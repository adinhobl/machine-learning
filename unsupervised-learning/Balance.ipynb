{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJ\n",
    "using MultivariateStats\n",
    "using Plots; gr()\n",
    "using StatsPlots\n",
    "using DataFrames\n",
    "using PyCall\n",
    "\n",
    "using CSV: read\n",
    "using StatsBase: countmap, kurtosis\n",
    "using Clustering: randindex, silhouettes, varinfo, vmeasure, mutualinfo\n",
    "using LinearAlgebra: diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV[\"LINES\"] = 100;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RNG = 133;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Data and Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Left_Weight</th><th>Left_Distance</th><th>Right_Weight</th><th>Right_Distance</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>625 rows × 4 columns</p><tr><th>1</th><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><th>2</th><td>1.0</td><td>1.0</td><td>1.0</td><td>2.0</td></tr><tr><th>3</th><td>1.0</td><td>1.0</td><td>1.0</td><td>3.0</td></tr><tr><th>4</th><td>1.0</td><td>1.0</td><td>1.0</td><td>4.0</td></tr><tr><th>5</th><td>1.0</td><td>1.0</td><td>1.0</td><td>5.0</td></tr><tr><th>6</th><td>1.0</td><td>1.0</td><td>2.0</td><td>1.0</td></tr><tr><th>7</th><td>1.0</td><td>1.0</td><td>2.0</td><td>2.0</td></tr><tr><th>8</th><td>1.0</td><td>1.0</td><td>2.0</td><td>3.0</td></tr><tr><th>9</th><td>1.0</td><td>1.0</td><td>2.0</td><td>4.0</td></tr><tr><th>10</th><td>1.0</td><td>1.0</td><td>2.0</td><td>5.0</td></tr><tr><th>11</th><td>1.0</td><td>1.0</td><td>3.0</td><td>1.0</td></tr><tr><th>12</th><td>1.0</td><td>1.0</td><td>3.0</td><td>2.0</td></tr><tr><th>13</th><td>1.0</td><td>1.0</td><td>3.0</td><td>3.0</td></tr><tr><th>14</th><td>1.0</td><td>1.0</td><td>3.0</td><td>4.0</td></tr><tr><th>15</th><td>1.0</td><td>1.0</td><td>3.0</td><td>5.0</td></tr><tr><th>16</th><td>1.0</td><td>1.0</td><td>4.0</td><td>1.0</td></tr><tr><th>17</th><td>1.0</td><td>1.0</td><td>4.0</td><td>2.0</td></tr><tr><th>18</th><td>1.0</td><td>1.0</td><td>4.0</td><td>3.0</td></tr><tr><th>19</th><td>1.0</td><td>1.0</td><td>4.0</td><td>4.0</td></tr><tr><th>20</th><td>1.0</td><td>1.0</td><td>4.0</td><td>5.0</td></tr><tr><th>21</th><td>1.0</td><td>1.0</td><td>5.0</td><td>1.0</td></tr><tr><th>22</th><td>1.0</td><td>1.0</td><td>5.0</td><td>2.0</td></tr><tr><th>23</th><td>1.0</td><td>1.0</td><td>5.0</td><td>3.0</td></tr><tr><th>24</th><td>1.0</td><td>1.0</td><td>5.0</td><td>4.0</td></tr><tr><th>25</th><td>1.0</td><td>1.0</td><td>5.0</td><td>5.0</td></tr><tr><th>26</th><td>1.0</td><td>2.0</td><td>1.0</td><td>1.0</td></tr><tr><th>27</th><td>1.0</td><td>2.0</td><td>1.0</td><td>2.0</td></tr><tr><th>28</th><td>1.0</td><td>2.0</td><td>1.0</td><td>3.0</td></tr><tr><th>29</th><td>1.0</td><td>2.0</td><td>1.0</td><td>4.0</td></tr><tr><th>30</th><td>1.0</td><td>2.0</td><td>1.0</td><td>5.0</td></tr><tr><th>31</th><td>1.0</td><td>2.0</td><td>2.0</td><td>1.0</td></tr><tr><th>32</th><td>1.0</td><td>2.0</td><td>2.0</td><td>2.0</td></tr><tr><th>33</th><td>1.0</td><td>2.0</td><td>2.0</td><td>3.0</td></tr><tr><th>34</th><td>1.0</td><td>2.0</td><td>2.0</td><td>4.0</td></tr><tr><th>35</th><td>1.0</td><td>2.0</td><td>2.0</td><td>5.0</td></tr><tr><th>36</th><td>1.0</td><td>2.0</td><td>3.0</td><td>1.0</td></tr><tr><th>37</th><td>1.0</td><td>2.0</td><td>3.0</td><td>2.0</td></tr><tr><th>38</th><td>1.0</td><td>2.0</td><td>3.0</td><td>3.0</td></tr><tr><th>39</th><td>1.0</td><td>2.0</td><td>3.0</td><td>4.0</td></tr><tr><th>40</th><td>1.0</td><td>2.0</td><td>3.0</td><td>5.0</td></tr><tr><th>41</th><td>1.0</td><td>2.0</td><td>4.0</td><td>1.0</td></tr><tr><th>42</th><td>1.0</td><td>2.0</td><td>4.0</td><td>2.0</td></tr><tr><th>43</th><td>1.0</td><td>2.0</td><td>4.0</td><td>3.0</td></tr><tr><th>44</th><td>1.0</td><td>2.0</td><td>4.0</td><td>4.0</td></tr><tr><th>45</th><td>1.0</td><td>2.0</td><td>4.0</td><td>5.0</td></tr><tr><th>46</th><td>1.0</td><td>2.0</td><td>5.0</td><td>1.0</td></tr><tr><th>47</th><td>1.0</td><td>2.0</td><td>5.0</td><td>2.0</td></tr><tr><th>48</th><td>1.0</td><td>2.0</td><td>5.0</td><td>3.0</td></tr><tr><th>49</th><td>1.0</td><td>2.0</td><td>5.0</td><td>4.0</td></tr><tr><th>50</th><td>1.0</td><td>2.0</td><td>5.0</td><td>5.0</td></tr><tr><th>51</th><td>1.0</td><td>3.0</td><td>1.0</td><td>1.0</td></tr><tr><th>52</th><td>1.0</td><td>3.0</td><td>1.0</td><td>2.0</td></tr><tr><th>53</th><td>1.0</td><td>3.0</td><td>1.0</td><td>3.0</td></tr><tr><th>54</th><td>1.0</td><td>3.0</td><td>1.0</td><td>4.0</td></tr><tr><th>55</th><td>1.0</td><td>3.0</td><td>1.0</td><td>5.0</td></tr><tr><th>56</th><td>1.0</td><td>3.0</td><td>2.0</td><td>1.0</td></tr><tr><th>57</th><td>1.0</td><td>3.0</td><td>2.0</td><td>2.0</td></tr><tr><th>58</th><td>1.0</td><td>3.0</td><td>2.0</td><td>3.0</td></tr><tr><th>59</th><td>1.0</td><td>3.0</td><td>2.0</td><td>4.0</td></tr><tr><th>60</th><td>1.0</td><td>3.0</td><td>2.0</td><td>5.0</td></tr><tr><th>61</th><td>1.0</td><td>3.0</td><td>3.0</td><td>1.0</td></tr><tr><th>62</th><td>1.0</td><td>3.0</td><td>3.0</td><td>2.0</td></tr><tr><th>63</th><td>1.0</td><td>3.0</td><td>3.0</td><td>3.0</td></tr><tr><th>64</th><td>1.0</td><td>3.0</td><td>3.0</td><td>4.0</td></tr><tr><th>65</th><td>1.0</td><td>3.0</td><td>3.0</td><td>5.0</td></tr><tr><th>66</th><td>1.0</td><td>3.0</td><td>4.0</td><td>1.0</td></tr><tr><th>67</th><td>1.0</td><td>3.0</td><td>4.0</td><td>2.0</td></tr><tr><th>68</th><td>1.0</td><td>3.0</td><td>4.0</td><td>3.0</td></tr><tr><th>69</th><td>1.0</td><td>3.0</td><td>4.0</td><td>4.0</td></tr><tr><th>70</th><td>1.0</td><td>3.0</td><td>4.0</td><td>5.0</td></tr><tr><th>71</th><td>1.0</td><td>3.0</td><td>5.0</td><td>1.0</td></tr><tr><th>72</th><td>1.0</td><td>3.0</td><td>5.0</td><td>2.0</td></tr><tr><th>73</th><td>1.0</td><td>3.0</td><td>5.0</td><td>3.0</td></tr><tr><th>74</th><td>1.0</td><td>3.0</td><td>5.0</td><td>4.0</td></tr><tr><th>75</th><td>1.0</td><td>3.0</td><td>5.0</td><td>5.0</td></tr><tr><th>76</th><td>1.0</td><td>4.0</td><td>1.0</td><td>1.0</td></tr><tr><th>77</th><td>1.0</td><td>4.0</td><td>1.0</td><td>2.0</td></tr><tr><th>78</th><td>1.0</td><td>4.0</td><td>1.0</td><td>3.0</td></tr><tr><th>79</th><td>1.0</td><td>4.0</td><td>1.0</td><td>4.0</td></tr><tr><th>80</th><td>1.0</td><td>4.0</td><td>1.0</td><td>5.0</td></tr><tr><th>81</th><td>1.0</td><td>4.0</td><td>2.0</td><td>1.0</td></tr><tr><th>82</th><td>1.0</td><td>4.0</td><td>2.0</td><td>2.0</td></tr><tr><th>83</th><td>1.0</td><td>4.0</td><td>2.0</td><td>3.0</td></tr><tr><th>84</th><td>1.0</td><td>4.0</td><td>2.0</td><td>4.0</td></tr><tr><th>85</th><td>1.0</td><td>4.0</td><td>2.0</td><td>5.0</td></tr><tr><th>86</th><td>1.0</td><td>4.0</td><td>3.0</td><td>1.0</td></tr><tr><th>87</th><td>1.0</td><td>4.0</td><td>3.0</td><td>2.0</td></tr><tr><th>88</th><td>1.0</td><td>4.0</td><td>3.0</td><td>3.0</td></tr><tr><th>89</th><td>1.0</td><td>4.0</td><td>3.0</td><td>4.0</td></tr><tr><th>90</th><td>1.0</td><td>4.0</td><td>3.0</td><td>5.0</td></tr><tr><th>91</th><td>1.0</td><td>4.0</td><td>4.0</td><td>1.0</td></tr><tr><th>92</th><td>1.0</td><td>4.0</td><td>4.0</td><td>2.0</td></tr><tr><th>93</th><td>1.0</td><td>4.0</td><td>4.0</td><td>3.0</td></tr><tr><th>94</th><td>1.0</td><td>4.0</td><td>4.0</td><td>4.0</td></tr><tr><th>95</th><td>1.0</td><td>4.0</td><td>4.0</td><td>5.0</td></tr><tr><th>96</th><td>1.0</td><td>4.0</td><td>5.0</td><td>1.0</td></tr><tr><th>97</th><td>1.0</td><td>4.0</td><td>5.0</td><td>2.0</td></tr><tr><th>98</th><td>1.0</td><td>4.0</td><td>5.0</td><td>3.0</td></tr><tr><th>99</th><td>1.0</td><td>4.0</td><td>5.0</td><td>4.0</td></tr><tr><th>100</th><td>1.0</td><td>4.0</td><td>5.0</td><td>5.0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& Left\\_Weight & Left\\_Distance & Right\\_Weight & Right\\_Distance\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1.0 & 1.0 & 1.0 & 1.0 \\\\\n",
       "\t2 & 1.0 & 1.0 & 1.0 & 2.0 \\\\\n",
       "\t3 & 1.0 & 1.0 & 1.0 & 3.0 \\\\\n",
       "\t4 & 1.0 & 1.0 & 1.0 & 4.0 \\\\\n",
       "\t5 & 1.0 & 1.0 & 1.0 & 5.0 \\\\\n",
       "\t6 & 1.0 & 1.0 & 2.0 & 1.0 \\\\\n",
       "\t7 & 1.0 & 1.0 & 2.0 & 2.0 \\\\\n",
       "\t8 & 1.0 & 1.0 & 2.0 & 3.0 \\\\\n",
       "\t9 & 1.0 & 1.0 & 2.0 & 4.0 \\\\\n",
       "\t10 & 1.0 & 1.0 & 2.0 & 5.0 \\\\\n",
       "\t11 & 1.0 & 1.0 & 3.0 & 1.0 \\\\\n",
       "\t12 & 1.0 & 1.0 & 3.0 & 2.0 \\\\\n",
       "\t13 & 1.0 & 1.0 & 3.0 & 3.0 \\\\\n",
       "\t14 & 1.0 & 1.0 & 3.0 & 4.0 \\\\\n",
       "\t15 & 1.0 & 1.0 & 3.0 & 5.0 \\\\\n",
       "\t16 & 1.0 & 1.0 & 4.0 & 1.0 \\\\\n",
       "\t17 & 1.0 & 1.0 & 4.0 & 2.0 \\\\\n",
       "\t18 & 1.0 & 1.0 & 4.0 & 3.0 \\\\\n",
       "\t19 & 1.0 & 1.0 & 4.0 & 4.0 \\\\\n",
       "\t20 & 1.0 & 1.0 & 4.0 & 5.0 \\\\\n",
       "\t21 & 1.0 & 1.0 & 5.0 & 1.0 \\\\\n",
       "\t22 & 1.0 & 1.0 & 5.0 & 2.0 \\\\\n",
       "\t23 & 1.0 & 1.0 & 5.0 & 3.0 \\\\\n",
       "\t24 & 1.0 & 1.0 & 5.0 & 4.0 \\\\\n",
       "\t25 & 1.0 & 1.0 & 5.0 & 5.0 \\\\\n",
       "\t26 & 1.0 & 2.0 & 1.0 & 1.0 \\\\\n",
       "\t27 & 1.0 & 2.0 & 1.0 & 2.0 \\\\\n",
       "\t28 & 1.0 & 2.0 & 1.0 & 3.0 \\\\\n",
       "\t29 & 1.0 & 2.0 & 1.0 & 4.0 \\\\\n",
       "\t30 & 1.0 & 2.0 & 1.0 & 5.0 \\\\\n",
       "\t31 & 1.0 & 2.0 & 2.0 & 1.0 \\\\\n",
       "\t32 & 1.0 & 2.0 & 2.0 & 2.0 \\\\\n",
       "\t33 & 1.0 & 2.0 & 2.0 & 3.0 \\\\\n",
       "\t34 & 1.0 & 2.0 & 2.0 & 4.0 \\\\\n",
       "\t35 & 1.0 & 2.0 & 2.0 & 5.0 \\\\\n",
       "\t36 & 1.0 & 2.0 & 3.0 & 1.0 \\\\\n",
       "\t37 & 1.0 & 2.0 & 3.0 & 2.0 \\\\\n",
       "\t38 & 1.0 & 2.0 & 3.0 & 3.0 \\\\\n",
       "\t39 & 1.0 & 2.0 & 3.0 & 4.0 \\\\\n",
       "\t40 & 1.0 & 2.0 & 3.0 & 5.0 \\\\\n",
       "\t41 & 1.0 & 2.0 & 4.0 & 1.0 \\\\\n",
       "\t42 & 1.0 & 2.0 & 4.0 & 2.0 \\\\\n",
       "\t43 & 1.0 & 2.0 & 4.0 & 3.0 \\\\\n",
       "\t44 & 1.0 & 2.0 & 4.0 & 4.0 \\\\\n",
       "\t45 & 1.0 & 2.0 & 4.0 & 5.0 \\\\\n",
       "\t46 & 1.0 & 2.0 & 5.0 & 1.0 \\\\\n",
       "\t47 & 1.0 & 2.0 & 5.0 & 2.0 \\\\\n",
       "\t48 & 1.0 & 2.0 & 5.0 & 3.0 \\\\\n",
       "\t49 & 1.0 & 2.0 & 5.0 & 4.0 \\\\\n",
       "\t50 & 1.0 & 2.0 & 5.0 & 5.0 \\\\\n",
       "\t51 & 1.0 & 3.0 & 1.0 & 1.0 \\\\\n",
       "\t52 & 1.0 & 3.0 & 1.0 & 2.0 \\\\\n",
       "\t53 & 1.0 & 3.0 & 1.0 & 3.0 \\\\\n",
       "\t54 & 1.0 & 3.0 & 1.0 & 4.0 \\\\\n",
       "\t55 & 1.0 & 3.0 & 1.0 & 5.0 \\\\\n",
       "\t56 & 1.0 & 3.0 & 2.0 & 1.0 \\\\\n",
       "\t57 & 1.0 & 3.0 & 2.0 & 2.0 \\\\\n",
       "\t58 & 1.0 & 3.0 & 2.0 & 3.0 \\\\\n",
       "\t59 & 1.0 & 3.0 & 2.0 & 4.0 \\\\\n",
       "\t60 & 1.0 & 3.0 & 2.0 & 5.0 \\\\\n",
       "\t61 & 1.0 & 3.0 & 3.0 & 1.0 \\\\\n",
       "\t62 & 1.0 & 3.0 & 3.0 & 2.0 \\\\\n",
       "\t63 & 1.0 & 3.0 & 3.0 & 3.0 \\\\\n",
       "\t64 & 1.0 & 3.0 & 3.0 & 4.0 \\\\\n",
       "\t65 & 1.0 & 3.0 & 3.0 & 5.0 \\\\\n",
       "\t66 & 1.0 & 3.0 & 4.0 & 1.0 \\\\\n",
       "\t67 & 1.0 & 3.0 & 4.0 & 2.0 \\\\\n",
       "\t68 & 1.0 & 3.0 & 4.0 & 3.0 \\\\\n",
       "\t69 & 1.0 & 3.0 & 4.0 & 4.0 \\\\\n",
       "\t70 & 1.0 & 3.0 & 4.0 & 5.0 \\\\\n",
       "\t71 & 1.0 & 3.0 & 5.0 & 1.0 \\\\\n",
       "\t72 & 1.0 & 3.0 & 5.0 & 2.0 \\\\\n",
       "\t73 & 1.0 & 3.0 & 5.0 & 3.0 \\\\\n",
       "\t74 & 1.0 & 3.0 & 5.0 & 4.0 \\\\\n",
       "\t75 & 1.0 & 3.0 & 5.0 & 5.0 \\\\\n",
       "\t76 & 1.0 & 4.0 & 1.0 & 1.0 \\\\\n",
       "\t77 & 1.0 & 4.0 & 1.0 & 2.0 \\\\\n",
       "\t78 & 1.0 & 4.0 & 1.0 & 3.0 \\\\\n",
       "\t79 & 1.0 & 4.0 & 1.0 & 4.0 \\\\\n",
       "\t80 & 1.0 & 4.0 & 1.0 & 5.0 \\\\\n",
       "\t81 & 1.0 & 4.0 & 2.0 & 1.0 \\\\\n",
       "\t82 & 1.0 & 4.0 & 2.0 & 2.0 \\\\\n",
       "\t83 & 1.0 & 4.0 & 2.0 & 3.0 \\\\\n",
       "\t84 & 1.0 & 4.0 & 2.0 & 4.0 \\\\\n",
       "\t85 & 1.0 & 4.0 & 2.0 & 5.0 \\\\\n",
       "\t86 & 1.0 & 4.0 & 3.0 & 1.0 \\\\\n",
       "\t87 & 1.0 & 4.0 & 3.0 & 2.0 \\\\\n",
       "\t88 & 1.0 & 4.0 & 3.0 & 3.0 \\\\\n",
       "\t89 & 1.0 & 4.0 & 3.0 & 4.0 \\\\\n",
       "\t90 & 1.0 & 4.0 & 3.0 & 5.0 \\\\\n",
       "\t91 & 1.0 & 4.0 & 4.0 & 1.0 \\\\\n",
       "\t92 & 1.0 & 4.0 & 4.0 & 2.0 \\\\\n",
       "\t93 & 1.0 & 4.0 & 4.0 & 3.0 \\\\\n",
       "\t94 & 1.0 & 4.0 & 4.0 & 4.0 \\\\\n",
       "\t95 & 1.0 & 4.0 & 4.0 & 5.0 \\\\\n",
       "\t96 & 1.0 & 4.0 & 5.0 & 1.0 \\\\\n",
       "\t97 & 1.0 & 4.0 & 5.0 & 2.0 \\\\\n",
       "\t98 & 1.0 & 4.0 & 5.0 & 3.0 \\\\\n",
       "\t99 & 1.0 & 4.0 & 5.0 & 4.0 \\\\\n",
       "\t100 & 1.0 & 4.0 & 5.0 & 5.0 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "625×4 DataFrame\n",
       "│ Row │ Left_Weight │ Left_Distance │ Right_Weight │ Right_Distance │\n",
       "│     │ \u001b[90mFloat64\u001b[39m     │ \u001b[90mFloat64\u001b[39m       │ \u001b[90mFloat64\u001b[39m      │ \u001b[90mFloat64\u001b[39m        │\n",
       "├─────┼─────────────┼───────────────┼──────────────┼────────────────┤\n",
       "│ 1   │ 1.0         │ 1.0           │ 1.0          │ 1.0            │\n",
       "│ 2   │ 1.0         │ 1.0           │ 1.0          │ 2.0            │\n",
       "│ 3   │ 1.0         │ 1.0           │ 1.0          │ 3.0            │\n",
       "│ 4   │ 1.0         │ 1.0           │ 1.0          │ 4.0            │\n",
       "│ 5   │ 1.0         │ 1.0           │ 1.0          │ 5.0            │\n",
       "│ 6   │ 1.0         │ 1.0           │ 2.0          │ 1.0            │\n",
       "│ 7   │ 1.0         │ 1.0           │ 2.0          │ 2.0            │\n",
       "│ 8   │ 1.0         │ 1.0           │ 2.0          │ 3.0            │\n",
       "│ 9   │ 1.0         │ 1.0           │ 2.0          │ 4.0            │\n",
       "│ 10  │ 1.0         │ 1.0           │ 2.0          │ 5.0            │\n",
       "│ 11  │ 1.0         │ 1.0           │ 3.0          │ 1.0            │\n",
       "│ 12  │ 1.0         │ 1.0           │ 3.0          │ 2.0            │\n",
       "│ 13  │ 1.0         │ 1.0           │ 3.0          │ 3.0            │\n",
       "│ 14  │ 1.0         │ 1.0           │ 3.0          │ 4.0            │\n",
       "│ 15  │ 1.0         │ 1.0           │ 3.0          │ 5.0            │\n",
       "│ 16  │ 1.0         │ 1.0           │ 4.0          │ 1.0            │\n",
       "│ 17  │ 1.0         │ 1.0           │ 4.0          │ 2.0            │\n",
       "│ 18  │ 1.0         │ 1.0           │ 4.0          │ 3.0            │\n",
       "│ 19  │ 1.0         │ 1.0           │ 4.0          │ 4.0            │\n",
       "│ 20  │ 1.0         │ 1.0           │ 4.0          │ 5.0            │\n",
       "│ 21  │ 1.0         │ 1.0           │ 5.0          │ 1.0            │\n",
       "│ 22  │ 1.0         │ 1.0           │ 5.0          │ 2.0            │\n",
       "│ 23  │ 1.0         │ 1.0           │ 5.0          │ 3.0            │\n",
       "│ 24  │ 1.0         │ 1.0           │ 5.0          │ 4.0            │\n",
       "│ 25  │ 1.0         │ 1.0           │ 5.0          │ 5.0            │\n",
       "│ 26  │ 1.0         │ 2.0           │ 1.0          │ 1.0            │\n",
       "│ 27  │ 1.0         │ 2.0           │ 1.0          │ 2.0            │\n",
       "│ 28  │ 1.0         │ 2.0           │ 1.0          │ 3.0            │\n",
       "│ 29  │ 1.0         │ 2.0           │ 1.0          │ 4.0            │\n",
       "│ 30  │ 1.0         │ 2.0           │ 1.0          │ 5.0            │\n",
       "│ 31  │ 1.0         │ 2.0           │ 2.0          │ 1.0            │\n",
       "│ 32  │ 1.0         │ 2.0           │ 2.0          │ 2.0            │\n",
       "│ 33  │ 1.0         │ 2.0           │ 2.0          │ 3.0            │\n",
       "│ 34  │ 1.0         │ 2.0           │ 2.0          │ 4.0            │\n",
       "│ 35  │ 1.0         │ 2.0           │ 2.0          │ 5.0            │\n",
       "│ 36  │ 1.0         │ 2.0           │ 3.0          │ 1.0            │\n",
       "│ 37  │ 1.0         │ 2.0           │ 3.0          │ 2.0            │\n",
       "│ 38  │ 1.0         │ 2.0           │ 3.0          │ 3.0            │\n",
       "│ 39  │ 1.0         │ 2.0           │ 3.0          │ 4.0            │\n",
       "│ 40  │ 1.0         │ 2.0           │ 3.0          │ 5.0            │\n",
       "│ 41  │ 1.0         │ 2.0           │ 4.0          │ 1.0            │\n",
       "│ 42  │ 1.0         │ 2.0           │ 4.0          │ 2.0            │\n",
       "│ 43  │ 1.0         │ 2.0           │ 4.0          │ 3.0            │\n",
       "│ 44  │ 1.0         │ 2.0           │ 4.0          │ 4.0            │\n",
       "│ 45  │ 1.0         │ 2.0           │ 4.0          │ 5.0            │\n",
       "⋮\n",
       "│ 580 │ 5.0         │ 4.0           │ 1.0          │ 5.0            │\n",
       "│ 581 │ 5.0         │ 4.0           │ 2.0          │ 1.0            │\n",
       "│ 582 │ 5.0         │ 4.0           │ 2.0          │ 2.0            │\n",
       "│ 583 │ 5.0         │ 4.0           │ 2.0          │ 3.0            │\n",
       "│ 584 │ 5.0         │ 4.0           │ 2.0          │ 4.0            │\n",
       "│ 585 │ 5.0         │ 4.0           │ 2.0          │ 5.0            │\n",
       "│ 586 │ 5.0         │ 4.0           │ 3.0          │ 1.0            │\n",
       "│ 587 │ 5.0         │ 4.0           │ 3.0          │ 2.0            │\n",
       "│ 588 │ 5.0         │ 4.0           │ 3.0          │ 3.0            │\n",
       "│ 589 │ 5.0         │ 4.0           │ 3.0          │ 4.0            │\n",
       "│ 590 │ 5.0         │ 4.0           │ 3.0          │ 5.0            │\n",
       "│ 591 │ 5.0         │ 4.0           │ 4.0          │ 1.0            │\n",
       "│ 592 │ 5.0         │ 4.0           │ 4.0          │ 2.0            │\n",
       "│ 593 │ 5.0         │ 4.0           │ 4.0          │ 3.0            │\n",
       "│ 594 │ 5.0         │ 4.0           │ 4.0          │ 4.0            │\n",
       "│ 595 │ 5.0         │ 4.0           │ 4.0          │ 5.0            │\n",
       "│ 596 │ 5.0         │ 4.0           │ 5.0          │ 1.0            │\n",
       "│ 597 │ 5.0         │ 4.0           │ 5.0          │ 2.0            │\n",
       "│ 598 │ 5.0         │ 4.0           │ 5.0          │ 3.0            │\n",
       "│ 599 │ 5.0         │ 4.0           │ 5.0          │ 4.0            │\n",
       "│ 600 │ 5.0         │ 4.0           │ 5.0          │ 5.0            │\n",
       "│ 601 │ 5.0         │ 5.0           │ 1.0          │ 1.0            │\n",
       "│ 602 │ 5.0         │ 5.0           │ 1.0          │ 2.0            │\n",
       "│ 603 │ 5.0         │ 5.0           │ 1.0          │ 3.0            │\n",
       "│ 604 │ 5.0         │ 5.0           │ 1.0          │ 4.0            │\n",
       "│ 605 │ 5.0         │ 5.0           │ 1.0          │ 5.0            │\n",
       "│ 606 │ 5.0         │ 5.0           │ 2.0          │ 1.0            │\n",
       "│ 607 │ 5.0         │ 5.0           │ 2.0          │ 2.0            │\n",
       "│ 608 │ 5.0         │ 5.0           │ 2.0          │ 3.0            │\n",
       "│ 609 │ 5.0         │ 5.0           │ 2.0          │ 4.0            │\n",
       "│ 610 │ 5.0         │ 5.0           │ 2.0          │ 5.0            │\n",
       "│ 611 │ 5.0         │ 5.0           │ 3.0          │ 1.0            │\n",
       "│ 612 │ 5.0         │ 5.0           │ 3.0          │ 2.0            │\n",
       "│ 613 │ 5.0         │ 5.0           │ 3.0          │ 3.0            │\n",
       "│ 614 │ 5.0         │ 5.0           │ 3.0          │ 4.0            │\n",
       "│ 615 │ 5.0         │ 5.0           │ 3.0          │ 5.0            │\n",
       "│ 616 │ 5.0         │ 5.0           │ 4.0          │ 1.0            │\n",
       "│ 617 │ 5.0         │ 5.0           │ 4.0          │ 2.0            │\n",
       "│ 618 │ 5.0         │ 5.0           │ 4.0          │ 3.0            │\n",
       "│ 619 │ 5.0         │ 5.0           │ 4.0          │ 4.0            │\n",
       "│ 620 │ 5.0         │ 5.0           │ 4.0          │ 5.0            │\n",
       "│ 621 │ 5.0         │ 5.0           │ 5.0          │ 1.0            │\n",
       "│ 622 │ 5.0         │ 5.0           │ 5.0          │ 2.0            │\n",
       "│ 623 │ 5.0         │ 5.0           │ 5.0          │ 3.0            │\n",
       "│ 624 │ 5.0         │ 5.0           │ 5.0          │ 4.0            │\n",
       "│ 625 │ 5.0         │ 5.0           │ 5.0          │ 5.0            │"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = read(\"balance.csv\")\n",
    "data[!,2:end] = float.(data[:,2:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nunique</th><th>nmissing</th><th>eltype</th></tr><tr><th></th><th>Symbol</th><th>Union…</th><th>Any</th><th>Union…</th><th>Any</th><th>Union…</th><th>Nothing</th><th>DataType</th></tr></thead><tbody><p>5 rows × 8 columns</p><tr><th>1</th><td>Class_Name</td><td></td><td>B</td><td></td><td>R</td><td>3</td><td></td><td>String</td></tr><tr><th>2</th><td>Left_Weight</td><td>3.0</td><td>1.0</td><td>3.0</td><td>5.0</td><td></td><td></td><td>Float64</td></tr><tr><th>3</th><td>Left_Distance</td><td>3.0</td><td>1.0</td><td>3.0</td><td>5.0</td><td></td><td></td><td>Float64</td></tr><tr><th>4</th><td>Right_Weight</td><td>3.0</td><td>1.0</td><td>3.0</td><td>5.0</td><td></td><td></td><td>Float64</td></tr><tr><th>5</th><td>Right_Distance</td><td>3.0</td><td>1.0</td><td>3.0</td><td>5.0</td><td></td><td></td><td>Float64</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& variable & mean & min & median & max & nunique & nmissing & eltype\\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & Union… & Nothing & DataType\\\\\n",
       "\t\\hline\n",
       "\t1 & Class\\_Name &  & B &  & R & 3 &  & String \\\\\n",
       "\t2 & Left\\_Weight & 3.0 & 1.0 & 3.0 & 5.0 &  &  & Float64 \\\\\n",
       "\t3 & Left\\_Distance & 3.0 & 1.0 & 3.0 & 5.0 &  &  & Float64 \\\\\n",
       "\t4 & Right\\_Weight & 3.0 & 1.0 & 3.0 & 5.0 &  &  & Float64 \\\\\n",
       "\t5 & Right\\_Distance & 3.0 & 1.0 & 3.0 & 5.0 &  &  & Float64 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×8 DataFrame. Omitted printing of 1 columns\n",
       "│ Row │ variable       │ mean   │ min │ median │ max │ nunique │ nmissing │\n",
       "│     │ \u001b[90mSymbol\u001b[39m         │ \u001b[90mUnion…\u001b[39m │ \u001b[90mAny\u001b[39m │ \u001b[90mUnion…\u001b[39m │ \u001b[90mAny\u001b[39m │ \u001b[90mUnion…\u001b[39m  │ \u001b[90mNothing\u001b[39m  │\n",
       "├─────┼────────────────┼────────┼─────┼────────┼─────┼─────────┼──────────┤\n",
       "│ 1   │ Class_Name     │        │ B   │        │ R   │ 3       │          │\n",
       "│ 2   │ Left_Weight    │ 3.0    │ 1.0 │ 3.0    │ 5.0 │         │          │\n",
       "│ 3   │ Left_Distance  │ 3.0    │ 1.0 │ 3.0    │ 5.0 │         │          │\n",
       "│ 4   │ Right_Weight   │ 3.0    │ 1.0 │ 3.0    │ 5.0 │         │          │\n",
       "│ 5   │ Right_Distance │ 3.0    │ 1.0 │ 3.0    │ 5.0 │         │          │"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.0784\n",
       " 0.4608\n",
       " 0.4608"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = countmap(data[:(Class_Name)])\n",
    "collect(label_counts[i] / size(data)[1] for i in keys(label_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m────────────────\u001b[0m┬\u001b[0m─────────────────────────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.names        \u001b[0m│\u001b[0m\u001b[22m _.types                         \u001b[0m│\u001b[0m\u001b[22m _.scitypes    \u001b[0m│\u001b[0m\n",
       "├\u001b[0m────────────────\u001b[0m┼\u001b[0m─────────────────────────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m Class_Name     \u001b[0m│\u001b[0m CategoricalValue{String,UInt32} \u001b[0m│\u001b[0m Multiclass{3} \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Left_Weight    \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Left_Distance  \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Right_Weight   \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Right_Distance \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "└\u001b[0m────────────────\u001b[0m┴\u001b[0m─────────────────────────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┘\u001b[0m\n",
       "_.nrows = 625\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coerce!(data, :Class_Name=>Multiclass)\n",
    "schema(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([342, 589, 478, 524, 282, 88, 9, 114, 564, 491  …  278, 274, 284, 467, 56, 407, 17, 109, 428, 401], [413, 566, 519, 240, 587, 61, 20, 312, 490, 334  …  356, 338, 361, 19, 567, 423, 176, 561, 371, 259])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, X = unpack(data, ==(:Class_Name), colname->true)\n",
    "train, test = partition(eachindex(y), 0.8, shuffle=true, rng=RNG, stratify=values(data[:Class_Name])) # gives 70:30 split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Confirming that data was stratified correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.0781563126252505\n",
       " 0.46092184368737477\n",
       " 0.46092184368737477"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts = countmap(data[train,:Class_Name])\n",
    "collect(train_counts[i] / size(train)[1] for i in keys(train_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.07936507936507936\n",
       " 0.4603174603174603\n",
       " 0.4603174603174603"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_counts = countmap(data[test,:Class_Name])\n",
    "collect(test_counts[i] / size(test)[1] for i in keys(test_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Not standardizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499×4 Array{Float64,2}:\n",
       " 3.0  4.0  4.0  2.0\n",
       " 5.0  4.0  3.0  4.0\n",
       " 4.0  5.0  1.0  3.0\n",
       " 5.0  1.0  5.0  4.0\n",
       " 3.0  2.0  2.0  2.0\n",
       " 1.0  4.0  3.0  3.0\n",
       " 1.0  1.0  2.0  4.0\n",
       " 1.0  5.0  3.0  4.0\n",
       " 5.0  3.0  3.0  4.0\n",
       " 4.0  5.0  4.0  1.0\n",
       " 3.0  1.0  5.0  5.0\n",
       " 3.0  2.0  3.0  3.0\n",
       " 5.0  3.0  1.0  4.0\n",
       " 3.0  3.0  3.0  1.0\n",
       " 5.0  2.0  2.0  1.0\n",
       " 2.0  4.0  1.0  4.0\n",
       " 5.0  1.0  2.0  5.0\n",
       " 3.0  3.0  2.0  1.0\n",
       " 3.0  4.0  2.0  1.0\n",
       " 3.0  1.0  1.0  5.0\n",
       " 3.0  2.0  4.0  2.0\n",
       " 3.0  2.0  4.0  1.0\n",
       " 4.0  2.0  4.0  1.0\n",
       " 2.0  2.0  4.0  5.0\n",
       " 5.0  4.0  1.0  2.0\n",
       " 2.0  1.0  3.0  4.0\n",
       " 3.0  4.0  4.0  1.0\n",
       " 5.0  5.0  2.0  5.0\n",
       " 2.0  2.0  4.0  4.0\n",
       " 1.0  2.0  2.0  2.0\n",
       " 5.0  1.0  2.0  3.0\n",
       " 2.0  1.0  1.0  2.0\n",
       " 3.0  5.0  5.0  2.0\n",
       " 2.0  3.0  3.0  5.0\n",
       " 5.0  5.0  4.0  2.0\n",
       " 4.0  3.0  5.0  5.0\n",
       " 2.0  5.0  4.0  1.0\n",
       " 5.0  4.0  2.0  5.0\n",
       " 1.0  2.0  4.0  5.0\n",
       " 5.0  2.0  5.0  3.0\n",
       " 5.0  2.0  2.0  5.0\n",
       " 2.0  5.0  3.0  2.0\n",
       " 4.0  2.0  5.0  2.0\n",
       " 4.0  5.0  4.0  2.0\n",
       " 1.0  4.0  5.0  3.0\n",
       " 5.0  5.0  1.0  1.0\n",
       " 3.0  2.0  2.0  5.0\n",
       " 5.0  4.0  3.0  3.0\n",
       " ⋮              \n",
       " 3.0  3.0  5.0  1.0\n",
       " 5.0  2.0  5.0  5.0\n",
       " 2.0  5.0  2.0  4.0\n",
       " 1.0  3.0  3.0  4.0\n",
       " 1.0  2.0  2.0  1.0\n",
       " 4.0  3.0  3.0  5.0\n",
       " 3.0  2.0  5.0  3.0\n",
       " 3.0  3.0  3.0  3.0\n",
       " 2.0  2.0  2.0  3.0\n",
       " 1.0  4.0  3.0  4.0\n",
       " 3.0  1.0  4.0  3.0\n",
       " 2.0  1.0  4.0  4.0\n",
       " 3.0  4.0  5.0  4.0\n",
       " 4.0  4.0  4.0  1.0\n",
       " 4.0  5.0  3.0  1.0\n",
       " 4.0  4.0  5.0  5.0\n",
       " 1.0  4.0  4.0  5.0\n",
       " 3.0  3.0  3.0  4.0\n",
       " 2.0  1.0  1.0  1.0\n",
       " 4.0  4.0  5.0  3.0\n",
       " 4.0  4.0  2.0  1.0\n",
       " 5.0  4.0  1.0  1.0\n",
       " 2.0  5.0  3.0  1.0\n",
       " 5.0  3.0  1.0  5.0\n",
       " 3.0  1.0  3.0  2.0\n",
       " 2.0  1.0  3.0  2.0\n",
       " 5.0  3.0  4.0  5.0\n",
       " 1.0  2.0  3.0  3.0\n",
       " 2.0  3.0  5.0  4.0\n",
       " 1.0  4.0  4.0  4.0\n",
       " 3.0  1.0  2.0  1.0\n",
       " 2.0  2.0  1.0  2.0\n",
       " 5.0  5.0  2.0  3.0\n",
       " 3.0  1.0  2.0  2.0\n",
       " 1.0  1.0  2.0  2.0\n",
       " 4.0  5.0  2.0  2.0\n",
       " 3.0  5.0  2.0  3.0\n",
       " 3.0  2.0  1.0  3.0\n",
       " 3.0  1.0  5.0  4.0\n",
       " 3.0  2.0  2.0  4.0\n",
       " 4.0  4.0  4.0  2.0\n",
       " 1.0  3.0  2.0  1.0\n",
       " 4.0  2.0  2.0  2.0\n",
       " 1.0  1.0  4.0  2.0\n",
       " 1.0  5.0  2.0  4.0\n",
       " 4.0  3.0  1.0  3.0\n",
       " 4.0  2.0  1.0  1.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = convert(Matrix,X[train,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32-element Array{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype),T} where T<:Tuple,1}:\n",
       " (name = AffinityPropagation, package_name = ScikitLearn, ... )\n",
       " (name = AgglomerativeClustering, package_name = ScikitLearn, ... )\n",
       " (name = Birch, package_name = ScikitLearn, ... )\n",
       " (name = ContinuousEncoder, package_name = MLJModels, ... )\n",
       " (name = DBSCAN, package_name = ScikitLearn, ... )\n",
       " (name = FactorAnalysis, package_name = MultivariateStats, ... )\n",
       " (name = FeatureAgglomeration, package_name = ScikitLearn, ... )\n",
       " (name = FeatureSelector, package_name = MLJModels, ... )\n",
       " (name = FillImputer, package_name = MLJModels, ... )\n",
       " (name = ICA, package_name = MultivariateStats, ... )\n",
       " (name = KMeans, package_name = Clustering, ... )\n",
       " (name = KMeans, package_name = ParallelKMeans, ... )\n",
       " (name = KMeans, package_name = ScikitLearn, ... )\n",
       " (name = KMedoids, package_name = Clustering, ... )\n",
       " (name = KernelPCA, package_name = MultivariateStats, ... )\n",
       " (name = MeanShift, package_name = ScikitLearn, ... )\n",
       " (name = MiniBatchKMeans, package_name = ScikitLearn, ... )\n",
       " (name = OPTICS, package_name = ScikitLearn, ... )\n",
       " (name = OneClassSVM, package_name = LIBSVM, ... )\n",
       " (name = OneHotEncoder, package_name = MLJModels, ... )\n",
       " (name = PCA, package_name = MultivariateStats, ... )\n",
       " (name = PPCA, package_name = MultivariateStats, ... )\n",
       " (name = SpectralClustering, package_name = ScikitLearn, ... )\n",
       " (name = Standardizer, package_name = MLJModels, ... )\n",
       " (name = StaticSurrogate, package_name = MLJBase, ... )\n",
       " (name = UnivariateBoxCoxTransformer, package_name = MLJModels, ... )\n",
       " (name = UnivariateDiscretizer, package_name = MLJModels, ... )\n",
       " (name = UnivariateFillImputer, package_name = MLJModels, ... )\n",
       " (name = UnivariateStandardizer, package_name = MLJModels, ... )\n",
       " (name = UnivariateTimeTypeToContinuous, package_name = MLJModels, ... )\n",
       " (name = UnsupervisedSurrogate, package_name = MLJBase, ... )\n",
       " (name = WrappedFunction, package_name = MLJBase, ... )"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task(model) = !model.is_supervised\n",
    "models(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Algorithms\n",
    "Run the clustering algorithms on the datasets and describe what you see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans\n",
    "* https://github.com/PyDataBlog/ParallelKMeans.jl/blob/master/src/hamerly.jl#L65\n",
    "* https://juliastats.org/Clustering.jl/stable/validate.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(\n",
       "    algo = ParallelKMeans.Hamerly(),\n",
       "    k_init = \"k-means++\",\n",
       "    k = 3,\n",
       "    tol = 1.0e-6,\n",
       "    max_iters = 300,\n",
       "    copy = true,\n",
       "    threads = 4,\n",
       "    rng = Random._GLOBAL_RNG(),\n",
       "    weights = nothing,\n",
       "    init = nothing)\u001b[34m @552\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load KMeans pkg=ParallelKMeans\n",
    "# @load KMeans pkg=Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dist_mat (generic function with 1 method)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://stackoverflow.com/questions/51181392/julia-vs-matlab-distance-matrix-run-time-test\n",
    "function dist_mat(X::Matrix)\n",
    "    G = X * X'\n",
    "    dG = diag(G)\n",
    "    return sqrt.(dG .+ dG' .- 2 .* G)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{KMeans} @542\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/Ov46j/src/machines.jl:319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Jclust = 4421.0\n",
      "Iteration 2: Jclust = 3903.390410059722\n",
      "Iteration 3: Jclust = 3933.408872707408\n",
      "Iteration 4: Jclust = 3879.817794847365\n",
      "Iteration 5: Jclust = 3824.6344015031214\n",
      "Iteration 6: Jclust = 3822.316220650946\n",
      "Iteration 7: Jclust = 3831.1352320367705\n",
      "Iteration 8: Jclust = 3776.363611876285\n",
      "Iteration 9: Jclust = 3783.784893877938\n",
      "Iteration 10: Jclust = 3776.3021069779024\n",
      "Iteration 11: Jclust = 3784.219707554757\n",
      "Iteration 12: Jclust = 3789.6485774607067\n",
      "Iteration 13: Jclust = 3740.4992408245976\n",
      "Iteration 14: Jclust = 3753.958293780855\n",
      "Iteration 15: Jclust = 3741.8581899346145\n",
      "Iteration 16: Jclust = 3747.4966968166145\n",
      "Iteration 17: Jclust = 3740.6438480627726\n",
      "Iteration 18: Jclust = 3749.971605031644\n",
      "Iteration 19: Jclust = 3715.1397171705926\n",
      "Iteration 20: Jclust = 3673.8189799613765\n",
      "Iteration 21: Jclust = 3683.1486548354146\n",
      "Iteration 22: Jclust = 3648.486077488995\n",
      "Iteration 23: Jclust = 3571.7907885821883\n",
      "Iteration 24: Jclust = 3550.48717945605\n",
      "Iteration 25: Jclust = 3546.624439832158\n",
      "Iteration 26: Jclust = 3532.131398176307\n",
      "Iteration 27: Jclust = 3554.7096638731705\n",
      "Iteration 28: Jclust = 3554.7096638731705\n",
      "Successfully terminated with convergence.\n",
      "mach.report.totalcost = 3261.3046575959347\n",
      "l = [(1, 258), (2, 241)]\n",
      "silhouette: 0.1830626307885436\n",
      "\n",
      "K = 3\n",
      "Iteration 1: Jclust = 4118.0\n",
      "Iteration 2: Jclust = 3276.6970366465007\n",
      "Iteration 3: Jclust = 3138.11855536009\n",
      "Iteration 4: Jclust = 3046.255859913119\n",
      "Iteration 5: Jclust = 3033.072418744619\n",
      "Iteration 6: Jclust = 3059.1210173787476\n",
      "Iteration 7: Jclust = 3077.6066111948103\n",
      "Iteration 8: Jclust = 3099.3121993470704\n",
      "Iteration 9: Jclust = 3085.431263487328\n",
      "Iteration 10: Jclust = 3081.0316755332806\n",
      "Iteration 11: Jclust = 3072.062718574553\n",
      "Iteration 12: Jclust = 3078.5713844320726\n",
      "Iteration 13: Jclust = 3092.2157772564387\n",
      "Iteration 14: Jclust = 3067.2808011082975\n",
      "Iteration 15: Jclust = 3079.2736765475083\n",
      "Iteration 16: Jclust = 3077.816811603864\n",
      "Iteration 17: Jclust = 3064.5100941165297\n",
      "Iteration 18: Jclust = 3057.3251415544178\n",
      "Iteration 19: Jclust = 3044.2123839846627\n",
      "Iteration 20: Jclust = 3029.6732028025785\n",
      "Iteration 21: Jclust = 3040.9776140946283\n",
      "Iteration 22: Jclust = 3031.439931207913\n",
      "Iteration 23: Jclust = 3039.197489748838\n",
      "Iteration 24: Jclust = 3040.787198994168\n",
      "Iteration 25: Jclust = 3040.787198994168\n",
      "Successfully terminated with convergence.\n",
      "mach.report.totalcost = 2778.824444074444\n",
      "l = [(1, 189), (2, 156), (3, 154)]\n",
      "silhouette: 0.1751423562914561\n",
      "\n",
      "K = 4\n",
      "Iteration 1: Jclust = 3895.0\n",
      "Iteration 2: Jclust = 2827.838698317927\n",
      "Iteration 3: Jclust = 2546.2650310559766\n",
      "Iteration 4: Jclust = 2647.579170878149\n",
      "Iteration 5: Jclust = 2614.9639680775203\n",
      "Iteration 6: Jclust = 2616.8903785632892\n",
      "Iteration 7: Jclust = 2601.096034304076\n",
      "Iteration 8: Jclust = 2612.396377162768\n",
      "Iteration 9: Jclust = 2617.6861976130967\n",
      "Iteration 10: Jclust = 2606.4194260726745\n",
      "Iteration 11: Jclust = 2623.137908070791\n",
      "Iteration 12: Jclust = 2640.6361723496125\n",
      "Iteration 13: Jclust = 2627.097755108615\n",
      "Iteration 14: Jclust = 2648.952647955293\n",
      "Iteration 15: Jclust = 2646.2346923536497\n",
      "Iteration 16: Jclust = 2646.2346923536497\n",
      "Successfully terminated with convergence.\n",
      "mach.report.totalcost = 2401.0271534411568\n",
      "l = [(1, 119), (2, 135), (3, 113), (4, 132)]\n",
      "silhouette: 0.18904541956950435\n",
      "\n",
      "K = 5\n",
      "Iteration 1: Jclust = 3862.0\n",
      "Iteration 2: Jclust = 2602.348857954719\n",
      "Iteration 3: Jclust = 2294.6428325629745\n",
      "Iteration 4: Jclust = 2338.5958908492603\n",
      "Iteration 5: Jclust = 2321.9981372034345\n",
      "Iteration 6: Jclust = 2341.106826371965\n",
      "Iteration 7: Jclust = 2318.2391299178184\n",
      "Iteration 8: Jclust = 2323.0097020955277\n",
      "Iteration 9: Jclust = 2334.467853467609\n",
      "Iteration 10: Jclust = 2347.5797164156206\n",
      "Iteration 11: Jclust = 2342.263692897223\n",
      "Iteration 12: Jclust = 2340.423411120643\n",
      "Iteration 13: Jclust = 2328.8449830249037\n",
      "Iteration 14: Jclust = 2337.1897953614007\n",
      "Iteration 15: Jclust = 2347.4486268712412\n",
      "Iteration 16: Jclust = 2337.0057131443195\n",
      "Iteration 17: Jclust = 2340.262775210368\n",
      "Iteration 18: Jclust = 2340.262775210368\n",
      "Successfully terminated with convergence.\n",
      "mach.report.totalcost = 2131.809968920191\n",
      "l = [(1, 109), (2, 104), (3, 92), (4, 110), (5, 84)]\n",
      "silhouette: 0.192487090398307\n",
      "\n",
      "K = 6\n",
      "Iteration 1: Jclust = 3346.0\n",
      "Iteration 2: Jclust = 2294.9450839394435\n",
      "Iteration 3: Jclust = 2107.700997425225\n",
      "Iteration 4: Jclust = 2117.861136840989\n",
      "Iteration 5: Jclust = 2102.072487620177\n",
      "Iteration 6: Jclust = 2132.627249780542\n",
      "Iteration 7: Jclust = 2147.7308816718346\n",
      "Iteration 8: Jclust = 2134.7380952856843\n",
      "Iteration 9: Jclust = 2121.684819301533\n",
      "Iteration 10: Jclust = 2131.3156932525703\n",
      "Iteration 11: Jclust = 2110.9841851737992\n",
      "Iteration 12: Jclust = 2109.297129618699\n",
      "Iteration 13: Jclust = 2125.236377034924\n",
      "Iteration 14: Jclust = 2102.568271987431\n",
      "Iteration 15: Jclust = 2093.116812231559\n",
      "Iteration 16: Jclust = 2100.926209948208\n",
      "Iteration 17: Jclust = 2107.7086416798657\n",
      "Iteration 18: Jclust = 2101.2847443426563\n",
      "Iteration 19: Jclust = 2101.2847443426563\n",
      "Successfully terminated with convergence.\n",
      "mach.report.totalcost = 1930.2715167419492\n",
      "l = [(1, 89), (2, 85), (3, 95), (4, 73), (5, 85), (6, 72)]\n",
      "silhouette: 0.19181067100848828\n",
      "\n",
      "K = 7\n",
      "Iteration 1: Jclust = 3266.0\n",
      "Iteration 2: Jclust = 2168.316185377804\n",
      "Iteration 3: Jclust = 1965.074340153609\n",
      "Iteration 4: Jclust = 1960.55784190415\n",
      "Iteration 5: Jclust = 1927.3297736762506\n",
      "Iteration 6: Jclust = 1932.9871025945044\n",
      "Iteration 7: Jclust = 1924.768313463706\n",
      "Iteration 8: Jclust = 1910.1874863195615\n",
      "Iteration 9: Jclust = 1891.4133397860842\n",
      "Iteration 10: Jclust = 1909.088449877714\n",
      "Iteration 11: Jclust = 1913.675334960725\n",
      "Iteration 12: Jclust = 1884.6339768326043\n",
      "Iteration 13: Jclust = 1908.7757105273545\n",
      "Iteration 14: Jclust = 1911.9498500401498\n",
      "Iteration 15: Jclust = 1898.5861587087009\n",
      "Iteration 16: Jclust = 1886.1583778426784\n",
      "Iteration 17: Jclust = 1849.3615074478341\n",
      "Iteration 18: Jclust = 1855.7717409356765\n",
      "Iteration 19: Jclust = 1859.3917691909137\n",
      "Iteration 20: Jclust = 1865.0997424101329\n",
      "Iteration 21: Jclust = 1861.4173117212265\n",
      "Iteration 22: Jclust = 1859.690740689254\n",
      "Iteration 23: Jclust = 1849.628954036886\n",
      "Iteration 24: Jclust = 1849.628954036886\n",
      "Successfully terminated with convergence.\n",
      "mach.report.totalcost = 1707.3775951208381\n",
      "l = [(1, 71), (2, 65), (3, 78), (4, 70), (5, 84), (6, 62), (7, 69)]\n",
      "silhouette: 0.20968953297347556\n",
      "\n",
      "K = 8\n",
      "Iteration 1: Jclust = 2745.0\n",
      "Iteration 2: Jclust = 1962.7514781023262\n",
      "Iteration 3: Jclust = 1752.436488049932\n",
      "Iteration 4: Jclust = 1757.2276321789843\n",
      "Iteration 5: Jclust = 1765.9088093679977\n",
      "Iteration 6: Jclust = 1781.6722157794404\n",
      "Iteration 7: Jclust = 1792.8806575647839\n",
      "Iteration 8: Jclust = 1775.3348227439506\n",
      "Iteration 9: Jclust = 1783.0472876755478\n",
      "Iteration 10: Jclust = 1773.3589919856886\n",
      "Iteration 11: Jclust = 1784.363595136927\n",
      "Iteration 12: Jclust = 1790.1722388627056\n",
      "Iteration 13: Jclust = 1764.1970740068728\n",
      "Iteration 14: Jclust = 1754.3314202642985\n",
      "Iteration 15: Jclust = 1723.6756802638756\n",
      "Iteration 16: Jclust = 1723.4184120046075\n",
      "Iteration 17: Jclust = 1734.0907653030622\n",
      "Iteration 18: Jclust = 1728.6274265097168\n",
      "Iteration 19: Jclust = 1721.4788054478374\n",
      "Iteration 20: Jclust = 1718.9108542788135\n",
      "Iteration 21: Jclust = 1719.1956727913605\n",
      "Iteration 22: Jclust = 1706.4882632791966\n",
      "Iteration 23: Jclust = 1704.5081430511361\n",
      "Iteration 24: Jclust = 1704.5081430511361\n",
      "Successfully terminated with convergence.\n",
      "mach.report.totalcost = 1596.7642252650965\n",
      "l = [(1, 66), (2, 64), (3, 61), (4, 63), (5, 54), (6, 61), (7, 65), (8, 65)]\n",
      "silhouette: 0.20473618880241207\n",
      "\n",
      "K = 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{KMeans} @233\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/Ov46j/src/machines.jl:319\n",
      "┌ Info: Training \u001b[34mMachine{KMeans} @850\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/Ov46j/src/machines.jl:319\n",
      "┌ Info: Training \u001b[34mMachine{KMeans} @759\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/Ov46j/src/machines.jl:319\n",
      "┌ Info: Training \u001b[34mMachine{KMeans} @243\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/Ov46j/src/machines.jl:319\n",
      "┌ Info: Training \u001b[34mMachine{KMeans} @004\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/Ov46j/src/machines.jl:319\n",
      "┌ Info: Training \u001b[34mMachine{KMeans} @212\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/Ov46j/src/machines.jl:319\n",
      "┌ Info: Training \u001b[34mMachine{KMeans} @365\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/Ov46j/src/machines.jl:319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Jclust = 2383.0\n",
      "Iteration 2: Jclust = 1781.959955219797\n",
      "Iteration 3: Jclust = 1639.538583405228\n",
      "Iteration 4: Jclust = 1613.8338123934711\n",
      "Iteration 5: Jclust = 1654.1315006283587\n",
      "Iteration 6: Jclust = 1660.5167139737136\n",
      "Iteration 7: Jclust = 1666.2544683221859\n",
      "Iteration 8: Jclust = 1664.3283959856515\n",
      "Iteration 9: Jclust = 1650.6549776263605\n",
      "Iteration 10: Jclust = 1650.0557648451022\n",
      "Iteration 11: Jclust = 1649.207365946503\n",
      "Iteration 12: Jclust = 1630.321314601865\n",
      "Iteration 13: Jclust = 1627.6542245842804\n",
      "Iteration 14: Jclust = 1627.6542245842804\n",
      "Successfully terminated with convergence.\n",
      "mach.report.totalcost = 1503.1477950212077\n",
      "l = [(1, 68), (2, 61), (3, 49), (4, 52), (5, 39), (6, 59), (7, 62), (8, 47), (9, 62)]\n",
      "silhouette: 0.19561172083997486\n",
      "\n",
      "K = 10\n",
      "Iteration 1: Jclust = 2188.0\n",
      "Iteration 2: Jclust = 1645.6366824972042\n",
      "Iteration 3: Jclust = 1544.0607464576308\n",
      "Iteration 4: Jclust = 1486.3943199798562\n",
      "Iteration 5: Jclust = 1513.987166348829\n",
      "Iteration 6: Jclust = 1531.2190198621918\n",
      "Iteration 7: Jclust = 1539.5532358963428\n",
      "Iteration 8: Jclust = 1515.5904818416827\n",
      "Iteration 9: Jclust = 1489.5338870677335\n",
      "Iteration 10: Jclust = 1496.303149417063\n",
      "Iteration 11: Jclust = 1496.5893633848134\n",
      "Iteration 12: Jclust = 1492.4669746548514\n",
      "Iteration 13: Jclust = 1492.4669746548514\n",
      "Successfully terminated with convergence.\n",
      "mach.report.totalcost = 1396.999794108121\n",
      "l = [(1, 57), (2, 54), (3, 52), (4, 47), (5, 53), (6, 49), (7, 40), (8, 40), (9, 59), (10, 48)]\n",
      "silhouette: 0.19348249136126539\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{KMeans} @213\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/Ov46j/src/machines.jl:319\n"
     ]
    }
   ],
   "source": [
    "upper = 10\n",
    "k_range = 2:upper\n",
    "total_costs = []\n",
    "sils = []\n",
    "ls = []\n",
    "sil_means = []\n",
    "km_assignments = []\n",
    "\n",
    "for i in k_range\n",
    "    println(\"K = $i\")\n",
    "    model = ParallelKMeans.KMeans(k=i, rng=RNG)\n",
    "    mach = machine(model, X)\n",
    "    MLJ.fit!(mach, rows=train)\n",
    "    \n",
    "#     @show report(mach) \n",
    "#     @show fitted_params(mach)\n",
    "    @show mach.report.totalcost # https://github.com/PyDataBlog/ParallelKMeans.jl/blob/87ce07d10796078aacffcbea0b2e9dc0c02f25d7/src/hamerly.jl#L65\n",
    "    d = countmap(mach.report.assignments)\n",
    "    \n",
    "    k = d |> keys\n",
    "    v = d |> values\n",
    "    l = sort(collect(zip(k,v)), by=x->x[1])\n",
    "    @show l\n",
    "    \n",
    "    # https://juliastats.org/Clustering.jl/stable/validate.html\n",
    "    s = silhouettes(mach.report.assignments, dist_mat(train_data))\n",
    "    println(\"silhouette: $(mean(s))\")\n",
    "    \n",
    "    push!(km_assignments, mach.report.assignments)\n",
    "    push!(ls, l)\n",
    "    push!(total_costs, mach.report.totalcost) \n",
    "    push!(sils, s)\n",
    "    push!(sil_means, mean(s))\n",
    "    println(\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip030\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip030)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip031\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip030)\" d=\"\n",
       "M190.788 1423.18 L2248.5 1423.18 L2248.5 47.2441 L190.788 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip032\">\n",
       "    <rect x=\"190\" y=\"47\" width=\"2059\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  249.025,1423.18 249.025,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  734.336,1423.18 734.336,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1219.65,1423.18 1219.65,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1704.96,1423.18 1704.96,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2190.27,1423.18 2190.27,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  190.788,1423.18 2248.5,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.025,1423.18 249.025,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  734.336,1423.18 734.336,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1219.65,1423.18 1219.65,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1704.96,1423.18 1704.96,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2190.27,1423.18 2190.27,1406.67 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip030)\" d=\"M 0 0 M243.678 1479.92 L259.997 1479.92 L259.997 1483.85 L238.053 1483.85 L238.053 1479.92 Q240.715 1477.16 245.298 1472.53 Q249.905 1467.88 251.085 1466.54 Q253.331 1464.01 254.21 1462.28 Q255.113 1460.52 255.113 1458.83 Q255.113 1456.07 253.169 1454.34 Q251.247 1452.6 248.145 1452.6 Q245.946 1452.6 243.493 1453.37 Q241.062 1454.13 238.284 1455.68 L238.284 1450.96 Q241.108 1449.82 243.562 1449.25 Q246.016 1448.67 248.053 1448.67 Q253.423 1448.67 256.618 1451.35 Q259.812 1454.04 259.812 1458.53 Q259.812 1460.66 259.002 1462.58 Q258.215 1464.48 256.108 1467.07 Q255.53 1467.74 252.428 1470.96 Q249.326 1474.15 243.678 1479.92 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M737.345 1453.37 L725.539 1471.81 L737.345 1471.81 L737.345 1453.37 M736.118 1449.29 L741.998 1449.29 L741.998 1471.81 L746.928 1471.81 L746.928 1475.7 L741.998 1475.7 L741.998 1483.85 L737.345 1483.85 L737.345 1475.7 L721.743 1475.7 L721.743 1471.19 L736.118 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1220.05 1464.71 Q1216.9 1464.71 1215.05 1466.86 Q1213.22 1469.01 1213.22 1472.76 Q1213.22 1476.49 1215.05 1478.67 Q1216.9 1480.82 1220.05 1480.82 Q1223.2 1480.82 1225.03 1478.67 Q1226.88 1476.49 1226.88 1472.76 Q1226.88 1469.01 1225.03 1466.86 Q1223.2 1464.71 1220.05 1464.71 M1229.33 1450.06 L1229.33 1454.31 Q1227.57 1453.48 1225.77 1453.04 Q1223.99 1452.6 1222.23 1452.6 Q1217.6 1452.6 1215.14 1455.73 Q1212.71 1458.85 1212.37 1465.17 Q1213.73 1463.16 1215.79 1462.09 Q1217.85 1461 1220.33 1461 Q1225.54 1461 1228.55 1464.18 Q1231.58 1467.32 1231.58 1472.76 Q1231.58 1478.09 1228.43 1481.31 Q1225.28 1484.52 1220.05 1484.52 Q1214.06 1484.52 1210.88 1479.94 Q1207.71 1475.33 1207.71 1466.61 Q1207.71 1458.41 1211.6 1453.55 Q1215.49 1448.67 1222.04 1448.67 Q1223.8 1448.67 1225.58 1449.01 Q1227.39 1449.36 1229.33 1450.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1704.96 1467.44 Q1701.62 1467.44 1699.7 1469.22 Q1697.8 1471 1697.8 1474.13 Q1697.8 1477.25 1699.7 1479.04 Q1701.62 1480.82 1704.96 1480.82 Q1708.29 1480.82 1710.21 1479.04 Q1712.13 1477.23 1712.13 1474.13 Q1712.13 1471 1710.21 1469.22 Q1708.31 1467.44 1704.96 1467.44 M1700.28 1465.45 Q1697.27 1464.71 1695.58 1462.65 Q1693.91 1460.59 1693.91 1457.63 Q1693.91 1453.48 1696.85 1451.07 Q1699.82 1448.67 1704.96 1448.67 Q1710.12 1448.67 1713.06 1451.07 Q1716 1453.48 1716 1457.63 Q1716 1460.59 1714.31 1462.65 Q1712.64 1464.71 1709.66 1465.45 Q1713.03 1466.24 1714.91 1468.53 Q1716.81 1470.82 1716.81 1474.13 Q1716.81 1479.15 1713.73 1481.84 Q1710.67 1484.52 1704.96 1484.52 Q1699.24 1484.52 1696.16 1481.84 Q1693.1 1479.15 1693.1 1474.13 Q1693.1 1470.82 1695 1468.53 Q1696.9 1466.24 1700.28 1465.45 M1698.57 1458.06 Q1698.57 1460.75 1700.23 1462.25 Q1701.92 1463.76 1704.96 1463.76 Q1707.97 1463.76 1709.66 1462.25 Q1711.37 1460.75 1711.37 1458.06 Q1711.37 1455.38 1709.66 1453.88 Q1707.97 1452.37 1704.96 1452.37 Q1701.92 1452.37 1700.23 1453.88 Q1698.57 1455.38 1698.57 1458.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2167.14 1479.92 L2174.78 1479.92 L2174.78 1453.55 L2166.47 1455.22 L2166.47 1450.96 L2174.73 1449.29 L2179.41 1449.29 L2179.41 1479.92 L2187.05 1479.92 L2187.05 1483.85 L2167.14 1483.85 L2167.14 1479.92 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2202.12 1452.37 Q2198.51 1452.37 2196.68 1455.94 Q2194.87 1459.48 2194.87 1466.61 Q2194.87 1473.71 2196.68 1477.28 Q2198.51 1480.82 2202.12 1480.82 Q2205.75 1480.82 2207.56 1477.28 Q2209.39 1473.71 2209.39 1466.61 Q2209.39 1459.48 2207.56 1455.94 Q2205.75 1452.37 2202.12 1452.37 M2202.12 1448.67 Q2207.93 1448.67 2210.98 1453.27 Q2214.06 1457.86 2214.06 1466.61 Q2214.06 1475.33 2210.98 1479.94 Q2207.93 1484.52 2202.12 1484.52 Q2196.31 1484.52 2193.23 1479.94 Q2190.17 1475.33 2190.17 1466.61 Q2190.17 1457.86 2193.23 1453.27 Q2196.31 1448.67 2202.12 1448.67 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M934.208 1508.52 L942.865 1508.52 L963.935 1548.28 L963.935 1508.52 L970.174 1508.52 L970.174 1556.04 L961.516 1556.04 L940.446 1516.29 L940.446 1556.04 L934.208 1556.04 L934.208 1508.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M975.712 1541.98 L975.712 1520.4 L981.568 1520.4 L981.568 1541.75 Q981.568 1546.81 983.542 1549.36 Q985.515 1551.87 989.462 1551.87 Q994.204 1551.87 996.942 1548.85 Q999.711 1545.83 999.711 1540.61 L999.711 1520.4 L1005.57 1520.4 L1005.57 1556.04 L999.711 1556.04 L999.711 1550.57 Q997.578 1553.82 994.745 1555.41 Q991.945 1556.97 988.221 1556.97 Q982.078 1556.97 978.895 1553.15 Q975.712 1549.33 975.712 1541.98 M990.449 1519.54 L990.449 1519.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1039.46 1527.24 Q1041.66 1523.29 1044.72 1521.41 Q1047.77 1519.54 1051.91 1519.54 Q1057.48 1519.54 1060.5 1523.45 Q1063.53 1527.33 1063.53 1534.53 L1063.53 1556.04 L1057.64 1556.04 L1057.64 1534.72 Q1057.64 1529.59 1055.82 1527.11 Q1054.01 1524.63 1050.29 1524.63 Q1045.73 1524.63 1043.09 1527.65 Q1040.45 1530.68 1040.45 1535.9 L1040.45 1556.04 L1034.56 1556.04 L1034.56 1534.72 Q1034.56 1529.56 1032.75 1527.11 Q1030.93 1524.63 1027.15 1524.63 Q1022.66 1524.63 1020.02 1527.68 Q1017.38 1530.71 1017.38 1535.9 L1017.38 1556.04 L1011.49 1556.04 L1011.49 1520.4 L1017.38 1520.4 L1017.38 1525.93 Q1019.38 1522.66 1022.18 1521.1 Q1024.98 1519.54 1028.83 1519.54 Q1032.72 1519.54 1035.42 1521.51 Q1038.16 1523.48 1039.46 1527.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1095.26 1538.25 Q1095.26 1531.79 1092.59 1528.13 Q1089.94 1524.44 1085.3 1524.44 Q1080.65 1524.44 1077.98 1528.13 Q1075.34 1531.79 1075.34 1538.25 Q1075.34 1544.71 1077.98 1548.4 Q1080.65 1552.07 1085.3 1552.07 Q1089.94 1552.07 1092.59 1548.4 Q1095.26 1544.71 1095.26 1538.25 M1075.34 1525.81 Q1077.18 1522.62 1079.98 1521.1 Q1082.82 1519.54 1086.73 1519.54 Q1093.22 1519.54 1097.27 1524.69 Q1101.34 1529.85 1101.34 1538.25 Q1101.34 1546.65 1097.27 1551.81 Q1093.22 1556.97 1086.73 1556.97 Q1082.82 1556.97 1079.98 1555.44 Q1077.18 1553.88 1075.34 1550.7 L1075.34 1556.04 L1069.45 1556.04 L1069.45 1506.52 L1075.34 1506.52 L1075.34 1525.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1137.97 1536.76 L1137.97 1539.62 L1111.05 1539.62 Q1111.43 1545.67 1114.68 1548.85 Q1117.95 1552 1123.78 1552 Q1127.15 1552 1130.3 1551.17 Q1133.49 1550.35 1136.61 1548.69 L1136.61 1554.23 Q1133.45 1555.57 1130.14 1556.27 Q1126.83 1556.97 1123.43 1556.97 Q1114.9 1556.97 1109.9 1552 Q1104.94 1547.04 1104.94 1538.57 Q1104.94 1529.82 1109.65 1524.69 Q1114.39 1519.54 1122.41 1519.54 Q1129.6 1519.54 1133.77 1524.18 Q1137.97 1528.8 1137.97 1536.76 M1132.12 1535.04 Q1132.05 1530.23 1129.41 1527.37 Q1126.8 1524.5 1122.47 1524.5 Q1117.57 1524.5 1114.61 1527.27 Q1111.68 1530.04 1111.24 1535.07 L1132.12 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1164.77 1525.87 Q1163.79 1525.3 1162.61 1525.04 Q1161.46 1524.76 1160.06 1524.76 Q1155.1 1524.76 1152.42 1528 Q1149.78 1531.22 1149.78 1537.27 L1149.78 1556.04 L1143.89 1556.04 L1143.89 1520.4 L1149.78 1520.4 L1149.78 1525.93 Q1151.63 1522.69 1154.59 1521.13 Q1157.55 1519.54 1161.78 1519.54 Q1162.39 1519.54 1163.12 1519.63 Q1163.85 1519.7 1164.74 1519.85 L1164.77 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1205.45 1524.5 Q1200.74 1524.5 1198 1528.19 Q1195.27 1531.85 1195.27 1538.25 Q1195.27 1544.65 1197.97 1548.34 Q1200.71 1552 1205.45 1552 Q1210.13 1552 1212.87 1548.31 Q1215.6 1544.62 1215.6 1538.25 Q1215.6 1531.92 1212.87 1528.23 Q1210.13 1524.5 1205.45 1524.5 M1205.45 1519.54 Q1213.09 1519.54 1217.45 1524.5 Q1221.81 1529.47 1221.81 1538.25 Q1221.81 1547 1217.45 1552 Q1213.09 1556.97 1205.45 1556.97 Q1197.78 1556.97 1193.42 1552 Q1189.09 1547 1189.09 1538.25 Q1189.09 1529.47 1193.42 1524.5 Q1197.78 1519.54 1205.45 1519.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1246 1506.52 L1246 1511.39 L1240.4 1511.39 Q1237.25 1511.39 1236.01 1512.66 Q1234.8 1513.93 1234.8 1517.24 L1234.8 1520.4 L1244.44 1520.4 L1244.44 1524.95 L1234.8 1524.95 L1234.8 1556.04 L1228.91 1556.04 L1228.91 1524.95 L1223.31 1524.95 L1223.31 1520.4 L1228.91 1520.4 L1228.91 1517.91 Q1228.91 1511.96 1231.68 1509.26 Q1234.45 1506.52 1240.46 1506.52 L1246 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1308.7 1512.18 L1308.7 1518.96 Q1305.46 1515.94 1301.76 1514.44 Q1298.1 1512.95 1293.97 1512.95 Q1285.82 1512.95 1281.49 1517.95 Q1277.16 1522.91 1277.16 1532.33 Q1277.16 1541.72 1281.49 1546.72 Q1285.82 1551.68 1293.97 1551.68 Q1298.1 1551.68 1301.76 1550.19 Q1305.46 1548.69 1308.7 1545.67 L1308.7 1552.38 Q1305.33 1554.68 1301.54 1555.82 Q1297.78 1556.97 1293.58 1556.97 Q1282.79 1556.97 1276.59 1550.38 Q1270.38 1543.76 1270.38 1532.33 Q1270.38 1520.87 1276.59 1514.28 Q1282.79 1507.66 1293.58 1507.66 Q1297.85 1507.66 1301.6 1508.81 Q1305.39 1509.92 1308.7 1512.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1314.85 1506.52 L1320.7 1506.52 L1320.7 1556.04 L1314.85 1556.04 L1314.85 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1326.24 1541.98 L1326.24 1520.4 L1332.1 1520.4 L1332.1 1541.75 Q1332.1 1546.81 1334.07 1549.36 Q1336.04 1551.87 1339.99 1551.87 Q1344.73 1551.87 1347.47 1548.85 Q1350.24 1545.83 1350.24 1540.61 L1350.24 1520.4 L1356.09 1520.4 L1356.09 1556.04 L1350.24 1556.04 L1350.24 1550.57 Q1348.11 1553.82 1345.27 1555.41 Q1342.47 1556.97 1338.75 1556.97 Q1332.61 1556.97 1329.42 1553.15 Q1326.24 1549.33 1326.24 1541.98 M1340.98 1519.54 L1340.98 1519.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1384.96 1521.45 L1384.96 1526.98 Q1382.48 1525.71 1379.81 1525.07 Q1377.13 1524.44 1374.27 1524.44 Q1369.91 1524.44 1367.71 1525.77 Q1365.55 1527.11 1365.55 1529.79 Q1365.55 1531.82 1367.11 1533 Q1368.67 1534.15 1373.38 1535.2 L1375.38 1535.64 Q1381.62 1536.98 1384.23 1539.43 Q1386.87 1541.85 1386.87 1546.21 Q1386.87 1551.17 1382.93 1554.07 Q1379.01 1556.97 1372.14 1556.97 Q1369.27 1556.97 1366.15 1556.39 Q1363.07 1555.85 1359.63 1554.74 L1359.63 1548.69 Q1362.87 1550.38 1366.03 1551.24 Q1369.18 1552.07 1372.26 1552.07 Q1376.4 1552.07 1378.63 1550.66 Q1380.86 1549.23 1380.86 1546.65 Q1380.86 1544.27 1379.23 1542.99 Q1377.64 1541.72 1372.2 1540.54 L1370.16 1540.07 Q1364.72 1538.92 1362.3 1536.56 Q1359.88 1534.18 1359.88 1530.04 Q1359.88 1525.01 1363.45 1522.27 Q1367.01 1519.54 1373.57 1519.54 Q1376.82 1519.54 1379.68 1520.01 Q1382.54 1520.49 1384.96 1521.45 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1398.81 1510.27 L1398.81 1520.4 L1410.87 1520.4 L1410.87 1524.95 L1398.81 1524.95 L1398.81 1544.3 Q1398.81 1548.66 1399.99 1549.9 Q1401.2 1551.14 1404.86 1551.14 L1410.87 1551.14 L1410.87 1556.04 L1404.86 1556.04 Q1398.08 1556.04 1395.5 1553.53 Q1392.92 1550.98 1392.92 1544.3 L1392.92 1524.95 L1388.62 1524.95 L1388.62 1520.4 L1392.92 1520.4 L1392.92 1510.27 L1398.81 1510.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1447.51 1536.76 L1447.51 1539.62 L1420.58 1539.62 Q1420.96 1545.67 1424.21 1548.85 Q1427.49 1552 1433.31 1552 Q1436.68 1552 1439.84 1551.17 Q1443.02 1550.35 1446.14 1548.69 L1446.14 1554.23 Q1442.99 1555.57 1439.68 1556.27 Q1436.37 1556.97 1432.96 1556.97 Q1424.43 1556.97 1419.43 1552 Q1414.47 1547.04 1414.47 1538.57 Q1414.47 1529.82 1419.18 1524.69 Q1423.92 1519.54 1431.94 1519.54 Q1439.14 1519.54 1443.31 1524.18 Q1447.51 1528.8 1447.51 1536.76 M1441.65 1535.04 Q1441.59 1530.23 1438.94 1527.37 Q1436.33 1524.5 1432.01 1524.5 Q1427.1 1524.5 1424.14 1527.27 Q1421.22 1530.04 1420.77 1535.07 L1441.65 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1474.31 1525.87 Q1473.32 1525.3 1472.14 1525.04 Q1471 1524.76 1469.6 1524.76 Q1464.63 1524.76 1461.96 1528 Q1459.31 1531.22 1459.31 1537.27 L1459.31 1556.04 L1453.43 1556.04 L1453.43 1520.4 L1459.31 1520.4 L1459.31 1525.93 Q1461.16 1522.69 1464.12 1521.13 Q1467.08 1519.54 1471.31 1519.54 Q1471.92 1519.54 1472.65 1519.63 Q1473.38 1519.7 1474.27 1519.85 L1474.31 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1503.17 1521.45 L1503.17 1526.98 Q1500.69 1525.71 1498.02 1525.07 Q1495.34 1524.44 1492.48 1524.44 Q1488.12 1524.44 1485.92 1525.77 Q1483.76 1527.11 1483.76 1529.79 Q1483.76 1531.82 1485.32 1533 Q1486.88 1534.15 1491.59 1535.2 L1493.59 1535.64 Q1499.83 1536.98 1502.44 1539.43 Q1505.08 1541.85 1505.08 1546.21 Q1505.08 1551.17 1501.14 1554.07 Q1497.22 1556.97 1490.35 1556.97 Q1487.48 1556.97 1484.36 1556.39 Q1481.28 1555.85 1477.84 1554.74 L1477.84 1548.69 Q1481.09 1550.38 1484.24 1551.24 Q1487.39 1552.07 1490.47 1552.07 Q1494.61 1552.07 1496.84 1550.66 Q1499.07 1549.23 1499.07 1546.65 Q1499.07 1544.27 1497.45 1542.99 Q1495.85 1541.72 1490.41 1540.54 L1488.37 1540.07 Q1482.93 1538.92 1480.51 1536.56 Q1478.09 1534.18 1478.09 1530.04 Q1478.09 1525.01 1481.66 1522.27 Q1485.22 1519.54 1491.78 1519.54 Q1495.03 1519.54 1497.89 1520.01 Q1500.76 1520.49 1503.17 1521.45 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  190.788,1312.52 2248.5,1312.52 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  190.788,964.39 2248.5,964.39 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  190.788,616.256 2248.5,616.256 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  190.788,268.123 2248.5,268.123 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  190.788,1423.18 190.788,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  190.788,1312.52 215.48,1312.52 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  190.788,964.39 215.48,964.39 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  190.788,616.256 215.48,616.256 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  190.788,268.123 215.48,268.123 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip030)\" d=\"M 0 0 M54.8347 1325.87 L62.4735 1325.87 L62.4735 1299.5 L54.1634 1301.17 L54.1634 1296.91 L62.4272 1295.24 L67.1032 1295.24 L67.1032 1325.87 L74.742 1325.87 L74.742 1329.8 L54.8347 1329.8 L54.8347 1325.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M79.8577 1295.24 L98.2141 1295.24 L98.2141 1299.18 L84.1401 1299.18 L84.1401 1307.65 Q85.1586 1307.3 86.1771 1307.14 Q87.1956 1306.96 88.2141 1306.96 Q94.0012 1306.96 97.3808 1310.13 Q100.76 1313.3 100.76 1318.71 Q100.76 1324.29 97.2882 1327.4 Q93.816 1330.47 87.4966 1330.47 Q85.3206 1330.47 83.0521 1330.1 Q80.8068 1329.73 78.3994 1328.99 L78.3994 1324.29 Q80.4827 1325.43 82.7049 1325.98 Q84.9271 1326.54 87.404 1326.54 Q91.4086 1326.54 93.7465 1324.43 Q96.0845 1322.33 96.0845 1318.71 Q96.0845 1315.1 93.7465 1313 Q91.4086 1310.89 87.404 1310.89 Q85.529 1310.89 83.654 1311.31 Q81.8021 1311.72 79.8577 1312.6 L79.8577 1295.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M115.83 1298.32 Q112.219 1298.32 110.39 1301.89 Q108.584 1305.43 108.584 1312.56 Q108.584 1319.66 110.39 1323.23 Q112.219 1326.77 115.83 1326.77 Q119.464 1326.77 121.27 1323.23 Q123.098 1319.66 123.098 1312.56 Q123.098 1305.43 121.27 1301.89 Q119.464 1298.32 115.83 1298.32 M115.83 1294.62 Q121.64 1294.62 124.695 1299.22 Q127.774 1303.81 127.774 1312.56 Q127.774 1321.28 124.695 1325.89 Q121.64 1330.47 115.83 1330.47 Q110.02 1330.47 106.941 1325.89 Q103.885 1321.28 103.885 1312.56 Q103.885 1303.81 106.941 1299.22 Q110.02 1294.62 115.83 1294.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M142.843 1298.32 Q139.232 1298.32 137.404 1301.89 Q135.598 1305.43 135.598 1312.56 Q135.598 1319.66 137.404 1323.23 Q139.232 1326.77 142.843 1326.77 Q146.478 1326.77 148.283 1323.23 Q150.112 1319.66 150.112 1312.56 Q150.112 1305.43 148.283 1301.89 Q146.478 1298.32 142.843 1298.32 M142.843 1294.62 Q148.654 1294.62 151.709 1299.22 Q154.788 1303.81 154.788 1312.56 Q154.788 1321.28 151.709 1325.89 Q148.654 1330.47 142.843 1330.47 Q137.033 1330.47 133.955 1325.89 Q130.899 1321.28 130.899 1312.56 Q130.899 1303.81 133.955 1299.22 Q137.033 1294.62 142.843 1294.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M57.4273 977.734 L73.7466 977.734 L73.7466 981.67 L51.8023 981.67 L51.8023 977.734 Q54.4643 974.98 59.0476 970.35 Q63.6541 965.697 64.8346 964.355 Q67.08 961.832 67.9596 960.096 Q68.8624 958.336 68.8624 956.647 Q68.8624 953.892 66.918 952.156 Q64.9967 950.42 61.8948 950.42 Q59.6958 950.42 57.2421 951.184 Q54.8115 951.947 52.0338 953.498 L52.0338 948.776 Q54.8578 947.642 57.3115 947.063 Q59.7652 946.485 61.8023 946.485 Q67.1726 946.485 70.367 949.17 Q73.5614 951.855 73.5614 956.346 Q73.5614 958.475 72.7513 960.397 Q71.9642 962.295 69.8578 964.887 Q69.2791 965.559 66.1772 968.776 Q63.0754 971.971 57.4273 977.734 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M88.816 950.188 Q85.2049 950.188 83.3762 953.753 Q81.5707 957.295 81.5707 964.424 Q81.5707 971.531 83.3762 975.096 Q85.2049 978.637 88.816 978.637 Q92.4502 978.637 94.2558 975.096 Q96.0845 971.531 96.0845 964.424 Q96.0845 957.295 94.2558 953.753 Q92.4502 950.188 88.816 950.188 M88.816 946.485 Q94.6262 946.485 97.6817 951.091 Q100.76 955.674 100.76 964.424 Q100.76 973.151 97.6817 977.758 Q94.6262 982.341 88.816 982.341 Q83.0058 982.341 79.9272 977.758 Q76.8716 973.151 76.8716 964.424 Q76.8716 955.674 79.9272 951.091 Q83.0058 946.485 88.816 946.485 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M115.83 950.188 Q112.219 950.188 110.39 953.753 Q108.584 957.295 108.584 964.424 Q108.584 971.531 110.39 975.096 Q112.219 978.637 115.83 978.637 Q119.464 978.637 121.27 975.096 Q123.098 971.531 123.098 964.424 Q123.098 957.295 121.27 953.753 Q119.464 950.188 115.83 950.188 M115.83 946.485 Q121.64 946.485 124.695 951.091 Q127.774 955.674 127.774 964.424 Q127.774 973.151 124.695 977.758 Q121.64 982.341 115.83 982.341 Q110.02 982.341 106.941 977.758 Q103.885 973.151 103.885 964.424 Q103.885 955.674 106.941 951.091 Q110.02 946.485 115.83 946.485 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M142.843 950.188 Q139.232 950.188 137.404 953.753 Q135.598 957.295 135.598 964.424 Q135.598 971.531 137.404 975.096 Q139.232 978.637 142.843 978.637 Q146.478 978.637 148.283 975.096 Q150.112 971.531 150.112 964.424 Q150.112 957.295 148.283 953.753 Q146.478 950.188 142.843 950.188 M142.843 946.485 Q148.654 946.485 151.709 951.091 Q154.788 955.674 154.788 964.424 Q154.788 973.151 151.709 977.758 Q148.654 982.341 142.843 982.341 Q137.033 982.341 133.955 977.758 Q130.899 973.151 130.899 964.424 Q130.899 955.674 133.955 951.091 Q137.033 946.485 142.843 946.485 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M58.4226 629.601 L74.742 629.601 L74.742 633.536 L52.7977 633.536 L52.7977 629.601 Q55.4597 626.847 60.043 622.217 Q64.6495 617.564 65.83 616.222 Q68.0754 613.699 68.955 611.962 Q69.8578 610.203 69.8578 608.513 Q69.8578 605.759 67.9133 604.023 Q65.992 602.287 62.8902 602.287 Q60.6911 602.287 58.2375 603.05 Q55.8069 603.814 53.0292 605.365 L53.0292 600.643 Q55.8532 599.509 58.3069 598.93 Q60.7606 598.351 62.7976 598.351 Q68.168 598.351 71.3624 601.037 Q74.5568 603.722 74.5568 608.212 Q74.5568 610.342 73.7466 612.263 Q72.9596 614.162 70.8531 616.754 Q70.2744 617.425 67.1726 620.643 Q64.0708 623.837 58.4226 629.601 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M79.8577 598.976 L98.2141 598.976 L98.2141 602.912 L84.1401 602.912 L84.1401 611.384 Q85.1586 611.037 86.1771 610.875 Q87.1956 610.689 88.2141 610.689 Q94.0012 610.689 97.3808 613.861 Q100.76 617.032 100.76 622.449 Q100.76 628.027 97.2882 631.129 Q93.816 634.208 87.4966 634.208 Q85.3206 634.208 83.0521 633.837 Q80.8068 633.467 78.3994 632.726 L78.3994 628.027 Q80.4827 629.161 82.7049 629.717 Q84.9271 630.273 87.404 630.273 Q91.4086 630.273 93.7465 628.166 Q96.0845 626.06 96.0845 622.449 Q96.0845 618.837 93.7465 616.731 Q91.4086 614.624 87.404 614.624 Q85.529 614.624 83.654 615.041 Q81.8021 615.458 79.8577 616.337 L79.8577 598.976 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M115.83 602.055 Q112.219 602.055 110.39 605.62 Q108.584 609.162 108.584 616.291 Q108.584 623.398 110.39 626.962 Q112.219 630.504 115.83 630.504 Q119.464 630.504 121.27 626.962 Q123.098 623.398 123.098 616.291 Q123.098 609.162 121.27 605.62 Q119.464 602.055 115.83 602.055 M115.83 598.351 Q121.64 598.351 124.695 602.958 Q127.774 607.541 127.774 616.291 Q127.774 625.018 124.695 629.624 Q121.64 634.208 115.83 634.208 Q110.02 634.208 106.941 629.624 Q103.885 625.018 103.885 616.291 Q103.885 607.541 106.941 602.958 Q110.02 598.351 115.83 598.351 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M142.843 602.055 Q139.232 602.055 137.404 605.62 Q135.598 609.162 135.598 616.291 Q135.598 623.398 137.404 626.962 Q139.232 630.504 142.843 630.504 Q146.478 630.504 148.283 626.962 Q150.112 623.398 150.112 616.291 Q150.112 609.162 148.283 605.62 Q146.478 602.055 142.843 602.055 M142.843 598.351 Q148.654 598.351 151.709 602.958 Q154.788 607.541 154.788 616.291 Q154.788 625.018 151.709 629.624 Q148.654 634.208 142.843 634.208 Q137.033 634.208 133.955 629.624 Q130.899 625.018 130.899 616.291 Q130.899 607.541 133.955 602.958 Q137.033 598.351 142.843 598.351 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M66.617 266.769 Q69.9735 267.487 71.8485 269.755 Q73.7466 272.024 73.7466 275.357 Q73.7466 280.473 70.2281 283.274 Q66.7096 286.075 60.2282 286.075 Q58.0523 286.075 55.7375 285.635 Q53.4458 285.218 50.9921 284.362 L50.9921 279.848 Q52.9366 280.982 55.2514 281.561 Q57.5662 282.139 60.0893 282.139 Q64.4874 282.139 66.7791 280.403 Q69.0939 278.667 69.0939 275.357 Q69.0939 272.302 66.9411 270.589 Q64.8115 268.852 60.9921 268.852 L56.9643 268.852 L56.9643 265.01 L61.1773 265.01 Q64.6263 265.01 66.455 263.644 Q68.2837 262.255 68.2837 259.663 Q68.2837 257.001 66.3856 255.589 Q64.5106 254.153 60.9921 254.153 Q59.0708 254.153 56.8717 254.57 Q54.6727 254.987 52.0338 255.866 L52.0338 251.7 Q54.6958 250.959 57.0106 250.589 Q59.3486 250.218 61.4087 250.218 Q66.7328 250.218 69.8346 252.649 Q72.9365 255.056 72.9365 259.177 Q72.9365 262.047 71.2929 264.038 Q69.6494 266.005 66.617 266.769 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M88.816 253.922 Q85.2049 253.922 83.3762 257.487 Q81.5707 261.028 81.5707 268.158 Q81.5707 275.264 83.3762 278.829 Q85.2049 282.371 88.816 282.371 Q92.4502 282.371 94.2558 278.829 Q96.0845 275.264 96.0845 268.158 Q96.0845 261.028 94.2558 257.487 Q92.4502 253.922 88.816 253.922 M88.816 250.218 Q94.6262 250.218 97.6817 254.825 Q100.76 259.408 100.76 268.158 Q100.76 276.885 97.6817 281.491 Q94.6262 286.075 88.816 286.075 Q83.0058 286.075 79.9272 281.491 Q76.8716 276.885 76.8716 268.158 Q76.8716 259.408 79.9272 254.825 Q83.0058 250.218 88.816 250.218 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M115.83 253.922 Q112.219 253.922 110.39 257.487 Q108.584 261.028 108.584 268.158 Q108.584 275.264 110.39 278.829 Q112.219 282.371 115.83 282.371 Q119.464 282.371 121.27 278.829 Q123.098 275.264 123.098 268.158 Q123.098 261.028 121.27 257.487 Q119.464 253.922 115.83 253.922 M115.83 250.218 Q121.64 250.218 124.695 254.825 Q127.774 259.408 127.774 268.158 Q127.774 276.885 124.695 281.491 Q121.64 286.075 115.83 286.075 Q110.02 286.075 106.941 281.491 Q103.885 276.885 103.885 268.158 Q103.885 259.408 106.941 254.825 Q110.02 250.218 115.83 250.218 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M142.843 253.922 Q139.232 253.922 137.404 257.487 Q135.598 261.028 135.598 268.158 Q135.598 275.264 137.404 278.829 Q139.232 282.371 142.843 282.371 Q146.478 282.371 148.283 278.829 Q150.112 275.264 150.112 268.158 Q150.112 261.028 148.283 257.487 Q146.478 253.922 142.843 253.922 M142.843 250.218 Q148.654 250.218 151.709 254.825 Q154.788 259.408 154.788 268.158 Q154.788 276.885 151.709 281.491 Q148.654 286.075 142.843 286.075 Q137.033 286.075 133.955 281.491 Q130.899 276.885 130.899 268.158 Q130.899 259.408 133.955 254.825 Q137.033 250.218 142.843 250.218 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip032)\" style=\"stroke:#008000; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.025,86.1857 491.68,422.12 734.336,685.168 976.991,872.615 1219.65,1012.94 1462.3,1168.13 1704.96,1245.15 1947.61,1310.33 2190.27,1384.24 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip030)\" d=\"\n",
       "M259.378 1377.32 L901.206 1377.32 L901.206 1256.36 L259.378 1256.36  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  259.378,1377.32 901.206,1377.32 901.206,1256.36 259.378,1256.36 259.378,1377.32 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#008000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  282.242,1316.84 419.423,1316.84 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip030)\" d=\"M 0 0 M442.286 1299.56 L471.522 1299.56 L471.522 1303.49 L459.254 1303.49 L459.254 1334.12 L454.555 1334.12 L454.555 1303.49 L442.286 1303.49 L442.286 1299.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M477.981 1311.18 Q474.555 1311.18 472.564 1313.86 Q470.573 1316.52 470.573 1321.18 Q470.573 1325.83 472.541 1328.51 Q474.532 1331.18 477.981 1331.18 Q481.384 1331.18 483.374 1328.49 Q485.365 1325.81 485.365 1321.18 Q485.365 1316.57 483.374 1313.88 Q481.384 1311.18 477.981 1311.18 M477.981 1307.56 Q483.536 1307.56 486.708 1311.18 Q489.879 1314.79 489.879 1321.18 Q489.879 1327.54 486.708 1331.18 Q483.536 1334.79 477.981 1334.79 Q472.402 1334.79 469.231 1331.18 Q466.083 1327.54 466.083 1321.18 Q466.083 1314.79 469.231 1311.18 Q472.402 1307.56 477.981 1307.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M498.559 1300.83 L498.559 1308.19 L507.332 1308.19 L507.332 1311.5 L498.559 1311.5 L498.559 1325.57 Q498.559 1328.74 499.416 1329.65 Q500.295 1330.55 502.957 1330.55 L507.332 1330.55 L507.332 1334.12 L502.957 1334.12 Q498.027 1334.12 496.152 1332.29 Q494.277 1330.43 494.277 1325.57 L494.277 1311.5 L491.152 1311.5 L491.152 1308.19 L494.277 1308.19 L494.277 1300.83 L498.559 1300.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M523.582 1321.08 Q518.42 1321.08 516.43 1322.26 Q514.439 1323.44 514.439 1326.29 Q514.439 1328.56 515.92 1329.9 Q517.425 1331.22 519.994 1331.22 Q523.536 1331.22 525.666 1328.72 Q527.818 1326.2 527.818 1322.03 L527.818 1321.08 L523.582 1321.08 M532.078 1319.32 L532.078 1334.12 L527.818 1334.12 L527.818 1330.18 Q526.36 1332.54 524.184 1333.68 Q522.008 1334.79 518.86 1334.79 Q514.879 1334.79 512.518 1332.56 Q510.18 1330.32 510.18 1326.57 Q510.18 1322.19 513.096 1319.97 Q516.036 1317.75 521.846 1317.75 L527.818 1317.75 L527.818 1317.33 Q527.818 1314.39 525.874 1312.8 Q523.953 1311.18 520.457 1311.18 Q518.235 1311.18 516.129 1311.71 Q514.022 1312.24 512.078 1313.31 L512.078 1309.37 Q514.416 1308.47 516.615 1308.03 Q518.814 1307.56 520.897 1307.56 Q526.522 1307.56 529.3 1310.48 Q532.078 1313.4 532.078 1319.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M536.545 1298.1 L540.804 1298.1 L540.804 1334.12 L536.545 1334.12 L536.545 1298.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M581.244 1300.69 L581.244 1305.25 Q578.582 1303.98 576.221 1303.35 Q573.86 1302.73 571.661 1302.73 Q567.841 1302.73 565.758 1304.21 Q563.698 1305.69 563.698 1308.42 Q563.698 1310.71 565.064 1311.89 Q566.452 1313.05 570.295 1313.77 L573.119 1314.35 Q578.351 1315.34 580.827 1317.87 Q583.327 1320.37 583.327 1324.58 Q583.327 1329.6 579.948 1332.19 Q576.591 1334.79 570.087 1334.79 Q567.633 1334.79 564.855 1334.23 Q562.101 1333.68 559.138 1332.59 L559.138 1327.77 Q561.985 1329.37 564.716 1330.18 Q567.448 1330.99 570.087 1330.99 Q574.091 1330.99 576.267 1329.42 Q578.443 1327.84 578.443 1324.93 Q578.443 1322.38 576.869 1320.94 Q575.318 1319.51 571.753 1318.79 L568.906 1318.24 Q563.675 1317.19 561.337 1314.97 Q558.999 1312.75 558.999 1308.79 Q558.999 1304.21 562.216 1301.57 Q565.457 1298.93 571.128 1298.93 Q573.559 1298.93 576.082 1299.37 Q578.605 1299.81 581.244 1300.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M590.341 1321.18 Q590.341 1325.87 592.263 1328.56 Q594.207 1331.22 597.587 1331.22 Q600.966 1331.22 602.911 1328.56 Q604.855 1325.87 604.855 1321.18 Q604.855 1316.48 602.911 1313.81 Q600.966 1311.13 597.587 1311.13 Q594.207 1311.13 592.263 1313.81 Q590.341 1316.48 590.341 1321.18 M604.855 1330.23 Q603.512 1332.54 601.452 1333.68 Q599.415 1334.79 596.545 1334.79 Q591.846 1334.79 588.883 1331.04 Q585.943 1327.29 585.943 1321.18 Q585.943 1315.06 588.883 1311.31 Q591.846 1307.56 596.545 1307.56 Q599.415 1307.56 601.452 1308.7 Q603.512 1309.81 604.855 1312.12 L604.855 1308.19 L609.114 1308.19 L609.114 1343.98 L604.855 1343.98 L604.855 1330.23 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M613.142 1323.88 L613.142 1308.19 L617.401 1308.19 L617.401 1323.72 Q617.401 1327.4 618.836 1329.25 Q620.272 1331.08 623.142 1331.08 Q626.591 1331.08 628.582 1328.88 Q630.596 1326.68 630.596 1322.89 L630.596 1308.19 L634.855 1308.19 L634.855 1334.12 L630.596 1334.12 L630.596 1330.13 Q629.045 1332.49 626.985 1333.65 Q624.948 1334.79 622.239 1334.79 Q617.772 1334.79 615.457 1332.01 Q613.142 1329.23 613.142 1323.88 M623.86 1307.56 L623.86 1307.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M651.105 1321.08 Q645.943 1321.08 643.952 1322.26 Q641.961 1323.44 641.961 1326.29 Q641.961 1328.56 643.443 1329.9 Q644.947 1331.22 647.517 1331.22 Q651.059 1331.22 653.188 1328.72 Q655.341 1326.2 655.341 1322.03 L655.341 1321.08 L651.105 1321.08 M659.6 1319.32 L659.6 1334.12 L655.341 1334.12 L655.341 1330.18 Q653.883 1332.54 651.707 1333.68 Q649.531 1334.79 646.383 1334.79 Q642.401 1334.79 640.04 1332.56 Q637.702 1330.32 637.702 1326.57 Q637.702 1322.19 640.619 1319.97 Q643.559 1317.75 649.369 1317.75 L655.341 1317.75 L655.341 1317.33 Q655.341 1314.39 653.396 1312.8 Q651.475 1311.18 647.98 1311.18 Q645.758 1311.18 643.651 1311.71 Q641.545 1312.24 639.6 1313.31 L639.6 1309.37 Q641.938 1308.47 644.137 1308.03 Q646.336 1307.56 648.42 1307.56 Q654.045 1307.56 656.822 1310.48 Q659.6 1313.4 659.6 1319.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M679.091 1312.17 Q678.373 1311.75 677.517 1311.57 Q676.683 1311.36 675.665 1311.36 Q672.054 1311.36 670.109 1313.72 Q668.188 1316.06 668.188 1320.46 L668.188 1334.12 L663.906 1334.12 L663.906 1308.19 L668.188 1308.19 L668.188 1312.22 Q669.531 1309.86 671.683 1308.72 Q673.836 1307.56 676.915 1307.56 Q677.355 1307.56 677.887 1307.63 Q678.419 1307.68 679.068 1307.8 L679.091 1312.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M704.692 1320.09 L704.692 1322.17 L685.109 1322.17 Q685.387 1326.57 687.748 1328.88 Q690.132 1331.18 694.368 1331.18 Q696.822 1331.18 699.114 1330.57 Q701.429 1329.97 703.697 1328.77 L703.697 1332.8 Q701.405 1333.77 698.998 1334.28 Q696.591 1334.79 694.114 1334.79 Q687.91 1334.79 684.276 1331.18 Q680.665 1327.56 680.665 1321.41 Q680.665 1315.04 684.091 1311.31 Q687.54 1307.56 693.373 1307.56 Q698.605 1307.56 701.637 1310.94 Q704.692 1314.3 704.692 1320.09 M700.433 1318.84 Q700.387 1315.34 698.466 1313.26 Q696.568 1311.18 693.419 1311.18 Q689.855 1311.18 687.702 1313.19 Q685.572 1315.2 685.248 1318.86 L700.433 1318.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M726.22 1312.12 L726.22 1298.1 L730.479 1298.1 L730.479 1334.12 L726.22 1334.12 L726.22 1330.23 Q724.878 1332.54 722.817 1333.68 Q720.78 1334.79 717.91 1334.79 Q713.211 1334.79 710.248 1331.04 Q707.308 1327.29 707.308 1321.18 Q707.308 1315.06 710.248 1311.31 Q713.211 1307.56 717.91 1307.56 Q720.78 1307.56 722.817 1308.7 Q724.878 1309.81 726.22 1312.12 M711.706 1321.18 Q711.706 1325.87 713.628 1328.56 Q715.572 1331.22 718.952 1331.22 Q722.331 1331.22 724.276 1328.56 Q726.22 1325.87 726.22 1321.18 Q726.22 1316.48 724.276 1313.81 Q722.331 1311.13 718.952 1311.13 Q715.572 1311.13 713.628 1313.81 Q711.706 1316.48 711.706 1321.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M750.202 1299.56 L772.053 1299.56 L772.053 1303.49 L754.877 1303.49 L754.877 1313.72 L771.336 1313.72 L771.336 1317.66 L754.877 1317.66 L754.877 1330.18 L772.47 1330.18 L772.47 1334.12 L750.202 1334.12 L750.202 1299.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M791.961 1312.17 Q791.243 1311.75 790.386 1311.57 Q789.553 1311.36 788.535 1311.36 Q784.924 1311.36 782.979 1313.72 Q781.058 1316.06 781.058 1320.46 L781.058 1334.12 L776.775 1334.12 L776.775 1308.19 L781.058 1308.19 L781.058 1312.22 Q782.4 1309.86 784.553 1308.72 Q786.706 1307.56 789.785 1307.56 Q790.224 1307.56 790.757 1307.63 Q791.289 1307.68 791.937 1307.8 L791.961 1312.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M810.618 1312.17 Q809.9 1311.75 809.044 1311.57 Q808.21 1311.36 807.192 1311.36 Q803.581 1311.36 801.636 1313.72 Q799.715 1316.06 799.715 1320.46 L799.715 1334.12 L795.433 1334.12 L795.433 1308.19 L799.715 1308.19 L799.715 1312.22 Q801.058 1309.86 803.21 1308.72 Q805.363 1307.56 808.442 1307.56 Q808.882 1307.56 809.414 1307.63 Q809.947 1307.68 810.595 1307.8 L810.618 1312.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M824.09 1311.18 Q820.664 1311.18 818.673 1313.86 Q816.683 1316.52 816.683 1321.18 Q816.683 1325.83 818.65 1328.51 Q820.641 1331.18 824.09 1331.18 Q827.493 1331.18 829.483 1328.49 Q831.474 1325.81 831.474 1321.18 Q831.474 1316.57 829.483 1313.88 Q827.493 1311.18 824.09 1311.18 M824.09 1307.56 Q829.646 1307.56 832.817 1311.18 Q835.988 1314.79 835.988 1321.18 Q835.988 1327.54 832.817 1331.18 Q829.646 1334.79 824.09 1334.79 Q818.511 1334.79 815.34 1331.18 Q812.192 1327.54 812.192 1321.18 Q812.192 1314.79 815.34 1311.18 Q818.511 1307.56 824.09 1307.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M855.479 1312.17 Q854.761 1311.75 853.905 1311.57 Q853.071 1311.36 852.053 1311.36 Q848.442 1311.36 846.497 1313.72 Q844.576 1316.06 844.576 1320.46 L844.576 1334.12 L840.294 1334.12 L840.294 1308.19 L844.576 1308.19 L844.576 1312.22 Q845.919 1309.86 848.071 1308.72 Q850.224 1307.56 853.303 1307.56 Q853.743 1307.56 854.275 1307.63 Q854.807 1307.68 855.456 1307.8 L855.479 1312.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"\n",
       "M190.788 1423.18 L2248.5 1423.18 L2248.5 47.2441 L190.788 47.2441  Z\n",
       "  \" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"0\"/>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  249.025,1423.18 249.025,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  734.336,1423.18 734.336,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1219.65,1423.18 1219.65,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1704.96,1423.18 1704.96,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2190.27,1423.18 2190.27,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  190.788,1423.18 2248.5,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.025,1423.18 249.025,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  734.336,1423.18 734.336,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1219.65,1423.18 1219.65,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1704.96,1423.18 1704.96,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2190.27,1423.18 2190.27,1406.67 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip030)\" d=\"M 0 0 M243.678 1479.92 L259.997 1479.92 L259.997 1483.85 L238.053 1483.85 L238.053 1479.92 Q240.715 1477.16 245.298 1472.53 Q249.905 1467.88 251.085 1466.54 Q253.331 1464.01 254.21 1462.28 Q255.113 1460.52 255.113 1458.83 Q255.113 1456.07 253.169 1454.34 Q251.247 1452.6 248.145 1452.6 Q245.946 1452.6 243.493 1453.37 Q241.062 1454.13 238.284 1455.68 L238.284 1450.96 Q241.108 1449.82 243.562 1449.25 Q246.016 1448.67 248.053 1448.67 Q253.423 1448.67 256.618 1451.35 Q259.812 1454.04 259.812 1458.53 Q259.812 1460.66 259.002 1462.58 Q258.215 1464.48 256.108 1467.07 Q255.53 1467.74 252.428 1470.96 Q249.326 1474.15 243.678 1479.92 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M737.345 1453.37 L725.539 1471.81 L737.345 1471.81 L737.345 1453.37 M736.118 1449.29 L741.998 1449.29 L741.998 1471.81 L746.928 1471.81 L746.928 1475.7 L741.998 1475.7 L741.998 1483.85 L737.345 1483.85 L737.345 1475.7 L721.743 1475.7 L721.743 1471.19 L736.118 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1220.05 1464.71 Q1216.9 1464.71 1215.05 1466.86 Q1213.22 1469.01 1213.22 1472.76 Q1213.22 1476.49 1215.05 1478.67 Q1216.9 1480.82 1220.05 1480.82 Q1223.2 1480.82 1225.03 1478.67 Q1226.88 1476.49 1226.88 1472.76 Q1226.88 1469.01 1225.03 1466.86 Q1223.2 1464.71 1220.05 1464.71 M1229.33 1450.06 L1229.33 1454.31 Q1227.57 1453.48 1225.77 1453.04 Q1223.99 1452.6 1222.23 1452.6 Q1217.6 1452.6 1215.14 1455.73 Q1212.71 1458.85 1212.37 1465.17 Q1213.73 1463.16 1215.79 1462.09 Q1217.85 1461 1220.33 1461 Q1225.54 1461 1228.55 1464.18 Q1231.58 1467.32 1231.58 1472.76 Q1231.58 1478.09 1228.43 1481.31 Q1225.28 1484.52 1220.05 1484.52 Q1214.06 1484.52 1210.88 1479.94 Q1207.71 1475.33 1207.71 1466.61 Q1207.71 1458.41 1211.6 1453.55 Q1215.49 1448.67 1222.04 1448.67 Q1223.8 1448.67 1225.58 1449.01 Q1227.39 1449.36 1229.33 1450.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1704.96 1467.44 Q1701.62 1467.44 1699.7 1469.22 Q1697.8 1471 1697.8 1474.13 Q1697.8 1477.25 1699.7 1479.04 Q1701.62 1480.82 1704.96 1480.82 Q1708.29 1480.82 1710.21 1479.04 Q1712.13 1477.23 1712.13 1474.13 Q1712.13 1471 1710.21 1469.22 Q1708.31 1467.44 1704.96 1467.44 M1700.28 1465.45 Q1697.27 1464.71 1695.58 1462.65 Q1693.91 1460.59 1693.91 1457.63 Q1693.91 1453.48 1696.85 1451.07 Q1699.82 1448.67 1704.96 1448.67 Q1710.12 1448.67 1713.06 1451.07 Q1716 1453.48 1716 1457.63 Q1716 1460.59 1714.31 1462.65 Q1712.64 1464.71 1709.66 1465.45 Q1713.03 1466.24 1714.91 1468.53 Q1716.81 1470.82 1716.81 1474.13 Q1716.81 1479.15 1713.73 1481.84 Q1710.67 1484.52 1704.96 1484.52 Q1699.24 1484.52 1696.16 1481.84 Q1693.1 1479.15 1693.1 1474.13 Q1693.1 1470.82 1695 1468.53 Q1696.9 1466.24 1700.28 1465.45 M1698.57 1458.06 Q1698.57 1460.75 1700.23 1462.25 Q1701.92 1463.76 1704.96 1463.76 Q1707.97 1463.76 1709.66 1462.25 Q1711.37 1460.75 1711.37 1458.06 Q1711.37 1455.38 1709.66 1453.88 Q1707.97 1452.37 1704.96 1452.37 Q1701.92 1452.37 1700.23 1453.88 Q1698.57 1455.38 1698.57 1458.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2167.14 1479.92 L2174.78 1479.92 L2174.78 1453.55 L2166.47 1455.22 L2166.47 1450.96 L2174.73 1449.29 L2179.41 1449.29 L2179.41 1479.92 L2187.05 1479.92 L2187.05 1483.85 L2167.14 1483.85 L2167.14 1479.92 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2202.12 1452.37 Q2198.51 1452.37 2196.68 1455.94 Q2194.87 1459.48 2194.87 1466.61 Q2194.87 1473.71 2196.68 1477.28 Q2198.51 1480.82 2202.12 1480.82 Q2205.75 1480.82 2207.56 1477.28 Q2209.39 1473.71 2209.39 1466.61 Q2209.39 1459.48 2207.56 1455.94 Q2205.75 1452.37 2202.12 1452.37 M2202.12 1448.67 Q2207.93 1448.67 2210.98 1453.27 Q2214.06 1457.86 2214.06 1466.61 Q2214.06 1475.33 2210.98 1479.94 Q2207.93 1484.52 2202.12 1484.52 Q2196.31 1484.52 2193.23 1479.94 Q2190.17 1475.33 2190.17 1466.61 Q2190.17 1457.86 2193.23 1453.27 Q2196.31 1448.67 2202.12 1448.67 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  190.788,1201.72 2248.5,1201.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  190.788,825.987 2248.5,825.987 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  190.788,450.254 2248.5,450.254 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip032)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  190.788,74.5204 2248.5,74.5204 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2248.5,1423.18 2248.5,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2248.5,1201.72 2223.81,1201.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2248.5,825.987 2223.81,825.987 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2248.5,450.254 2223.81,450.254 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2248.5,74.5204 2223.81,74.5204 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip030)\" d=\"M 0 0 M2296.45 1187.52 Q2292.84 1187.52 2291.01 1191.08 Q2289.2 1194.63 2289.2 1201.76 Q2289.2 1208.86 2291.01 1212.43 Q2292.84 1215.97 2296.45 1215.97 Q2300.08 1215.97 2301.89 1212.43 Q2303.72 1208.86 2303.72 1201.76 Q2303.72 1194.63 2301.89 1191.08 Q2300.08 1187.52 2296.45 1187.52 M2296.45 1183.82 Q2302.26 1183.82 2305.31 1188.42 Q2308.39 1193.01 2308.39 1201.76 Q2308.39 1210.48 2305.31 1215.09 Q2302.26 1219.67 2296.45 1219.67 Q2290.64 1219.67 2287.56 1215.09 Q2284.5 1210.48 2284.5 1201.76 Q2284.5 1193.01 2287.56 1188.42 Q2290.64 1183.82 2296.45 1183.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2313.46 1213.12 L2318.35 1213.12 L2318.35 1219 L2313.46 1219 L2313.46 1213.12 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2324.23 1215.07 L2331.86 1215.07 L2331.86 1188.7 L2323.55 1190.37 L2323.55 1186.11 L2331.82 1184.44 L2336.49 1184.44 L2336.49 1215.07 L2344.13 1215.07 L2344.13 1219 L2324.23 1219 L2324.23 1215.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2359.2 1202.59 Q2355.87 1202.59 2353.95 1204.37 Q2352.05 1206.15 2352.05 1209.28 Q2352.05 1212.4 2353.95 1214.19 Q2355.87 1215.97 2359.2 1215.97 Q2362.54 1215.97 2364.46 1214.19 Q2366.38 1212.38 2366.38 1209.28 Q2366.38 1206.15 2364.46 1204.37 Q2362.56 1202.59 2359.2 1202.59 M2354.53 1200.6 Q2351.52 1199.86 2349.83 1197.8 Q2348.16 1195.74 2348.16 1192.77 Q2348.16 1188.63 2351.1 1186.22 Q2354.06 1183.82 2359.2 1183.82 Q2364.36 1183.82 2367.3 1186.22 Q2370.24 1188.63 2370.24 1192.77 Q2370.24 1195.74 2368.55 1197.8 Q2366.89 1199.86 2363.9 1200.6 Q2367.28 1201.38 2369.16 1203.68 Q2371.05 1205.97 2371.05 1209.28 Q2371.05 1214.3 2367.98 1216.99 Q2364.92 1219.67 2359.2 1219.67 Q2353.49 1219.67 2350.41 1216.99 Q2347.35 1214.3 2347.35 1209.28 Q2347.35 1205.97 2349.25 1203.68 Q2351.15 1201.38 2354.53 1200.6 M2352.81 1193.21 Q2352.81 1195.9 2354.48 1197.4 Q2356.17 1198.91 2359.2 1198.91 Q2362.21 1198.91 2363.9 1197.4 Q2365.61 1195.9 2365.61 1193.21 Q2365.61 1190.53 2363.9 1189.02 Q2362.21 1187.52 2359.2 1187.52 Q2356.17 1187.52 2354.48 1189.02 Q2352.81 1190.53 2352.81 1193.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2296.45 811.786 Q2292.84 811.786 2291.01 815.351 Q2289.2 818.892 2289.2 826.022 Q2289.2 833.128 2291.01 836.693 Q2292.84 840.235 2296.45 840.235 Q2300.08 840.235 2301.89 836.693 Q2303.72 833.128 2303.72 826.022 Q2303.72 818.892 2301.89 815.351 Q2300.08 811.786 2296.45 811.786 M2296.45 808.082 Q2302.26 808.082 2305.31 812.689 Q2308.39 817.272 2308.39 826.022 Q2308.39 834.749 2305.31 839.355 Q2302.26 843.938 2296.45 843.938 Q2290.64 843.938 2287.56 839.355 Q2284.5 834.749 2284.5 826.022 Q2284.5 817.272 2287.56 812.689 Q2290.64 808.082 2296.45 808.082 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2313.46 837.387 L2318.35 837.387 L2318.35 843.267 L2313.46 843.267 L2313.46 837.387 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2324.23 839.332 L2331.86 839.332 L2331.86 812.966 L2323.55 814.633 L2323.55 810.374 L2331.82 808.707 L2336.49 808.707 L2336.49 839.332 L2344.13 839.332 L2344.13 843.267 L2324.23 843.267 L2324.23 839.332 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2349.34 842.549 L2349.34 838.29 Q2351.1 839.124 2352.91 839.563 Q2354.71 840.003 2356.45 840.003 Q2361.08 840.003 2363.51 836.901 Q2365.96 833.776 2366.31 827.434 Q2364.97 829.425 2362.91 830.489 Q2360.85 831.554 2358.35 831.554 Q2353.16 831.554 2350.13 828.429 Q2347.12 825.281 2347.12 819.841 Q2347.12 814.517 2350.27 811.3 Q2353.42 808.082 2358.65 808.082 Q2364.64 808.082 2367.79 812.689 Q2370.96 817.272 2370.96 826.022 Q2370.96 834.193 2367.07 839.077 Q2363.21 843.938 2356.66 843.938 Q2354.9 843.938 2353.09 843.591 Q2351.29 843.244 2349.34 842.549 M2358.65 827.897 Q2361.8 827.897 2363.62 825.744 Q2365.48 823.591 2365.48 819.841 Q2365.48 816.114 2363.62 813.962 Q2361.8 811.786 2358.65 811.786 Q2355.5 811.786 2353.65 813.962 Q2351.82 816.114 2351.82 819.841 Q2351.82 823.591 2353.65 825.744 Q2355.5 827.897 2358.65 827.897 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2296.45 436.052 Q2292.84 436.052 2291.01 439.617 Q2289.2 443.159 2289.2 450.288 Q2289.2 457.395 2291.01 460.96 Q2292.84 464.501 2296.45 464.501 Q2300.08 464.501 2301.89 460.96 Q2303.72 457.395 2303.72 450.288 Q2303.72 443.159 2301.89 439.617 Q2300.08 436.052 2296.45 436.052 M2296.45 432.349 Q2302.26 432.349 2305.31 436.955 Q2308.39 441.538 2308.39 450.288 Q2308.39 459.015 2305.31 463.622 Q2302.26 468.205 2296.45 468.205 Q2290.64 468.205 2287.56 463.622 Q2284.5 459.015 2284.5 450.288 Q2284.5 441.538 2287.56 436.955 Q2290.64 432.349 2296.45 432.349 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2313.46 461.654 L2318.35 461.654 L2318.35 467.534 L2313.46 467.534 L2313.46 461.654 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2327.44 463.599 L2343.76 463.599 L2343.76 467.534 L2321.82 467.534 L2321.82 463.599 Q2324.48 460.844 2329.06 456.214 Q2333.67 451.562 2334.85 450.219 Q2337.1 447.696 2337.98 445.96 Q2338.88 444.201 2338.88 442.511 Q2338.88 439.756 2336.93 438.02 Q2335.01 436.284 2331.91 436.284 Q2329.71 436.284 2327.26 437.048 Q2324.83 437.812 2322.05 439.363 L2322.05 434.64 Q2324.87 433.506 2327.33 432.927 Q2329.78 432.349 2331.82 432.349 Q2337.19 432.349 2340.38 435.034 Q2343.58 437.719 2343.58 442.21 Q2343.58 444.339 2342.77 446.261 Q2341.98 448.159 2339.87 450.751 Q2339.3 451.423 2336.19 454.64 Q2333.09 457.835 2327.44 463.599 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2358.83 436.052 Q2355.22 436.052 2353.39 439.617 Q2351.59 443.159 2351.59 450.288 Q2351.59 457.395 2353.39 460.96 Q2355.22 464.501 2358.83 464.501 Q2362.47 464.501 2364.27 460.96 Q2366.1 457.395 2366.1 450.288 Q2366.1 443.159 2364.27 439.617 Q2362.47 436.052 2358.83 436.052 M2358.83 432.349 Q2364.64 432.349 2367.7 436.955 Q2370.78 441.538 2370.78 450.288 Q2370.78 459.015 2367.7 463.622 Q2364.64 468.205 2358.83 468.205 Q2353.02 468.205 2349.94 463.622 Q2346.89 459.015 2346.89 450.288 Q2346.89 441.538 2349.94 436.955 Q2353.02 432.349 2358.83 432.349 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2296.45 60.3191 Q2292.84 60.3191 2291.01 63.8839 Q2289.2 67.4255 2289.2 74.5551 Q2289.2 81.6616 2291.01 85.2264 Q2292.84 88.768 2296.45 88.768 Q2300.08 88.768 2301.89 85.2264 Q2303.72 81.6616 2303.72 74.5551 Q2303.72 67.4255 2301.89 63.8839 Q2300.08 60.3191 2296.45 60.3191 M2296.45 56.6154 Q2302.26 56.6154 2305.31 61.2219 Q2308.39 65.8052 2308.39 74.5551 Q2308.39 83.2819 2305.31 87.8884 Q2302.26 92.4717 2296.45 92.4717 Q2290.64 92.4717 2287.56 87.8884 Q2284.5 83.2819 2284.5 74.5551 Q2284.5 65.8052 2287.56 61.2219 Q2290.64 56.6154 2296.45 56.6154 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2313.46 85.9208 L2318.35 85.9208 L2318.35 91.8004 L2313.46 91.8004 L2313.46 85.9208 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2327.44 87.8652 L2343.76 87.8652 L2343.76 91.8004 L2321.82 91.8004 L2321.82 87.8652 Q2324.48 85.1106 2329.06 80.481 Q2333.67 75.8283 2334.85 74.4857 Q2337.1 71.9625 2337.98 70.2264 Q2338.88 68.4672 2338.88 66.7774 Q2338.88 64.0228 2336.93 62.2867 Q2335.01 60.5506 2331.91 60.5506 Q2329.71 60.5506 2327.26 61.3144 Q2324.83 62.0783 2322.05 63.6292 L2322.05 58.907 Q2324.87 57.7728 2327.33 57.1941 Q2329.78 56.6154 2331.82 56.6154 Q2337.19 56.6154 2340.38 59.3006 Q2343.58 61.9857 2343.58 66.4765 Q2343.58 68.6061 2342.77 70.5274 Q2341.98 72.4255 2339.87 75.0181 Q2339.3 75.6894 2336.19 78.9069 Q2333.09 82.1014 2327.44 87.8652 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2349.64 87.8652 L2357.28 87.8652 L2357.28 61.4996 L2348.97 63.1663 L2348.97 58.907 L2357.24 57.2404 L2361.91 57.2404 L2361.91 87.8652 L2369.55 87.8652 L2369.55 91.8004 L2349.64 91.8004 L2349.64 87.8652 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip032)\" style=\"stroke:#0000ff; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  249.025,1086.65 491.68,1384.24 734.336,861.854 976.991,732.539 1219.65,757.954 1462.3,86.1857 1704.96,272.299 1947.61,615.136 2190.27,695.138 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip030)\" d=\"\n",
       "M1605.98 214.069 L2179.91 214.069 L2179.91 93.1086 L1605.98 93.1086  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1605.98,214.069 2179.91,214.069 2179.91,93.1086 1605.98,93.1086 1605.98,214.069 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip030)\" style=\"stroke:#0000ff; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1628.84,153.589 1766.02,153.589 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip030)\" d=\"M 0 0 M1811.13 137.443 L1811.13 142.003 Q1808.47 140.73 1806.11 140.105 Q1803.75 139.48 1801.55 139.48 Q1797.73 139.48 1795.65 140.961 Q1793.59 142.443 1793.59 145.174 Q1793.59 147.466 1794.95 148.647 Q1796.34 149.804 1800.18 150.522 L1803.01 151.1 Q1808.24 152.096 1810.72 154.619 Q1813.22 157.119 1813.22 161.332 Q1813.22 166.355 1809.84 168.947 Q1806.48 171.54 1799.98 171.54 Q1797.52 171.54 1794.74 170.984 Q1791.99 170.429 1789.03 169.341 L1789.03 164.526 Q1791.87 166.123 1794.6 166.933 Q1797.34 167.744 1799.98 167.744 Q1803.98 167.744 1806.16 166.17 Q1808.33 164.596 1808.33 161.679 Q1808.33 159.133 1806.76 157.697 Q1805.21 156.262 1801.64 155.545 L1798.79 154.989 Q1793.56 153.947 1791.23 151.725 Q1788.89 149.503 1788.89 145.545 Q1788.89 140.961 1792.1 138.322 Q1795.35 135.684 1801.02 135.684 Q1803.45 135.684 1805.97 136.123 Q1808.49 136.563 1811.13 137.443 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1817.68 144.943 L1821.94 144.943 L1821.94 170.869 L1817.68 170.869 L1817.68 144.943 M1817.68 134.85 L1821.94 134.85 L1821.94 140.244 L1817.68 140.244 L1817.68 134.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1826.41 134.85 L1830.67 134.85 L1830.67 170.869 L1826.41 170.869 L1826.41 134.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1856.69 155.221 L1856.69 170.869 L1852.43 170.869 L1852.43 155.359 Q1852.43 151.679 1850.99 149.85 Q1849.56 148.022 1846.69 148.022 Q1843.24 148.022 1841.25 150.221 Q1839.26 152.42 1839.26 156.216 L1839.26 170.869 L1834.98 170.869 L1834.98 134.85 L1839.26 134.85 L1839.26 148.971 Q1840.79 146.633 1842.85 145.475 Q1844.93 144.318 1847.64 144.318 Q1852.1 144.318 1854.4 147.096 Q1856.69 149.85 1856.69 155.221 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1871.2 147.929 Q1867.78 147.929 1865.79 150.614 Q1863.79 153.276 1863.79 157.929 Q1863.79 162.582 1865.76 165.267 Q1867.75 167.929 1871.2 167.929 Q1874.6 167.929 1876.6 165.244 Q1878.59 162.558 1878.59 157.929 Q1878.59 153.322 1876.6 150.637 Q1874.6 147.929 1871.2 147.929 M1871.2 144.318 Q1876.76 144.318 1879.93 147.929 Q1883.1 151.54 1883.1 157.929 Q1883.1 164.295 1879.93 167.929 Q1876.76 171.54 1871.2 171.54 Q1865.62 171.54 1862.45 167.929 Q1859.3 164.295 1859.3 157.929 Q1859.3 151.54 1862.45 147.929 Q1865.62 144.318 1871.2 144.318 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1887.13 160.637 L1887.13 144.943 L1891.39 144.943 L1891.39 160.475 Q1891.39 164.156 1892.82 166.008 Q1894.26 167.836 1897.13 167.836 Q1900.58 167.836 1902.57 165.637 Q1904.58 163.438 1904.58 159.642 L1904.58 144.943 L1908.84 144.943 L1908.84 170.869 L1904.58 170.869 L1904.58 166.887 Q1903.03 169.248 1900.97 170.406 Q1898.93 171.54 1896.22 171.54 Q1891.76 171.54 1889.44 168.762 Q1887.13 165.984 1887.13 160.637 M1897.85 144.318 L1897.85 144.318 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1935.48 156.841 L1935.48 158.924 L1915.9 158.924 Q1916.18 163.322 1918.54 165.637 Q1920.92 167.929 1925.16 167.929 Q1927.61 167.929 1929.91 167.327 Q1932.22 166.725 1934.49 165.521 L1934.49 169.549 Q1932.2 170.521 1929.79 171.031 Q1927.38 171.54 1924.91 171.54 Q1918.7 171.54 1915.07 167.929 Q1911.46 164.318 1911.46 158.16 Q1911.46 151.795 1914.88 148.068 Q1918.33 144.318 1924.16 144.318 Q1929.4 144.318 1932.43 147.697 Q1935.48 151.054 1935.48 156.841 M1931.22 155.591 Q1931.18 152.096 1929.26 150.012 Q1927.36 147.929 1924.21 147.929 Q1920.65 147.929 1918.49 149.943 Q1916.36 151.957 1916.04 155.614 L1931.22 155.591 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1944.16 137.582 L1944.16 144.943 L1952.94 144.943 L1952.94 148.253 L1944.16 148.253 L1944.16 162.327 Q1944.16 165.498 1945.02 166.401 Q1945.9 167.304 1948.56 167.304 L1952.94 167.304 L1952.94 170.869 L1948.56 170.869 Q1943.63 170.869 1941.76 169.04 Q1939.88 167.188 1939.88 162.327 L1939.88 148.253 L1936.76 148.253 L1936.76 144.943 L1939.88 144.943 L1939.88 137.582 L1944.16 137.582 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1961.62 137.582 L1961.62 144.943 L1970.39 144.943 L1970.39 148.253 L1961.62 148.253 L1961.62 162.327 Q1961.62 165.498 1962.47 166.401 Q1963.35 167.304 1966.02 167.304 L1970.39 167.304 L1970.39 170.869 L1966.02 170.869 Q1961.09 170.869 1959.21 169.04 Q1957.34 167.188 1957.34 162.327 L1957.34 148.253 L1954.21 148.253 L1954.21 144.943 L1957.34 144.943 L1957.34 137.582 L1961.62 137.582 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M1997.03 156.841 L1997.03 158.924 L1977.45 158.924 Q1977.73 163.322 1980.09 165.637 Q1982.47 167.929 1986.71 167.929 Q1989.16 167.929 1991.46 167.327 Q1993.77 166.725 1996.04 165.521 L1996.04 169.549 Q1993.75 170.521 1991.34 171.031 Q1988.93 171.54 1986.46 171.54 Q1980.25 171.54 1976.62 167.929 Q1973.01 164.318 1973.01 158.16 Q1973.01 151.795 1976.43 148.068 Q1979.88 144.318 1985.71 144.318 Q1990.95 144.318 1993.98 147.697 Q1997.03 151.054 1997.03 156.841 M1992.78 155.591 Q1992.73 152.096 1990.81 150.012 Q1988.91 147.929 1985.76 147.929 Q1982.2 147.929 1980.04 149.943 Q1977.91 151.957 1977.59 155.614 L1992.78 155.591 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2037.47 137.443 L2037.47 142.003 Q2034.81 140.73 2032.45 140.105 Q2030.09 139.48 2027.89 139.48 Q2024.07 139.48 2021.99 140.961 Q2019.93 142.443 2019.93 145.174 Q2019.93 147.466 2021.29 148.647 Q2022.68 149.804 2026.52 150.522 L2029.35 151.1 Q2034.58 152.096 2037.06 154.619 Q2039.56 157.119 2039.56 161.332 Q2039.56 166.355 2036.18 168.947 Q2032.82 171.54 2026.32 171.54 Q2023.86 171.54 2021.09 170.984 Q2018.33 170.429 2015.37 169.341 L2015.37 164.526 Q2018.21 166.123 2020.95 166.933 Q2023.68 167.744 2026.32 167.744 Q2030.32 167.744 2032.5 166.17 Q2034.67 164.596 2034.67 161.679 Q2034.67 159.133 2033.1 157.697 Q2031.55 156.262 2027.98 155.545 L2025.14 154.989 Q2019.9 153.947 2017.57 151.725 Q2015.23 149.503 2015.23 145.545 Q2015.23 140.961 2018.45 138.322 Q2021.69 135.684 2027.36 135.684 Q2029.79 135.684 2032.31 136.123 Q2034.84 136.563 2037.47 137.443 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2062.68 145.938 L2062.68 149.92 Q2060.88 148.924 2059.05 148.438 Q2057.24 147.929 2055.39 147.929 Q2051.25 147.929 2048.96 150.568 Q2046.66 153.184 2046.66 157.929 Q2046.66 162.674 2048.96 165.313 Q2051.25 167.929 2055.39 167.929 Q2057.24 167.929 2059.05 167.443 Q2060.88 166.933 2062.68 165.938 L2062.68 169.873 Q2060.9 170.707 2058.98 171.123 Q2057.08 171.54 2054.93 171.54 Q2049.07 171.54 2045.62 167.859 Q2042.17 164.179 2042.17 157.929 Q2042.17 151.586 2045.65 147.952 Q2049.14 144.318 2055.21 144.318 Q2057.17 144.318 2059.05 144.735 Q2060.92 145.128 2062.68 145.938 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2077.2 147.929 Q2073.77 147.929 2071.78 150.614 Q2069.79 153.276 2069.79 157.929 Q2069.79 162.582 2071.76 165.267 Q2073.75 167.929 2077.2 167.929 Q2080.6 167.929 2082.59 165.244 Q2084.58 162.558 2084.58 157.929 Q2084.58 153.322 2082.59 150.637 Q2080.6 147.929 2077.2 147.929 M2077.2 144.318 Q2082.75 144.318 2085.92 147.929 Q2089.09 151.54 2089.09 157.929 Q2089.09 164.295 2085.92 167.929 Q2082.75 171.54 2077.2 171.54 Q2071.62 171.54 2068.45 167.929 Q2065.3 164.295 2065.3 157.929 Q2065.3 151.54 2068.45 147.929 Q2071.62 144.318 2077.2 144.318 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2108.58 148.924 Q2107.87 148.508 2107.01 148.322 Q2106.18 148.114 2105.16 148.114 Q2101.55 148.114 2099.6 150.475 Q2097.68 152.813 2097.68 157.211 L2097.68 170.869 L2093.4 170.869 L2093.4 144.943 L2097.68 144.943 L2097.68 148.971 Q2099.02 146.609 2101.18 145.475 Q2103.33 144.318 2106.41 144.318 Q2106.85 144.318 2107.38 144.387 Q2107.91 144.434 2108.56 144.549 L2108.58 148.924 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip030)\" d=\"M 0 0 M2134.19 156.841 L2134.19 158.924 L2114.6 158.924 Q2114.88 163.322 2117.24 165.637 Q2119.63 167.929 2123.86 167.929 Q2126.32 167.929 2128.61 167.327 Q2130.92 166.725 2133.19 165.521 L2133.19 169.549 Q2130.9 170.521 2128.49 171.031 Q2126.08 171.54 2123.61 171.54 Q2117.4 171.54 2113.77 167.929 Q2110.16 164.318 2110.16 158.16 Q2110.16 151.795 2113.58 148.068 Q2117.03 144.318 2122.87 144.318 Q2128.1 144.318 2131.13 147.697 Q2134.19 151.054 2134.19 156.841 M2129.93 155.591 Q2129.88 152.096 2127.96 150.012 Q2126.06 147.929 2122.91 147.929 Q2119.35 147.929 2117.2 149.943 Q2115.07 151.957 2114.74 155.614 L2129.93 155.591 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(k_range, total_costs, legend=:bottomleft, label=\"Total Squared Error\", color=:green, lw=2)\n",
    "xlabel!(\"Number of Clusters\")\n",
    "plot!(twinx(), k_range, sil_means, legend=:topright, label=\"Silhouette Score\", color=:blue, lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_kmeans_metrics_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prepare_portfolio (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function prepare_portfolio(ls)\n",
    "    N = size(ls)[1]\n",
    "    D = size(ls[end])[1]\n",
    "    mat = zeros(N, D)\n",
    "    for i in 1:N\n",
    "#         print(\"\\n\")\n",
    "#         @show i\n",
    "        for j in 1:size(ls[i])[1]\n",
    "#             @show j\n",
    "            mat[i,j] = ls[i][j][2]\n",
    "        end\n",
    "    end\n",
    "    return mat\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cum_columns (generic function with 1 method)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cum_columns(mat; normalize=false)\n",
    "    mat2 = deepcopy(mat)\n",
    "    normalize && (mat2 ./= sum(mat2, dims = 2)) # if you want to normalize each row\n",
    "    for i in 2:size(mat2)[2]\n",
    "       mat2[:,i] = mat2[:,i-1] + mat2[:,i]\n",
    "    end\n",
    "    return mat2'\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cum_plot (generic function with 1 method)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cum_plot(mat)\n",
    "    N = size(mat)[2]\n",
    "    p = plot(legend=:outertopright, palette=palette(:Accent_8))\n",
    "    for i in N+1:-1:1\n",
    "        plot!(1:N, mat[i,:], label=\"Cluster $(i)\", fill=0, α=1)\n",
    "    end\n",
    "    xticks!(collect(1:N),string.(collect(2:N+1)))\n",
    "    ylabel!(\"Cluster Proportion\")\n",
    "    xlabel!(\"Number of Clusters\")\n",
    "    display(p)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fillrange {Number or AbstractVector}\n",
      "fill_between, fillbetween, fillranges, fillrng, fillto, frange\n",
      "\n",
      "Fills area between fillrange and y for line-types, sets the base for bar/stick types, and similar for other types.\n",
      "Series attribute,  default: nothing\n"
     ]
    }
   ],
   "source": [
    "# plotattr(:Series)\n",
    "plotattr(\"fillrange\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10×9 LinearAlgebra.Adjoint{Float64,Array{Float64,2}}:\n",
       " 0.517034  0.378758  0.238477  0.218437  …  0.132265  0.136273  0.114228\n",
       " 1.0       0.691383  0.509018  0.426854     0.260521  0.258517  0.222445\n",
       " 1.0       1.0       0.735471  0.611222     0.382766  0.356713  0.326653\n",
       " 1.0       1.0       1.0       0.831663     0.509018  0.460922  0.420842\n",
       " 1.0       1.0       1.0       1.0          0.617234  0.539078  0.527054\n",
       " 1.0       1.0       1.0       1.0       …  0.739479  0.657315  0.625251\n",
       " 1.0       1.0       1.0       1.0          0.869739  0.781563  0.705411\n",
       " 1.0       1.0       1.0       1.0          1.0       0.875752  0.785571\n",
       " 1.0       1.0       1.0       1.0          1.0       1.0       0.903808\n",
       " 1.0       1.0       1.0       1.0          1.0       1.0       1.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = prepare_portfolio(ls)\n",
    "plotmat = cum_columns(mat, normalize=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Explanation\n",
    "For the plot, the area below each line indicates the proportion of the instances that are contained in that cluster. X-axis is number of clusters in that Kmeans run. So for 2 means, there are 2 clusters. For 5 means, there is a large cluster 1, a large cluster 4, and a large cluster 5. Up to runs as large as 6 clusters, there are really only 3 prevalent clusters. Farther than that, it becomes more fragmented. 3 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip080\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip080)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip081\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip080)\" d=\"\n",
       "M238.13 1423.18 L1831.13 1423.18 L1831.13 47.2441 L238.13 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip082\">\n",
       "    <rect x=\"238\" y=\"47\" width=\"1594\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  283.215,1423.18 283.215,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  471.069,1423.18 471.069,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  658.923,1423.18 658.923,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  846.776,1423.18 846.776,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1034.63,1423.18 1034.63,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1222.48,1423.18 1222.48,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1410.34,1423.18 1410.34,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1598.19,1423.18 1598.19,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1786.05,1423.18 1786.05,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  238.13,1423.18 1831.13,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.215,1423.18 283.215,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  471.069,1423.18 471.069,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  658.923,1423.18 658.923,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  846.776,1423.18 846.776,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1034.63,1423.18 1034.63,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1222.48,1423.18 1222.48,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1410.34,1423.18 1410.34,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1598.19,1423.18 1598.19,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1786.05,1423.18 1786.05,1406.67 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M277.868 1479.92 L294.187 1479.92 L294.187 1483.85 L272.243 1483.85 L272.243 1479.92 Q274.905 1477.16 279.488 1472.53 Q284.095 1467.88 285.275 1466.54 Q287.521 1464.01 288.4 1462.28 Q289.303 1460.52 289.303 1458.83 Q289.303 1456.07 287.358 1454.34 Q285.437 1452.6 282.335 1452.6 Q280.136 1452.6 277.683 1453.37 Q275.252 1454.13 272.474 1455.68 L272.474 1450.96 Q275.298 1449.82 277.752 1449.25 Q280.206 1448.67 282.243 1448.67 Q287.613 1448.67 290.808 1451.35 Q294.002 1454.04 294.002 1458.53 Q294.002 1460.66 293.192 1462.58 Q292.405 1464.48 290.298 1467.07 Q289.72 1467.74 286.618 1470.96 Q283.516 1474.15 277.868 1479.92 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M475.316 1465.22 Q478.673 1465.94 480.548 1468.2 Q482.446 1470.47 482.446 1473.81 Q482.446 1478.92 478.928 1481.72 Q475.409 1484.52 468.928 1484.52 Q466.752 1484.52 464.437 1484.08 Q462.145 1483.67 459.692 1482.81 L459.692 1478.3 Q461.636 1479.43 463.951 1480.01 Q466.266 1480.59 468.789 1480.59 Q473.187 1480.59 475.479 1478.85 Q477.793 1477.12 477.793 1473.81 Q477.793 1470.75 475.641 1469.04 Q473.511 1467.3 469.692 1467.3 L465.664 1467.3 L465.664 1463.46 L469.877 1463.46 Q473.326 1463.46 475.154 1462.09 Q476.983 1460.7 476.983 1458.11 Q476.983 1455.45 475.085 1454.04 Q473.21 1452.6 469.692 1452.6 Q467.77 1452.6 465.571 1453.02 Q463.372 1453.44 460.733 1454.31 L460.733 1450.15 Q463.395 1449.41 465.71 1449.04 Q468.048 1448.67 470.108 1448.67 Q475.432 1448.67 478.534 1451.1 Q481.636 1453.5 481.636 1457.63 Q481.636 1460.5 479.992 1462.49 Q478.349 1464.45 475.316 1465.22 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M661.932 1453.37 L650.126 1471.81 L661.932 1471.81 L661.932 1453.37 M660.705 1449.29 L666.585 1449.29 L666.585 1471.81 L671.515 1471.81 L671.515 1475.7 L666.585 1475.7 L666.585 1483.85 L661.932 1483.85 L661.932 1475.7 L646.33 1475.7 L646.33 1471.19 L660.705 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M837.054 1449.29 L855.411 1449.29 L855.411 1453.23 L841.337 1453.23 L841.337 1461.7 Q842.355 1461.35 843.374 1461.19 Q844.392 1461 845.411 1461 Q851.198 1461 854.577 1464.18 Q857.957 1467.35 857.957 1472.76 Q857.957 1478.34 854.485 1481.44 Q851.013 1484.52 844.693 1484.52 Q842.517 1484.52 840.249 1484.15 Q838.003 1483.78 835.596 1483.04 L835.596 1478.34 Q837.679 1479.48 839.902 1480.03 Q842.124 1480.59 844.601 1480.59 Q848.605 1480.59 850.943 1478.48 Q853.281 1476.38 853.281 1472.76 Q853.281 1469.15 850.943 1467.05 Q848.605 1464.94 844.601 1464.94 Q842.726 1464.94 840.851 1465.36 Q838.999 1465.77 837.054 1466.65 L837.054 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1035.04 1464.71 Q1031.89 1464.71 1030.04 1466.86 Q1028.21 1469.01 1028.21 1472.76 Q1028.21 1476.49 1030.04 1478.67 Q1031.89 1480.82 1035.04 1480.82 Q1038.18 1480.82 1040.01 1478.67 Q1041.86 1476.49 1041.86 1472.76 Q1041.86 1469.01 1040.01 1466.86 Q1038.18 1464.71 1035.04 1464.71 M1044.32 1450.06 L1044.32 1454.31 Q1042.56 1453.48 1040.75 1453.04 Q1038.97 1452.6 1037.21 1452.6 Q1032.58 1452.6 1030.13 1455.73 Q1027.7 1458.85 1027.35 1465.17 Q1028.72 1463.16 1030.78 1462.09 Q1032.84 1461 1035.31 1461 Q1040.52 1461 1043.53 1464.18 Q1046.56 1467.32 1046.56 1472.76 Q1046.56 1478.09 1043.41 1481.31 Q1040.27 1484.52 1035.04 1484.52 Q1029.04 1484.52 1025.87 1479.94 Q1022.7 1475.33 1022.7 1466.61 Q1022.7 1458.41 1026.59 1453.55 Q1030.48 1448.67 1037.03 1448.67 Q1038.79 1448.67 1040.57 1449.01 Q1042.37 1449.36 1044.32 1450.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1211.37 1449.29 L1233.6 1449.29 L1233.6 1451.28 L1221.05 1483.85 L1216.16 1483.85 L1227.97 1453.23 L1211.37 1453.23 L1211.37 1449.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1410.34 1467.44 Q1407 1467.44 1405.08 1469.22 Q1403.19 1471 1403.19 1474.13 Q1403.19 1477.25 1405.08 1479.04 Q1407 1480.82 1410.34 1480.82 Q1413.67 1480.82 1415.59 1479.04 Q1417.51 1477.23 1417.51 1474.13 Q1417.51 1471 1415.59 1469.22 Q1413.69 1467.44 1410.34 1467.44 M1405.66 1465.45 Q1402.65 1464.71 1400.96 1462.65 Q1399.3 1460.59 1399.3 1457.63 Q1399.3 1453.48 1402.24 1451.07 Q1405.2 1448.67 1410.34 1448.67 Q1415.5 1448.67 1418.44 1451.07 Q1421.38 1453.48 1421.38 1457.63 Q1421.38 1460.59 1419.69 1462.65 Q1418.02 1464.71 1415.04 1465.45 Q1418.42 1466.24 1420.29 1468.53 Q1422.19 1470.82 1422.19 1474.13 Q1422.19 1479.15 1419.11 1481.84 Q1416.06 1484.52 1410.34 1484.52 Q1404.62 1484.52 1401.54 1481.84 Q1398.49 1479.15 1398.49 1474.13 Q1398.49 1470.82 1400.38 1468.53 Q1402.28 1466.24 1405.66 1465.45 M1403.95 1458.06 Q1403.95 1460.75 1405.62 1462.25 Q1407.31 1463.76 1410.34 1463.76 Q1413.35 1463.76 1415.04 1462.25 Q1416.75 1460.75 1416.75 1458.06 Q1416.75 1455.38 1415.04 1453.88 Q1413.35 1452.37 1410.34 1452.37 Q1407.31 1452.37 1405.62 1453.88 Q1403.95 1455.38 1403.95 1458.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1588.49 1483.13 L1588.49 1478.88 Q1590.25 1479.71 1592.06 1480.15 Q1593.86 1480.59 1595.6 1480.59 Q1600.23 1480.59 1602.66 1477.49 Q1605.11 1474.36 1605.46 1468.02 Q1604.12 1470.01 1602.06 1471.07 Q1600 1472.14 1597.5 1472.14 Q1592.31 1472.14 1589.28 1469.01 Q1586.27 1465.87 1586.27 1460.43 Q1586.27 1455.1 1589.42 1451.88 Q1592.57 1448.67 1597.8 1448.67 Q1603.79 1448.67 1606.94 1453.27 Q1610.11 1457.86 1610.11 1466.61 Q1610.11 1474.78 1606.22 1479.66 Q1602.36 1484.52 1595.81 1484.52 Q1594.05 1484.52 1592.24 1484.18 Q1590.44 1483.83 1588.49 1483.13 M1597.8 1468.48 Q1600.95 1468.48 1602.78 1466.33 Q1604.63 1464.18 1604.63 1460.43 Q1604.63 1456.7 1602.78 1454.55 Q1600.95 1452.37 1597.8 1452.37 Q1594.65 1452.37 1592.8 1454.55 Q1590.97 1456.7 1590.97 1460.43 Q1590.97 1464.18 1592.8 1466.33 Q1594.65 1468.48 1597.8 1468.48 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1762.92 1479.92 L1770.56 1479.92 L1770.56 1453.55 L1762.25 1455.22 L1762.25 1450.96 L1770.51 1449.29 L1775.19 1449.29 L1775.19 1479.92 L1782.83 1479.92 L1782.83 1483.85 L1762.92 1483.85 L1762.92 1479.92 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1797.9 1452.37 Q1794.29 1452.37 1792.46 1455.94 Q1790.65 1459.48 1790.65 1466.61 Q1790.65 1473.71 1792.46 1477.28 Q1794.29 1480.82 1797.9 1480.82 Q1801.53 1480.82 1803.34 1477.28 Q1805.17 1473.71 1805.17 1466.61 Q1805.17 1459.48 1803.34 1455.94 Q1801.53 1452.37 1797.9 1452.37 M1797.9 1448.67 Q1803.71 1448.67 1806.76 1453.27 Q1809.84 1457.86 1809.84 1466.61 Q1809.84 1475.33 1806.76 1479.94 Q1803.71 1484.52 1797.9 1484.52 Q1792.09 1484.52 1789.01 1479.94 Q1785.95 1475.33 1785.95 1466.61 Q1785.95 1457.86 1789.01 1453.27 Q1792.09 1448.67 1797.9 1448.67 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M749.192 1508.52 L757.849 1508.52 L778.92 1548.28 L778.92 1508.52 L785.158 1508.52 L785.158 1556.04 L776.501 1556.04 L755.43 1516.29 L755.43 1556.04 L749.192 1556.04 L749.192 1508.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M790.696 1541.98 L790.696 1520.4 L796.553 1520.4 L796.553 1541.75 Q796.553 1546.81 798.526 1549.36 Q800.5 1551.87 804.446 1551.87 Q809.189 1551.87 811.926 1548.85 Q814.695 1545.83 814.695 1540.61 L814.695 1520.4 L820.552 1520.4 L820.552 1556.04 L814.695 1556.04 L814.695 1550.57 Q812.563 1553.82 809.73 1555.41 Q806.929 1556.97 803.205 1556.97 Q797.062 1556.97 793.879 1553.15 Q790.696 1549.33 790.696 1541.98 M805.433 1519.54 L805.433 1519.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M854.449 1527.24 Q856.645 1523.29 859.701 1521.41 Q862.756 1519.54 866.894 1519.54 Q872.464 1519.54 875.488 1523.45 Q878.511 1527.33 878.511 1534.53 L878.511 1556.04 L872.623 1556.04 L872.623 1534.72 Q872.623 1529.59 870.809 1527.11 Q868.995 1524.63 865.271 1524.63 Q860.719 1524.63 858.077 1527.65 Q855.436 1530.68 855.436 1535.9 L855.436 1556.04 L849.547 1556.04 L849.547 1534.72 Q849.547 1529.56 847.733 1527.11 Q845.919 1524.63 842.131 1524.63 Q837.644 1524.63 835.002 1527.68 Q832.36 1530.71 832.36 1535.9 L832.36 1556.04 L826.472 1556.04 L826.472 1520.4 L832.36 1520.4 L832.36 1525.93 Q834.365 1522.66 837.166 1521.1 Q839.967 1519.54 843.818 1519.54 Q847.701 1519.54 850.407 1521.51 Q853.144 1523.48 854.449 1527.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M910.244 1538.25 Q910.244 1531.79 907.571 1528.13 Q904.929 1524.44 900.282 1524.44 Q895.635 1524.44 892.962 1528.13 Q890.32 1531.79 890.32 1538.25 Q890.32 1544.71 892.962 1548.4 Q895.635 1552.07 900.282 1552.07 Q904.929 1552.07 907.571 1548.4 Q910.244 1544.71 910.244 1538.25 M890.32 1525.81 Q892.166 1522.62 894.967 1521.1 Q897.799 1519.54 901.714 1519.54 Q908.207 1519.54 912.25 1524.69 Q916.324 1529.85 916.324 1538.25 Q916.324 1546.65 912.25 1551.81 Q908.207 1556.97 901.714 1556.97 Q897.799 1556.97 894.967 1555.44 Q892.166 1553.88 890.32 1550.7 L890.32 1556.04 L884.431 1556.04 L884.431 1506.52 L890.32 1506.52 L890.32 1525.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M952.958 1536.76 L952.958 1539.62 L926.031 1539.62 Q926.413 1545.67 929.66 1548.85 Q932.938 1552 938.763 1552 Q942.137 1552 945.288 1551.17 Q948.47 1550.35 951.59 1548.69 L951.59 1554.23 Q948.439 1555.57 945.128 1556.27 Q941.818 1556.97 938.413 1556.97 Q929.883 1556.97 924.886 1552 Q919.92 1547.04 919.92 1538.57 Q919.92 1529.82 924.631 1524.69 Q929.373 1519.54 937.394 1519.54 Q944.587 1519.54 948.757 1524.18 Q952.958 1528.8 952.958 1536.76 M947.102 1535.04 Q947.038 1530.23 944.396 1527.37 Q941.786 1524.5 937.458 1524.5 Q932.556 1524.5 929.596 1527.27 Q926.668 1530.04 926.222 1535.07 L947.102 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M979.758 1525.87 Q978.771 1525.3 977.594 1525.04 Q976.448 1524.76 975.047 1524.76 Q970.082 1524.76 967.408 1528 Q964.767 1531.22 964.767 1537.27 L964.767 1556.04 L958.878 1556.04 L958.878 1520.4 L964.767 1520.4 L964.767 1525.93 Q966.613 1522.69 969.573 1521.13 Q972.533 1519.54 976.766 1519.54 Q977.371 1519.54 978.103 1519.63 Q978.835 1519.7 979.726 1519.85 L979.758 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1020.43 1524.5 Q1015.72 1524.5 1012.99 1528.19 Q1010.25 1531.85 1010.25 1538.25 Q1010.25 1544.65 1012.96 1548.34 Q1015.69 1552 1020.43 1552 Q1025.11 1552 1027.85 1548.31 Q1030.59 1544.62 1030.59 1538.25 Q1030.59 1531.92 1027.85 1528.23 Q1025.11 1524.5 1020.43 1524.5 M1020.43 1519.54 Q1028.07 1519.54 1032.43 1524.5 Q1036.79 1529.47 1036.79 1538.25 Q1036.79 1547 1032.43 1552 Q1028.07 1556.97 1020.43 1556.97 Q1012.76 1556.97 1008.4 1552 Q1004.07 1547 1004.07 1538.25 Q1004.07 1529.47 1008.4 1524.5 Q1012.76 1519.54 1020.43 1519.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1060.98 1506.52 L1060.98 1511.39 L1055.38 1511.39 Q1052.23 1511.39 1050.99 1512.66 Q1049.78 1513.93 1049.78 1517.24 L1049.78 1520.4 L1059.42 1520.4 L1059.42 1524.95 L1049.78 1524.95 L1049.78 1556.04 L1043.89 1556.04 L1043.89 1524.95 L1038.29 1524.95 L1038.29 1520.4 L1043.89 1520.4 L1043.89 1517.91 Q1043.89 1511.96 1046.66 1509.26 Q1049.43 1506.52 1055.45 1506.52 L1060.98 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1123.69 1512.18 L1123.69 1518.96 Q1120.44 1515.94 1116.75 1514.44 Q1113.09 1512.95 1108.95 1512.95 Q1100.8 1512.95 1096.47 1517.95 Q1092.14 1522.91 1092.14 1532.33 Q1092.14 1541.72 1096.47 1546.72 Q1100.8 1551.68 1108.95 1551.68 Q1113.09 1551.68 1116.75 1550.19 Q1120.44 1548.69 1123.69 1545.67 L1123.69 1552.38 Q1120.31 1554.68 1116.53 1555.82 Q1112.77 1556.97 1108.57 1556.97 Q1097.78 1556.97 1091.57 1550.38 Q1085.36 1543.76 1085.36 1532.33 Q1085.36 1520.87 1091.57 1514.28 Q1097.78 1507.66 1108.57 1507.66 Q1112.83 1507.66 1116.59 1508.81 Q1120.38 1509.92 1123.69 1512.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1129.83 1506.52 L1135.69 1506.52 L1135.69 1556.04 L1129.83 1556.04 L1129.83 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1141.22 1541.98 L1141.22 1520.4 L1147.08 1520.4 L1147.08 1541.75 Q1147.08 1546.81 1149.05 1549.36 Q1151.03 1551.87 1154.97 1551.87 Q1159.72 1551.87 1162.45 1548.85 Q1165.22 1545.83 1165.22 1540.61 L1165.22 1520.4 L1171.08 1520.4 L1171.08 1556.04 L1165.22 1556.04 L1165.22 1550.57 Q1163.09 1553.82 1160.26 1555.41 Q1157.46 1556.97 1153.73 1556.97 Q1147.59 1556.97 1144.41 1553.15 Q1141.22 1549.33 1141.22 1541.98 M1155.96 1519.54 L1155.96 1519.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1199.95 1521.45 L1199.95 1526.98 Q1197.47 1525.71 1194.79 1525.07 Q1192.12 1524.44 1189.25 1524.44 Q1184.89 1524.44 1182.7 1525.77 Q1180.53 1527.11 1180.53 1529.79 Q1180.53 1531.82 1182.09 1533 Q1183.65 1534.15 1188.36 1535.2 L1190.37 1535.64 Q1196.61 1536.98 1199.22 1539.43 Q1201.86 1541.85 1201.86 1546.21 Q1201.86 1551.17 1197.91 1554.07 Q1194 1556.97 1187.12 1556.97 Q1184.26 1556.97 1181.14 1556.39 Q1178.05 1555.85 1174.61 1554.74 L1174.61 1548.69 Q1177.86 1550.38 1181.01 1551.24 Q1184.16 1552.07 1187.25 1552.07 Q1191.39 1552.07 1193.61 1550.66 Q1195.84 1549.23 1195.84 1546.65 Q1195.84 1544.27 1194.22 1542.99 Q1192.63 1541.72 1187.18 1540.54 L1185.15 1540.07 Q1179.7 1538.92 1177.29 1536.56 Q1174.87 1534.18 1174.87 1530.04 Q1174.87 1525.01 1178.43 1522.27 Q1182 1519.54 1188.55 1519.54 Q1191.8 1519.54 1194.66 1520.01 Q1197.53 1520.49 1199.95 1521.45 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1213.79 1510.27 L1213.79 1520.4 L1225.86 1520.4 L1225.86 1524.95 L1213.79 1524.95 L1213.79 1544.3 Q1213.79 1548.66 1214.97 1549.9 Q1216.18 1551.14 1219.84 1551.14 L1225.86 1551.14 L1225.86 1556.04 L1219.84 1556.04 Q1213.06 1556.04 1210.48 1553.53 Q1207.9 1550.98 1207.9 1544.3 L1207.9 1524.95 L1203.61 1524.95 L1203.61 1520.4 L1207.9 1520.4 L1207.9 1510.27 L1213.79 1510.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1262.49 1536.76 L1262.49 1539.62 L1235.56 1539.62 Q1235.95 1545.67 1239.19 1548.85 Q1242.47 1552 1248.3 1552 Q1251.67 1552 1254.82 1551.17 Q1258 1550.35 1261.12 1548.69 L1261.12 1554.23 Q1257.97 1555.57 1254.66 1556.27 Q1251.35 1556.97 1247.95 1556.97 Q1239.42 1556.97 1234.42 1552 Q1229.45 1547.04 1229.45 1538.57 Q1229.45 1529.82 1234.16 1524.69 Q1238.91 1519.54 1246.93 1519.54 Q1254.12 1519.54 1258.29 1524.18 Q1262.49 1528.8 1262.49 1536.76 M1256.63 1535.04 Q1256.57 1530.23 1253.93 1527.37 Q1251.32 1524.5 1246.99 1524.5 Q1242.09 1524.5 1239.13 1527.27 Q1236.2 1530.04 1235.75 1535.07 L1256.63 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1289.29 1525.87 Q1288.3 1525.3 1287.13 1525.04 Q1285.98 1524.76 1284.58 1524.76 Q1279.61 1524.76 1276.94 1528 Q1274.3 1531.22 1274.3 1537.27 L1274.3 1556.04 L1268.41 1556.04 L1268.41 1520.4 L1274.3 1520.4 L1274.3 1525.93 Q1276.15 1522.69 1279.11 1521.13 Q1282.07 1519.54 1286.3 1519.54 Q1286.9 1519.54 1287.64 1519.63 Q1288.37 1519.7 1289.26 1519.85 L1289.29 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1318.16 1521.45 L1318.16 1526.98 Q1315.68 1525.71 1313 1525.07 Q1310.33 1524.44 1307.46 1524.44 Q1303.1 1524.44 1300.91 1525.77 Q1298.74 1527.11 1298.74 1529.79 Q1298.74 1531.82 1300.3 1533 Q1301.86 1534.15 1306.57 1535.2 L1308.58 1535.64 Q1314.82 1536.98 1317.43 1539.43 Q1320.07 1541.85 1320.07 1546.21 Q1320.07 1551.17 1316.12 1554.07 Q1312.21 1556.97 1305.33 1556.97 Q1302.47 1556.97 1299.35 1556.39 Q1296.26 1555.85 1292.82 1554.74 L1292.82 1548.69 Q1296.07 1550.38 1299.22 1551.24 Q1302.37 1552.07 1305.46 1552.07 Q1309.6 1552.07 1311.82 1550.66 Q1314.05 1549.23 1314.05 1546.65 Q1314.05 1544.27 1312.43 1542.99 Q1310.84 1541.72 1305.4 1540.54 L1303.36 1540.07 Q1297.92 1538.92 1295.5 1536.56 Q1293.08 1534.18 1293.08 1530.04 Q1293.08 1525.01 1296.64 1522.27 Q1300.21 1519.54 1306.76 1519.54 Q1310.01 1519.54 1312.88 1520.01 Q1315.74 1520.49 1318.16 1521.45 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  238.13,1384.24 1831.13,1384.24 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  238.13,1059.73 1831.13,1059.73 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  238.13,735.212 1831.13,735.212 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  238.13,410.699 1831.13,410.699 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  238.13,86.1857 1831.13,86.1857 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  238.13,1423.18 238.13,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  238.13,1384.24 257.246,1384.24 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  238.13,1059.73 257.246,1059.73 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  238.13,735.212 257.246,735.212 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  238.13,410.699 257.246,410.699 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  238.13,86.1857 257.246,86.1857 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M126.205 1370.04 Q122.593 1370.04 120.765 1373.6 Q118.959 1377.14 118.959 1384.27 Q118.959 1391.38 120.765 1394.94 Q122.593 1398.49 126.205 1398.49 Q129.839 1398.49 131.644 1394.94 Q133.473 1391.38 133.473 1384.27 Q133.473 1377.14 131.644 1373.6 Q129.839 1370.04 126.205 1370.04 M126.205 1366.33 Q132.015 1366.33 135.07 1370.94 Q138.149 1375.52 138.149 1384.27 Q138.149 1393 135.07 1397.61 Q132.015 1402.19 126.205 1402.19 Q120.394 1402.19 117.316 1397.61 Q114.26 1393 114.26 1384.27 Q114.26 1375.52 117.316 1370.94 Q120.394 1366.33 126.205 1366.33 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M143.218 1395.64 L148.103 1395.64 L148.103 1401.52 L143.218 1401.52 L143.218 1395.64 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M163.172 1370.04 Q159.561 1370.04 157.732 1373.6 Q155.927 1377.14 155.927 1384.27 Q155.927 1391.38 157.732 1394.94 Q159.561 1398.49 163.172 1398.49 Q166.806 1398.49 168.612 1394.94 Q170.44 1391.38 170.44 1384.27 Q170.44 1377.14 168.612 1373.6 Q166.806 1370.04 163.172 1370.04 M163.172 1366.33 Q168.982 1366.33 172.038 1370.94 Q175.116 1375.52 175.116 1384.27 Q175.116 1393 172.038 1397.61 Q168.982 1402.19 163.172 1402.19 Q157.362 1402.19 154.283 1397.61 Q151.228 1393 151.228 1384.27 Q151.228 1375.52 154.283 1370.94 Q157.362 1366.33 163.172 1366.33 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M190.186 1370.04 Q186.575 1370.04 184.746 1373.6 Q182.94 1377.14 182.94 1384.27 Q182.94 1391.38 184.746 1394.94 Q186.575 1398.49 190.186 1398.49 Q193.82 1398.49 195.625 1394.94 Q197.454 1391.38 197.454 1384.27 Q197.454 1377.14 195.625 1373.6 Q193.82 1370.04 190.186 1370.04 M190.186 1366.33 Q195.996 1366.33 199.051 1370.94 Q202.13 1375.52 202.13 1384.27 Q202.13 1393 199.051 1397.61 Q195.996 1402.19 190.186 1402.19 Q184.376 1402.19 181.297 1397.61 Q178.241 1393 178.241 1384.27 Q178.241 1375.52 181.297 1370.94 Q184.376 1366.33 190.186 1366.33 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M128.797 1045.52 Q125.186 1045.52 123.357 1049.09 Q121.552 1052.63 121.552 1059.76 Q121.552 1066.87 123.357 1070.43 Q125.186 1073.97 128.797 1073.97 Q132.431 1073.97 134.237 1070.43 Q136.066 1066.87 136.066 1059.76 Q136.066 1052.63 134.237 1049.09 Q132.431 1045.52 128.797 1045.52 M128.797 1041.82 Q134.607 1041.82 137.663 1046.43 Q140.741 1051.01 140.741 1059.76 Q140.741 1068.49 137.663 1073.09 Q134.607 1077.68 128.797 1077.68 Q122.987 1077.68 119.908 1073.09 Q116.853 1068.49 116.853 1059.76 Q116.853 1051.01 119.908 1046.43 Q122.987 1041.82 128.797 1041.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M145.811 1071.13 L150.695 1071.13 L150.695 1077.01 L145.811 1077.01 L145.811 1071.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M159.792 1073.07 L176.112 1073.07 L176.112 1077.01 L154.167 1077.01 L154.167 1073.07 Q156.829 1070.32 161.413 1065.69 Q166.019 1061.03 167.2 1059.69 Q169.445 1057.17 170.325 1055.43 Q171.227 1053.67 171.227 1051.98 Q171.227 1049.23 169.283 1047.49 Q167.362 1045.76 164.26 1045.76 Q162.061 1045.76 159.607 1046.52 Q157.177 1047.28 154.399 1048.83 L154.399 1044.11 Q157.223 1042.98 159.677 1042.4 Q162.13 1041.82 164.167 1041.82 Q169.538 1041.82 172.732 1044.51 Q175.926 1047.19 175.926 1051.68 Q175.926 1053.81 175.116 1055.73 Q174.329 1057.63 172.223 1060.22 Q171.644 1060.89 168.542 1064.11 Q165.44 1067.31 159.792 1073.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M181.227 1042.45 L199.584 1042.45 L199.584 1046.38 L185.51 1046.38 L185.51 1054.85 Q186.528 1054.51 187.547 1054.34 Q188.565 1054.16 189.584 1054.16 Q195.371 1054.16 198.75 1057.33 Q202.13 1060.5 202.13 1065.92 Q202.13 1071.5 198.658 1074.6 Q195.186 1077.68 188.866 1077.68 Q186.69 1077.68 184.422 1077.31 Q182.176 1076.94 179.769 1076.19 L179.769 1071.5 Q181.852 1072.63 184.075 1073.19 Q186.297 1073.74 188.774 1073.74 Q192.778 1073.74 195.116 1071.63 Q197.454 1069.53 197.454 1065.92 Q197.454 1062.31 195.116 1060.2 Q192.778 1058.09 188.774 1058.09 Q186.899 1058.09 185.024 1058.51 Q183.172 1058.93 181.227 1059.81 L181.227 1042.45 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M127.2 721.011 Q123.589 721.011 121.76 724.575 Q119.955 728.117 119.955 735.247 Q119.955 742.353 121.76 745.918 Q123.589 749.46 127.2 749.46 Q130.834 749.46 132.64 745.918 Q134.468 742.353 134.468 735.247 Q134.468 728.117 132.64 724.575 Q130.834 721.011 127.2 721.011 M127.2 717.307 Q133.01 717.307 136.066 721.913 Q139.144 726.497 139.144 735.247 Q139.144 743.973 136.066 748.58 Q133.01 753.163 127.2 753.163 Q121.39 753.163 118.311 748.58 Q115.256 743.973 115.256 735.247 Q115.256 726.497 118.311 721.913 Q121.39 717.307 127.2 717.307 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M144.214 746.612 L149.098 746.612 L149.098 752.492 L144.214 752.492 L144.214 746.612 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M154.214 717.932 L172.57 717.932 L172.57 721.867 L158.496 721.867 L158.496 730.339 Q159.515 729.992 160.533 729.83 Q161.552 729.645 162.57 729.645 Q168.357 729.645 171.737 732.816 Q175.116 735.987 175.116 741.404 Q175.116 746.983 171.644 750.085 Q168.172 753.163 161.852 753.163 Q159.677 753.163 157.408 752.793 Q155.163 752.423 152.755 751.682 L152.755 746.983 Q154.839 748.117 157.061 748.673 Q159.283 749.228 161.76 749.228 Q165.765 749.228 168.102 747.122 Q170.44 745.015 170.44 741.404 Q170.44 737.793 168.102 735.687 Q165.765 733.58 161.76 733.58 Q159.885 733.58 158.01 733.997 Q156.158 734.413 154.214 735.293 L154.214 717.932 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M190.186 721.011 Q186.575 721.011 184.746 724.575 Q182.94 728.117 182.94 735.247 Q182.94 742.353 184.746 745.918 Q186.575 749.46 190.186 749.46 Q193.82 749.46 195.625 745.918 Q197.454 742.353 197.454 735.247 Q197.454 728.117 195.625 724.575 Q193.82 721.011 190.186 721.011 M190.186 717.307 Q195.996 717.307 199.051 721.913 Q202.13 726.497 202.13 735.247 Q202.13 743.973 199.051 748.58 Q195.996 753.163 190.186 753.163 Q184.376 753.163 181.297 748.58 Q178.241 743.973 178.241 735.247 Q178.241 726.497 181.297 721.913 Q184.376 717.307 190.186 717.307 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M128.103 396.498 Q124.492 396.498 122.663 400.062 Q120.857 403.604 120.857 410.734 Q120.857 417.84 122.663 421.405 Q124.492 424.946 128.103 424.946 Q131.737 424.946 133.542 421.405 Q135.371 417.84 135.371 410.734 Q135.371 403.604 133.542 400.062 Q131.737 396.498 128.103 396.498 M128.103 392.794 Q133.913 392.794 136.968 397.4 Q140.047 401.984 140.047 410.734 Q140.047 419.46 136.968 424.067 Q133.913 428.65 128.103 428.65 Q122.293 428.65 119.214 424.067 Q116.158 419.46 116.158 410.734 Q116.158 401.984 119.214 397.4 Q122.293 392.794 128.103 392.794 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M145.116 422.099 L150.001 422.099 L150.001 427.979 L145.116 427.979 L145.116 422.099 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M153.89 393.419 L176.112 393.419 L176.112 395.41 L163.565 427.979 L158.681 427.979 L170.487 397.354 L153.89 397.354 L153.89 393.419 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M181.227 393.419 L199.584 393.419 L199.584 397.354 L185.51 397.354 L185.51 405.826 Q186.528 405.479 187.547 405.317 Q188.565 405.132 189.584 405.132 Q195.371 405.132 198.75 408.303 Q202.13 411.474 202.13 416.891 Q202.13 422.47 198.658 425.571 Q195.186 428.65 188.866 428.65 Q186.69 428.65 184.422 428.28 Q182.176 427.909 179.769 427.169 L179.769 422.47 Q181.852 423.604 184.075 424.159 Q186.297 424.715 188.774 424.715 Q192.778 424.715 195.116 422.608 Q197.454 420.502 197.454 416.891 Q197.454 413.28 195.116 411.173 Q192.778 409.067 188.774 409.067 Q186.899 409.067 185.024 409.484 Q183.172 409.9 181.227 410.78 L181.227 393.419 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M118.242 99.5305 L125.88 99.5305 L125.88 73.1649 L117.57 74.8316 L117.57 70.5723 L125.834 68.9057 L130.51 68.9057 L130.51 99.5305 L138.149 99.5305 L138.149 103.466 L118.242 103.466 L118.242 99.5305 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M143.218 97.5861 L148.103 97.5861 L148.103 103.466 L143.218 103.466 L143.218 97.5861 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M163.172 71.9844 Q159.561 71.9844 157.732 75.5492 Q155.927 79.0908 155.927 86.2204 Q155.927 93.3268 157.732 96.8916 Q159.561 100.433 163.172 100.433 Q166.806 100.433 168.612 96.8916 Q170.44 93.3268 170.44 86.2204 Q170.44 79.0908 168.612 75.5492 Q166.806 71.9844 163.172 71.9844 M163.172 68.2807 Q168.982 68.2807 172.038 72.8871 Q175.116 77.4704 175.116 86.2204 Q175.116 94.9472 172.038 99.5537 Q168.982 104.137 163.172 104.137 Q157.362 104.137 154.283 99.5537 Q151.228 94.9472 151.228 86.2204 Q151.228 77.4704 154.283 72.8871 Q157.362 68.2807 163.172 68.2807 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M190.186 71.9844 Q186.575 71.9844 184.746 75.5492 Q182.94 79.0908 182.94 86.2204 Q182.94 93.3268 184.746 96.8916 Q186.575 100.433 190.186 100.433 Q193.82 100.433 195.625 96.8916 Q197.454 93.3268 197.454 86.2204 Q197.454 79.0908 195.625 75.5492 Q193.82 71.9844 190.186 71.9844 M190.186 68.2807 Q195.996 68.2807 199.051 72.8871 Q202.13 77.4704 202.13 86.2204 Q202.13 94.9472 199.051 99.5537 Q195.996 104.137 190.186 104.137 Q184.376 104.137 181.297 99.5537 Q178.241 94.9472 178.241 86.2204 Q178.241 77.4704 181.297 72.8871 Q184.376 68.2807 190.186 68.2807 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M44.1444 962.531 L50.9239 962.531 Q47.9002 965.778 46.4043 969.47 Q44.9083 973.13 44.9083 977.268 Q44.9083 985.416 49.9054 989.745 Q54.8707 994.073 64.2919 994.073 Q73.6813 994.073 78.6784 989.745 Q83.6436 985.416 83.6436 977.268 Q83.6436 973.13 82.1477 969.47 Q80.6518 965.778 77.6281 962.531 L84.3439 962.531 Q86.6355 965.905 87.7814 969.693 Q88.9272 973.449 88.9272 977.65 Q88.9272 988.44 82.3387 994.646 Q75.7183 1000.85 64.2919 1000.85 Q52.8336 1000.85 46.2451 994.646 Q39.6248 988.44 39.6248 977.65 Q39.6248 973.385 40.7706 969.629 Q41.8846 965.842 44.1444 962.531 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M38.479 956.388 L38.479 950.532 L88.0042 950.532 L88.0042 956.388 L38.479 956.388 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M73.9359 944.994 L52.3562 944.994 L52.3562 939.137 L73.7131 939.137 Q78.7739 939.137 81.3202 937.164 Q83.8346 935.191 83.8346 931.244 Q83.8346 926.501 80.8109 923.764 Q77.7872 920.995 72.5673 920.995 L52.3562 920.995 L52.3562 915.139 L88.0042 915.139 L88.0042 920.995 L82.5296 920.995 Q85.7762 923.128 87.3676 925.96 Q88.9272 928.761 88.9272 932.485 Q88.9272 938.628 85.1078 941.811 Q81.2883 944.994 73.9359 944.994 M51.4968 930.257 L51.4968 930.257 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M53.4065 886.27 L58.9447 886.27 Q57.6716 888.753 57.035 891.426 Q56.3984 894.1 56.3984 896.965 Q56.3984 901.325 57.7352 903.521 Q59.072 905.686 61.7456 905.686 Q63.7826 905.686 64.9603 904.126 Q66.1061 902.566 67.1565 897.856 L67.6021 895.851 Q68.9389 889.612 71.3897 887.002 Q73.8086 884.36 78.1691 884.36 Q83.1344 884.36 86.0308 888.307 Q88.9272 892.222 88.9272 899.097 Q88.9272 901.962 88.3543 905.081 Q87.8132 908.168 86.6992 911.606 L80.6518 911.606 Q82.3387 908.359 83.198 905.208 Q84.0256 902.057 84.0256 898.97 Q84.0256 894.832 82.6251 892.604 Q81.1929 890.376 78.6147 890.376 Q76.2276 890.376 74.9545 891.999 Q73.6813 893.591 72.5037 899.033 L72.0262 901.07 Q70.8804 906.513 68.5251 908.932 Q66.138 911.351 62.0002 911.351 Q56.9713 911.351 54.2341 907.786 Q51.4968 904.221 51.4968 897.665 Q51.4968 894.418 51.9743 891.554 Q52.4517 888.689 53.4065 886.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M42.2347 872.425 L52.3562 872.425 L52.3562 860.362 L56.9077 860.362 L56.9077 872.425 L76.2594 872.425 Q80.6199 872.425 81.8613 871.247 Q83.1026 870.038 83.1026 866.377 L83.1026 860.362 L88.0042 860.362 L88.0042 866.377 Q88.0042 873.157 85.4897 875.735 Q82.9434 878.313 76.2594 878.313 L56.9077 878.313 L56.9077 882.61 L52.3562 882.61 L52.3562 878.313 L42.2347 878.313 L42.2347 872.425 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M68.7161 823.727 L71.5806 823.727 L71.5806 850.654 Q77.6281 850.272 80.8109 847.026 Q83.9619 843.747 83.9619 837.923 Q83.9619 834.549 83.1344 831.398 Q82.3069 828.215 80.6518 825.096 L86.1899 825.096 Q87.5267 828.247 88.227 831.557 Q88.9272 834.867 88.9272 838.273 Q88.9272 846.803 83.9619 851.8 Q78.9967 856.765 70.5303 856.765 Q61.7774 856.765 56.6531 852.055 Q51.4968 847.312 51.4968 839.291 Q51.4968 832.098 56.1438 827.928 Q60.7589 823.727 68.7161 823.727 M66.9973 829.584 Q62.1912 829.647 59.3266 832.289 Q56.4621 834.899 56.4621 839.228 Q56.4621 844.129 59.2312 847.089 Q62.0002 850.017 67.0292 850.463 L66.9973 829.584 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M57.8307 796.927 Q57.2578 797.914 57.0032 799.092 Q56.7167 800.238 56.7167 801.638 Q56.7167 806.603 59.9632 809.277 Q63.1779 811.919 69.2253 811.919 L88.0042 811.919 L88.0042 817.807 L52.3562 817.807 L52.3562 811.919 L57.8944 811.919 Q54.6479 810.073 53.0883 807.113 Q51.4968 804.153 51.4968 799.919 Q51.4968 799.315 51.5923 798.583 Q51.656 797.851 51.8151 796.959 L57.8307 796.927 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M45.7677 763.38 L63.6235 763.38 L63.6235 755.296 Q63.6235 750.808 61.3 748.357 Q58.9765 745.906 54.6797 745.906 Q50.4147 745.906 48.0912 748.357 Q45.7677 750.808 45.7677 755.296 L45.7677 763.38 M40.4842 769.81 L40.4842 755.296 Q40.4842 747.307 44.1126 743.233 Q47.7092 739.127 54.6797 739.127 Q61.7138 739.127 65.3104 743.233 Q68.907 747.307 68.907 755.296 L68.907 763.38 L88.0042 763.38 L88.0042 769.81 L40.4842 769.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M57.8307 713.473 Q57.2578 714.46 57.0032 715.637 Q56.7167 716.783 56.7167 718.184 Q56.7167 723.149 59.9632 725.823 Q63.1779 728.464 69.2253 728.464 L88.0042 728.464 L88.0042 734.353 L52.3562 734.353 L52.3562 728.464 L57.8944 728.464 Q54.6479 726.618 53.0883 723.658 Q51.4968 720.698 51.4968 716.465 Q51.4968 715.86 51.5923 715.128 Q51.656 714.396 51.8151 713.505 L57.8307 713.473 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M56.4621 694.949 Q56.4621 699.659 60.1542 702.397 Q63.8145 705.134 70.212 705.134 Q76.6095 705.134 80.3017 702.429 Q83.9619 699.691 83.9619 694.949 Q83.9619 690.27 80.2698 687.533 Q76.5777 684.796 70.212 684.796 Q63.8781 684.796 60.186 687.533 Q56.4621 690.27 56.4621 694.949 M51.4968 694.949 Q51.4968 687.31 56.4621 682.95 Q61.4273 678.589 70.212 678.589 Q78.9649 678.589 83.9619 682.95 Q88.9272 687.31 88.9272 694.949 Q88.9272 702.62 83.9619 706.98 Q78.9649 711.309 70.212 711.309 Q61.4273 711.309 56.4621 706.98 Q51.4968 702.62 51.4968 694.949 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M82.657 666.781 L101.563 666.781 L101.563 672.669 L52.3562 672.669 L52.3562 666.781 L57.7671 666.781 Q54.5842 664.935 53.0564 662.134 Q51.4968 659.301 51.4968 655.386 Q51.4968 648.893 56.6531 644.851 Q61.8093 640.777 70.212 640.777 Q78.6147 640.777 83.771 644.851 Q88.9272 648.893 88.9272 655.386 Q88.9272 659.301 87.3994 662.134 Q85.8398 664.935 82.657 666.781 M70.212 646.856 Q63.7508 646.856 60.0905 649.53 Q56.3984 652.171 56.3984 656.818 Q56.3984 661.465 60.0905 664.139 Q63.7508 666.781 70.212 666.781 Q76.6732 666.781 80.3653 664.139 Q84.0256 661.465 84.0256 656.818 Q84.0256 652.171 80.3653 649.53 Q76.6732 646.856 70.212 646.856 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M56.4621 620.82 Q56.4621 625.531 60.1542 628.268 Q63.8145 631.005 70.212 631.005 Q76.6095 631.005 80.3017 628.3 Q83.9619 625.563 83.9619 620.82 Q83.9619 616.141 80.2698 613.404 Q76.5777 610.667 70.212 610.667 Q63.8781 610.667 60.186 613.404 Q56.4621 616.141 56.4621 620.82 M51.4968 620.82 Q51.4968 613.181 56.4621 608.821 Q61.4273 604.46 70.212 604.46 Q78.9649 604.46 83.9619 608.821 Q88.9272 613.181 88.9272 620.82 Q88.9272 628.491 83.9619 632.851 Q78.9649 637.18 70.212 637.18 Q61.4273 637.18 56.4621 632.851 Q51.4968 628.491 51.4968 620.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M57.8307 577.661 Q57.2578 578.647 57.0032 579.825 Q56.7167 580.971 56.7167 582.371 Q56.7167 587.337 59.9632 590.01 Q63.1779 592.652 69.2253 592.652 L88.0042 592.652 L88.0042 598.54 L52.3562 598.54 L52.3562 592.652 L57.8944 592.652 Q54.6479 590.806 53.0883 587.846 Q51.4968 584.886 51.4968 580.653 Q51.4968 580.048 51.5923 579.316 Q51.656 578.584 51.8151 577.693 L57.8307 577.661 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M42.2347 565.725 L52.3562 565.725 L52.3562 553.662 L56.9077 553.662 L56.9077 565.725 L76.2594 565.725 Q80.6199 565.725 81.8613 564.547 Q83.1026 563.338 83.1026 559.678 L83.1026 553.662 L88.0042 553.662 L88.0042 559.678 Q88.0042 566.457 85.4897 569.035 Q82.9434 571.613 76.2594 571.613 L56.9077 571.613 L56.9077 575.91 L52.3562 575.91 L52.3562 571.613 L42.2347 571.613 L42.2347 565.725 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M52.3562 547.519 L52.3562 541.663 L88.0042 541.663 L88.0042 547.519 L52.3562 547.519 M38.479 547.519 L38.479 541.663 L45.895 541.663 L45.895 547.519 L38.479 547.519 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M56.4621 521.706 Q56.4621 526.417 60.1542 529.154 Q63.8145 531.891 70.212 531.891 Q76.6095 531.891 80.3017 529.186 Q83.9619 526.449 83.9619 521.706 Q83.9619 517.027 80.2698 514.29 Q76.5777 511.553 70.212 511.553 Q63.8781 511.553 60.186 514.29 Q56.4621 517.027 56.4621 521.706 M51.4968 521.706 Q51.4968 514.067 56.4621 509.707 Q61.4273 505.346 70.212 505.346 Q78.9649 505.346 83.9619 509.707 Q88.9272 514.067 88.9272 521.706 Q88.9272 529.377 83.9619 533.737 Q78.9649 538.066 70.212 538.066 Q61.4273 538.066 56.4621 533.737 Q51.4968 529.377 51.4968 521.706 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M66.4881 469.571 L88.0042 469.571 L88.0042 475.427 L66.679 475.427 Q61.6183 475.427 59.1038 477.401 Q56.5894 479.374 56.5894 483.321 Q56.5894 488.063 59.6131 490.801 Q62.6368 493.538 67.8567 493.538 L88.0042 493.538 L88.0042 499.426 L52.3562 499.426 L52.3562 493.538 L57.8944 493.538 Q54.6797 491.437 53.0883 488.604 Q51.4968 485.74 51.4968 482.016 Q51.4968 475.873 55.3163 472.722 Q59.1038 469.571 66.4881 469.571 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip082)\" d=\"\n",
       "M283.215 86.1857 L471.069 86.1857 L658.923 86.1857 L846.776 86.1857 L1034.63 86.1857 L1222.48 86.1857 L1410.34 86.1857 L1598.19 86.1857 L1786.05 86.1857 L1786.05 1384.24 \n",
       "  L1598.19 1384.24 L1410.34 1384.24 L1222.48 1384.24 L1034.63 1384.24 L846.776 1384.24 L658.923 1384.24 L471.069 1384.24 L283.215 1384.24  Z\n",
       "  \" fill=\"#7ec87e\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#7ec87e; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.215,86.1857 471.069,86.1857 658.923,86.1857 846.776,86.1857 1034.63,86.1857 1222.48,86.1857 1410.34,86.1857 1598.19,86.1857 1786.05,86.1857 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip082)\" d=\"\n",
       "M283.215 86.1857 L471.069 86.1857 L658.923 86.1857 L846.776 86.1857 L1034.63 86.1857 L1222.48 86.1857 L1410.34 86.1857 L1598.19 86.1857 L1786.05 211.048 L1786.05 1384.24 \n",
       "  L1598.19 1384.24 L1410.34 1384.24 L1222.48 1384.24 L1034.63 1384.24 L846.776 1384.24 L658.923 1384.24 L471.069 1384.24 L283.215 1384.24  Z\n",
       "  \" fill=\"#bdadd3\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#bdadd3; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.215,86.1857 471.069,86.1857 658.923,86.1857 846.776,86.1857 1034.63,86.1857 1222.48,86.1857 1410.34,86.1857 1598.19,86.1857 1786.05,211.048 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip082)\" d=\"\n",
       "M283.215 86.1857 L471.069 86.1857 L658.923 86.1857 L846.776 86.1857 L1034.63 86.1857 L1222.48 86.1857 L1410.34 86.1857 L1598.19 247.467 L1786.05 364.526 L1786.05 1384.24 \n",
       "  L1598.19 1384.24 L1410.34 1384.24 L1222.48 1384.24 L1034.63 1384.24 L846.776 1384.24 L658.923 1384.24 L471.069 1384.24 L283.215 1384.24  Z\n",
       "  \" fill=\"#fcc085\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#fcc085; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.215,86.1857 471.069,86.1857 658.923,86.1857 846.776,86.1857 1034.63,86.1857 1222.48,86.1857 1410.34,86.1857 1598.19,247.467 1786.05,364.526 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip082)\" d=\"\n",
       "M283.215 86.1857 L471.069 86.1857 L658.923 86.1857 L846.776 86.1857 L1034.63 86.1857 L1222.48 86.1857 L1410.34 255.271 L1598.19 369.728 L1786.05 468.578 L1786.05 1384.24 \n",
       "  L1598.19 1384.24 L1410.34 1384.24 L1222.48 1384.24 L1034.63 1384.24 L846.776 1384.24 L658.923 1384.24 L471.069 1384.24 L283.215 1384.24  Z\n",
       "  \" fill=\"#ffff99\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#ffff99; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.215,86.1857 471.069,86.1857 658.923,86.1857 846.776,86.1857 1034.63,86.1857 1222.48,86.1857 1410.34,255.271 1598.19,369.728 1786.05,468.578 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip082)\" d=\"\n",
       "M283.215 86.1857 L471.069 86.1857 L658.923 86.1857 L846.776 86.1857 L1034.63 86.1857 L1222.48 265.676 L1410.34 424.356 L1598.19 531.009 L1786.05 572.63 L1786.05 1384.24 \n",
       "  L1598.19 1384.24 L1410.34 1384.24 L1222.48 1384.24 L1034.63 1384.24 L846.776 1384.24 L658.923 1384.24 L471.069 1384.24 L283.215 1384.24  Z\n",
       "  \" fill=\"#386caf\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#386caf; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.215,86.1857 471.069,86.1857 658.923,86.1857 846.776,86.1857 1034.63,86.1857 1222.48,265.676 1410.34,424.356 1598.19,531.009 1786.05,572.63 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip082)\" d=\"\n",
       "M283.215 86.1857 L471.069 86.1857 L658.923 86.1857 L846.776 86.1857 L1034.63 273.48 L1222.48 426.957 L1410.34 583.035 L1598.19 684.486 L1786.05 700.094 L1786.05 1384.24 \n",
       "  L1598.19 1384.24 L1410.34 1384.24 L1222.48 1384.24 L1034.63 1384.24 L846.776 1384.24 L658.923 1384.24 L471.069 1384.24 L283.215 1384.24  Z\n",
       "  \" fill=\"#ef027e\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#ef027e; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.215,86.1857 471.069,86.1857 658.923,86.1857 846.776,86.1857 1034.63,273.48 1222.48,426.957 1410.34,583.035 1598.19,684.486 1786.05,700.094 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip082)\" d=\"\n",
       "M283.215 86.1857 L471.069 86.1857 L658.923 86.1857 L846.776 304.696 L1034.63 494.591 L1222.48 645.467 L1410.34 723.506 L1598.19 785.937 L1786.05 837.964 L1786.05 1384.24 \n",
       "  L1598.19 1384.24 L1410.34 1384.24 L1222.48 1384.24 L1034.63 1384.24 L846.776 1384.24 L658.923 1384.24 L471.069 1384.24 L283.215 1384.24  Z\n",
       "  \" fill=\"#be5b16\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#be5b16; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.215,86.1857 471.069,86.1857 658.923,86.1857 846.776,304.696 1034.63,494.591 1222.48,645.467 1410.34,723.506 1598.19,785.937 1786.05,837.964 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip082)\" d=\"\n",
       "M283.215 86.1857 L471.069 86.1857 L658.923 429.558 L846.776 590.839 L1034.63 684.486 L1222.48 827.558 L1410.34 887.388 L1598.19 921.205 L1786.05 960.225 L1786.05 1384.24 \n",
       "  L1598.19 1384.24 L1410.34 1384.24 L1222.48 1384.24 L1034.63 1384.24 L846.776 1384.24 L658.923 1384.24 L471.069 1384.24 L283.215 1384.24  Z\n",
       "  \" fill=\"#666666\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#666666; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.215,86.1857 471.069,86.1857 658.923,429.558 846.776,590.839 1034.63,684.486 1222.48,827.558 1410.34,887.388 1598.19,921.205 1786.05,960.225 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip082)\" d=\"\n",
       "M283.215 86.1857 L471.069 486.787 L658.923 723.506 L846.776 830.16 L1034.63 931.611 L1222.48 1030.46 L1410.34 1046.07 L1598.19 1048.67 L1786.05 1095.49 L1786.05 1384.24 \n",
       "  L1598.19 1384.24 L1410.34 1384.24 L1222.48 1384.24 L1034.63 1384.24 L846.776 1384.24 L658.923 1384.24 L471.069 1384.24 L283.215 1384.24  Z\n",
       "  \" fill=\"#7ec87e\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#7ec87e; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.215,86.1857 471.069,486.787 658.923,723.506 846.776,830.16 1034.63,931.611 1222.48,1030.46 1410.34,1046.07 1598.19,1048.67 1786.05,1095.49 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip082)\" d=\"\n",
       "M283.215 713.101 L471.069 892.591 L658.923 1074.68 L846.776 1100.7 L1034.63 1152.72 L1222.48 1199.55 L1410.34 1212.55 L1598.19 1207.35 L1786.05 1235.96 L1786.05 1384.24 \n",
       "  L1598.19 1384.24 L1410.34 1384.24 L1222.48 1384.24 L1034.63 1384.24 L846.776 1384.24 L658.923 1384.24 L471.069 1384.24 L283.215 1384.24  Z\n",
       "  \" fill=\"#bdadd3\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#bdadd3; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  283.215,713.101 471.069,892.591 658.923,1074.68 846.776,1100.7 1034.63,1152.72 1222.48,1199.55 1410.34,1212.55 1598.19,1207.35 1786.05,1235.96 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"\n",
       "M1901.62 758.389 L2352.76 758.389 L2352.76 93.1086 L1901.62 93.1086  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1901.62,758.389 2352.76,758.389 2352.76,93.1086 1901.62,93.1086 1901.62,758.389 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"\n",
       "M1925.11 177.781 L2066.09 177.781 L2066.09 129.397 L1925.11 129.397 L1925.11 177.781  Z\n",
       "  \" fill=\"#7ec87e\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#7ec87e; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1925.11,129.397 2066.09,129.397 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M2117.46 138.971 L2117.46 143.901 Q2115.09 141.702 2112.41 140.614 Q2109.75 139.526 2106.74 139.526 Q2100.81 139.526 2097.66 143.16 Q2094.52 146.772 2094.52 153.623 Q2094.52 160.452 2097.66 164.086 Q2100.81 167.697 2106.74 167.697 Q2109.75 167.697 2112.41 166.609 Q2115.09 165.521 2117.46 163.322 L2117.46 168.207 Q2115 169.873 2112.25 170.707 Q2109.52 171.54 2106.46 171.54 Q2098.61 171.54 2094.1 166.748 Q2089.58 161.933 2089.58 153.623 Q2089.58 145.29 2094.1 140.498 Q2098.61 135.684 2106.46 135.684 Q2109.56 135.684 2112.29 136.517 Q2115.05 137.327 2117.46 138.971 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2121.92 134.85 L2126.18 134.85 L2126.18 170.869 L2121.92 170.869 L2121.92 134.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2130.21 160.637 L2130.21 144.943 L2134.47 144.943 L2134.47 160.475 Q2134.47 164.156 2135.9 166.008 Q2137.34 167.836 2140.21 167.836 Q2143.66 167.836 2145.65 165.637 Q2147.66 163.438 2147.66 159.642 L2147.66 144.943 L2151.92 144.943 L2151.92 170.869 L2147.66 170.869 L2147.66 166.887 Q2146.11 169.248 2144.05 170.406 Q2142.02 171.54 2139.31 171.54 Q2134.84 171.54 2132.52 168.762 Q2130.21 165.984 2130.21 160.637 M2140.93 144.318 L2140.93 144.318 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2172.92 145.707 L2172.92 149.734 Q2171.11 148.809 2169.17 148.346 Q2167.22 147.883 2165.14 147.883 Q2161.97 147.883 2160.37 148.855 Q2158.8 149.827 2158.8 151.771 Q2158.8 153.253 2159.93 154.109 Q2161.07 154.943 2164.49 155.707 L2165.95 156.031 Q2170.49 157.003 2172.39 158.785 Q2174.31 160.545 2174.31 163.716 Q2174.31 167.327 2171.44 169.433 Q2168.59 171.54 2163.59 171.54 Q2161.51 171.54 2159.24 171.123 Q2156.99 170.73 2154.49 169.92 L2154.49 165.521 Q2156.85 166.748 2159.14 167.373 Q2161.44 167.975 2163.68 167.975 Q2166.69 167.975 2168.31 166.957 Q2169.93 165.915 2169.93 164.04 Q2169.93 162.304 2168.75 161.378 Q2167.59 160.452 2163.64 159.596 L2162.15 159.248 Q2158.2 158.415 2156.44 156.702 Q2154.68 154.966 2154.68 151.957 Q2154.68 148.299 2157.27 146.309 Q2159.86 144.318 2164.63 144.318 Q2166.99 144.318 2169.08 144.665 Q2171.16 145.012 2172.92 145.707 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2182.99 137.582 L2182.99 144.943 L2191.76 144.943 L2191.76 148.253 L2182.99 148.253 L2182.99 162.327 Q2182.99 165.498 2183.84 166.401 Q2184.72 167.304 2187.39 167.304 L2191.76 167.304 L2191.76 170.869 L2187.39 170.869 Q2182.45 170.869 2180.58 169.04 Q2178.7 167.188 2178.7 162.327 L2178.7 148.253 L2175.58 148.253 L2175.58 144.943 L2178.7 144.943 L2178.7 137.582 L2182.99 137.582 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2218.4 156.841 L2218.4 158.924 L2198.82 158.924 Q2199.1 163.322 2201.46 165.637 Q2203.84 167.929 2208.08 167.929 Q2210.53 167.929 2212.82 167.327 Q2215.14 166.725 2217.41 165.521 L2217.41 169.549 Q2215.12 170.521 2212.71 171.031 Q2210.3 171.54 2207.82 171.54 Q2201.62 171.54 2197.99 167.929 Q2194.38 164.318 2194.38 158.16 Q2194.38 151.795 2197.8 148.068 Q2201.25 144.318 2207.08 144.318 Q2212.32 144.318 2215.35 147.697 Q2218.4 151.054 2218.4 156.841 M2214.14 155.591 Q2214.1 152.096 2212.18 150.012 Q2210.28 147.929 2207.13 147.929 Q2203.57 147.929 2201.41 149.943 Q2199.28 151.957 2198.96 155.614 L2214.14 155.591 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2237.89 148.924 Q2237.18 148.508 2236.32 148.322 Q2235.49 148.114 2234.47 148.114 Q2230.86 148.114 2228.91 150.475 Q2226.99 152.813 2226.99 157.211 L2226.99 170.869 L2222.71 170.869 L2222.71 144.943 L2226.99 144.943 L2226.99 148.971 Q2228.33 146.609 2230.49 145.475 Q2232.64 144.318 2235.72 144.318 Q2236.16 144.318 2236.69 144.387 Q2237.22 144.434 2237.87 144.549 L2237.89 148.924 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2258.84 166.933 L2266.48 166.933 L2266.48 140.568 L2258.17 142.235 L2258.17 137.975 L2266.44 136.309 L2271.11 136.309 L2271.11 166.933 L2278.75 166.933 L2278.75 170.869 L2258.84 170.869 L2258.84 166.933 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2293.82 139.387 Q2290.21 139.387 2288.38 142.952 Q2286.57 146.494 2286.57 153.623 Q2286.57 160.73 2288.38 164.295 Q2290.21 167.836 2293.82 167.836 Q2297.45 167.836 2299.26 164.295 Q2301.09 160.73 2301.09 153.623 Q2301.09 146.494 2299.26 142.952 Q2297.45 139.387 2293.82 139.387 M2293.82 135.684 Q2299.63 135.684 2302.69 140.29 Q2305.76 144.873 2305.76 153.623 Q2305.76 162.35 2302.69 166.957 Q2299.63 171.54 2293.82 171.54 Q2288.01 171.54 2284.93 166.957 Q2281.88 162.35 2281.88 153.623 Q2281.88 144.873 2284.93 140.29 Q2288.01 135.684 2293.82 135.684 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"\n",
       "M1925.11 238.261 L2066.09 238.261 L2066.09 189.877 L1925.11 189.877 L1925.11 238.261  Z\n",
       "  \" fill=\"#bdadd3\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#bdadd3; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1925.11,189.877 2066.09,189.877 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M2117.46 199.451 L2117.46 204.381 Q2115.09 202.182 2112.41 201.094 Q2109.75 200.006 2106.74 200.006 Q2100.81 200.006 2097.66 203.64 Q2094.52 207.252 2094.52 214.103 Q2094.52 220.932 2097.66 224.566 Q2100.81 228.177 2106.74 228.177 Q2109.75 228.177 2112.41 227.089 Q2115.09 226.001 2117.46 223.802 L2117.46 228.687 Q2115 230.353 2112.25 231.187 Q2109.52 232.02 2106.46 232.02 Q2098.61 232.02 2094.1 227.228 Q2089.58 222.413 2089.58 214.103 Q2089.58 205.77 2094.1 200.978 Q2098.61 196.164 2106.46 196.164 Q2109.56 196.164 2112.29 196.997 Q2115.05 197.807 2117.46 199.451 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2121.92 195.33 L2126.18 195.33 L2126.18 231.349 L2121.92 231.349 L2121.92 195.33 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2130.21 221.117 L2130.21 205.423 L2134.47 205.423 L2134.47 220.955 Q2134.47 224.636 2135.9 226.488 Q2137.34 228.316 2140.21 228.316 Q2143.66 228.316 2145.65 226.117 Q2147.66 223.918 2147.66 220.122 L2147.66 205.423 L2151.92 205.423 L2151.92 231.349 L2147.66 231.349 L2147.66 227.367 Q2146.11 229.728 2144.05 230.886 Q2142.02 232.02 2139.31 232.02 Q2134.84 232.02 2132.52 229.242 Q2130.21 226.464 2130.21 221.117 M2140.93 204.798 L2140.93 204.798 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2172.92 206.187 L2172.92 210.214 Q2171.11 209.289 2169.17 208.826 Q2167.22 208.363 2165.14 208.363 Q2161.97 208.363 2160.37 209.335 Q2158.8 210.307 2158.8 212.251 Q2158.8 213.733 2159.93 214.589 Q2161.07 215.423 2164.49 216.187 L2165.95 216.511 Q2170.49 217.483 2172.39 219.265 Q2174.31 221.025 2174.31 224.196 Q2174.31 227.807 2171.44 229.913 Q2168.59 232.02 2163.59 232.02 Q2161.51 232.02 2159.24 231.603 Q2156.99 231.21 2154.49 230.4 L2154.49 226.001 Q2156.85 227.228 2159.14 227.853 Q2161.44 228.455 2163.68 228.455 Q2166.69 228.455 2168.31 227.437 Q2169.93 226.395 2169.93 224.52 Q2169.93 222.784 2168.75 221.858 Q2167.59 220.932 2163.64 220.076 L2162.15 219.728 Q2158.2 218.895 2156.44 217.182 Q2154.68 215.446 2154.68 212.437 Q2154.68 208.779 2157.27 206.789 Q2159.86 204.798 2164.63 204.798 Q2166.99 204.798 2169.08 205.145 Q2171.16 205.492 2172.92 206.187 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2182.99 198.062 L2182.99 205.423 L2191.76 205.423 L2191.76 208.733 L2182.99 208.733 L2182.99 222.807 Q2182.99 225.978 2183.84 226.881 Q2184.72 227.784 2187.39 227.784 L2191.76 227.784 L2191.76 231.349 L2187.39 231.349 Q2182.45 231.349 2180.58 229.52 Q2178.7 227.668 2178.7 222.807 L2178.7 208.733 L2175.58 208.733 L2175.58 205.423 L2178.7 205.423 L2178.7 198.062 L2182.99 198.062 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2218.4 217.321 L2218.4 219.404 L2198.82 219.404 Q2199.1 223.802 2201.46 226.117 Q2203.84 228.409 2208.08 228.409 Q2210.53 228.409 2212.82 227.807 Q2215.14 227.205 2217.41 226.001 L2217.41 230.029 Q2215.12 231.001 2212.71 231.511 Q2210.3 232.02 2207.82 232.02 Q2201.62 232.02 2197.99 228.409 Q2194.38 224.798 2194.38 218.64 Q2194.38 212.275 2197.8 208.548 Q2201.25 204.798 2207.08 204.798 Q2212.32 204.798 2215.35 208.177 Q2218.4 211.534 2218.4 217.321 M2214.14 216.071 Q2214.1 212.576 2212.18 210.492 Q2210.28 208.409 2207.13 208.409 Q2203.57 208.409 2201.41 210.423 Q2199.28 212.437 2198.96 216.094 L2214.14 216.071 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2237.89 209.404 Q2237.18 208.988 2236.32 208.802 Q2235.49 208.594 2234.47 208.594 Q2230.86 208.594 2228.91 210.955 Q2226.99 213.293 2226.99 217.691 L2226.99 231.349 L2222.71 231.349 L2222.71 205.423 L2226.99 205.423 L2226.99 209.451 Q2228.33 207.089 2230.49 205.955 Q2232.64 204.798 2235.72 204.798 Q2236.16 204.798 2236.69 204.867 Q2237.22 204.914 2237.87 205.029 L2237.89 209.404 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2258.17 230.631 L2258.17 226.372 Q2259.93 227.205 2261.74 227.645 Q2263.54 228.085 2265.28 228.085 Q2269.91 228.085 2272.34 224.983 Q2274.79 221.858 2275.14 215.515 Q2273.8 217.506 2271.74 218.571 Q2269.68 219.636 2267.18 219.636 Q2261.99 219.636 2258.96 216.511 Q2255.95 213.363 2255.95 207.923 Q2255.95 202.599 2259.1 199.381 Q2262.25 196.164 2267.48 196.164 Q2273.47 196.164 2276.62 200.77 Q2279.79 205.353 2279.79 214.103 Q2279.79 222.275 2275.9 227.159 Q2272.04 232.02 2265.49 232.02 Q2263.73 232.02 2261.92 231.673 Q2260.12 231.325 2258.17 230.631 M2267.48 215.978 Q2270.63 215.978 2272.45 213.826 Q2274.31 211.673 2274.31 207.923 Q2274.31 204.196 2272.45 202.043 Q2270.63 199.867 2267.48 199.867 Q2264.33 199.867 2262.48 202.043 Q2260.65 204.196 2260.65 207.923 Q2260.65 211.673 2262.48 213.826 Q2264.33 215.978 2267.48 215.978 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"\n",
       "M1925.11 298.741 L2066.09 298.741 L2066.09 250.357 L1925.11 250.357 L1925.11 298.741  Z\n",
       "  \" fill=\"#fcc085\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#fcc085; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1925.11,250.357 2066.09,250.357 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M2117.46 259.931 L2117.46 264.861 Q2115.09 262.662 2112.41 261.574 Q2109.75 260.486 2106.74 260.486 Q2100.81 260.486 2097.66 264.12 Q2094.52 267.732 2094.52 274.583 Q2094.52 281.412 2097.66 285.046 Q2100.81 288.657 2106.74 288.657 Q2109.75 288.657 2112.41 287.569 Q2115.09 286.481 2117.46 284.282 L2117.46 289.167 Q2115 290.833 2112.25 291.667 Q2109.52 292.5 2106.46 292.5 Q2098.61 292.5 2094.1 287.708 Q2089.58 282.893 2089.58 274.583 Q2089.58 266.25 2094.1 261.458 Q2098.61 256.644 2106.46 256.644 Q2109.56 256.644 2112.29 257.477 Q2115.05 258.287 2117.46 259.931 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2121.92 255.81 L2126.18 255.81 L2126.18 291.829 L2121.92 291.829 L2121.92 255.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2130.21 281.597 L2130.21 265.903 L2134.47 265.903 L2134.47 281.435 Q2134.47 285.116 2135.9 286.968 Q2137.34 288.796 2140.21 288.796 Q2143.66 288.796 2145.65 286.597 Q2147.66 284.398 2147.66 280.602 L2147.66 265.903 L2151.92 265.903 L2151.92 291.829 L2147.66 291.829 L2147.66 287.847 Q2146.11 290.208 2144.05 291.366 Q2142.02 292.5 2139.31 292.5 Q2134.84 292.5 2132.52 289.722 Q2130.21 286.944 2130.21 281.597 M2140.93 265.278 L2140.93 265.278 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2172.92 266.667 L2172.92 270.694 Q2171.11 269.769 2169.17 269.306 Q2167.22 268.843 2165.14 268.843 Q2161.97 268.843 2160.37 269.815 Q2158.8 270.787 2158.8 272.731 Q2158.8 274.213 2159.93 275.069 Q2161.07 275.903 2164.49 276.667 L2165.95 276.991 Q2170.49 277.963 2172.39 279.745 Q2174.31 281.505 2174.31 284.676 Q2174.31 288.287 2171.44 290.393 Q2168.59 292.5 2163.59 292.5 Q2161.51 292.5 2159.24 292.083 Q2156.99 291.69 2154.49 290.88 L2154.49 286.481 Q2156.85 287.708 2159.14 288.333 Q2161.44 288.935 2163.68 288.935 Q2166.69 288.935 2168.31 287.917 Q2169.93 286.875 2169.93 285 Q2169.93 283.264 2168.75 282.338 Q2167.59 281.412 2163.64 280.556 L2162.15 280.208 Q2158.2 279.375 2156.44 277.662 Q2154.68 275.926 2154.68 272.917 Q2154.68 269.259 2157.27 267.269 Q2159.86 265.278 2164.63 265.278 Q2166.99 265.278 2169.08 265.625 Q2171.16 265.972 2172.92 266.667 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2182.99 258.542 L2182.99 265.903 L2191.76 265.903 L2191.76 269.213 L2182.99 269.213 L2182.99 283.287 Q2182.99 286.458 2183.84 287.361 Q2184.72 288.264 2187.39 288.264 L2191.76 288.264 L2191.76 291.829 L2187.39 291.829 Q2182.45 291.829 2180.58 290 Q2178.7 288.148 2178.7 283.287 L2178.7 269.213 L2175.58 269.213 L2175.58 265.903 L2178.7 265.903 L2178.7 258.542 L2182.99 258.542 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2218.4 277.801 L2218.4 279.884 L2198.82 279.884 Q2199.1 284.282 2201.46 286.597 Q2203.84 288.889 2208.08 288.889 Q2210.53 288.889 2212.82 288.287 Q2215.14 287.685 2217.41 286.481 L2217.41 290.509 Q2215.12 291.481 2212.71 291.991 Q2210.3 292.5 2207.82 292.5 Q2201.62 292.5 2197.99 288.889 Q2194.38 285.278 2194.38 279.12 Q2194.38 272.755 2197.8 269.028 Q2201.25 265.278 2207.08 265.278 Q2212.32 265.278 2215.35 268.657 Q2218.4 272.014 2218.4 277.801 M2214.14 276.551 Q2214.1 273.056 2212.18 270.972 Q2210.28 268.889 2207.13 268.889 Q2203.57 268.889 2201.41 270.903 Q2199.28 272.917 2198.96 276.574 L2214.14 276.551 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2237.89 269.884 Q2237.18 269.468 2236.32 269.282 Q2235.49 269.074 2234.47 269.074 Q2230.86 269.074 2228.91 271.435 Q2226.99 273.773 2226.99 278.171 L2226.99 291.829 L2222.71 291.829 L2222.71 265.903 L2226.99 265.903 L2226.99 269.931 Q2228.33 267.569 2230.49 266.435 Q2232.64 265.278 2235.72 265.278 Q2236.16 265.278 2236.69 265.347 Q2237.22 265.394 2237.87 265.509 L2237.89 269.884 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2268.03 275.417 Q2264.7 275.417 2262.78 277.199 Q2260.88 278.981 2260.88 282.106 Q2260.88 285.231 2262.78 287.014 Q2264.7 288.796 2268.03 288.796 Q2271.37 288.796 2273.29 287.014 Q2275.21 285.208 2275.21 282.106 Q2275.21 278.981 2273.29 277.199 Q2271.39 275.417 2268.03 275.417 M2263.36 273.426 Q2260.35 272.685 2258.66 270.625 Q2256.99 268.565 2256.99 265.602 Q2256.99 261.458 2259.93 259.051 Q2262.89 256.644 2268.03 256.644 Q2273.19 256.644 2276.13 259.051 Q2279.07 261.458 2279.07 265.602 Q2279.07 268.565 2277.38 270.625 Q2275.72 272.685 2272.73 273.426 Q2276.11 274.213 2277.99 276.505 Q2279.88 278.796 2279.88 282.106 Q2279.88 287.13 2276.81 289.815 Q2273.75 292.5 2268.03 292.5 Q2262.32 292.5 2259.24 289.815 Q2256.18 287.13 2256.18 282.106 Q2256.18 278.796 2258.08 276.505 Q2259.98 274.213 2263.36 273.426 M2261.64 266.042 Q2261.64 268.727 2263.31 270.232 Q2265 271.736 2268.03 271.736 Q2271.04 271.736 2272.73 270.232 Q2274.44 268.727 2274.44 266.042 Q2274.44 263.357 2272.73 261.852 Q2271.04 260.347 2268.03 260.347 Q2265 260.347 2263.31 261.852 Q2261.64 263.357 2261.64 266.042 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"\n",
       "M1925.11 359.221 L2066.09 359.221 L2066.09 310.837 L1925.11 310.837 L1925.11 359.221  Z\n",
       "  \" fill=\"#ffff99\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#ffff99; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1925.11,310.837 2066.09,310.837 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M2117.46 320.411 L2117.46 325.341 Q2115.09 323.142 2112.41 322.054 Q2109.75 320.966 2106.74 320.966 Q2100.81 320.966 2097.66 324.6 Q2094.52 328.212 2094.52 335.063 Q2094.52 341.892 2097.66 345.526 Q2100.81 349.137 2106.74 349.137 Q2109.75 349.137 2112.41 348.049 Q2115.09 346.961 2117.46 344.762 L2117.46 349.647 Q2115 351.313 2112.25 352.147 Q2109.52 352.98 2106.46 352.98 Q2098.61 352.98 2094.1 348.188 Q2089.58 343.373 2089.58 335.063 Q2089.58 326.73 2094.1 321.938 Q2098.61 317.124 2106.46 317.124 Q2109.56 317.124 2112.29 317.957 Q2115.05 318.767 2117.46 320.411 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2121.92 316.29 L2126.18 316.29 L2126.18 352.309 L2121.92 352.309 L2121.92 316.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2130.21 342.077 L2130.21 326.383 L2134.47 326.383 L2134.47 341.915 Q2134.47 345.596 2135.9 347.448 Q2137.34 349.276 2140.21 349.276 Q2143.66 349.276 2145.65 347.077 Q2147.66 344.878 2147.66 341.082 L2147.66 326.383 L2151.92 326.383 L2151.92 352.309 L2147.66 352.309 L2147.66 348.327 Q2146.11 350.688 2144.05 351.846 Q2142.02 352.98 2139.31 352.98 Q2134.84 352.98 2132.52 350.202 Q2130.21 347.424 2130.21 342.077 M2140.93 325.758 L2140.93 325.758 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2172.92 327.147 L2172.92 331.174 Q2171.11 330.249 2169.17 329.786 Q2167.22 329.323 2165.14 329.323 Q2161.97 329.323 2160.37 330.295 Q2158.8 331.267 2158.8 333.211 Q2158.8 334.693 2159.93 335.549 Q2161.07 336.383 2164.49 337.147 L2165.95 337.471 Q2170.49 338.443 2172.39 340.225 Q2174.31 341.985 2174.31 345.156 Q2174.31 348.767 2171.44 350.873 Q2168.59 352.98 2163.59 352.98 Q2161.51 352.98 2159.24 352.563 Q2156.99 352.17 2154.49 351.36 L2154.49 346.961 Q2156.85 348.188 2159.14 348.813 Q2161.44 349.415 2163.68 349.415 Q2166.69 349.415 2168.31 348.397 Q2169.93 347.355 2169.93 345.48 Q2169.93 343.744 2168.75 342.818 Q2167.59 341.892 2163.64 341.036 L2162.15 340.688 Q2158.2 339.855 2156.44 338.142 Q2154.68 336.406 2154.68 333.397 Q2154.68 329.739 2157.27 327.749 Q2159.86 325.758 2164.63 325.758 Q2166.99 325.758 2169.08 326.105 Q2171.16 326.452 2172.92 327.147 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2182.99 319.022 L2182.99 326.383 L2191.76 326.383 L2191.76 329.693 L2182.99 329.693 L2182.99 343.767 Q2182.99 346.938 2183.84 347.841 Q2184.72 348.744 2187.39 348.744 L2191.76 348.744 L2191.76 352.309 L2187.39 352.309 Q2182.45 352.309 2180.58 350.48 Q2178.7 348.628 2178.7 343.767 L2178.7 329.693 L2175.58 329.693 L2175.58 326.383 L2178.7 326.383 L2178.7 319.022 L2182.99 319.022 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2218.4 338.281 L2218.4 340.364 L2198.82 340.364 Q2199.1 344.762 2201.46 347.077 Q2203.84 349.369 2208.08 349.369 Q2210.53 349.369 2212.82 348.767 Q2215.14 348.165 2217.41 346.961 L2217.41 350.989 Q2215.12 351.961 2212.71 352.471 Q2210.3 352.98 2207.82 352.98 Q2201.62 352.98 2197.99 349.369 Q2194.38 345.758 2194.38 339.6 Q2194.38 333.235 2197.8 329.508 Q2201.25 325.758 2207.08 325.758 Q2212.32 325.758 2215.35 329.137 Q2218.4 332.494 2218.4 338.281 M2214.14 337.031 Q2214.1 333.536 2212.18 331.452 Q2210.28 329.369 2207.13 329.369 Q2203.57 329.369 2201.41 331.383 Q2199.28 333.397 2198.96 337.054 L2214.14 337.031 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2237.89 330.364 Q2237.18 329.948 2236.32 329.762 Q2235.49 329.554 2234.47 329.554 Q2230.86 329.554 2228.91 331.915 Q2226.99 334.253 2226.99 338.651 L2226.99 352.309 L2222.71 352.309 L2222.71 326.383 L2226.99 326.383 L2226.99 330.411 Q2228.33 328.049 2230.49 326.915 Q2232.64 325.758 2235.72 325.758 Q2236.16 325.758 2236.69 325.827 Q2237.22 325.874 2237.87 325.989 L2237.89 330.364 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2256.85 317.749 L2279.07 317.749 L2279.07 319.739 L2266.53 352.309 L2261.64 352.309 L2273.45 321.684 L2256.85 321.684 L2256.85 317.749 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"\n",
       "M1925.11 419.701 L2066.09 419.701 L2066.09 371.317 L1925.11 371.317 L1925.11 419.701  Z\n",
       "  \" fill=\"#386caf\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#386caf; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1925.11,371.317 2066.09,371.317 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M2117.46 380.891 L2117.46 385.821 Q2115.09 383.622 2112.41 382.534 Q2109.75 381.446 2106.74 381.446 Q2100.81 381.446 2097.66 385.08 Q2094.52 388.692 2094.52 395.543 Q2094.52 402.372 2097.66 406.006 Q2100.81 409.617 2106.74 409.617 Q2109.75 409.617 2112.41 408.529 Q2115.09 407.441 2117.46 405.242 L2117.46 410.127 Q2115 411.793 2112.25 412.627 Q2109.52 413.46 2106.46 413.46 Q2098.61 413.46 2094.1 408.668 Q2089.58 403.853 2089.58 395.543 Q2089.58 387.21 2094.1 382.418 Q2098.61 377.604 2106.46 377.604 Q2109.56 377.604 2112.29 378.437 Q2115.05 379.247 2117.46 380.891 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2121.92 376.77 L2126.18 376.77 L2126.18 412.789 L2121.92 412.789 L2121.92 376.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2130.21 402.557 L2130.21 386.863 L2134.47 386.863 L2134.47 402.395 Q2134.47 406.076 2135.9 407.928 Q2137.34 409.756 2140.21 409.756 Q2143.66 409.756 2145.65 407.557 Q2147.66 405.358 2147.66 401.562 L2147.66 386.863 L2151.92 386.863 L2151.92 412.789 L2147.66 412.789 L2147.66 408.807 Q2146.11 411.168 2144.05 412.326 Q2142.02 413.46 2139.31 413.46 Q2134.84 413.46 2132.52 410.682 Q2130.21 407.904 2130.21 402.557 M2140.93 386.238 L2140.93 386.238 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2172.92 387.627 L2172.92 391.654 Q2171.11 390.729 2169.17 390.266 Q2167.22 389.803 2165.14 389.803 Q2161.97 389.803 2160.37 390.775 Q2158.8 391.747 2158.8 393.691 Q2158.8 395.173 2159.93 396.029 Q2161.07 396.863 2164.49 397.627 L2165.95 397.951 Q2170.49 398.923 2172.39 400.705 Q2174.31 402.465 2174.31 405.636 Q2174.31 409.247 2171.44 411.353 Q2168.59 413.46 2163.59 413.46 Q2161.51 413.46 2159.24 413.043 Q2156.99 412.65 2154.49 411.84 L2154.49 407.441 Q2156.85 408.668 2159.14 409.293 Q2161.44 409.895 2163.68 409.895 Q2166.69 409.895 2168.31 408.877 Q2169.93 407.835 2169.93 405.96 Q2169.93 404.224 2168.75 403.298 Q2167.59 402.372 2163.64 401.516 L2162.15 401.168 Q2158.2 400.335 2156.44 398.622 Q2154.68 396.886 2154.68 393.877 Q2154.68 390.219 2157.27 388.229 Q2159.86 386.238 2164.63 386.238 Q2166.99 386.238 2169.08 386.585 Q2171.16 386.932 2172.92 387.627 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2182.99 379.502 L2182.99 386.863 L2191.76 386.863 L2191.76 390.173 L2182.99 390.173 L2182.99 404.247 Q2182.99 407.418 2183.84 408.321 Q2184.72 409.224 2187.39 409.224 L2191.76 409.224 L2191.76 412.789 L2187.39 412.789 Q2182.45 412.789 2180.58 410.96 Q2178.7 409.108 2178.7 404.247 L2178.7 390.173 L2175.58 390.173 L2175.58 386.863 L2178.7 386.863 L2178.7 379.502 L2182.99 379.502 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2218.4 398.761 L2218.4 400.844 L2198.82 400.844 Q2199.1 405.242 2201.46 407.557 Q2203.84 409.849 2208.08 409.849 Q2210.53 409.849 2212.82 409.247 Q2215.14 408.645 2217.41 407.441 L2217.41 411.469 Q2215.12 412.441 2212.71 412.951 Q2210.3 413.46 2207.82 413.46 Q2201.62 413.46 2197.99 409.849 Q2194.38 406.238 2194.38 400.08 Q2194.38 393.715 2197.8 389.988 Q2201.25 386.238 2207.08 386.238 Q2212.32 386.238 2215.35 389.617 Q2218.4 392.974 2218.4 398.761 M2214.14 397.511 Q2214.1 394.016 2212.18 391.932 Q2210.28 389.849 2207.13 389.849 Q2203.57 389.849 2201.41 391.863 Q2199.28 393.877 2198.96 397.534 L2214.14 397.511 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2237.89 390.844 Q2237.18 390.428 2236.32 390.242 Q2235.49 390.034 2234.47 390.034 Q2230.86 390.034 2228.91 392.395 Q2226.99 394.733 2226.99 399.131 L2226.99 412.789 L2222.71 412.789 L2222.71 386.863 L2226.99 386.863 L2226.99 390.891 Q2228.33 388.529 2230.49 387.395 Q2232.64 386.238 2235.72 386.238 Q2236.16 386.238 2236.69 386.307 Q2237.22 386.354 2237.87 386.469 L2237.89 390.844 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2268.61 393.645 Q2265.46 393.645 2263.61 395.798 Q2261.78 397.951 2261.78 401.701 Q2261.78 405.428 2263.61 407.603 Q2265.46 409.756 2268.61 409.756 Q2271.76 409.756 2273.59 407.603 Q2275.44 405.428 2275.44 401.701 Q2275.44 397.951 2273.59 395.798 Q2271.76 393.645 2268.61 393.645 M2277.89 378.993 L2277.89 383.252 Q2276.13 382.418 2274.33 381.979 Q2272.55 381.539 2270.79 381.539 Q2266.16 381.539 2263.7 384.664 Q2261.27 387.789 2260.93 394.108 Q2262.29 392.094 2264.35 391.029 Q2266.41 389.942 2268.89 389.942 Q2274.1 389.942 2277.11 393.113 Q2280.14 396.261 2280.14 401.701 Q2280.14 407.025 2276.99 410.242 Q2273.84 413.46 2268.61 413.46 Q2262.62 413.46 2259.45 408.877 Q2256.27 404.27 2256.27 395.543 Q2256.27 387.349 2260.16 382.488 Q2264.05 377.604 2270.6 377.604 Q2272.36 377.604 2274.14 377.951 Q2275.95 378.298 2277.89 378.993 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"\n",
       "M1925.11 480.181 L2066.09 480.181 L2066.09 431.797 L1925.11 431.797 L1925.11 480.181  Z\n",
       "  \" fill=\"#ef027e\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#ef027e; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1925.11,431.797 2066.09,431.797 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M2117.46 441.371 L2117.46 446.301 Q2115.09 444.102 2112.41 443.014 Q2109.75 441.926 2106.74 441.926 Q2100.81 441.926 2097.66 445.56 Q2094.52 449.172 2094.52 456.023 Q2094.52 462.852 2097.66 466.486 Q2100.81 470.097 2106.74 470.097 Q2109.75 470.097 2112.41 469.009 Q2115.09 467.921 2117.46 465.722 L2117.46 470.607 Q2115 472.273 2112.25 473.107 Q2109.52 473.94 2106.46 473.94 Q2098.61 473.94 2094.1 469.148 Q2089.58 464.333 2089.58 456.023 Q2089.58 447.69 2094.1 442.898 Q2098.61 438.084 2106.46 438.084 Q2109.56 438.084 2112.29 438.917 Q2115.05 439.727 2117.46 441.371 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2121.92 437.25 L2126.18 437.25 L2126.18 473.269 L2121.92 473.269 L2121.92 437.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2130.21 463.037 L2130.21 447.343 L2134.47 447.343 L2134.47 462.875 Q2134.47 466.556 2135.9 468.408 Q2137.34 470.236 2140.21 470.236 Q2143.66 470.236 2145.65 468.037 Q2147.66 465.838 2147.66 462.042 L2147.66 447.343 L2151.92 447.343 L2151.92 473.269 L2147.66 473.269 L2147.66 469.287 Q2146.11 471.648 2144.05 472.806 Q2142.02 473.94 2139.31 473.94 Q2134.84 473.94 2132.52 471.162 Q2130.21 468.384 2130.21 463.037 M2140.93 446.718 L2140.93 446.718 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2172.92 448.107 L2172.92 452.134 Q2171.11 451.209 2169.17 450.746 Q2167.22 450.283 2165.14 450.283 Q2161.97 450.283 2160.37 451.255 Q2158.8 452.227 2158.8 454.171 Q2158.8 455.653 2159.93 456.509 Q2161.07 457.343 2164.49 458.107 L2165.95 458.431 Q2170.49 459.403 2172.39 461.185 Q2174.31 462.945 2174.31 466.116 Q2174.31 469.727 2171.44 471.833 Q2168.59 473.94 2163.59 473.94 Q2161.51 473.94 2159.24 473.523 Q2156.99 473.13 2154.49 472.32 L2154.49 467.921 Q2156.85 469.148 2159.14 469.773 Q2161.44 470.375 2163.68 470.375 Q2166.69 470.375 2168.31 469.357 Q2169.93 468.315 2169.93 466.44 Q2169.93 464.704 2168.75 463.778 Q2167.59 462.852 2163.64 461.996 L2162.15 461.648 Q2158.2 460.815 2156.44 459.102 Q2154.68 457.366 2154.68 454.357 Q2154.68 450.699 2157.27 448.709 Q2159.86 446.718 2164.63 446.718 Q2166.99 446.718 2169.08 447.065 Q2171.16 447.412 2172.92 448.107 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2182.99 439.982 L2182.99 447.343 L2191.76 447.343 L2191.76 450.653 L2182.99 450.653 L2182.99 464.727 Q2182.99 467.898 2183.84 468.801 Q2184.72 469.704 2187.39 469.704 L2191.76 469.704 L2191.76 473.269 L2187.39 473.269 Q2182.45 473.269 2180.58 471.44 Q2178.7 469.588 2178.7 464.727 L2178.7 450.653 L2175.58 450.653 L2175.58 447.343 L2178.7 447.343 L2178.7 439.982 L2182.99 439.982 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2218.4 459.241 L2218.4 461.324 L2198.82 461.324 Q2199.1 465.722 2201.46 468.037 Q2203.84 470.329 2208.08 470.329 Q2210.53 470.329 2212.82 469.727 Q2215.14 469.125 2217.41 467.921 L2217.41 471.949 Q2215.12 472.921 2212.71 473.431 Q2210.3 473.94 2207.82 473.94 Q2201.62 473.94 2197.99 470.329 Q2194.38 466.718 2194.38 460.56 Q2194.38 454.195 2197.8 450.468 Q2201.25 446.718 2207.08 446.718 Q2212.32 446.718 2215.35 450.097 Q2218.4 453.454 2218.4 459.241 M2214.14 457.991 Q2214.1 454.496 2212.18 452.412 Q2210.28 450.329 2207.13 450.329 Q2203.57 450.329 2201.41 452.343 Q2199.28 454.357 2198.96 458.014 L2214.14 457.991 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2237.89 451.324 Q2237.18 450.908 2236.32 450.722 Q2235.49 450.514 2234.47 450.514 Q2230.86 450.514 2228.91 452.875 Q2226.99 455.213 2226.99 459.611 L2226.99 473.269 L2222.71 473.269 L2222.71 447.343 L2226.99 447.343 L2226.99 451.371 Q2228.33 449.009 2230.49 447.875 Q2232.64 446.718 2235.72 446.718 Q2236.16 446.718 2236.69 446.787 Q2237.22 446.834 2237.87 446.949 L2237.89 451.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2258.08 438.709 L2276.44 438.709 L2276.44 442.644 L2262.36 442.644 L2262.36 451.116 Q2263.38 450.769 2264.4 450.607 Q2265.42 450.422 2266.44 450.422 Q2272.22 450.422 2275.6 453.593 Q2278.98 456.764 2278.98 462.181 Q2278.98 467.759 2275.51 470.861 Q2272.04 473.94 2265.72 473.94 Q2263.54 473.94 2261.27 473.57 Q2259.03 473.199 2256.62 472.458 L2256.62 467.759 Q2258.7 468.894 2260.93 469.449 Q2263.15 470.005 2265.63 470.005 Q2269.63 470.005 2271.97 467.898 Q2274.31 465.792 2274.31 462.181 Q2274.31 458.57 2271.97 456.463 Q2269.63 454.357 2265.63 454.357 Q2263.75 454.357 2261.88 454.773 Q2260.02 455.19 2258.08 456.07 L2258.08 438.709 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"\n",
       "M1925.11 540.661 L2066.09 540.661 L2066.09 492.277 L1925.11 492.277 L1925.11 540.661  Z\n",
       "  \" fill=\"#be5b16\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#be5b16; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1925.11,492.277 2066.09,492.277 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M2117.46 501.851 L2117.46 506.781 Q2115.09 504.582 2112.41 503.494 Q2109.75 502.406 2106.74 502.406 Q2100.81 502.406 2097.66 506.04 Q2094.52 509.652 2094.52 516.503 Q2094.52 523.332 2097.66 526.966 Q2100.81 530.577 2106.74 530.577 Q2109.75 530.577 2112.41 529.489 Q2115.09 528.401 2117.46 526.202 L2117.46 531.087 Q2115 532.753 2112.25 533.587 Q2109.52 534.42 2106.46 534.42 Q2098.61 534.42 2094.1 529.628 Q2089.58 524.813 2089.58 516.503 Q2089.58 508.17 2094.1 503.378 Q2098.61 498.564 2106.46 498.564 Q2109.56 498.564 2112.29 499.397 Q2115.05 500.207 2117.46 501.851 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2121.92 497.73 L2126.18 497.73 L2126.18 533.749 L2121.92 533.749 L2121.92 497.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2130.21 523.517 L2130.21 507.823 L2134.47 507.823 L2134.47 523.355 Q2134.47 527.036 2135.9 528.888 Q2137.34 530.716 2140.21 530.716 Q2143.66 530.716 2145.65 528.517 Q2147.66 526.318 2147.66 522.522 L2147.66 507.823 L2151.92 507.823 L2151.92 533.749 L2147.66 533.749 L2147.66 529.767 Q2146.11 532.128 2144.05 533.286 Q2142.02 534.42 2139.31 534.42 Q2134.84 534.42 2132.52 531.642 Q2130.21 528.864 2130.21 523.517 M2140.93 507.198 L2140.93 507.198 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2172.92 508.587 L2172.92 512.614 Q2171.11 511.689 2169.17 511.226 Q2167.22 510.763 2165.14 510.763 Q2161.97 510.763 2160.37 511.735 Q2158.8 512.707 2158.8 514.651 Q2158.8 516.133 2159.93 516.989 Q2161.07 517.823 2164.49 518.587 L2165.95 518.911 Q2170.49 519.883 2172.39 521.665 Q2174.31 523.425 2174.31 526.596 Q2174.31 530.207 2171.44 532.313 Q2168.59 534.42 2163.59 534.42 Q2161.51 534.42 2159.24 534.003 Q2156.99 533.61 2154.49 532.8 L2154.49 528.401 Q2156.85 529.628 2159.14 530.253 Q2161.44 530.855 2163.68 530.855 Q2166.69 530.855 2168.31 529.837 Q2169.93 528.795 2169.93 526.92 Q2169.93 525.184 2168.75 524.258 Q2167.59 523.332 2163.64 522.476 L2162.15 522.128 Q2158.2 521.295 2156.44 519.582 Q2154.68 517.846 2154.68 514.837 Q2154.68 511.179 2157.27 509.189 Q2159.86 507.198 2164.63 507.198 Q2166.99 507.198 2169.08 507.545 Q2171.16 507.892 2172.92 508.587 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2182.99 500.462 L2182.99 507.823 L2191.76 507.823 L2191.76 511.133 L2182.99 511.133 L2182.99 525.207 Q2182.99 528.378 2183.84 529.281 Q2184.72 530.184 2187.39 530.184 L2191.76 530.184 L2191.76 533.749 L2187.39 533.749 Q2182.45 533.749 2180.58 531.92 Q2178.7 530.068 2178.7 525.207 L2178.7 511.133 L2175.58 511.133 L2175.58 507.823 L2178.7 507.823 L2178.7 500.462 L2182.99 500.462 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2218.4 519.721 L2218.4 521.804 L2198.82 521.804 Q2199.1 526.202 2201.46 528.517 Q2203.84 530.809 2208.08 530.809 Q2210.53 530.809 2212.82 530.207 Q2215.14 529.605 2217.41 528.401 L2217.41 532.429 Q2215.12 533.401 2212.71 533.911 Q2210.3 534.42 2207.82 534.42 Q2201.62 534.42 2197.99 530.809 Q2194.38 527.198 2194.38 521.04 Q2194.38 514.675 2197.8 510.948 Q2201.25 507.198 2207.08 507.198 Q2212.32 507.198 2215.35 510.577 Q2218.4 513.934 2218.4 519.721 M2214.14 518.471 Q2214.1 514.976 2212.18 512.892 Q2210.28 510.809 2207.13 510.809 Q2203.57 510.809 2201.41 512.823 Q2199.28 514.837 2198.96 518.494 L2214.14 518.471 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2237.89 511.804 Q2237.18 511.388 2236.32 511.202 Q2235.49 510.994 2234.47 510.994 Q2230.86 510.994 2228.91 513.355 Q2226.99 515.693 2226.99 520.091 L2226.99 533.749 L2222.71 533.749 L2222.71 507.823 L2226.99 507.823 L2226.99 511.851 Q2228.33 509.489 2230.49 508.355 Q2232.64 507.198 2235.72 507.198 Q2236.16 507.198 2236.69 507.267 Q2237.22 507.314 2237.87 507.429 L2237.89 511.804 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2270.88 503.263 L2259.07 521.712 L2270.88 521.712 L2270.88 503.263 M2269.65 499.189 L2275.53 499.189 L2275.53 521.712 L2280.46 521.712 L2280.46 525.601 L2275.53 525.601 L2275.53 533.749 L2270.88 533.749 L2270.88 525.601 L2255.28 525.601 L2255.28 521.087 L2269.65 499.189 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"\n",
       "M1925.11 601.141 L2066.09 601.141 L2066.09 552.757 L1925.11 552.757 L1925.11 601.141  Z\n",
       "  \" fill=\"#666666\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#666666; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1925.11,552.757 2066.09,552.757 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M2117.46 562.331 L2117.46 567.261 Q2115.09 565.062 2112.41 563.974 Q2109.75 562.886 2106.74 562.886 Q2100.81 562.886 2097.66 566.52 Q2094.52 570.132 2094.52 576.983 Q2094.52 583.812 2097.66 587.446 Q2100.81 591.057 2106.74 591.057 Q2109.75 591.057 2112.41 589.969 Q2115.09 588.881 2117.46 586.682 L2117.46 591.567 Q2115 593.233 2112.25 594.067 Q2109.52 594.9 2106.46 594.9 Q2098.61 594.9 2094.1 590.108 Q2089.58 585.293 2089.58 576.983 Q2089.58 568.65 2094.1 563.858 Q2098.61 559.044 2106.46 559.044 Q2109.56 559.044 2112.29 559.877 Q2115.05 560.687 2117.46 562.331 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2121.92 558.21 L2126.18 558.21 L2126.18 594.229 L2121.92 594.229 L2121.92 558.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2130.21 583.997 L2130.21 568.303 L2134.47 568.303 L2134.47 583.835 Q2134.47 587.516 2135.9 589.368 Q2137.34 591.196 2140.21 591.196 Q2143.66 591.196 2145.65 588.997 Q2147.66 586.798 2147.66 583.002 L2147.66 568.303 L2151.92 568.303 L2151.92 594.229 L2147.66 594.229 L2147.66 590.247 Q2146.11 592.608 2144.05 593.766 Q2142.02 594.9 2139.31 594.9 Q2134.84 594.9 2132.52 592.122 Q2130.21 589.344 2130.21 583.997 M2140.93 567.678 L2140.93 567.678 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2172.92 569.067 L2172.92 573.094 Q2171.11 572.169 2169.17 571.706 Q2167.22 571.243 2165.14 571.243 Q2161.97 571.243 2160.37 572.215 Q2158.8 573.187 2158.8 575.131 Q2158.8 576.613 2159.93 577.469 Q2161.07 578.303 2164.49 579.067 L2165.95 579.391 Q2170.49 580.363 2172.39 582.145 Q2174.31 583.905 2174.31 587.076 Q2174.31 590.687 2171.44 592.793 Q2168.59 594.9 2163.59 594.9 Q2161.51 594.9 2159.24 594.483 Q2156.99 594.09 2154.49 593.28 L2154.49 588.881 Q2156.85 590.108 2159.14 590.733 Q2161.44 591.335 2163.68 591.335 Q2166.69 591.335 2168.31 590.317 Q2169.93 589.275 2169.93 587.4 Q2169.93 585.664 2168.75 584.738 Q2167.59 583.812 2163.64 582.956 L2162.15 582.608 Q2158.2 581.775 2156.44 580.062 Q2154.68 578.326 2154.68 575.317 Q2154.68 571.659 2157.27 569.669 Q2159.86 567.678 2164.63 567.678 Q2166.99 567.678 2169.08 568.025 Q2171.16 568.372 2172.92 569.067 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2182.99 560.942 L2182.99 568.303 L2191.76 568.303 L2191.76 571.613 L2182.99 571.613 L2182.99 585.687 Q2182.99 588.858 2183.84 589.761 Q2184.72 590.664 2187.39 590.664 L2191.76 590.664 L2191.76 594.229 L2187.39 594.229 Q2182.45 594.229 2180.58 592.4 Q2178.7 590.548 2178.7 585.687 L2178.7 571.613 L2175.58 571.613 L2175.58 568.303 L2178.7 568.303 L2178.7 560.942 L2182.99 560.942 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2218.4 580.201 L2218.4 582.284 L2198.82 582.284 Q2199.1 586.682 2201.46 588.997 Q2203.84 591.289 2208.08 591.289 Q2210.53 591.289 2212.82 590.687 Q2215.14 590.085 2217.41 588.881 L2217.41 592.909 Q2215.12 593.881 2212.71 594.391 Q2210.3 594.9 2207.82 594.9 Q2201.62 594.9 2197.99 591.289 Q2194.38 587.678 2194.38 581.52 Q2194.38 575.155 2197.8 571.428 Q2201.25 567.678 2207.08 567.678 Q2212.32 567.678 2215.35 571.057 Q2218.4 574.414 2218.4 580.201 M2214.14 578.951 Q2214.1 575.456 2212.18 573.372 Q2210.28 571.289 2207.13 571.289 Q2203.57 571.289 2201.41 573.303 Q2199.28 575.317 2198.96 578.974 L2214.14 578.951 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2237.89 572.284 Q2237.18 571.868 2236.32 571.682 Q2235.49 571.474 2234.47 571.474 Q2230.86 571.474 2228.91 573.835 Q2226.99 576.173 2226.99 580.571 L2226.99 594.229 L2222.71 594.229 L2222.71 568.303 L2226.99 568.303 L2226.99 572.331 Q2228.33 569.969 2230.49 568.835 Q2232.64 567.678 2235.72 567.678 Q2236.16 567.678 2236.69 567.747 Q2237.22 567.794 2237.87 567.909 L2237.89 572.284 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2272.2 575.594 Q2275.56 576.312 2277.43 578.581 Q2279.33 580.849 2279.33 584.182 Q2279.33 589.298 2275.81 592.099 Q2272.29 594.9 2265.81 594.9 Q2263.63 594.9 2261.32 594.46 Q2259.03 594.043 2256.57 593.187 L2256.57 588.673 Q2258.52 589.807 2260.83 590.386 Q2263.15 590.965 2265.67 590.965 Q2270.07 590.965 2272.36 589.229 Q2274.68 587.493 2274.68 584.182 Q2274.68 581.127 2272.52 579.414 Q2270.39 577.678 2266.57 577.678 L2262.55 577.678 L2262.55 573.835 L2266.76 573.835 Q2270.21 573.835 2272.04 572.469 Q2273.87 571.081 2273.87 568.488 Q2273.87 565.826 2271.97 564.414 Q2270.09 562.979 2266.57 562.979 Q2264.65 562.979 2262.45 563.395 Q2260.26 563.812 2257.62 564.692 L2257.62 560.525 Q2260.28 559.784 2262.59 559.414 Q2264.93 559.044 2266.99 559.044 Q2272.32 559.044 2275.42 561.474 Q2278.52 563.882 2278.52 568.002 Q2278.52 570.872 2276.88 572.863 Q2275.23 574.831 2272.2 575.594 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"\n",
       "M1925.11 661.621 L2066.09 661.621 L2066.09 613.237 L1925.11 613.237 L1925.11 661.621  Z\n",
       "  \" fill=\"#7ec87e\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#7ec87e; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1925.11,613.237 2066.09,613.237 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M2117.46 622.811 L2117.46 627.741 Q2115.09 625.542 2112.41 624.454 Q2109.75 623.366 2106.74 623.366 Q2100.81 623.366 2097.66 627 Q2094.52 630.612 2094.52 637.463 Q2094.52 644.292 2097.66 647.926 Q2100.81 651.537 2106.74 651.537 Q2109.75 651.537 2112.41 650.449 Q2115.09 649.361 2117.46 647.162 L2117.46 652.047 Q2115 653.713 2112.25 654.547 Q2109.52 655.38 2106.46 655.38 Q2098.61 655.38 2094.1 650.588 Q2089.58 645.773 2089.58 637.463 Q2089.58 629.13 2094.1 624.338 Q2098.61 619.524 2106.46 619.524 Q2109.56 619.524 2112.29 620.357 Q2115.05 621.167 2117.46 622.811 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2121.92 618.69 L2126.18 618.69 L2126.18 654.709 L2121.92 654.709 L2121.92 618.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2130.21 644.477 L2130.21 628.783 L2134.47 628.783 L2134.47 644.315 Q2134.47 647.996 2135.9 649.848 Q2137.34 651.676 2140.21 651.676 Q2143.66 651.676 2145.65 649.477 Q2147.66 647.278 2147.66 643.482 L2147.66 628.783 L2151.92 628.783 L2151.92 654.709 L2147.66 654.709 L2147.66 650.727 Q2146.11 653.088 2144.05 654.246 Q2142.02 655.38 2139.31 655.38 Q2134.84 655.38 2132.52 652.602 Q2130.21 649.824 2130.21 644.477 M2140.93 628.158 L2140.93 628.158 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2172.92 629.547 L2172.92 633.574 Q2171.11 632.649 2169.17 632.186 Q2167.22 631.723 2165.14 631.723 Q2161.97 631.723 2160.37 632.695 Q2158.8 633.667 2158.8 635.611 Q2158.8 637.093 2159.93 637.949 Q2161.07 638.783 2164.49 639.547 L2165.95 639.871 Q2170.49 640.843 2172.39 642.625 Q2174.31 644.385 2174.31 647.556 Q2174.31 651.167 2171.44 653.273 Q2168.59 655.38 2163.59 655.38 Q2161.51 655.38 2159.24 654.963 Q2156.99 654.57 2154.49 653.76 L2154.49 649.361 Q2156.85 650.588 2159.14 651.213 Q2161.44 651.815 2163.68 651.815 Q2166.69 651.815 2168.31 650.797 Q2169.93 649.755 2169.93 647.88 Q2169.93 646.144 2168.75 645.218 Q2167.59 644.292 2163.64 643.436 L2162.15 643.088 Q2158.2 642.255 2156.44 640.542 Q2154.68 638.806 2154.68 635.797 Q2154.68 632.139 2157.27 630.149 Q2159.86 628.158 2164.63 628.158 Q2166.99 628.158 2169.08 628.505 Q2171.16 628.852 2172.92 629.547 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2182.99 621.422 L2182.99 628.783 L2191.76 628.783 L2191.76 632.093 L2182.99 632.093 L2182.99 646.167 Q2182.99 649.338 2183.84 650.241 Q2184.72 651.144 2187.39 651.144 L2191.76 651.144 L2191.76 654.709 L2187.39 654.709 Q2182.45 654.709 2180.58 652.88 Q2178.7 651.028 2178.7 646.167 L2178.7 632.093 L2175.58 632.093 L2175.58 628.783 L2178.7 628.783 L2178.7 621.422 L2182.99 621.422 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2218.4 640.681 L2218.4 642.764 L2198.82 642.764 Q2199.1 647.162 2201.46 649.477 Q2203.84 651.769 2208.08 651.769 Q2210.53 651.769 2212.82 651.167 Q2215.14 650.565 2217.41 649.361 L2217.41 653.389 Q2215.12 654.361 2212.71 654.871 Q2210.3 655.38 2207.82 655.38 Q2201.62 655.38 2197.99 651.769 Q2194.38 648.158 2194.38 642 Q2194.38 635.635 2197.8 631.908 Q2201.25 628.158 2207.08 628.158 Q2212.32 628.158 2215.35 631.537 Q2218.4 634.894 2218.4 640.681 M2214.14 639.431 Q2214.1 635.936 2212.18 633.852 Q2210.28 631.769 2207.13 631.769 Q2203.57 631.769 2201.41 633.783 Q2199.28 635.797 2198.96 639.454 L2214.14 639.431 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2237.89 632.764 Q2237.18 632.348 2236.32 632.162 Q2235.49 631.954 2234.47 631.954 Q2230.86 631.954 2228.91 634.315 Q2226.99 636.653 2226.99 641.051 L2226.99 654.709 L2222.71 654.709 L2222.71 628.783 L2226.99 628.783 L2226.99 632.811 Q2228.33 630.449 2230.49 629.315 Q2232.64 628.158 2235.72 628.158 Q2236.16 628.158 2236.69 628.227 Q2237.22 628.274 2237.87 628.389 L2237.89 632.764 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2262.06 650.773 L2278.38 650.773 L2278.38 654.709 L2256.44 654.709 L2256.44 650.773 Q2259.1 648.019 2263.68 643.389 Q2268.29 638.736 2269.47 637.394 Q2271.71 634.871 2272.59 633.135 Q2273.5 631.375 2273.5 629.686 Q2273.5 626.931 2271.55 625.195 Q2269.63 623.459 2266.53 623.459 Q2264.33 623.459 2261.88 624.223 Q2259.45 624.987 2256.67 626.537 L2256.67 621.815 Q2259.49 620.681 2261.95 620.102 Q2264.4 619.524 2266.44 619.524 Q2271.81 619.524 2275 622.209 Q2278.19 624.894 2278.19 629.385 Q2278.19 631.514 2277.38 633.436 Q2276.6 635.334 2274.49 637.926 Q2273.91 638.598 2270.81 641.815 Q2267.71 645.01 2262.06 650.773 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"\n",
       "M1925.11 722.101 L2066.09 722.101 L2066.09 673.717 L1925.11 673.717 L1925.11 722.101  Z\n",
       "  \" fill=\"#bdadd3\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#bdadd3; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1925.11,673.717 2066.09,673.717 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M2117.46 683.291 L2117.46 688.221 Q2115.09 686.022 2112.41 684.934 Q2109.75 683.846 2106.74 683.846 Q2100.81 683.846 2097.66 687.48 Q2094.52 691.092 2094.52 697.943 Q2094.52 704.772 2097.66 708.406 Q2100.81 712.017 2106.74 712.017 Q2109.75 712.017 2112.41 710.929 Q2115.09 709.841 2117.46 707.642 L2117.46 712.527 Q2115 714.193 2112.25 715.027 Q2109.52 715.86 2106.46 715.86 Q2098.61 715.86 2094.1 711.068 Q2089.58 706.253 2089.58 697.943 Q2089.58 689.61 2094.1 684.818 Q2098.61 680.004 2106.46 680.004 Q2109.56 680.004 2112.29 680.837 Q2115.05 681.647 2117.46 683.291 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2121.92 679.17 L2126.18 679.17 L2126.18 715.189 L2121.92 715.189 L2121.92 679.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2130.21 704.957 L2130.21 689.263 L2134.47 689.263 L2134.47 704.795 Q2134.47 708.476 2135.9 710.328 Q2137.34 712.156 2140.21 712.156 Q2143.66 712.156 2145.65 709.957 Q2147.66 707.758 2147.66 703.962 L2147.66 689.263 L2151.92 689.263 L2151.92 715.189 L2147.66 715.189 L2147.66 711.207 Q2146.11 713.568 2144.05 714.726 Q2142.02 715.86 2139.31 715.86 Q2134.84 715.86 2132.52 713.082 Q2130.21 710.304 2130.21 704.957 M2140.93 688.638 L2140.93 688.638 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2172.92 690.027 L2172.92 694.054 Q2171.11 693.129 2169.17 692.666 Q2167.22 692.203 2165.14 692.203 Q2161.97 692.203 2160.37 693.175 Q2158.8 694.147 2158.8 696.091 Q2158.8 697.573 2159.93 698.429 Q2161.07 699.263 2164.49 700.027 L2165.95 700.351 Q2170.49 701.323 2172.39 703.105 Q2174.31 704.865 2174.31 708.036 Q2174.31 711.647 2171.44 713.753 Q2168.59 715.86 2163.59 715.86 Q2161.51 715.86 2159.24 715.443 Q2156.99 715.05 2154.49 714.24 L2154.49 709.841 Q2156.85 711.068 2159.14 711.693 Q2161.44 712.295 2163.68 712.295 Q2166.69 712.295 2168.31 711.277 Q2169.93 710.235 2169.93 708.36 Q2169.93 706.624 2168.75 705.698 Q2167.59 704.772 2163.64 703.916 L2162.15 703.568 Q2158.2 702.735 2156.44 701.022 Q2154.68 699.286 2154.68 696.277 Q2154.68 692.619 2157.27 690.629 Q2159.86 688.638 2164.63 688.638 Q2166.99 688.638 2169.08 688.985 Q2171.16 689.332 2172.92 690.027 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2182.99 681.902 L2182.99 689.263 L2191.76 689.263 L2191.76 692.573 L2182.99 692.573 L2182.99 706.647 Q2182.99 709.818 2183.84 710.721 Q2184.72 711.624 2187.39 711.624 L2191.76 711.624 L2191.76 715.189 L2187.39 715.189 Q2182.45 715.189 2180.58 713.36 Q2178.7 711.508 2178.7 706.647 L2178.7 692.573 L2175.58 692.573 L2175.58 689.263 L2178.7 689.263 L2178.7 681.902 L2182.99 681.902 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2218.4 701.161 L2218.4 703.244 L2198.82 703.244 Q2199.1 707.642 2201.46 709.957 Q2203.84 712.249 2208.08 712.249 Q2210.53 712.249 2212.82 711.647 Q2215.14 711.045 2217.41 709.841 L2217.41 713.869 Q2215.12 714.841 2212.71 715.351 Q2210.3 715.86 2207.82 715.86 Q2201.62 715.86 2197.99 712.249 Q2194.38 708.638 2194.38 702.48 Q2194.38 696.115 2197.8 692.388 Q2201.25 688.638 2207.08 688.638 Q2212.32 688.638 2215.35 692.017 Q2218.4 695.374 2218.4 701.161 M2214.14 699.911 Q2214.1 696.416 2212.18 694.332 Q2210.28 692.249 2207.13 692.249 Q2203.57 692.249 2201.41 694.263 Q2199.28 696.277 2198.96 699.934 L2214.14 699.911 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2237.89 693.244 Q2237.18 692.828 2236.32 692.642 Q2235.49 692.434 2234.47 692.434 Q2230.86 692.434 2228.91 694.795 Q2226.99 697.133 2226.99 701.531 L2226.99 715.189 L2222.71 715.189 L2222.71 689.263 L2226.99 689.263 L2226.99 693.291 Q2228.33 690.929 2230.49 689.795 Q2232.64 688.638 2235.72 688.638 Q2236.16 688.638 2236.69 688.707 Q2237.22 688.754 2237.87 688.869 L2237.89 693.244 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2258.84 711.253 L2266.48 711.253 L2266.48 684.888 L2258.17 686.555 L2258.17 682.295 L2266.44 680.629 L2271.11 680.629 L2271.11 711.253 L2278.75 711.253 L2278.75 715.189 L2258.84 715.189 L2258.84 711.253 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cum_plot(plotmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_kmeans_portfolio_$upper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using methods above, determined that 7 clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499-element Array{Int64,1}:\n",
       " 3\n",
       " 5\n",
       " 2\n",
       " 5\n",
       " 1\n",
       " 3\n",
       " 7\n",
       " 3\n",
       " 5\n",
       " 4\n",
       " 6\n",
       " 6\n",
       " 2\n",
       " 4\n",
       " 4\n",
       " 7\n",
       " 5\n",
       " 1\n",
       " 2\n",
       " 7\n",
       " 4\n",
       " 4\n",
       " 4\n",
       " 6\n",
       " 2\n",
       " 6\n",
       " 4\n",
       " 2\n",
       " 6\n",
       " 1\n",
       " 5\n",
       " 1\n",
       " 3\n",
       " 7\n",
       " 4\n",
       " 5\n",
       " 3\n",
       " 5\n",
       " 6\n",
       " 4\n",
       " 5\n",
       " 3\n",
       " 4\n",
       " 4\n",
       " 3\n",
       " 2\n",
       " 7\n",
       " 2\n",
       " ⋮\n",
       " 4\n",
       " 5\n",
       " 7\n",
       " 7\n",
       " 1\n",
       " 5\n",
       " 6\n",
       " 5\n",
       " 1\n",
       " 3\n",
       " 6\n",
       " 6\n",
       " 3\n",
       " 4\n",
       " 2\n",
       " 5\n",
       " 3\n",
       " 5\n",
       " 1\n",
       " 4\n",
       " 2\n",
       " 2\n",
       " 3\n",
       " 5\n",
       " 1\n",
       " 1\n",
       " 5\n",
       " 6\n",
       " 6\n",
       " 3\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 1\n",
       " 1\n",
       " 2\n",
       " 2\n",
       " 1\n",
       " 6\n",
       " 7\n",
       " 4\n",
       " 1\n",
       " 1\n",
       " 6\n",
       " 7\n",
       " 2\n",
       " 1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km_assignments[6] # is 7 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499-element Array{Tuple{Int64,Int64},1}:\n",
       " (3, 1)\n",
       " (5, 1)\n",
       " (2, 1)\n",
       " (5, 2)\n",
       " (1, 1)\n",
       " (3, 2)\n",
       " (7, 2)\n",
       " (3, 2)\n",
       " (5, 1)\n",
       " (4, 1)\n",
       " (6, 2)\n",
       " (6, 2)\n",
       " (2, 1)\n",
       " (4, 1)\n",
       " (4, 1)\n",
       " (7, 1)\n",
       " (5, 2)\n",
       " (1, 1)\n",
       " (2, 1)\n",
       " (7, 2)\n",
       " (4, 2)\n",
       " (4, 1)\n",
       " (4, 1)\n",
       " (6, 2)\n",
       " (2, 1)\n",
       " (6, 2)\n",
       " (4, 1)\n",
       " (2, 1)\n",
       " (6, 2)\n",
       " (1, 2)\n",
       " (5, 2)\n",
       " (1, 3)\n",
       " (3, 1)\n",
       " (7, 2)\n",
       " (4, 1)\n",
       " (5, 2)\n",
       " (3, 1)\n",
       " (5, 1)\n",
       " (6, 2)\n",
       " (4, 2)\n",
       " (5, 3)\n",
       " (3, 1)\n",
       " (4, 2)\n",
       " (4, 1)\n",
       " (3, 2)\n",
       " (2, 1)\n",
       " (7, 2)\n",
       " (2, 1)\n",
       " ⋮\n",
       " (4, 1)\n",
       " (5, 2)\n",
       " (7, 1)\n",
       " (7, 2)\n",
       " (1, 3)\n",
       " (5, 2)\n",
       " (6, 2)\n",
       " (5, 3)\n",
       " (1, 2)\n",
       " (3, 2)\n",
       " (6, 2)\n",
       " (6, 2)\n",
       " (3, 2)\n",
       " (4, 1)\n",
       " (2, 1)\n",
       " (5, 2)\n",
       " (3, 2)\n",
       " (5, 2)\n",
       " (1, 1)\n",
       " (4, 1)\n",
       " (2, 1)\n",
       " (2, 1)\n",
       " (3, 1)\n",
       " (5, 1)\n",
       " (1, 2)\n",
       " (1, 2)\n",
       " (5, 2)\n",
       " (6, 2)\n",
       " (6, 2)\n",
       " (3, 2)\n",
       " (1, 1)\n",
       " (1, 1)\n",
       " (2, 1)\n",
       " (1, 2)\n",
       " (1, 2)\n",
       " (2, 1)\n",
       " (2, 1)\n",
       " (1, 1)\n",
       " (6, 2)\n",
       " (7, 2)\n",
       " (4, 1)\n",
       " (1, 1)\n",
       " (1, 1)\n",
       " (6, 2)\n",
       " (7, 2)\n",
       " (2, 1)\n",
       " (1, 1)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_1h = map(x-> if (x == \"L\") 1 elseif (x == \"R\") 2 else 3 end , y[train])\n",
    "collect(zip(km_assignments[6], y_1h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17217083630640004"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mutualinfo(km_assignments[6], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.17217083630639998"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vmeasure(km_assignments[6], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09238207444103792, 0.5917054993521179, 0.4082945006478821, 0.18341099870423577)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randindex(km_assignments[6], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3629758146848374"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "varinfo(km_assignments[6], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cgrad(:bluesreds)\n",
    "scatter(X[:,1].*X[:,2], X[:,3].*X[:,4], label=y_1h, color=:bluesreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_pair_plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 and 27, 27 and 13. central mass of majority class, in different axes there are differences for the minority class. this meshes with the metrics that use the labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectation Maximization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture\n",
    "* https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_pdf.html#sphx-glr-auto-examples-mixture-plot-gmm-pdf-py\n",
    "* https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_covariances.html#sphx-glr-auto-examples-mixture-plot-gmm-covariances-py\n",
    "* https://scikit-learn.org/stable/auto_examples/mixture/plot_gmm_selection.html#sphx-glr-auto-examples-mixture-plot-gmm-selection-py\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html#sklearn.mixture.GaussianMixture\n",
    "* https://github.com/JuliaPy/PyCall.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_m = pyimport(\"sklearn.mixture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upper = 8\n",
    "k_range = 2:upper\n",
    "bics = []\n",
    "sils = []\n",
    "ls = []\n",
    "sil_means = []\n",
    "em_assignments = []\n",
    "\n",
    "for i in k_range\n",
    "    println(\"Gaussians = $i\")\n",
    "    clf = sklearn_m.GaussianMixture(n_components=i, covariance_type=\"full\", random_state=RNG)\n",
    "    labels = clf.fit_predict(train_data) .+ 1; # indexing issues from python to julia functions\n",
    "    \n",
    "    d = countmap(labels)\n",
    "    k = d |> keys\n",
    "    v = d |> values\n",
    "    l = sort(collect(zip(k,v)), by=x->x[1])\n",
    "    @show l\n",
    "    \n",
    "    s = silhouettes(labels, dist_mat(train_data))\n",
    "    println(\"silhouette: $(mean(s))\")\n",
    "    \n",
    "    bayes_ic = clf.bic(train_data)\n",
    "    @show bayes_ic\n",
    "    \n",
    "    push!(ls, l)\n",
    "    push!(sils, s)\n",
    "    push!(sil_means, mean(s))\n",
    "    push!(bics, bayes_ic)\n",
    "    push!(em_assignments, labels)\n",
    "    println(\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(k_range, bics, legend=:bottomleft, label=\"Bayesian Info Criteria\", color=:green, lw=2)\n",
    "xlabel!(\"Number of Clusters\")\n",
    "plot!(twinx(),k_range, sil_means, legend=:topright, label=\"Silhouette Score\", color=:blue, lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_em_metrics_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = prepare_portfolio(ls)\n",
    "plotmat = cum_columns(mat, normalize=true)\n",
    "cum_plot(plotmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_em_portfolio_$upper\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html#sphx-glr-auto-examples-cluster-plot-kmeans-silhouette-analysis-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "using methods above, determined that 6 clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_1h = map(x-> if (x == \"RB\") 1 else 2 end , y[train])\n",
    "collect(zip(em_assignments[5], y_1h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualinfo(em_assignments[5], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmeasure(em_assignments[5], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randindex(em_assignments[5], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varinfo(em_assignments[5], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "em_assignments[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction\n",
    "Apply the dimensionality reduction algorithms to the two datasets and describe what you see.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info(\"PCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = @load PCA pkg=\"MultivariateStats\" \n",
    "mach = machine(model, X)\n",
    "MLJ.fit!(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report(mach)[:principalvars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fitted_params(mach)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = MLJ.transform(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scitype(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report(mach)[:mean]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot explained variance\n",
    "max_dims=31\n",
    "ex_vars = []\n",
    "err_vars = []\n",
    "\n",
    "for i in 1:max_dims\n",
    "    model.maxoutdim = i\n",
    "    mach = machine(model, X)\n",
    "    MLJ.fit!(mach, rows=train) \n",
    "    d = MLJ.transform(mach, rows=train)\n",
    "    \n",
    "    r = mach.report\n",
    "    push!(ex_vars, r[:tprincipalvar] / r[:tvar])   \n",
    "    \n",
    "    sqerr = (train_data - (Array(d) * fitted_params(mach)[1]' .+ r[:mean]')).^2 |> sum\n",
    "    push!(err_vars, sqerr) \n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#elbow method\n",
    "plot(1:max_dims, ex_vars, label=\"Explained Variance\", legend=false)\n",
    "ylims!(0,1)\n",
    "yticks!(0:0.1:1)\n",
    "xticks!(2:2:30)\n",
    "xlabel!(\"Principal Components\")\n",
    "ylabel!(\"Explained Variance vs. No. Components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_pca_explained_variance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weak elbow, that has low explained variance. probably going to opt for around 20 after which it doesn't improve quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:max_dims, err_vars, legend=false)\n",
    "xlabel!(\"Principal Dimensions\")\n",
    "ylabel!(\"Squared Projection Loss\")\n",
    "xticks!(1:max_dims)\n",
    "xticks!(2:2:30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_pca_sqerr\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualize top 2 or 3 principal components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(mach.report[:principalvars][1:3]) / sum(mach.report[:principalvars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.maxoutdim = 3\n",
    "mach = machine(model, X)\n",
    "MLJ.fit!(mach, rows=train) \n",
    "data_trans = MLJ.transform(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plotattr(:Series)\n",
    "plotattr(\"markercolor\")\n",
    "plotattr(\"seriescolor\")\n",
    "plotattr(\"camera\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(data_trans[:,1],data_trans[:,2],data_trans[:,3], leg=false, c=y_1h, camera=(60,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_pca_$(model.maxoutdim)_comps_3D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scatter(data_trans[:,1],data_trans[:,2],data_trans[:,3], leg=false, c=y_1h, camera=(50,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PCA_best_comps = 22;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.maxoutdim = PCA_best_comps\n",
    "mach = machine(model, X)\n",
    "MLJ.fit!(mach, rows=train) \n",
    "PCA_data = MLJ.transform(mach, rows=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info(\"ICA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = @load ICA pkg=\"MultivariateStats\" \n",
    "model.k = 2\n",
    "model.do_whiten = true\n",
    "model.maxiter= 200\n",
    "mach = machine(model, X)\n",
    "MLJ.fit!(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "report(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mach.fitresult.W, mach.fitresult.mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_trans = MLJ.transform(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kurtosis(convert(Array, data_trans))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_comps = 10\n",
    "model.maxiter= 1000\n",
    "kurts = []\n",
    "best_kurt = -Inf\n",
    "best_ica_mach = nothing\n",
    "best_i = 0\n",
    "err_vars = []\n",
    "\n",
    "for i in 1:max_comps\n",
    "    @show i\n",
    "    model.k = i\n",
    "    model.do_whiten = true\n",
    "    mach = machine(model, X)\n",
    "    MLJ.fit!(mach, rows=train)\n",
    "    d = MLJ.transform(mach, rows=train)\n",
    "    r = mach.report\n",
    "    krtoes = kurtosis(convert(Array, d))\n",
    "    push!(kurts, krtoes)\n",
    "    \n",
    "    sqerr = (train_data - (Array(d) * mach.fitresult.W' .+ mach.fitresult.mean')).^2 |> sum\n",
    "    push!(err_vars, sqerr) \n",
    "    \n",
    "    if krtoes > best_kurt\n",
    "        best_ica_mach = mach\n",
    "        best_i = i\n",
    "        best_kurt = krtoes\n",
    "    end    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kurts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_i, best_kurt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ica_mach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(kurts, legend=false, xlabel=\"Components\", ylabel=\"Kurtosis\", title=\"ICA Total Kurtosis vs. No. of Components\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_ica_kurtosis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:max_comps, err_vars, legend=false)\n",
    "xlabel!(\"Independent Dimensions\")\n",
    "ylabel!(\"Squared Projection Loss\")\n",
    "xticks!(1:max_dims)\n",
    "# ylims!(0.5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_ica_sqerr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_trans = MLJ.transform(best_ica_mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_component = 0\n",
    "best_k_so_far = 0\n",
    "second_best = 0\n",
    "for i in 1:size(ica_trans)[2]\n",
    "    curr_k = kurtosis(ica_trans[:,i])\n",
    "    @show i curr_k\n",
    "    if curr_k > best_k_so_far\n",
    "        second_best = best_component\n",
    "        best_component = i\n",
    "        best_k_so_far = curr_k\n",
    "    end\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "weight_mat = best_ica_mach.fitresult.W[:,best_component]\n",
    "\n",
    "# https://stackoverflow.com/questions/3989016/how-to-find-all-positions-of-the-maximum-value-in-a-list\n",
    "m = sort!(collect(zip((abs.(weight_mat)), 1:size(weight_mat)[1])), by=x->x[1], rev=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "largest weighted features above deal with the number of carbon bonds and the number of halogen atoms in the molecule. these are actually largely independent, e.g. one is not a physical property that could be influenced by the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "marginalscatter(ica_trans[:,best_component],ica_trans[:,second_best], leg=false, c=y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_ica_pair\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 components have very high kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "pairplot(data_trans, y_1h, c=1:6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = @load ICA pkg=\"MultivariateStats\" \n",
    "model.k = best_i\n",
    "model.do_whiten = true\n",
    "model.maxiter= 1000\n",
    "best_ica_mach = machine(model, X)\n",
    "MLJ.fit!(best_ica_mach, rows=train)\n",
    "ICA_data = MLJ.transform(best_ica_mach, rows=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Projections\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.random_projection.SparseRandomProjection.html#sklearn.random_projection.SparseRandomProjection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_rp = pyimport(\"sklearn.random_projection\") # may be necessary to run this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RP = sklearn_rp.SparseRandomProjection(n_components=6, random_state=RNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_RP = RP.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data - convert(Array, X_RP * RP.components_)).^2 |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot reconstruction error\n",
    "max_dims=25\n",
    "err_vars_all = []\n",
    "\n",
    "for z in rand(1:1000, 7)\n",
    "    err_vars = []\n",
    "\n",
    "    for i in 1:max_dims\n",
    "        RP = sklearn_rp.SparseRandomProjection(n_components=i, random_state=z)\n",
    "        X_RP = RP.fit_transform(train_data)\n",
    "        sqerr = (train_data - convert(Array, X_RP * RP.components_)).^2 |> sum\n",
    "\n",
    "        push!(err_vars, sqerr)   \n",
    "    end\n",
    "    push!(err_vars_all, err_vars)\n",
    "end\n",
    "\n",
    "# https://stackoverflow.com/questions/36566844/pca-projection-and-reconstruction-in-scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:max_dims, err_vars_all, legend=false)\n",
    "xlabel!(\"Random Dimensions\")\n",
    "ylabel!(\"Squared Projection Loss\")\n",
    "xticks!(1:2:max_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_rp_sqerr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RP = sklearn_rp.SparseRandomProjection(n_components=23, random_state=RNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RP_data = RP.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic PCA\n",
    "* https://multivariatestatsjl.readthedocs.io/en/stable/kpca.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models(\"MultivariateStats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "info(\"PPCA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = @load PPCA pkg=\"MultivariateStats\"\n",
    "model.maxoutdim = 8\n",
    "model.method = :ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach = machine(model, X)\n",
    "MLJ.fit!(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "report(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fitted_params(mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fitted_params(mach)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = MLJ.transform(mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data - (Array(d) * fitted_params(mach)[1]' .+ report(mach)[:mean]')).^2 |> sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot reconstruction error\n",
    "max_dims=15\n",
    "err_vars = []\n",
    "\n",
    "for i in 1:max_dims\n",
    "    model.maxoutdim = i\n",
    "    mach = machine(model, X)\n",
    "    MLJ.fit!(mach, rows=train) \n",
    "    d = MLJ.transform(mach, rows=train)\n",
    "    r = mach.report\n",
    "    \n",
    "    sqerr = (train_data - (Array(d) * fitted_params(mach)[1]' .+ r[:mean]')).^2 |> sum\n",
    "    push!(err_vars, sqerr) \n",
    "    \n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(1:max_dims, err_vars, legend=false)\n",
    "xlabel!(\"Principal Dimensions\")\n",
    "ylabel!(\"Squared Projection Loss\")\n",
    "xticks!(2:2:max_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_ppca_sqerr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PPCA_best_dim = 7\n",
    "model.maxoutdim = PPCA_best_dim\n",
    "model.method = :ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mach = machine(model, X)\n",
    "MLJ.fit!(mach, rows=train)\n",
    "PPCA_data = MLJ.transform(mach, rows=train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Pt 2\n",
    "Reproduce your clustering experiments, but on the data after you've run dimensionality reduction on it. Yes, that’s 16 combinations of datasets, dimensionality reduction, and clustering method. You should look at all of them, but focus on the more interesting findings in your report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PCA_data = convert(Array, PCA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = 7\n",
    "k_range = 2:upper\n",
    "total_costs = []\n",
    "sils = []\n",
    "ls = []\n",
    "sil_means = []\n",
    "assignments = []\n",
    "\n",
    "for i in k_range\n",
    "    println(\"K = $i\")\n",
    "    model = ParallelKMeans.KMeans(k=i, rng=RNG)\n",
    "    mach = machine(model, PCA_data)\n",
    "    MLJ.fit!(mach)\n",
    "    \n",
    "#     @show report(mach) \n",
    "#     @show fitted_params(mach)\n",
    "    @show mach.report.totalcost # https://github.com/PyDataBlog/ParallelKMeans.jl/blob/87ce07d10796078aacffcbea0b2e9dc0c02f25d7/src/hamerly.jl#L65\n",
    "    d = countmap(mach.report.assignments)\n",
    "    \n",
    "    k = d |> keys\n",
    "    v = d |> values\n",
    "    l = sort(collect(zip(k,v)), by=x->x[1])\n",
    "    @show l\n",
    "    \n",
    "    # https://juliastats.org/Clustering.jl/stable/validate.html\n",
    "    s = silhouettes(mach.report.assignments, dist_mat(PCA_data))\n",
    "    println(\"silhouette: $(mean(s))\")\n",
    "    \n",
    "    push!(assignments, mach.report.assignments)\n",
    "    push!(ls, l)\n",
    "    push!(total_costs, mach.report.totalcost) \n",
    "    push!(sils, s)\n",
    "    push!(sil_means, mean(s))\n",
    "    println(\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(k_range, total_costs, legend=:bottomleft, label=\"Total Squared Error\", color=:green, lw=2)\n",
    "xlabel!(\"Number of Clusters\")\n",
    "plot!(twinx(), k_range, sil_means, legend=:topright, label=\"Silhouette Score\", color=:blue, lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_pca_kmeans_metrics_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = prepare_portfolio(ls)\n",
    "plotmat = cum_columns(mat, normalize=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cum_plot(plotmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_pca_kmeans_portfolio_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_1h = map(x-> if (x == \"RB\") 1 else 2 end , y[train])\n",
    "collect(zip(assignments[2], y_1h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmeasure(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randindex(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA - EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upper = 8\n",
    "k_range = 2:upper\n",
    "bics = []\n",
    "sils = []\n",
    "ls = []\n",
    "sil_means = []\n",
    "assignments = []\n",
    "\n",
    "for i in k_range\n",
    "    println(\"Gaussians = $i\")\n",
    "    clf = sklearn_m.GaussianMixture(n_components=i, covariance_type=\"full\", random_state=RNG)\n",
    "    labels = clf.fit_predict(PCA_data) .+ 1; # indexing issues from python to julia functions\n",
    "    \n",
    "    d = countmap(labels)\n",
    "    k = d |> keys\n",
    "    v = d |> values\n",
    "    l = sort(collect(zip(k,v)), by=x->x[1])\n",
    "    @show l\n",
    "    \n",
    "    s = silhouettes(labels, dist_mat(PCA_data))\n",
    "    println(\"silhouette: $(mean(s))\")\n",
    "    \n",
    "    bayes_ic = clf.bic(PCA_data)\n",
    "    @show bayes_ic\n",
    "    \n",
    "    push!(ls, l)\n",
    "    push!(sils, s)\n",
    "    push!(sil_means, mean(s))\n",
    "    push!(bics, bayes_ic)\n",
    "    push!(assignments, labels)\n",
    "    println(\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(k_range, bics, legend=:bottomleft, label=\"Bayesian Info Criteria\", color=:green, lw=2)\n",
    "xlabel!(\"Number of Clusters\")\n",
    "plot!(twinx(),k_range, sil_means, legend=:topright, label=\"Silhouette Score\", color=:blue, lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_pca_em_metrics_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = prepare_portfolio(ls)\n",
    "plotmat = cum_columns(mat, normalize=true)\n",
    "cum_plot(plotmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_pca_em_portfolio_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_1h = map(x-> if (x == \"RB\") 1 else 2 end , y[train])\n",
    "collect(zip(assignments[2], y_1h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmeasure(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randindex(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICA - Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ICA_data = convert(Array, ICA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = 8\n",
    "k_range = 2:upper\n",
    "total_costs = []\n",
    "sils = []\n",
    "ls = []\n",
    "sil_means = []\n",
    "assignments = []\n",
    "\n",
    "for i in k_range\n",
    "    println(\"K = $i\")\n",
    "    model = ParallelKMeans.KMeans(k=i, rng=RNG)\n",
    "    mach = machine(model, ICA_data)\n",
    "    MLJ.fit!(mach)\n",
    "    \n",
    "#     @show report(mach) \n",
    "#     @show fitted_params(mach)\n",
    "    @show mach.report.totalcost # https://github.com/PyDataBlog/ParallelKMeans.jl/blob/87ce07d10796078aacffcbea0b2e9dc0c02f25d7/src/hamerly.jl#L65\n",
    "    d = countmap(mach.report.assignments)\n",
    "    \n",
    "    k = d |> keys\n",
    "    v = d |> values\n",
    "    l = sort(collect(zip(k,v)), by=x->x[1])\n",
    "    @show l\n",
    "    \n",
    "    # https://juliastats.org/Clustering.jl/stable/validate.html\n",
    "    s = silhouettes(mach.report.assignments, dist_mat(ICA_data))\n",
    "    println(\"silhouette: $(mean(s))\")\n",
    "    \n",
    "    push!(assignments, mach.report.assignments)\n",
    "    push!(ls, l)\n",
    "    push!(total_costs, mach.report.totalcost) \n",
    "    push!(sils, s)\n",
    "    push!(sil_means, mean(s))\n",
    "    println(\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(k_range, total_costs, legend=:bottomleft, label=\"Total Squared Error\", color=:green, lw=2)\n",
    "xlabel!(\"Number of Clusters\")\n",
    "plot!(twinx(), k_range, sil_means, legend=:topright, label=\"Silhouette Score\", color=:blue, lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_ica_kmeans_metrics_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = prepare_portfolio(ls)\n",
    "plotmat = cum_columns(mat, normalize=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cum_plot(plotmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_ica_kmeans_portfolio_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_1h = map(x-> if (x == \"RB\") 1 else 2 end , y[train])\n",
    "collect(zip(assignments[2], y_1h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmeasure(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randindex(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICA - EM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upper = 8\n",
    "k_range = 2:upper\n",
    "bics = []\n",
    "sils = []\n",
    "ls = []\n",
    "sil_means = []\n",
    "assignments = []\n",
    "\n",
    "for i in k_range\n",
    "    println(\"Gaussians = $i\")\n",
    "    clf = sklearn_m.GaussianMixture(n_components=i, covariance_type=\"full\", random_state=RNG)\n",
    "    labels = clf.fit_predict(PCA_data) .+ 1; # indexing issues from python to julia functions\n",
    "    \n",
    "    d = countmap(labels)\n",
    "    k = d |> keys\n",
    "    v = d |> values\n",
    "    l = sort(collect(zip(k,v)), by=x->x[1])\n",
    "    @show l\n",
    "    \n",
    "    s = silhouettes(labels, dist_mat(PCA_data))\n",
    "    println(\"silhouette: $(mean(s))\")\n",
    "    \n",
    "    bayes_ic = clf.bic(PCA_data)\n",
    "    @show bayes_ic\n",
    "    \n",
    "    push!(ls, l)\n",
    "    push!(sils, s)\n",
    "    push!(sil_means, mean(s))\n",
    "    push!(bics, bayes_ic)\n",
    "    push!(assignments, labels)\n",
    "    println(\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(k_range, bics, legend=:bottomleft, label=\"Bayesian Info Criteria\", color=:green, lw=2)\n",
    "xlabel!(\"Number of Clusters\")\n",
    "plot!(twinx(),k_range, sil_means, legend=:topright, label=\"Silhouette Score\", color=:blue, lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_ica_em_metrics_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = prepare_portfolio(ls)\n",
    "plotmat = cum_columns(mat, normalize=true)\n",
    "cum_plot(plotmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_ica_em_portfolio_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_1h = map(x-> if (x == \"RB\") 1 else 2 end , y[train])\n",
    "collect(zip(assignments[2], y_1h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmeasure(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randindex(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Projections - Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = 8\n",
    "k_range = 2:upper\n",
    "total_costs = []\n",
    "sils = []\n",
    "ls = []\n",
    "sil_means = []\n",
    "assignments = []\n",
    "\n",
    "for i in k_range\n",
    "    println(\"K = $i\")\n",
    "    model = ParallelKMeans.KMeans(k=i, rng=RNG)\n",
    "    mach = machine(model, RP_data)\n",
    "    MLJ.fit!(mach)\n",
    "    \n",
    "#     @show report(mach) \n",
    "#     @show fitted_params(mach)\n",
    "    @show mach.report.totalcost # https://github.com/PyDataBlog/ParallelKMeans.jl/blob/87ce07d10796078aacffcbea0b2e9dc0c02f25d7/src/hamerly.jl#L65\n",
    "    d = countmap(mach.report.assignments)\n",
    "    \n",
    "    k = d |> keys\n",
    "    v = d |> values\n",
    "    l = sort(collect(zip(k,v)), by=x->x[1])\n",
    "    @show l\n",
    "    \n",
    "    # https://juliastats.org/Clustering.jl/stable/validate.html\n",
    "    s = silhouettes(mach.report.assignments, dist_mat(RP_data))\n",
    "    println(\"silhouette: $(mean(s))\")\n",
    "    \n",
    "    push!(assignments, mach.report.assignments)\n",
    "    push!(ls, l)\n",
    "    push!(total_costs, mach.report.totalcost) \n",
    "    push!(sils, s)\n",
    "    push!(sil_means, mean(s))\n",
    "    println(\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(k_range, total_costs, legend=:bottomleft, label=\"Total Squared Error\", color=:green, lw=2)\n",
    "xlabel!(\"Number of Clusters\")\n",
    "plot!(twinx(), k_range, sil_means, legend=:topright, label=\"Silhouette Score\", color=:blue, lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_rp_kmeans_metrics_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = prepare_portfolio(ls)\n",
    "plotmat = cum_columns(mat, normalize=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cum_plot(plotmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_rp_kmeans_portfolio_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_1h = map(x-> if (x == \"RB\") 1 else 2 end , y[train])\n",
    "collect(zip(assignments[2], y_1h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmeasure(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randindex(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized Projections - EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upper = 8\n",
    "k_range = 2:upper\n",
    "bics = []\n",
    "sils = []\n",
    "ls = []\n",
    "sil_means = []\n",
    "assignments = []\n",
    "\n",
    "for i in k_range\n",
    "    println(\"Gaussians = $i\")\n",
    "    clf = sklearn_m.GaussianMixture(n_components=i, covariance_type=\"full\", random_state=RNG)\n",
    "    labels = clf.fit_predict(RP_data) .+ 1; # indexing issues from python to julia functions\n",
    "    \n",
    "    d = countmap(labels)\n",
    "    k = d |> keys\n",
    "    v = d |> values\n",
    "    l = sort(collect(zip(k,v)), by=x->x[1])\n",
    "    @show l\n",
    "    \n",
    "    s = silhouettes(labels, dist_mat(RP_data))\n",
    "    println(\"silhouette: $(mean(s))\")\n",
    "    \n",
    "    bayes_ic = clf.bic(RP_data)\n",
    "    @show bayes_ic\n",
    "    \n",
    "    push!(ls, l)\n",
    "    push!(sils, s)\n",
    "    push!(sil_means, mean(s))\n",
    "    push!(bics, bayes_ic)\n",
    "    push!(assignments, labels)\n",
    "    println(\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(k_range, bics, legend=:bottomleft, label=\"Bayesian Info Criteria\", color=:green, lw=2)\n",
    "xlabel!(\"Number of Clusters\")\n",
    "plot!(twinx(),k_range, sil_means, legend=:topright, label=\"Silhouette Score\", color=:blue, lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_rp_em_metrics_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = prepare_portfolio(ls)\n",
    "plotmat = cum_columns(mat, normalize=true)\n",
    "cum_plot(plotmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_rp_em_portfolio_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_1h = map(x-> if (x == \"RB\") 1 else 2 end , y[train])\n",
    "collect(zip(assignments[2], y_1h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmeasure(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randindex(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPCA - Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PPCA_data = Array(PPCA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = 8\n",
    "k_range = 2:upper\n",
    "total_costs = []\n",
    "sils = []\n",
    "ls = []\n",
    "sil_means = []\n",
    "assignments = []\n",
    "\n",
    "for i in k_range\n",
    "    println(\"K = $i\")\n",
    "    model = ParallelKMeans.KMeans(k=i, rng=RNG)\n",
    "    mach = machine(model, PPCA_data)\n",
    "    MLJ.fit!(mach)\n",
    "    \n",
    "#     @show report(mach) \n",
    "#     @show fitted_params(mach)\n",
    "    @show mach.report.totalcost # https://github.com/PyDataBlog/ParallelKMeans.jl/blob/87ce07d10796078aacffcbea0b2e9dc0c02f25d7/src/hamerly.jl#L65\n",
    "    d = countmap(mach.report.assignments)\n",
    "    \n",
    "    k = d |> keys\n",
    "    v = d |> values\n",
    "    l = sort(collect(zip(k,v)), by=x->x[1])\n",
    "    @show l\n",
    "    \n",
    "    # https://juliastats.org/Clustering.jl/stable/validate.html\n",
    "    s = silhouettes(mach.report.assignments, dist_mat(PPCA_data))\n",
    "    println(\"silhouette: $(mean(s))\")\n",
    "    \n",
    "    push!(assignments, mach.report.assignments)\n",
    "    push!(ls, l)\n",
    "    push!(total_costs, mach.report.totalcost) \n",
    "    push!(sils, s)\n",
    "    push!(sil_means, mean(s))\n",
    "    println(\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "values for loss are likely a lot smaller because there are fewer dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(k_range, total_costs, legend=:bottomleft, label=\"Total Squared Error\", color=:green, lw=2)\n",
    "xlabel!(\"Number of Clusters\")\n",
    "plot!(twinx(), k_range, sil_means, legend=:topright, label=\"Silhouette Score\", color=:blue, lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_ppca_kmeans_metrics_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = prepare_portfolio(ls)\n",
    "plotmat = cum_columns(mat, normalize=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cum_plot(plotmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_ppca_kmeans_portfolio_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_1h = map(x-> if (x == \"RB\") 1 else 2 end , y[train])\n",
    "collect(zip(assignments[2], y_1h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmeasure(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randindex(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPCA - EM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "upper = 8\n",
    "k_range = 2:upper\n",
    "bics = []\n",
    "sils = []\n",
    "ls = []\n",
    "sil_means = []\n",
    "assignments = []\n",
    "\n",
    "for i in k_range\n",
    "    println(\"Gaussians = $i\")\n",
    "    clf = sklearn_m.GaussianMixture(n_components=i, covariance_type=\"full\", random_state=RNG)\n",
    "    labels = clf.fit_predict(PPCA_data) .+ 1; # indexing issues from python to julia functions\n",
    "    \n",
    "    d = countmap(labels)\n",
    "    k = d |> keys\n",
    "    v = d |> values\n",
    "    l = sort(collect(zip(k,v)), by=x->x[1])\n",
    "    @show l\n",
    "    \n",
    "    s = silhouettes(labels, dist_mat(PPCA_data))\n",
    "    println(\"silhouette: $(mean(s))\")\n",
    "    \n",
    "    bayes_ic = clf.bic(PPCA_data)\n",
    "    @show bayes_ic\n",
    "    \n",
    "    push!(ls, l)\n",
    "    push!(sils, s)\n",
    "    push!(sil_means, mean(s))\n",
    "    push!(bics, bayes_ic)\n",
    "    push!(assignments, labels)\n",
    "    println(\"\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(k_range, bics, legend=:bottomleft, label=\"Bayesian Info Criteria\", color=:green, lw=2)\n",
    "xlabel!(\"Number of Clusters\")\n",
    "plot!(twinx(),k_range, sil_means, legend=:topright, label=\"Silhouette Score\", color=:blue, lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_ppca_em_metrics_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = prepare_portfolio(ls)\n",
    "plotmat = cum_columns(mat, normalize=true)\n",
    "cum_plot(plotmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefig(\"figures/bal_ppca_em_portfolio_$upper\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_1h = map(x-> if (x == \"RB\") 1 else 2 end , y[train])\n",
    "collect(zip(assignments[2], y_1h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutualinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmeasure(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randindex(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "varinfo(assignments[2], y_1h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BELOW ONLY FOR 1 DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimensionality Reduction + NN \n",
    "Apply the dimensionality reduction algorithms to one of your datasets from assignment #1. (if you've reused the datasets from assignment #1 to do experiments 1-3 above then you've already done this) and rerun your neural network learner on the newly projected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using MLJFlux\n",
    "using Flux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc(ŷ, y) = (mode.(ŷ) .== y) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom network\n",
    "mutable struct CustomNN <:MLJFlux.Builder\n",
    "    n1 ::Int\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function MLJFlux.build(nn::CustomNN, n_in, n_out)\n",
    "    return Chain(\n",
    "        Flux.Dense(n_in, nn.n1, σ),\n",
    "        Flux.Dense(nn.n1, n_out, σ)\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@load NeuralNetworkClassifier\n",
    "# nn = NeuralNetworkClassifier(builder=CustomNN(132))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sz = 16;\n",
    "max_epochs = 1000;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipe = @pipeline PCA2 NeuralNetworkClassifier(builder=CustomNN(132))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipe.pca.maxoutdim = PCA_best_comps\n",
    "pca_pipe.neural_network_classifier.batch_size = batch_sz;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipe.neural_network_classifier.epochs = max_epochs\n",
    "pca_pipe.neural_network_classifier.lambda = 0.01\n",
    "pca_pipe.neural_network_classifier.optimiser.eta = 0.0015;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_nn_mach = machine(pca_pipe, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(pca_nn_mach, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = :(neural_network_classifier.epochs)\n",
    "param2 = :(neural_network_classifier.optimiser.eta)\n",
    "\n",
    "r1 = range(pca_pipe, param1, lower=10, upper=max_epochs, scale=:linear)\n",
    "r2 = range(pca_pipe, param2, lower=0.001, upper=0.01, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_nn_tuning = TunedModel(model=pca_pipe,\n",
    "                                    tuning=Grid(goal=144, rng=RNG),\n",
    "                                    resampling=Holdout(fraction_train=0.7), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1, r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_nn_tuning_mach = machine(pca_nn_tuning, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(pca_nn_tuning_mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(pca_nn_tuning_mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int1 = fitted_params(pca_nn_tuning_mach)[:best_model].neural_network_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipe.neural_network_classifier = int1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = :(neural_network_classifier.epochs)\n",
    "param2 = :(neural_network_classifier.lambda)\n",
    "\n",
    "r1 = range(pca_pipe, param1, lower=10, upper=max_epochs, scale=:linear)\n",
    "r2 = range(pca_pipe, param2, lower=0.001, upper=0.5, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca_nn_tuning = TunedModel(model=pca_pipe,\n",
    "                                    tuning=Grid(goal=144, rng=RNG),\n",
    "                                    resampling=Holdout(fraction_train=0.7), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1, r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_nn_tuning_mach = machine(pca_nn_tuning, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = fit!(pca_nn_tuning_mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(pca_nn_tuning_mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2 = fitted_params(pca_nn_tuning_mach)[:best_model].neural_network_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipe.neural_network_classifier = int2\n",
    "pca_nn_mach = machine(pca_pipe, X, y)\n",
    "pca_nn_acc = evaluate!(pca_nn_mach, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = collect(0:5:max_epochs)\n",
    "r = range(pca_pipe, :(neural_network_classifier.epochs), lower=1, upper=max_epochs, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve = MLJ.learning_curve(pca_nn_mach, \n",
    "                        range=r, \n",
    "#                         resampling=Holdout(fraction_train=0.7), \n",
    "                        resampling=CV(nfolds=4), \n",
    "                        measure=cross_entropy, \n",
    "                        acceleration=CPUProcesses()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(Net.report.training_losses, label=\"Training\", lw=2)\n",
    "plot(curve.parameter_values,\n",
    "     curve.measurements,\n",
    "     xlab=curve.parameter_name,\n",
    "     ylab=\"Cross Entropy\",\n",
    "     label=\"Validation\", lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = round(pca_pipe.neural_network_classifier.optimiser.eta, digits=5)\n",
    "b = round(minimum(curve.measurements), digits=5)\n",
    "best_epochs = curve.parameter_values[argmin(curve.measurements)]\n",
    "a,b, best_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_pipe.neural_network_classifier.epochs = best_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Net = machine(pca_pipe, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(Final_Net, rows=train, force=true, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_acc = evaluate!(Final_Net, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = MLJ.predict(Final_Net, X[test,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy(ŷ, y[test]) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc(ŷ, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = confusion_matrix(mode.(ŷ), y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision(c),recall(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_pipe = NeuralNetworkClassifier(builder=CustomNN(132))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_pipe.batch_size = batch_sz\n",
    "ica_pipe.epochs = max_epochs\n",
    "ica_pipe.lambda = 0.01\n",
    "ica_pipe.optimiser.eta = 0.0015;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_nn_mach = machine(ica_pipe, ica_trans, y[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(ica_nn_mach, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = :(epochs)\n",
    "param2 = :(optimiser.eta)\n",
    "\n",
    "r1 = range(ica_pipe, param1, lower=10, upper=max_epochs, scale=:linear)\n",
    "r2 = range(ica_pipe, param2, lower=10^-3, upper=10^-1.4, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ica_nn_tuning = TunedModel(model=ica_pipe,\n",
    "                                    tuning=Grid(goal=289, rng=RNG),\n",
    "                                    resampling=Holdout(fraction_train=0.7, rng=RNG), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1, r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_nn_tuning_mach = machine(ica_nn_tuning, ica_trans, y[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(ica_nn_tuning_mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(ica_nn_tuning_mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int1 = fitted_params(ica_nn_tuning_mach)[:best_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_pipe = int1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = :(epochs)\n",
    "param2 = :(lambda)\n",
    "\n",
    "r1 = range(ica_pipe, param1, lower=10, upper=max_epochs, scale=:linear)\n",
    "r2 = range(ica_pipe, param2, lower=0.001, upper=0.3, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ica_nn_tuning = TunedModel(model=ica_pipe,\n",
    "                                    tuning=Grid(goal=144, rng=RNG),\n",
    "                                    resampling=Holdout(fraction_train=0.7), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1, r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_nn_tuning_mach = machine(ica_nn_tuning, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = fit!(ica_nn_tuning_mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(ica_nn_tuning_mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2 = fitted_params(ica_nn_tuning_mach)[:best_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_pipe = int2\n",
    "ica_nn_mach = machine(ica_pipe, X, y)\n",
    "ica_nn_acc = evaluate!(ica_nn_mach, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = collect(0:5:max_epochs)\n",
    "r = range(ica_pipe, :(epochs), lower=1, upper=max_epochs, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve = MLJ.learning_curve(ica_nn_mach, \n",
    "                        range=r, \n",
    "#                         resampling=Holdout(fraction_train=0.7), \n",
    "                        resampling=CV(nfolds=4), \n",
    "                        measure=cross_entropy, \n",
    "                        acceleration=CPUProcesses()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(Net.report.training_losses, label=\"Training\", lw=2)\n",
    "plot(curve.parameter_values,\n",
    "     curve.measurements,\n",
    "     xlab=curve.parameter_name,\n",
    "     ylab=\"Cross Entropy\",\n",
    "     label=\"Validation\", lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = round(ica_pipe.optimiser.eta, digits=5)\n",
    "b = round(minimum(curve.measurements), digits=5)\n",
    "best_epochs = curve.parameter_values[argmin(curve.measurements)]\n",
    "a,b, best_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_pipe.epochs = best_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Net = machine(ica_pipe, ica_trans, y[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(Final_Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_acc = evaluate!(Final_Net, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ica_nn_test = MLJ.transform(best_ica_mach, rows=test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = MLJ.predict(Final_Net, ica_nn_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy(ŷ, y[test]) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc(ŷ, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = confusion_matrix(mode.(ŷ), y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision(c),recall(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "RP_data = DataFrame(RP_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_pipe = NeuralNetworkClassifier(builder=CustomNN(132))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_pipe.batch_size = batch_sz\n",
    "rp_pipe.epochs = max_epochs\n",
    "rp_pipe.lambda = 0.01\n",
    "rp_pipe.optimiser.eta = 0.0015;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_nn_mach = machine(rp_pipe, RP_data, y[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(rp_nn_mach, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = :(epochs)\n",
    "param2 = :(optimiser.eta)\n",
    "\n",
    "r1 = range(rp_pipe, param1, lower=10, upper=max_epochs, scale=:linear)\n",
    "r2 = range(rp_pipe, param2, lower=10^-3, upper=10^-1.4, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rp_nn_tuning = TunedModel(model=rp_pipe,\n",
    "                                    tuning=Grid(goal=289, rng=RNG),\n",
    "                                    resampling=Holdout(fraction_train=0.7, rng=RNG), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1, r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_nn_tuning_mach = machine(rp_nn_tuning, RP_data, y[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(rp_nn_tuning_mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(rp_nn_tuning_mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int1 = fitted_params(rp_nn_tuning_mach)[:best_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_pipe = int1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = :(epochs)\n",
    "param2 = :(lambda)\n",
    "\n",
    "r1 = range(rp_pipe, param1, lower=10, upper=max_epochs, scale=:linear)\n",
    "r2 = range(rp_pipe, param2, lower=0.001, upper=0.3, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rp_nn_tuning = TunedModel(model=rp_pipe,\n",
    "                                    tuning=Grid(goal=144, rng=RNG),\n",
    "                                    resampling=Holdout(fraction_train=0.7), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1, r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_nn_tuning_mach = machine(rp_nn_tuning, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = fit!(rp_nn_tuning_mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(rp_nn_tuning_mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2 = fitted_params(rp_nn_tuning_mach)[:best_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_pipe = int2\n",
    "rp_nn_mach = machine(rp_pipe, RP_data, y[train])\n",
    "rp_nn_acc = evaluate!(rp_nn_mach, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = collect(0:5:max_epochs)\n",
    "r = range(rp_pipe, :(epochs), lower=1, upper=max_epochs, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve = MLJ.learning_curve(rp_nn_mach, \n",
    "                        range=r, \n",
    "#                         resampling=Holdout(fraction_train=0.7), \n",
    "                        resampling=CV(nfolds=4), \n",
    "                        measure=cross_entropy, \n",
    "                        acceleration=CPUProcesses()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(Net.report.training_losses, label=\"Training\", lw=2)\n",
    "plot(curve.parameter_values,\n",
    "     curve.measurements,\n",
    "     xlab=curve.parameter_name,\n",
    "     ylab=\"Cross Entropy\",\n",
    "     label=\"Validation\", lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = round(rp_pipe.optimiser.eta, digits=5)\n",
    "b = round(minimum(curve.measurements), digits=5)\n",
    "best_epochs = curve.parameter_values[argmin(curve.measurements)]\n",
    "a,b, best_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_pipe.epochs = best_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Net = machine(rp_pipe, RP_data, y[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(Final_Net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_acc = evaluate!(Final_Net, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_data = convert(Matrix,X[test,:])\n",
    "RP_test = RP.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = MLJ.predict(Final_Net, RP_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy(ŷ, y[test]) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc(ŷ, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = confusion_matrix(mode.(ŷ), y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision(c),recall(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@load PPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppca_pipe = @pipeline PPCA2 NeuralNetworkClassifier(builder=CustomNN(132))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppca_pipe.ppca.maxoutdim = PPCA_best_dim\n",
    "ppca_pipe.neural_network_classifier.batch_size = batch_sz;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppca_pipe.neural_network_classifier.epochs = max_epochs\n",
    "ppca_pipe.neural_network_classifier.lambda = 0.01\n",
    "ppca_pipe.neural_network_classifier.optimiser.eta = 0.0015;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppca_nn_mach = machine(ppca_pipe, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(ppca_nn_mach, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = :(neural_network_classifier.epochs)\n",
    "param2 = :(neural_network_classifier.optimiser.eta)\n",
    "\n",
    "r1 = range(ppca_pipe, param1, lower=10, upper=max_epochs, scale=:linear)\n",
    "r2 = range(ppca_pipe, param2, lower=10^-2.5, upper=0.01, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppca_nn_tuning = TunedModel(model=ppca_pipe,\n",
    "                                    tuning=Grid(goal=144, rng=RNG),\n",
    "                                    resampling=Holdout(fraction_train=0.7), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1, r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppca_nn_tuning_mach = machine(ppca_nn_tuning, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(ppca_nn_tuning_mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(ppca_nn_tuning_mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int1 = fitted_params(ppca_nn_tuning_mach)[:best_model].neural_network_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppca_pipe.neural_network_classifier = int1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param1 = :(neural_network_classifier.epochs)\n",
    "param2 = :(neural_network_classifier.lambda)\n",
    "\n",
    "r1 = range(ppca_pipe, param1, lower=10, upper=max_epochs, scale=:linear)\n",
    "r2 = range(ppca_pipe, param2, lower=0.001, upper=0.5, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ppca_nn_tuning = TunedModel(model=ppca_pipe,\n",
    "                                    tuning=Grid(goal=144, rng=RNG),\n",
    "                                    resampling=Holdout(fraction_train=0.7), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1, r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppca_nn_tuning_mach = machine(ppca_nn_tuning, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = fit!(ppca_nn_tuning_mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot(ppca_nn_tuning_mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2 = fitted_params(ppca_nn_tuning_mach)[:best_model].neural_network_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppca_pipe.neural_network_classifier = int2\n",
    "ppca_nn_mach = machine(ppca_pipe, X, y)\n",
    "ppca_nn_acc = evaluate!(ppca_nn_mach, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vals = collect(0:5:max_epochs)\n",
    "r = range(ppca_pipe, :(neural_network_classifier.epochs), lower=1, upper=max_epochs, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curve = MLJ.learning_curve(ppca_nn_mach, \n",
    "                        range=r, \n",
    "#                         resampling=Holdout(fraction_train=0.7), \n",
    "                        resampling=CV(nfolds=4), \n",
    "                        measure=cross_entropy, \n",
    "                        acceleration=CPUProcesses()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot(Net.report.training_losses, label=\"Training\", lw=2)\n",
    "plot(curve.parameter_values,\n",
    "     curve.measurements,\n",
    "     xlab=curve.parameter_name,\n",
    "     ylab=\"Cross Entropy\",\n",
    "     label=\"Validation\", lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = round(ppca_pipe.neural_network_classifier.optimiser.eta, digits=5)\n",
    "b = round(minimum(curve.measurements), digits=5)\n",
    "best_epochs = curve.parameter_values[argmin(curve.measurements)]\n",
    "a,b, best_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppca_pipe.neural_network_classifier.epochs = best_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_Net = machine(ppca_pipe, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(Final_Net, rows=train, force=true, verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_acc = evaluate!(Final_Net, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = MLJ.predict(Final_Net, X[test,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy(ŷ, y[test]) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc(ŷ, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = confusion_matrix(mode.(ŷ), y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision(c),recall(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.2",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
