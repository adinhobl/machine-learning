{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using CSV\n",
    "using MLJ\n",
    "using Plots\n",
    "using StatsBase\n",
    "\n",
    "include(\"../../lib.jl\")\n",
    "\n",
    "ENV[\"LINES\"]=50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 2. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 3. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 4. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 5. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 6. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 7. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 8. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 9. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 10. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 11. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 12. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 13. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 14. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 15. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 16. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 17. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 18. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 19. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 20. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 21. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 22. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 23. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 24. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 25. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 26. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 27. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 28. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 29. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 30. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 31. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 32. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 33. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 34. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 35. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 36. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 37. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 38. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 39. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 40. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 41. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 42. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 43. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 44. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 45. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 46. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 47. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 48. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 49. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 50. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 51. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 52. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 53. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 54. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 55. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 56. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 57. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 58. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 59. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 60. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 61. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 62. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 63. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 64. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 65. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 66. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 67. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 68. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 69. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 70. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 71. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 72. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 73. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 74. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 75. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 76. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 77. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 78. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 79. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 80. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 81. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 82. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 83. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 84. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 85. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 86. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 87. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 88. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 89. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 90. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 91. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 92. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 93. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 94. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 95. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 96. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 97. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 98. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 99. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 100. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 101. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 102. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 103. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 104. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 105. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 106. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 107. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 108. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 109. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 110. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 111. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 112. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 113. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 114. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 115. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 116. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 117. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 118. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 119. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 120. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 121. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 122. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 123. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 124. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 125. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 126. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 127. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 128. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 129. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 130. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 131. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 132. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 133. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 134. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 135. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 136. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 137. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 138. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 139. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 140. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 141. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 142. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 143. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 144. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 145. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 146. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 147. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 148. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 149. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 150. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 151. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 152. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 153. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 154. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 155. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 156. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 157. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 158. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 159. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 160. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 161. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 162. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 163. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 164. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 165. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 166. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 167. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 168. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 169. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 170. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 171. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 172. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 173. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 174. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 175. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 176. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 177. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 178. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 179. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 180. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 181. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 182. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 183. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 184. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 185. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 186. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 187. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 188. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 189. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 190. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 191. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 192. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 193. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 194. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 195. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 196. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 197. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 198. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 199. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 200. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 201. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 202. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 203. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 204. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 205. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 206. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 207. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 208. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 209. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 210. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 211. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 212. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 213. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 214. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 215. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 216. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 217. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 218. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 219. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 220. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 221. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 222. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 223. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 224. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 225. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 226. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 227. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 228. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 229. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 230. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 231. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 232. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 233. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 234. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 235. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 236. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 237. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 238. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 239. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 240. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 241. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 242. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 243. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 244. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 245. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 246. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 247. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 248. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 249. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 250. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 251. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 252. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 253. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 254. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 255. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 256. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 257. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 258. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 259. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 260. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 261. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 262. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 263. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 264. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 265. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 266. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 267. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 268. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 269. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 270. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 271. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 272. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 273. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 274. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 275. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 276. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 277. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 278. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 279. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 280. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 281. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 282. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 283. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 284. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 285. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 286. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 287. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 288. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 289. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 290. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 291. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 292. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 293. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 294. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 295. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 296. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 297. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 298. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 299. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 300. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 301. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 302. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 303. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 304. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 305. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 306. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 307. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 308. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 309. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 310. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 311. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 312. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 313. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 314. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 315. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 316. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 317. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 318. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 319. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 320. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 321. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 322. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 323. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 324. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 325. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 326. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 327. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 328. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 329. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 330. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 331. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 332. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 333. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 334. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 335. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 336. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 337. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 338. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 339. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 340. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 341. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 342. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 343. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 344. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 345. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 346. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 347. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 348. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 349. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 350. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 351. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 352. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 353. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 354. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 355. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 356. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 357. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 358. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 359. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 360. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 361. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 362. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 363. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 364. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 365. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 366. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 367. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 368. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 369. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 370. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 371. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 372. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 373. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 374. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 375. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 376. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 377. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 378. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 379. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 380. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 381. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 382. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 383. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 384. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 385. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 386. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 387. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 388. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 389. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 390. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 391. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 392. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 393. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 394. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 395. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 396. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 397. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 398. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 399. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 400. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 401. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 402. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 403. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 404. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 405. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 406. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 407. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 408. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 409. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 410. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 411. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 412. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 413. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 414. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 415. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 416. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 417. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 418. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 419. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 420. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 421. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 422. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 423. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 424. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 425. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 426. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 427. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 428. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 429. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 430. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 431. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 432. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 433. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 434. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 435. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 436. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 437. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 438. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 439. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 440. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 441. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 442. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 443. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 444. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 445. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 446. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 447. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 448. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 449. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 450. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 451. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 452. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 453. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 454. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 455. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 456. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 457. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 458. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 459. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 460. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 461. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 462. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 463. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 464. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 465. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 466. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 467. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 468. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 469. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 470. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 471. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 472. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 473. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 474. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 475. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 476. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 477. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 478. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 479. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 480. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 481. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 482. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 483. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 484. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 485. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 486. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 487. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 488. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 489. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 490. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 491. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 492. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 493. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 494. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 495. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 496. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 497. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 498. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 499. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 500. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 501. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 502. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 503. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 504. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 505. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 506. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 507. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 508. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 509. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 510. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 511. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 512. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 513. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 514. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 515. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 516. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 517. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 518. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 519. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 520. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 521. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 522. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 523. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 524. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 525. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 526. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 527. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 528. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 529. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 530. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 531. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 532. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 533. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 534. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 535. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 536. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 537. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 538. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 539. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 540. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 541. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 542. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 543. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 544. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 545. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 546. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 547. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 548. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 549. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 550. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 551. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 552. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 553. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 554. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 555. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 556. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 557. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 558. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 559. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 560. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 561. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 562. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 563. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 564. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 565. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 566. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 567. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 568. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 569. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 570. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>id</th><th>diagnosis</th><th>radius_mean</th><th>texture_mean</th><th>perimeter_mean</th><th>area_mean</th><th>smoothness_mean</th></tr><tr><th></th><th>Int64</th><th>String</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>569 rows  33 columns (omitted printing of 26 columns)</p><tr><th>1</th><td>842302</td><td>M</td><td>17.99</td><td>10.38</td><td>122.8</td><td>1001.0</td><td>0.1184</td></tr><tr><th>2</th><td>842517</td><td>M</td><td>20.57</td><td>17.77</td><td>132.9</td><td>1326.0</td><td>0.08474</td></tr><tr><th>3</th><td>84300903</td><td>M</td><td>19.69</td><td>21.25</td><td>130.0</td><td>1203.0</td><td>0.1096</td></tr><tr><th>4</th><td>84348301</td><td>M</td><td>11.42</td><td>20.38</td><td>77.58</td><td>386.1</td><td>0.1425</td></tr><tr><th>5</th><td>84358402</td><td>M</td><td>20.29</td><td>14.34</td><td>135.1</td><td>1297.0</td><td>0.1003</td></tr><tr><th>6</th><td>843786</td><td>M</td><td>12.45</td><td>15.7</td><td>82.57</td><td>477.1</td><td>0.1278</td></tr><tr><th>7</th><td>844359</td><td>M</td><td>18.25</td><td>19.98</td><td>119.6</td><td>1040.0</td><td>0.09463</td></tr><tr><th>8</th><td>84458202</td><td>M</td><td>13.71</td><td>20.83</td><td>90.2</td><td>577.9</td><td>0.1189</td></tr><tr><th>9</th><td>844981</td><td>M</td><td>13.0</td><td>21.82</td><td>87.5</td><td>519.8</td><td>0.1273</td></tr><tr><th>10</th><td>84501001</td><td>M</td><td>12.46</td><td>24.04</td><td>83.97</td><td>475.9</td><td>0.1186</td></tr><tr><th>11</th><td>845636</td><td>M</td><td>16.02</td><td>23.24</td><td>102.7</td><td>797.8</td><td>0.08206</td></tr><tr><th>12</th><td>84610002</td><td>M</td><td>15.78</td><td>17.89</td><td>103.6</td><td>781.0</td><td>0.0971</td></tr><tr><th>13</th><td>846226</td><td>M</td><td>19.17</td><td>24.8</td><td>132.4</td><td>1123.0</td><td>0.0974</td></tr><tr><th>14</th><td>846381</td><td>M</td><td>15.85</td><td>23.95</td><td>103.7</td><td>782.7</td><td>0.08401</td></tr><tr><th>15</th><td>84667401</td><td>M</td><td>13.73</td><td>22.61</td><td>93.6</td><td>578.3</td><td>0.1131</td></tr><tr><th>16</th><td>84799002</td><td>M</td><td>14.54</td><td>27.54</td><td>96.73</td><td>658.8</td><td>0.1139</td></tr><tr><th>17</th><td>848406</td><td>M</td><td>14.68</td><td>20.13</td><td>94.74</td><td>684.5</td><td>0.09867</td></tr><tr><th>18</th><td>84862001</td><td>M</td><td>16.13</td><td>20.68</td><td>108.1</td><td>798.8</td><td>0.117</td></tr><tr><th>19</th><td>849014</td><td>M</td><td>19.81</td><td>22.15</td><td>130.0</td><td>1260.0</td><td>0.09831</td></tr><tr><th>20</th><td>8510426</td><td>B</td><td>13.54</td><td>14.36</td><td>87.46</td><td>566.3</td><td>0.09779</td></tr><tr><th>21</th><td>8510653</td><td>B</td><td>13.08</td><td>15.71</td><td>85.63</td><td>520.0</td><td>0.1075</td></tr><tr><th>22</th><td>8510824</td><td>B</td><td>9.504</td><td>12.44</td><td>60.34</td><td>273.9</td><td>0.1024</td></tr><tr><th>23</th><td>8511133</td><td>M</td><td>15.34</td><td>14.26</td><td>102.5</td><td>704.4</td><td>0.1073</td></tr><tr><th>24</th><td>851509</td><td>M</td><td>21.16</td><td>23.04</td><td>137.2</td><td>1404.0</td><td>0.09428</td></tr><tr><th>25</th><td>852552</td><td>M</td><td>16.65</td><td>21.38</td><td>110.0</td><td>904.6</td><td>0.1121</td></tr><tr><th>26</th><td>852631</td><td>M</td><td>17.14</td><td>16.4</td><td>116.0</td><td>912.7</td><td>0.1186</td></tr><tr><th>27</th><td>852763</td><td>M</td><td>14.58</td><td>21.53</td><td>97.41</td><td>644.8</td><td>0.1054</td></tr><tr><th>28</th><td>852781</td><td>M</td><td>18.61</td><td>20.25</td><td>122.1</td><td>1094.0</td><td>0.0944</td></tr><tr><th>29</th><td>852973</td><td>M</td><td>15.3</td><td>25.27</td><td>102.4</td><td>732.4</td><td>0.1082</td></tr><tr><th>30</th><td>853201</td><td>M</td><td>17.57</td><td>15.05</td><td>115.0</td><td>955.1</td><td>0.09847</td></tr><tr><th>31</th><td>853401</td><td>M</td><td>18.63</td><td>25.11</td><td>124.8</td><td>1088.0</td><td>0.1064</td></tr><tr><th>32</th><td>853612</td><td>M</td><td>11.84</td><td>18.7</td><td>77.93</td><td>440.6</td><td>0.1109</td></tr><tr><th>33</th><td>85382601</td><td>M</td><td>17.02</td><td>23.98</td><td>112.8</td><td>899.3</td><td>0.1197</td></tr><tr><th>34</th><td>854002</td><td>M</td><td>19.27</td><td>26.47</td><td>127.9</td><td>1162.0</td><td>0.09401</td></tr><tr><th>35</th><td>854039</td><td>M</td><td>16.13</td><td>17.88</td><td>107.0</td><td>807.2</td><td>0.104</td></tr><tr><th>36</th><td>854253</td><td>M</td><td>16.74</td><td>21.59</td><td>110.1</td><td>869.5</td><td>0.0961</td></tr><tr><th>37</th><td>854268</td><td>M</td><td>14.25</td><td>21.72</td><td>93.63</td><td>633.0</td><td>0.09823</td></tr><tr><th>38</th><td>854941</td><td>B</td><td>13.03</td><td>18.42</td><td>82.61</td><td>523.8</td><td>0.08983</td></tr><tr><th>39</th><td>855133</td><td>M</td><td>14.99</td><td>25.2</td><td>95.54</td><td>698.8</td><td>0.09387</td></tr><tr><th>40</th><td>855138</td><td>M</td><td>13.48</td><td>20.82</td><td>88.4</td><td>559.2</td><td>0.1016</td></tr><tr><th>41</th><td>855167</td><td>M</td><td>13.44</td><td>21.58</td><td>86.18</td><td>563.0</td><td>0.08162</td></tr><tr><th>42</th><td>855563</td><td>M</td><td>10.95</td><td>21.35</td><td>71.9</td><td>371.1</td><td>0.1227</td></tr><tr><th>43</th><td>855625</td><td>M</td><td>19.07</td><td>24.81</td><td>128.3</td><td>1104.0</td><td>0.09081</td></tr><tr><th>44</th><td>856106</td><td>M</td><td>13.28</td><td>20.28</td><td>87.32</td><td>545.2</td><td>0.1041</td></tr><tr><th>45</th><td>85638502</td><td>M</td><td>13.17</td><td>21.81</td><td>85.42</td><td>531.5</td><td>0.09714</td></tr><tr><th>46</th><td>857010</td><td>M</td><td>18.65</td><td>17.6</td><td>123.7</td><td>1076.0</td><td>0.1099</td></tr><tr><th>47</th><td>85713702</td><td>B</td><td>8.196</td><td>16.84</td><td>51.71</td><td>201.9</td><td>0.086</td></tr><tr><th>48</th><td>85715</td><td>M</td><td>13.17</td><td>18.66</td><td>85.98</td><td>534.6</td><td>0.1158</td></tr><tr><th>49</th><td>857155</td><td>B</td><td>12.05</td><td>14.63</td><td>78.04</td><td>449.3</td><td>0.1031</td></tr><tr><th>50</th><td>857156</td><td>B</td><td>13.49</td><td>22.3</td><td>86.91</td><td>561.0</td><td>0.08752</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& id & diagnosis & radius\\_mean & texture\\_mean & perimeter\\_mean & area\\_mean & smoothness\\_mean & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & String & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 842302 & M & 17.99 & 10.38 & 122.8 & 1001.0 & 0.1184 & $\\dots$ \\\\\n",
       "\t2 & 842517 & M & 20.57 & 17.77 & 132.9 & 1326.0 & 0.08474 & $\\dots$ \\\\\n",
       "\t3 & 84300903 & M & 19.69 & 21.25 & 130.0 & 1203.0 & 0.1096 & $\\dots$ \\\\\n",
       "\t4 & 84348301 & M & 11.42 & 20.38 & 77.58 & 386.1 & 0.1425 & $\\dots$ \\\\\n",
       "\t5 & 84358402 & M & 20.29 & 14.34 & 135.1 & 1297.0 & 0.1003 & $\\dots$ \\\\\n",
       "\t6 & 843786 & M & 12.45 & 15.7 & 82.57 & 477.1 & 0.1278 & $\\dots$ \\\\\n",
       "\t7 & 844359 & M & 18.25 & 19.98 & 119.6 & 1040.0 & 0.09463 & $\\dots$ \\\\\n",
       "\t8 & 84458202 & M & 13.71 & 20.83 & 90.2 & 577.9 & 0.1189 & $\\dots$ \\\\\n",
       "\t9 & 844981 & M & 13.0 & 21.82 & 87.5 & 519.8 & 0.1273 & $\\dots$ \\\\\n",
       "\t10 & 84501001 & M & 12.46 & 24.04 & 83.97 & 475.9 & 0.1186 & $\\dots$ \\\\\n",
       "\t11 & 845636 & M & 16.02 & 23.24 & 102.7 & 797.8 & 0.08206 & $\\dots$ \\\\\n",
       "\t12 & 84610002 & M & 15.78 & 17.89 & 103.6 & 781.0 & 0.0971 & $\\dots$ \\\\\n",
       "\t13 & 846226 & M & 19.17 & 24.8 & 132.4 & 1123.0 & 0.0974 & $\\dots$ \\\\\n",
       "\t14 & 846381 & M & 15.85 & 23.95 & 103.7 & 782.7 & 0.08401 & $\\dots$ \\\\\n",
       "\t15 & 84667401 & M & 13.73 & 22.61 & 93.6 & 578.3 & 0.1131 & $\\dots$ \\\\\n",
       "\t16 & 84799002 & M & 14.54 & 27.54 & 96.73 & 658.8 & 0.1139 & $\\dots$ \\\\\n",
       "\t17 & 848406 & M & 14.68 & 20.13 & 94.74 & 684.5 & 0.09867 & $\\dots$ \\\\\n",
       "\t18 & 84862001 & M & 16.13 & 20.68 & 108.1 & 798.8 & 0.117 & $\\dots$ \\\\\n",
       "\t19 & 849014 & M & 19.81 & 22.15 & 130.0 & 1260.0 & 0.09831 & $\\dots$ \\\\\n",
       "\t20 & 8510426 & B & 13.54 & 14.36 & 87.46 & 566.3 & 0.09779 & $\\dots$ \\\\\n",
       "\t21 & 8510653 & B & 13.08 & 15.71 & 85.63 & 520.0 & 0.1075 & $\\dots$ \\\\\n",
       "\t22 & 8510824 & B & 9.504 & 12.44 & 60.34 & 273.9 & 0.1024 & $\\dots$ \\\\\n",
       "\t23 & 8511133 & M & 15.34 & 14.26 & 102.5 & 704.4 & 0.1073 & $\\dots$ \\\\\n",
       "\t24 & 851509 & M & 21.16 & 23.04 & 137.2 & 1404.0 & 0.09428 & $\\dots$ \\\\\n",
       "\t25 & 852552 & M & 16.65 & 21.38 & 110.0 & 904.6 & 0.1121 & $\\dots$ \\\\\n",
       "\t26 & 852631 & M & 17.14 & 16.4 & 116.0 & 912.7 & 0.1186 & $\\dots$ \\\\\n",
       "\t27 & 852763 & M & 14.58 & 21.53 & 97.41 & 644.8 & 0.1054 & $\\dots$ \\\\\n",
       "\t28 & 852781 & M & 18.61 & 20.25 & 122.1 & 1094.0 & 0.0944 & $\\dots$ \\\\\n",
       "\t29 & 852973 & M & 15.3 & 25.27 & 102.4 & 732.4 & 0.1082 & $\\dots$ \\\\\n",
       "\t30 & 853201 & M & 17.57 & 15.05 & 115.0 & 955.1 & 0.09847 & $\\dots$ \\\\\n",
       "\t31 & 853401 & M & 18.63 & 25.11 & 124.8 & 1088.0 & 0.1064 & $\\dots$ \\\\\n",
       "\t32 & 853612 & M & 11.84 & 18.7 & 77.93 & 440.6 & 0.1109 & $\\dots$ \\\\\n",
       "\t33 & 85382601 & M & 17.02 & 23.98 & 112.8 & 899.3 & 0.1197 & $\\dots$ \\\\\n",
       "\t34 & 854002 & M & 19.27 & 26.47 & 127.9 & 1162.0 & 0.09401 & $\\dots$ \\\\\n",
       "\t35 & 854039 & M & 16.13 & 17.88 & 107.0 & 807.2 & 0.104 & $\\dots$ \\\\\n",
       "\t36 & 854253 & M & 16.74 & 21.59 & 110.1 & 869.5 & 0.0961 & $\\dots$ \\\\\n",
       "\t37 & 854268 & M & 14.25 & 21.72 & 93.63 & 633.0 & 0.09823 & $\\dots$ \\\\\n",
       "\t38 & 854941 & B & 13.03 & 18.42 & 82.61 & 523.8 & 0.08983 & $\\dots$ \\\\\n",
       "\t39 & 855133 & M & 14.99 & 25.2 & 95.54 & 698.8 & 0.09387 & $\\dots$ \\\\\n",
       "\t40 & 855138 & M & 13.48 & 20.82 & 88.4 & 559.2 & 0.1016 & $\\dots$ \\\\\n",
       "\t41 & 855167 & M & 13.44 & 21.58 & 86.18 & 563.0 & 0.08162 & $\\dots$ \\\\\n",
       "\t42 & 855563 & M & 10.95 & 21.35 & 71.9 & 371.1 & 0.1227 & $\\dots$ \\\\\n",
       "\t43 & 855625 & M & 19.07 & 24.81 & 128.3 & 1104.0 & 0.09081 & $\\dots$ \\\\\n",
       "\t44 & 856106 & M & 13.28 & 20.28 & 87.32 & 545.2 & 0.1041 & $\\dots$ \\\\\n",
       "\t45 & 85638502 & M & 13.17 & 21.81 & 85.42 & 531.5 & 0.09714 & $\\dots$ \\\\\n",
       "\t46 & 857010 & M & 18.65 & 17.6 & 123.7 & 1076.0 & 0.1099 & $\\dots$ \\\\\n",
       "\t47 & 85713702 & B & 8.196 & 16.84 & 51.71 & 201.9 & 0.086 & $\\dots$ \\\\\n",
       "\t48 & 85715 & M & 13.17 & 18.66 & 85.98 & 534.6 & 0.1158 & $\\dots$ \\\\\n",
       "\t49 & 857155 & B & 12.05 & 14.63 & 78.04 & 449.3 & 0.1031 & $\\dots$ \\\\\n",
       "\t50 & 857156 & B & 13.49 & 22.3 & 86.91 & 561.0 & 0.08752 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "56933 DataFrame. Omitted printing of 28 columns\n",
       " Row  id        diagnosis  radius_mean  texture_mean  perimeter_mean \n",
       "      \u001b[90mInt64\u001b[39m     \u001b[90mString\u001b[39m     \u001b[90mFloat64\u001b[39m      \u001b[90mFloat64\u001b[39m       \u001b[90mFloat64\u001b[39m        \n",
       "\n",
       " 1    842302    M          17.99        10.38         122.8          \n",
       " 2    842517    M          20.57        17.77         132.9          \n",
       " 3    84300903  M          19.69        21.25         130.0          \n",
       " 4    84348301  M          11.42        20.38         77.58          \n",
       " 5    84358402  M          20.29        14.34         135.1          \n",
       " 6    843786    M          12.45        15.7          82.57          \n",
       " 7    844359    M          18.25        19.98         119.6          \n",
       " 8    84458202  M          13.71        20.83         90.2           \n",
       " 9    844981    M          13.0         21.82         87.5           \n",
       " 10   84501001  M          12.46        24.04         83.97          \n",
       " 11   845636    M          16.02        23.24         102.7          \n",
       " 12   84610002  M          15.78        17.89         103.6          \n",
       " 13   846226    M          19.17        24.8          132.4          \n",
       " 14   846381    M          15.85        23.95         103.7          \n",
       " 15   84667401  M          13.73        22.61         93.6           \n",
       " 16   84799002  M          14.54        27.54         96.73          \n",
       " 17   848406    M          14.68        20.13         94.74          \n",
       " 18   84862001  M          16.13        20.68         108.1          \n",
       " 19   849014    M          19.81        22.15         130.0          \n",
       " 20   8510426   B          13.54        14.36         87.46          \n",
       "\n",
       " 549  923169    B          9.683        19.34         61.05          \n",
       " 550  923465    B          10.82        24.21         68.89          \n",
       " 551  923748    B          10.86        21.48         68.51          \n",
       " 552  923780    B          11.13        22.44         71.49          \n",
       " 553  924084    B          12.77        29.43         81.35          \n",
       " 554  924342    B          9.333        21.94         59.01          \n",
       " 555  924632    B          12.88        28.92         82.5           \n",
       " 556  924934    B          10.29        27.61         65.67          \n",
       " 557  924964    B          10.16        19.59         64.73          \n",
       " 558  925236    B          9.423        27.88         59.26          \n",
       " 559  925277    B          14.59        22.68         96.39          \n",
       " 560  925291    B          11.51        23.93         74.52          \n",
       " 561  925292    B          14.05        27.15         91.38          \n",
       " 562  925311    B          11.2         29.37         70.67          \n",
       " 563  925622    M          15.22        30.62         103.4          \n",
       " 564  926125    M          20.92        25.09         143.0          \n",
       " 565  926424    M          21.56        22.39         142.0          \n",
       " 566  926682    M          20.13        28.25         131.2          \n",
       " 567  926954    M          16.6         28.08         108.3          \n",
       " 568  927241    M          20.6         29.33         140.1          \n",
       " 569  92751     B          7.76         24.54         47.92          "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CSV.read(\"./data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nunique</th><th>nmissing</th></tr><tr><th></th><th>Symbol</th><th>Union</th><th>Any</th><th>Union</th><th>Any</th><th>Union</th><th>Nothing</th></tr></thead><tbody><p>31 rows  8 columns (omitted printing of 1 columns)</p><tr><th>1</th><td>diagnosis</td><td></td><td>B</td><td></td><td>M</td><td>2</td><td></td></tr><tr><th>2</th><td>radius_mean</td><td>14.1273</td><td>6.981</td><td>13.37</td><td>28.11</td><td></td><td></td></tr><tr><th>3</th><td>texture_mean</td><td>19.2896</td><td>9.71</td><td>18.84</td><td>39.28</td><td></td><td></td></tr><tr><th>4</th><td>perimeter_mean</td><td>91.969</td><td>43.79</td><td>86.24</td><td>188.5</td><td></td><td></td></tr><tr><th>5</th><td>area_mean</td><td>654.889</td><td>143.5</td><td>551.1</td><td>2501.0</td><td></td><td></td></tr><tr><th>6</th><td>smoothness_mean</td><td>0.0963603</td><td>0.05263</td><td>0.09587</td><td>0.1634</td><td></td><td></td></tr><tr><th>7</th><td>compactness_mean</td><td>0.104341</td><td>0.01938</td><td>0.09263</td><td>0.3454</td><td></td><td></td></tr><tr><th>8</th><td>concavity_mean</td><td>0.0887993</td><td>0.0</td><td>0.06154</td><td>0.4268</td><td></td><td></td></tr><tr><th>9</th><td>concave points_mean</td><td>0.0489191</td><td>0.0</td><td>0.0335</td><td>0.2012</td><td></td><td></td></tr><tr><th>10</th><td>symmetry_mean</td><td>0.181162</td><td>0.106</td><td>0.1792</td><td>0.304</td><td></td><td></td></tr><tr><th>11</th><td>fractal_dimension_mean</td><td>0.0627976</td><td>0.04996</td><td>0.06154</td><td>0.09744</td><td></td><td></td></tr><tr><th>12</th><td>radius_se</td><td>0.405172</td><td>0.1115</td><td>0.3242</td><td>2.873</td><td></td><td></td></tr><tr><th>13</th><td>texture_se</td><td>1.21685</td><td>0.3602</td><td>1.108</td><td>4.885</td><td></td><td></td></tr><tr><th>14</th><td>perimeter_se</td><td>2.86606</td><td>0.757</td><td>2.287</td><td>21.98</td><td></td><td></td></tr><tr><th>15</th><td>area_se</td><td>40.3371</td><td>6.802</td><td>24.53</td><td>542.2</td><td></td><td></td></tr><tr><th>16</th><td>smoothness_se</td><td>0.00704098</td><td>0.001713</td><td>0.00638</td><td>0.03113</td><td></td><td></td></tr><tr><th>17</th><td>compactness_se</td><td>0.0254781</td><td>0.002252</td><td>0.02045</td><td>0.1354</td><td></td><td></td></tr><tr><th>18</th><td>concavity_se</td><td>0.0318937</td><td>0.0</td><td>0.02589</td><td>0.396</td><td></td><td></td></tr><tr><th>19</th><td>concave points_se</td><td>0.0117961</td><td>0.0</td><td>0.01093</td><td>0.05279</td><td></td><td></td></tr><tr><th>20</th><td>symmetry_se</td><td>0.0205423</td><td>0.007882</td><td>0.01873</td><td>0.07895</td><td></td><td></td></tr><tr><th>21</th><td>fractal_dimension_se</td><td>0.0037949</td><td>0.0008948</td><td>0.003187</td><td>0.02984</td><td></td><td></td></tr><tr><th>22</th><td>radius_worst</td><td>16.2692</td><td>7.93</td><td>14.97</td><td>36.04</td><td></td><td></td></tr><tr><th>23</th><td>texture_worst</td><td>25.6772</td><td>12.02</td><td>25.41</td><td>49.54</td><td></td><td></td></tr><tr><th>24</th><td>perimeter_worst</td><td>107.261</td><td>50.41</td><td>97.66</td><td>251.2</td><td></td><td></td></tr><tr><th>25</th><td>area_worst</td><td>880.583</td><td>185.2</td><td>686.5</td><td>4254.0</td><td></td><td></td></tr><tr><th>26</th><td>smoothness_worst</td><td>0.132369</td><td>0.07117</td><td>0.1313</td><td>0.2226</td><td></td><td></td></tr><tr><th>27</th><td>compactness_worst</td><td>0.254265</td><td>0.02729</td><td>0.2119</td><td>1.058</td><td></td><td></td></tr><tr><th>28</th><td>concavity_worst</td><td>0.272188</td><td>0.0</td><td>0.2267</td><td>1.252</td><td></td><td></td></tr><tr><th>29</th><td>concave points_worst</td><td>0.114606</td><td>0.0</td><td>0.09993</td><td>0.291</td><td></td><td></td></tr><tr><th>30</th><td>symmetry_worst</td><td>0.290076</td><td>0.1565</td><td>0.2822</td><td>0.6638</td><td></td><td></td></tr><tr><th>31</th><td>fractal_dimension_worst</td><td>0.0839458</td><td>0.05504</td><td>0.08004</td><td>0.2075</td><td></td><td></td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& variable & mean & min & median & max & nunique & nmissing & \\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union & Any & Union & Any & Union & Nothing & \\\\\n",
       "\t\\hline\n",
       "\t1 & diagnosis &  & B &  & M & 2 &  & $\\dots$ \\\\\n",
       "\t2 & radius\\_mean & 14.1273 & 6.981 & 13.37 & 28.11 &  &  & $\\dots$ \\\\\n",
       "\t3 & texture\\_mean & 19.2896 & 9.71 & 18.84 & 39.28 &  &  & $\\dots$ \\\\\n",
       "\t4 & perimeter\\_mean & 91.969 & 43.79 & 86.24 & 188.5 &  &  & $\\dots$ \\\\\n",
       "\t5 & area\\_mean & 654.889 & 143.5 & 551.1 & 2501.0 &  &  & $\\dots$ \\\\\n",
       "\t6 & smoothness\\_mean & 0.0963603 & 0.05263 & 0.09587 & 0.1634 &  &  & $\\dots$ \\\\\n",
       "\t7 & compactness\\_mean & 0.104341 & 0.01938 & 0.09263 & 0.3454 &  &  & $\\dots$ \\\\\n",
       "\t8 & concavity\\_mean & 0.0887993 & 0.0 & 0.06154 & 0.4268 &  &  & $\\dots$ \\\\\n",
       "\t9 & concave points\\_mean & 0.0489191 & 0.0 & 0.0335 & 0.2012 &  &  & $\\dots$ \\\\\n",
       "\t10 & symmetry\\_mean & 0.181162 & 0.106 & 0.1792 & 0.304 &  &  & $\\dots$ \\\\\n",
       "\t11 & fractal\\_dimension\\_mean & 0.0627976 & 0.04996 & 0.06154 & 0.09744 &  &  & $\\dots$ \\\\\n",
       "\t12 & radius\\_se & 0.405172 & 0.1115 & 0.3242 & 2.873 &  &  & $\\dots$ \\\\\n",
       "\t13 & texture\\_se & 1.21685 & 0.3602 & 1.108 & 4.885 &  &  & $\\dots$ \\\\\n",
       "\t14 & perimeter\\_se & 2.86606 & 0.757 & 2.287 & 21.98 &  &  & $\\dots$ \\\\\n",
       "\t15 & area\\_se & 40.3371 & 6.802 & 24.53 & 542.2 &  &  & $\\dots$ \\\\\n",
       "\t16 & smoothness\\_se & 0.00704098 & 0.001713 & 0.00638 & 0.03113 &  &  & $\\dots$ \\\\\n",
       "\t17 & compactness\\_se & 0.0254781 & 0.002252 & 0.02045 & 0.1354 &  &  & $\\dots$ \\\\\n",
       "\t18 & concavity\\_se & 0.0318937 & 0.0 & 0.02589 & 0.396 &  &  & $\\dots$ \\\\\n",
       "\t19 & concave points\\_se & 0.0117961 & 0.0 & 0.01093 & 0.05279 &  &  & $\\dots$ \\\\\n",
       "\t20 & symmetry\\_se & 0.0205423 & 0.007882 & 0.01873 & 0.07895 &  &  & $\\dots$ \\\\\n",
       "\t21 & fractal\\_dimension\\_se & 0.0037949 & 0.0008948 & 0.003187 & 0.02984 &  &  & $\\dots$ \\\\\n",
       "\t22 & radius\\_worst & 16.2692 & 7.93 & 14.97 & 36.04 &  &  & $\\dots$ \\\\\n",
       "\t23 & texture\\_worst & 25.6772 & 12.02 & 25.41 & 49.54 &  &  & $\\dots$ \\\\\n",
       "\t24 & perimeter\\_worst & 107.261 & 50.41 & 97.66 & 251.2 &  &  & $\\dots$ \\\\\n",
       "\t25 & area\\_worst & 880.583 & 185.2 & 686.5 & 4254.0 &  &  & $\\dots$ \\\\\n",
       "\t26 & smoothness\\_worst & 0.132369 & 0.07117 & 0.1313 & 0.2226 &  &  & $\\dots$ \\\\\n",
       "\t27 & compactness\\_worst & 0.254265 & 0.02729 & 0.2119 & 1.058 &  &  & $\\dots$ \\\\\n",
       "\t28 & concavity\\_worst & 0.272188 & 0.0 & 0.2267 & 1.252 &  &  & $\\dots$ \\\\\n",
       "\t29 & concave points\\_worst & 0.114606 & 0.0 & 0.09993 & 0.291 &  &  & $\\dots$ \\\\\n",
       "\t30 & symmetry\\_worst & 0.290076 & 0.1565 & 0.2822 & 0.6638 &  &  & $\\dots$ \\\\\n",
       "\t31 & fractal\\_dimension\\_worst & 0.0839458 & 0.05504 & 0.08004 & 0.2075 &  &  & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "318 DataFrame. Omitted printing of 3 columns\n",
       " Row  variable                 mean        min        median    max     \n",
       "      \u001b[90mSymbol\u001b[39m                   \u001b[90mUnion\u001b[39m      \u001b[90mAny\u001b[39m        \u001b[90mUnion\u001b[39m    \u001b[90mAny\u001b[39m     \n",
       "\n",
       " 1    diagnosis                            B                    M       \n",
       " 2    radius_mean              14.1273     6.981      13.37     28.11   \n",
       " 3    texture_mean             19.2896     9.71       18.84     39.28   \n",
       " 4    perimeter_mean           91.969      43.79      86.24     188.5   \n",
       " 5    area_mean                654.889     143.5      551.1     2501.0  \n",
       " 6    smoothness_mean          0.0963603   0.05263    0.09587   0.1634  \n",
       " 7    compactness_mean         0.104341    0.01938    0.09263   0.3454  \n",
       " 8    concavity_mean           0.0887993   0.0        0.06154   0.4268  \n",
       " 9    concave points_mean      0.0489191   0.0        0.0335    0.2012  \n",
       " 10   symmetry_mean            0.181162    0.106      0.1792    0.304   \n",
       " 11   fractal_dimension_mean   0.0627976   0.04996    0.06154   0.09744 \n",
       " 12   radius_se                0.405172    0.1115     0.3242    2.873   \n",
       " 13   texture_se               1.21685     0.3602     1.108     4.885   \n",
       " 14   perimeter_se             2.86606     0.757      2.287     21.98   \n",
       " 15   area_se                  40.3371     6.802      24.53     542.2   \n",
       " 16   smoothness_se            0.00704098  0.001713   0.00638   0.03113 \n",
       " 17   compactness_se           0.0254781   0.002252   0.02045   0.1354  \n",
       " 18   concavity_se             0.0318937   0.0        0.02589   0.396   \n",
       " 19   concave points_se        0.0117961   0.0        0.01093   0.05279 \n",
       " 20   symmetry_se              0.0205423   0.007882   0.01873   0.07895 \n",
       " 21   fractal_dimension_se     0.0037949   0.0008948  0.003187  0.02984 \n",
       " 22   radius_worst             16.2692     7.93       14.97     36.04   \n",
       " 23   texture_worst            25.6772     12.02      25.41     49.54   \n",
       " 24   perimeter_worst          107.261     50.41      97.66     251.2   \n",
       " 25   area_worst               880.583     185.2      686.5     4254.0  \n",
       " 26   smoothness_worst         0.132369    0.07117    0.1313    0.2226  \n",
       " 27   compactness_worst        0.254265    0.02729    0.2119    1.058   \n",
       " 28   concavity_worst          0.272188    0.0        0.2267    1.252   \n",
       " 29   concave points_worst     0.114606    0.0        0.09993   0.291   \n",
       " 30   symmetry_worst           0.290076    0.1565     0.2822    0.6638  \n",
       " 31   fractal_dimension_worst  0.0839458   0.05504    0.08004   0.2075  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[:, Not([33, 1])]\n",
    "describe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at class labels to see if dataset is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Int64} with 2 entries:\n",
       "  \"B\" => 357\n",
       "  \"M\" => 212"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = countmap(data[:diagnosis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.6274165202108963\n",
       " 0.37258347978910367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(label_counts[i] / size(data)[1] for i in keys(label_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[22m _.names                 \u001b[0m\u001b[0m\u001b[22m _.types                         \u001b[0m\u001b[0m\u001b[22m _.scitypes    \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m diagnosis               \u001b[0m\u001b[0m CategoricalValue{String,UInt32} \u001b[0m\u001b[0m Multiclass{2} \u001b[0m\u001b[0m\n",
       "\u001b[0m radius_mean             \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m texture_mean            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m perimeter_mean          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m area_mean               \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m smoothness_mean         \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m compactness_mean        \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concavity_mean          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concave points_mean     \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m symmetry_mean           \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m fractal_dimension_mean  \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m radius_se               \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m texture_se              \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m perimeter_se            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m area_se                 \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m smoothness_se           \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m compactness_se          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concavity_se            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concave points_se       \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m symmetry_se             \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m fractal_dimension_se    \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m radius_worst            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m texture_worst           \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m perimeter_worst         \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m area_worst              \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m smoothness_worst        \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m compactness_worst       \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concavity_worst         \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concave points_worst    \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m symmetry_worst          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m fractal_dimension_worst \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "_.nrows = 569\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coerce!(data, :diagnosis=>Multiclass)\n",
    "schema(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CategoricalValue{String,UInt32}[\"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\"    \"B\", \"B\", \"B\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"B\"], 56930 DataFrame. Omitted printing of 26 columns\n",
       " Row  radius_mean  texture_mean  perimeter_mean  area_mean \n",
       "      \u001b[90mFloat64\u001b[39m      \u001b[90mFloat64\u001b[39m       \u001b[90mFloat64\u001b[39m         \u001b[90mFloat64\u001b[39m   \n",
       "\n",
       " 1    17.99        10.38         122.8           1001.0    \n",
       " 2    20.57        17.77         132.9           1326.0    \n",
       " 3    19.69        21.25         130.0           1203.0    \n",
       " 4    11.42        20.38         77.58           386.1     \n",
       " 5    20.29        14.34         135.1           1297.0    \n",
       " 6    12.45        15.7          82.57           477.1     \n",
       " 7    18.25        19.98         119.6           1040.0    \n",
       " 8    13.71        20.83         90.2            577.9     \n",
       " 9    13.0         21.82         87.5            519.8     \n",
       " 10   12.46        24.04         83.97           475.9     \n",
       " 11   16.02        23.24         102.7           797.8     \n",
       " 12   15.78        17.89         103.6           781.0     \n",
       " 13   19.17        24.8          132.4           1123.0    \n",
       " 14   15.85        23.95         103.7           782.7     \n",
       " 15   13.73        22.61         93.6            578.3     \n",
       " 16   14.54        27.54         96.73           658.8     \n",
       " 17   14.68        20.13         94.74           684.5     \n",
       " 18   16.13        20.68         108.1           798.8     \n",
       " 19   19.81        22.15         130.0           1260.0    \n",
       " 20   13.54        14.36         87.46           566.3     \n",
       "\n",
       " 549  9.683        19.34         61.05           285.7     \n",
       " 550  10.82        24.21         68.89           361.6     \n",
       " 551  10.86        21.48         68.51           360.5     \n",
       " 552  11.13        22.44         71.49           378.4     \n",
       " 553  12.77        29.43         81.35           507.9     \n",
       " 554  9.333        21.94         59.01           264.0     \n",
       " 555  12.88        28.92         82.5            514.3     \n",
       " 556  10.29        27.61         65.67           321.4     \n",
       " 557  10.16        19.59         64.73           311.7     \n",
       " 558  9.423        27.88         59.26           271.3     \n",
       " 559  14.59        22.68         96.39           657.1     \n",
       " 560  11.51        23.93         74.52           403.5     \n",
       " 561  14.05        27.15         91.38           600.4     \n",
       " 562  11.2         29.37         70.67           386.0     \n",
       " 563  15.22        30.62         103.4           716.9     \n",
       " 564  20.92        25.09         143.0           1347.0    \n",
       " 565  21.56        22.39         142.0           1479.0    \n",
       " 566  20.13        28.25         131.2           1261.0    \n",
       " 567  16.6         28.08         108.3           858.1     \n",
       " 568  20.6         29.33         140.1           1265.0    \n",
       " 569  7.76         24.54         47.92           181.0     )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, X = unpack(data, ==(:diagnosis), colname->true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition train and test data accoring to class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([483, 534, 159, 31, 170, 416, 231, 43, 161, 286    134, 500, 395, 533, 112, 396, 297, 106, 303, 261], [392, 390, 320, 27, 328, 477, 19, 356, 518, 444    136, 559, 505, 274, 508, 358, 90, 296, 79, 415])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data to use when trying to fit a single validation set\n",
    "train, test = partition(eachindex(y), 0.7, shuffle=true, rng=123, stratify=values(data[:diagnosis])) # gives 70:30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.628140703517588\n",
       " 0.37185929648241206"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts = countmap(data[train,:diagnosis])\n",
    "collect(train_counts[i] / size(train)[1] for i in keys(train_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.6257309941520468\n",
       " 0.3742690058479532"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_counts = countmap(data[test,:diagnosis])\n",
    "collect(test_counts[i] / size(test)[1] for i in keys(test_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five Learning Algorithms\n",
    "\n",
    "* Decision trees with some form of pruning\n",
    "* Neural networks\n",
    "* Boosting\n",
    "* Support Vector Machines\n",
    "* k-nearest neighbors\n",
    "\n",
    "\n",
    "##### Testing\n",
    "* Implement the algorithms\n",
    "* Design two *interesting* classification problems. For the purposes of this assignment, a classification problem is just a set of training examples and a set of test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43-element Array{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype),T} where T<:Tuple,1}:\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianLDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianQDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = ScikitLearn, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " (name = ExtraTreesClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n",
       " (name = GaussianNBClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GaussianProcessClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GradientBoostingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = KNNClassifier, package_name = NearestNeighbors, ... )\n",
       " (name = KNeighborsClassifier, package_name = ScikitLearn, ... )\n",
       " (name = LDA, package_name = MultivariateStats, ... )\n",
       " (name = LGBMClassifier, package_name = LightGBM, ... )\n",
       " (name = LinearBinaryClassifier, package_name = GLM, ... )\n",
       " (name = LinearSVC, package_name = LIBSVM, ... )\n",
       " (name = LogisticCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = LogisticClassifier, package_name = ScikitLearn, ... )\n",
       " (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n",
       " (name = NuSVC, package_name = LIBSVM, ... )\n",
       " (name = PassiveAggressiveClassifier, package_name = ScikitLearn, ... )\n",
       " (name = PerceptronClassifier, package_name = ScikitLearn, ... )\n",
       " (name = ProbabilisticSGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVC, package_name = LIBSVM, ... )\n",
       " (name = SVMClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(matching(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJScikitLearnInterface "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Loading into module \"Main\": \n",
      " @ MLJModels /home/andrew/.julia/packages/MLJModels/5DFoi/src/loading.jl:70\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "import MLJScikitLearnInterface \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(\n",
       "    base_estimator = nothing,\n",
       "    n_estimators = 50,\n",
       "    learning_rate = 1.0,\n",
       "    algorithm = \"SAMME.R\",\n",
       "    random_state = nothing)\u001b[34m @878\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load AdaBoostClassifier verbosity=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "* Implement or steal a boosted version of your decision trees. \n",
    "* As before, you will want to use some form of pruning, but presumably because you're using boosting you can afford to be much more aggressive about your pruning.\n",
    "\n",
    "**Chooses the hardest examples** talk about in write-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(\n",
       "    base_estimator = nothing,\n",
       "    n_estimators = 50,\n",
       "    learning_rate = 1.0,\n",
       "    algorithm = \"SAMME.R\",\n",
       "    random_state = nothing)\u001b[34m @100\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_model = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{AdaBoostClassifier} @651\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @203\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @732\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_mach = machine(boost_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{AdaBoostClassifier} @651\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/machines.jl:322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{AdaBoostClassifier} @651\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @203\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @732\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(boost_mach, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "MethodError: no method matching confusion_matrix(::MLJBase.UnivariateFiniteArray{Multiclass{2},String,UInt32,Float64,1}, ::CategoricalArray{String,1,UInt32,String,CategoricalValue{String,UInt32},Union{}}; rev=nothing)\nClosest candidates are:\n  confusion_matrix(!Matched::AbstractArray{var\"#s581\",1} where var\"#s581\"<:CategoricalValue, ::AbstractArray{var\"#s580\",1} where var\"#s580\"<:CategoricalValue; rev, perm, warn) at /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:64",
     "output_type": "error",
     "traceback": [
      "MethodError: no method matching confusion_matrix(::MLJBase.UnivariateFiniteArray{Multiclass{2},String,UInt32,Float64,1}, ::CategoricalArray{String,1,UInt32,String,CategoricalValue{String,UInt32},Union{}}; rev=nothing)\nClosest candidates are:\n  confusion_matrix(!Matched::AbstractArray{var\"#s581\",1} where var\"#s581\"<:CategoricalValue, ::AbstractArray{var\"#s580\",1} where var\"#s580\"<:CategoricalValue; rev, perm, warn) at /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:64",
      "",
      "Stacktrace:",
      " [1] (::FalseNegativeRate)(::MLJBase.UnivariateFiniteArray{Multiclass{2},String,UInt32,Float64,1}, ::CategoricalArray{String,1,UInt32,String,CategoricalValue{String,UInt32},Union{}}) at /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/finite.jl:867",
      " [2] value at /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/measures.jl:71 [inlined]",
      " [3] value at /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/measures.jl:64 [inlined]",
      " [4] (::MLJBase.var\"#267#272\"{DataFrame,CategoricalArray{String,1,UInt32,String,CategoricalValue{String,UInt32},Union{}},MLJBase.UnivariateFiniteArray{Multiclass{2},String,UInt32,Float64,1}})(::FalseNegativeRate) at ./none:0",
      " [5] iterate at ./generator.jl:47 [inlined]",
      " [6] collect_to!(::Array{Any,1}, ::Base.Generator{Array{Any,1},MLJBase.var\"#267#272\"{DataFrame,CategoricalArray{String,1,UInt32,String,CategoricalValue{String,UInt32},Union{}},MLJBase.UnivariateFiniteArray{Multiclass{2},String,UInt32,Float64,1}}}, ::Int64, ::Int64) at ./array.jl:732",
      " [7] collect_to!(::Array{Array{Float64,1},1}, ::Base.Generator{Array{Any,1},MLJBase.var\"#267#272\"{DataFrame,CategoricalArray{String,1,UInt32,String,CategoricalValue{String,UInt32},Union{}},MLJBase.UnivariateFiniteArray{Multiclass{2},String,UInt32,Float64,1}}}, ::Int64, ::Int64) at ./array.jl:740",
      " [8] collect_to_with_first!(::Array{Array{Float64,1},1}, ::Array{Float64,1}, ::Base.Generator{Array{Any,1},MLJBase.var\"#267#272\"{DataFrame,CategoricalArray{String,1,UInt32,String,CategoricalValue{String,UInt32},Union{}},MLJBase.UnivariateFiniteArray{Multiclass{2},String,UInt32,Float64,1}}}, ::Int64) at ./array.jl:710",
      " [9] collect(::Base.Generator{Array{Any,1},MLJBase.var\"#267#272\"{DataFrame,CategoricalArray{String,1,UInt32,String,CategoricalValue{String,UInt32},Union{}},MLJBase.UnivariateFiniteArray{Multiclass{2},String,UInt32,Float64,1}}}) at ./array.jl:691",
      " [10] (::MLJBase.var\"#get_measurements#271\"{Array{Tuple{Array{Int64,1},Array{Int64,1}},1},Nothing,Int64,Array{Any,1},typeof(MLJModelInterface.predict),Bool,DataFrame,CategoricalArray{String,1,UInt32,String,CategoricalValue{String,UInt32},Union{}}})(::Machine{MLJScikitLearnInterface.AdaBoostClassifier}, ::Int64) at /home/andrew/.julia/packages/MLJBase/cJmIS/src/resampling.jl:779",
      " [11] #251 at /home/andrew/.julia/packages/MLJBase/cJmIS/src/resampling.jl:644 [inlined]",
      " [12] _mapreduce(::MLJBase.var\"#251#252\"{MLJBase.var\"#get_measurements#271\"{Array{Tuple{Array{Int64,1},Array{Int64,1}},1},Nothing,Int64,Array{Any,1},typeof(MLJModelInterface.predict),Bool,DataFrame,CategoricalArray{String,1,UInt32,String,CategoricalValue{String,UInt32},Union{}}},Machine{MLJScikitLearnInterface.AdaBoostClassifier},Int64,ProgressMeter.Progress}, ::typeof(vcat), ::IndexLinear, ::UnitRange{Int64}) at ./reduce.jl:408",
      " [13] _mapreduce_dim at ./reducedim.jl:318 [inlined]",
      " [14] #mapreduce#620 at ./reducedim.jl:310 [inlined]",
      " [15] mapreduce at ./reducedim.jl:310 [inlined]",
      " [16] _evaluate!(::MLJBase.var\"#get_measurements#271\"{Array{Tuple{Array{Int64,1},Array{Int64,1}},1},Nothing,Int64,Array{Any,1},typeof(MLJModelInterface.predict),Bool,DataFrame,CategoricalArray{String,1,UInt32,String,CategoricalValue{String,UInt32},Union{}}}, ::Machine{MLJScikitLearnInterface.AdaBoostClassifier}, ::CPU1{Nothing}, ::Int64, ::Int64) at /home/andrew/.julia/packages/MLJBase/cJmIS/src/resampling.jl:643",
      " [17] evaluate!(::Machine{MLJScikitLearnInterface.AdaBoostClassifier}, ::Array{Tuple{Array{Int64,1},Array{Int64,1}},1}, ::Nothing, ::Nothing, ::Int64, ::Int64, ::Array{Any,1}, ::typeof(MLJModelInterface.predict), ::CPU1{Nothing}, ::Bool) at /home/andrew/.julia/packages/MLJBase/cJmIS/src/resampling.jl:796",
      " [18] evaluate!(::Machine{MLJScikitLearnInterface.AdaBoostClassifier}, ::CV, ::Nothing, ::Nothing, ::Int64, ::Int64, ::Array{Any,1}, ::Function, ::CPU1{Nothing}, ::Bool) at /home/andrew/.julia/packages/MLJBase/cJmIS/src/resampling.jl:859",
      " [19] evaluate!(::Machine{MLJScikitLearnInterface.AdaBoostClassifier}; resampling::CV, measures::Nothing, measure::Array{Any,1}, weights::Nothing, operation::Function, acceleration::CPU1{Nothing}, rows::Nothing, repeats::Int64, force::Bool, check_measure::Bool, verbosity::Int64) at /home/andrew/.julia/packages/MLJBase/cJmIS/src/resampling.jl:608",
      " [20] top-level scope at In[39]:1",
      " [21] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "boost_acc = evaluate!(boost_mach, resampling=CV(shuffle=true), measure=[cross_entropy, acc], \n",
    "                      verbosity=1, check_measure=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate!(boost_mach, resampling=CV(shuffle=true), measure=[tnr,tpr,fnr,fpr], verbosity=1, operation=predict_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_params(boost_mach);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch \n",
    "number of estimators vs learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJBase.NumericRange(Int64, :n_estimators, ... )"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param1 = :learning_rate\n",
    "param2 = :n_estimators\n",
    "\n",
    "r1 = range(boost_model, param1, lower=0.1, upper=1, scale=:linear)\n",
    "r2 = range(boost_model, param2, lower=10, upper=100, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "    model = AdaBoostClassifier(\n",
       "            base_estimator = nothing,\n",
       "            n_estimators = 50,\n",
       "            learning_rate = 1.0,\n",
       "            algorithm = \"SAMME.R\",\n",
       "            random_state = nothing),\n",
       "    tuning = Grid(\n",
       "            goal = nothing,\n",
       "            resolution = 10,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = CV(\n",
       "            nfolds = 6,\n",
       "            shuffle = false,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = cross_entropy(\n",
       "            eps = 2.220446049250313e-16),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = MLJBase.NumericRange{T,MLJBase.Bounded,Symbol} where T[\u001b[34mNumericRange{Float64,} @023\u001b[39m, \u001b[34mNumericRange{Int64,} @909\u001b[39m],\n",
       "    train_best = true,\n",
       "    repeats = 1,\n",
       "    n = nothing,\n",
       "    acceleration = CPUThreads{Int64}(1),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true)\u001b[34m @953\u001b[39m"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_boost_model = TunedModel(model=boost_model,\n",
    "                                    tuning=Grid(),\n",
    "                                    resampling=CV(), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1, r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,}} @100\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @569\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @828\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_boost = machine(self_tuning_boost_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,}} @100\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/machines.jl:322\n",
      " Info: Attempting to evaluate 100 models.\n",
      " @ MLJTuning /home/andrew/.julia/packages/MLJTuning/nuvTc/src/tuned_models.jl:501\n",
      "\u001b[33mEvaluating over 100 metamodels: 100%[=========================] Time: 0:00:48\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,}} @100\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @569\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @828\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = fit!(self_tuning_boost, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip760\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip760)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip761\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip760)\" d=\"\n",
       "M322.319 1423.18 L2112.76 1423.18 L2112.76 47.2441 L322.319 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip762\">\n",
       "    <rect x=\"322\" y=\"47\" width=\"1791\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  560.669,1423.18 560.669,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  936.022,1423.18 936.022,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1311.38,1423.18 1311.38,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1686.73,1423.18 1686.73,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2062.08,1423.18 2062.08,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  322.319,1384.24 2112.76,1384.24 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  322.319,1059.73 2112.76,1059.73 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  322.319,735.212 2112.76,735.212 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  322.319,410.699 2112.76,410.699 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  322.319,86.1857 2112.76,86.1857 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  322.319,1423.18 2112.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  322.319,1423.18 322.319,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  560.669,1423.18 560.669,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  936.022,1423.18 936.022,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1311.38,1423.18 1311.38,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1686.73,1423.18 1686.73,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2062.08,1423.18 2062.08,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  322.319,1384.24 343.805,1384.24 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  322.319,1059.73 343.805,1059.73 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  322.319,735.212 343.805,735.212 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  322.319,410.699 343.805,410.699 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  322.319,86.1857 343.805,86.1857 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip760)\" d=\"M 0 0 M542.984 1445.17 Q539.373 1445.17 537.544 1448.74 Q535.739 1452.28 535.739 1459.41 Q535.739 1466.51 537.544 1470.08 Q539.373 1473.62 542.984 1473.62 Q546.618 1473.62 548.424 1470.08 Q550.252 1466.51 550.252 1459.41 Q550.252 1452.28 548.424 1448.74 Q546.618 1445.17 542.984 1445.17 M542.984 1441.47 Q548.794 1441.47 551.85 1446.07 Q554.928 1450.66 554.928 1459.41 Q554.928 1468.13 551.85 1472.74 Q548.794 1477.32 542.984 1477.32 Q537.174 1477.32 534.095 1472.74 Q531.039 1468.13 531.039 1459.41 Q531.039 1450.66 534.095 1446.07 Q537.174 1441.47 542.984 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M559.998 1470.77 L564.882 1470.77 L564.882 1476.65 L559.998 1476.65 L559.998 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M573.979 1472.72 L590.298 1472.72 L590.298 1476.65 L568.354 1476.65 L568.354 1472.72 Q571.016 1469.96 575.599 1465.33 Q580.206 1460.68 581.386 1459.34 Q583.632 1456.81 584.511 1455.08 Q585.414 1453.32 585.414 1451.63 Q585.414 1448.87 583.47 1447.14 Q581.548 1445.4 578.447 1445.4 Q576.248 1445.4 573.794 1446.17 Q571.363 1446.93 568.586 1448.48 L568.586 1443.76 Q571.41 1442.62 573.863 1442.05 Q576.317 1441.47 578.354 1441.47 Q583.724 1441.47 586.919 1444.15 Q590.113 1446.84 590.113 1451.33 Q590.113 1453.46 589.303 1455.38 Q588.516 1457.28 586.41 1459.87 Q585.831 1460.54 582.729 1463.76 Q579.627 1466.95 573.979 1472.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M917.296 1445.17 Q913.685 1445.17 911.856 1448.74 Q910.05 1452.28 910.05 1459.41 Q910.05 1466.51 911.856 1470.08 Q913.685 1473.62 917.296 1473.62 Q920.93 1473.62 922.736 1470.08 Q924.564 1466.51 924.564 1459.41 Q924.564 1452.28 922.736 1448.74 Q920.93 1445.17 917.296 1445.17 M917.296 1441.47 Q923.106 1441.47 926.161 1446.07 Q929.24 1450.66 929.24 1459.41 Q929.24 1468.13 926.161 1472.74 Q923.106 1477.32 917.296 1477.32 Q911.486 1477.32 908.407 1472.74 Q905.351 1468.13 905.351 1459.41 Q905.351 1450.66 908.407 1446.07 Q911.486 1441.47 917.296 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M934.31 1470.77 L939.194 1470.77 L939.194 1476.65 L934.31 1476.65 L934.31 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M957.11 1446.17 L945.305 1464.61 L957.11 1464.61 L957.11 1446.17 M955.883 1442.09 L961.763 1442.09 L961.763 1464.61 L966.694 1464.61 L966.694 1468.5 L961.763 1468.5 L961.763 1476.65 L957.11 1476.65 L957.11 1468.5 L941.509 1468.5 L941.509 1463.99 L955.883 1442.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1292.81 1445.17 Q1289.2 1445.17 1287.37 1448.74 Q1285.57 1452.28 1285.57 1459.41 Q1285.57 1466.51 1287.37 1470.08 Q1289.2 1473.62 1292.81 1473.62 Q1296.45 1473.62 1298.25 1470.08 Q1300.08 1466.51 1300.08 1459.41 Q1300.08 1452.28 1298.25 1448.74 Q1296.45 1445.17 1292.81 1445.17 M1292.81 1441.47 Q1298.62 1441.47 1301.68 1446.07 Q1304.76 1450.66 1304.76 1459.41 Q1304.76 1468.13 1301.68 1472.74 Q1298.62 1477.32 1292.81 1477.32 Q1287 1477.32 1283.92 1472.74 Q1280.87 1468.13 1280.87 1459.41 Q1280.87 1450.66 1283.92 1446.07 Q1287 1441.47 1292.81 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1309.83 1470.77 L1314.71 1470.77 L1314.71 1476.65 L1309.83 1476.65 L1309.83 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1330.36 1457.51 Q1327.21 1457.51 1325.36 1459.66 Q1323.53 1461.81 1323.53 1465.56 Q1323.53 1469.29 1325.36 1471.47 Q1327.21 1473.62 1330.36 1473.62 Q1333.51 1473.62 1335.33 1471.47 Q1337.19 1469.29 1337.19 1465.56 Q1337.19 1461.81 1335.33 1459.66 Q1333.51 1457.51 1330.36 1457.51 M1339.64 1442.86 L1339.64 1447.11 Q1337.88 1446.28 1336.07 1445.84 Q1334.29 1445.4 1332.53 1445.4 Q1327.9 1445.4 1325.45 1448.53 Q1323.02 1451.65 1322.67 1457.97 Q1324.04 1455.96 1326.1 1454.89 Q1328.16 1453.8 1330.64 1453.8 Q1335.84 1453.8 1338.85 1456.98 Q1341.89 1460.12 1341.89 1465.56 Q1341.89 1470.89 1338.74 1474.11 Q1335.59 1477.32 1330.36 1477.32 Q1324.36 1477.32 1321.19 1472.74 Q1318.02 1468.13 1318.02 1459.41 Q1318.02 1451.21 1321.91 1446.35 Q1325.8 1441.47 1332.35 1441.47 Q1334.11 1441.47 1335.89 1441.81 Q1337.7 1442.16 1339.64 1442.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1668.29 1445.17 Q1664.68 1445.17 1662.85 1448.74 Q1661.05 1452.28 1661.05 1459.41 Q1661.05 1466.51 1662.85 1470.08 Q1664.68 1473.62 1668.29 1473.62 Q1671.93 1473.62 1673.73 1470.08 Q1675.56 1466.51 1675.56 1459.41 Q1675.56 1452.28 1673.73 1448.74 Q1671.93 1445.17 1668.29 1445.17 M1668.29 1441.47 Q1674.1 1441.47 1677.16 1446.07 Q1680.24 1450.66 1680.24 1459.41 Q1680.24 1468.13 1677.16 1472.74 Q1674.1 1477.32 1668.29 1477.32 Q1662.48 1477.32 1659.4 1472.74 Q1656.35 1468.13 1656.35 1459.41 Q1656.35 1450.66 1659.4 1446.07 Q1662.48 1441.47 1668.29 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1685.31 1470.77 L1690.19 1470.77 L1690.19 1476.65 L1685.31 1476.65 L1685.31 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1705.26 1460.24 Q1701.93 1460.24 1700.01 1462.02 Q1698.11 1463.8 1698.11 1466.93 Q1698.11 1470.05 1700.01 1471.84 Q1701.93 1473.62 1705.26 1473.62 Q1708.59 1473.62 1710.51 1471.84 Q1712.44 1470.03 1712.44 1466.93 Q1712.44 1463.8 1710.51 1462.02 Q1708.62 1460.24 1705.26 1460.24 M1700.58 1458.25 Q1697.57 1457.51 1695.88 1455.45 Q1694.22 1453.39 1694.22 1450.43 Q1694.22 1446.28 1697.16 1443.87 Q1700.12 1441.47 1705.26 1441.47 Q1710.42 1441.47 1713.36 1443.87 Q1716.3 1446.28 1716.3 1450.43 Q1716.3 1453.39 1714.61 1455.45 Q1712.94 1457.51 1709.96 1458.25 Q1713.34 1459.04 1715.21 1461.33 Q1717.11 1463.62 1717.11 1466.93 Q1717.11 1471.95 1714.03 1474.64 Q1710.98 1477.32 1705.26 1477.32 Q1699.54 1477.32 1696.46 1474.64 Q1693.41 1471.95 1693.41 1466.93 Q1693.41 1463.62 1695.31 1461.33 Q1697.2 1459.04 1700.58 1458.25 M1698.87 1450.86 Q1698.87 1453.55 1700.54 1455.05 Q1702.23 1456.56 1705.26 1456.56 Q1708.27 1456.56 1709.96 1455.05 Q1711.67 1453.55 1711.67 1450.86 Q1711.67 1448.18 1709.96 1446.68 Q1708.27 1445.17 1705.26 1445.17 Q1702.23 1445.17 1700.54 1446.68 Q1698.87 1448.18 1698.87 1450.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2033.98 1472.72 L2041.62 1472.72 L2041.62 1446.35 L2033.31 1448.02 L2033.31 1443.76 L2041.57 1442.09 L2046.25 1442.09 L2046.25 1472.72 L2053.89 1472.72 L2053.89 1476.65 L2033.98 1476.65 L2033.98 1472.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2058.96 1470.77 L2063.84 1470.77 L2063.84 1476.65 L2058.96 1476.65 L2058.96 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2078.91 1445.17 Q2075.3 1445.17 2073.47 1448.74 Q2071.67 1452.28 2071.67 1459.41 Q2071.67 1466.51 2073.47 1470.08 Q2075.3 1473.62 2078.91 1473.62 Q2082.55 1473.62 2084.35 1470.08 Q2086.18 1466.51 2086.18 1459.41 Q2086.18 1452.28 2084.35 1448.74 Q2082.55 1445.17 2078.91 1445.17 M2078.91 1441.47 Q2084.72 1441.47 2087.78 1446.07 Q2090.86 1450.66 2090.86 1459.41 Q2090.86 1468.13 2087.78 1472.74 Q2084.72 1477.32 2078.91 1477.32 Q2073.1 1477.32 2070.02 1472.74 Q2066.97 1468.13 2066.97 1459.41 Q2066.97 1450.66 2070.02 1446.07 Q2073.1 1441.47 2078.91 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M182.694 1404.03 L190.332 1404.03 L190.332 1377.67 L182.022 1379.33 L182.022 1375.07 L190.286 1373.41 L194.962 1373.41 L194.962 1404.03 L202.601 1404.03 L202.601 1407.97 L182.694 1407.97 L182.694 1404.03 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M217.67 1376.48 Q214.059 1376.48 212.23 1380.05 Q210.425 1383.59 210.425 1390.72 Q210.425 1397.83 212.23 1401.39 Q214.059 1404.93 217.67 1404.93 Q221.304 1404.93 223.11 1401.39 Q224.939 1397.83 224.939 1390.72 Q224.939 1383.59 223.11 1380.05 Q221.304 1376.48 217.67 1376.48 M217.67 1372.78 Q223.48 1372.78 226.536 1377.39 Q229.615 1381.97 229.615 1390.72 Q229.615 1399.45 226.536 1404.05 Q223.48 1408.64 217.67 1408.64 Q211.86 1408.64 208.781 1404.05 Q205.726 1399.45 205.726 1390.72 Q205.726 1381.97 208.781 1377.39 Q211.86 1372.78 217.67 1372.78 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M230.16 1377.36 L236.367 1377.36 L236.367 1355.94 L229.615 1357.29 L229.615 1353.83 L236.329 1352.48 L240.128 1352.48 L240.128 1377.36 L246.335 1377.36 L246.335 1380.56 L230.16 1380.56 L230.16 1377.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M250.454 1375.78 L254.422 1375.78 L254.422 1380.56 L250.454 1380.56 L250.454 1375.78 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M266.666 1354.98 Q263.732 1354.98 262.246 1357.87 Q260.779 1360.75 260.779 1366.54 Q260.779 1372.32 262.246 1375.21 Q263.732 1378.09 266.666 1378.09 Q269.619 1378.09 271.086 1375.21 Q272.572 1372.32 272.572 1366.54 Q272.572 1360.75 271.086 1357.87 Q269.619 1354.98 266.666 1354.98 M266.666 1351.97 Q271.387 1351.97 273.869 1355.71 Q276.371 1359.43 276.371 1366.54 Q276.371 1373.63 273.869 1377.38 Q271.387 1381.1 266.666 1381.1 Q261.945 1381.1 259.444 1377.38 Q256.961 1373.63 256.961 1366.54 Q256.961 1359.43 259.444 1355.71 Q261.945 1351.97 266.666 1351.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M288.615 1354.98 Q285.681 1354.98 284.195 1357.87 Q282.728 1360.75 282.728 1366.54 Q282.728 1372.32 284.195 1375.21 Q285.681 1378.09 288.615 1378.09 Q291.567 1378.09 293.034 1375.21 Q294.52 1372.32 294.52 1366.54 Q294.52 1360.75 293.034 1357.87 Q291.567 1354.98 288.615 1354.98 M288.615 1351.97 Q293.335 1351.97 295.818 1355.71 Q298.319 1359.43 298.319 1366.54 Q298.319 1373.63 295.818 1377.38 Q293.335 1381.1 288.615 1381.1 Q283.894 1381.1 281.392 1377.38 Q278.91 1373.63 278.91 1366.54 Q278.91 1359.43 281.392 1355.71 Q283.894 1351.97 288.615 1351.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M184.8 1079.52 L192.439 1079.52 L192.439 1053.15 L184.129 1054.82 L184.129 1050.56 L192.393 1048.89 L197.069 1048.89 L197.069 1079.52 L204.707 1079.52 L204.707 1083.45 L184.8 1083.45 L184.8 1079.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M219.777 1051.97 Q216.166 1051.97 214.337 1055.54 Q212.531 1059.08 212.531 1066.21 Q212.531 1073.31 214.337 1076.88 Q216.166 1080.42 219.777 1080.42 Q223.411 1080.42 225.217 1076.88 Q227.045 1073.31 227.045 1066.21 Q227.045 1059.08 225.217 1055.54 Q223.411 1051.97 219.777 1051.97 M219.777 1048.27 Q225.587 1048.27 228.642 1052.87 Q231.721 1057.46 231.721 1066.21 Q231.721 1074.93 228.642 1079.54 Q225.587 1084.12 219.777 1084.12 Q213.967 1084.12 210.888 1079.54 Q207.832 1074.93 207.832 1066.21 Q207.832 1057.46 210.888 1052.87 Q213.967 1048.27 219.777 1048.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M232.267 1052.84 L238.473 1052.84 L238.473 1031.42 L231.721 1032.78 L231.721 1029.32 L238.435 1027.96 L242.235 1027.96 L242.235 1052.84 L248.441 1052.84 L248.441 1056.04 L232.267 1056.04 L232.267 1052.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M252.56 1051.27 L256.529 1051.27 L256.529 1056.04 L252.56 1056.04 L252.56 1051.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M263.92 1052.84 L277.179 1052.84 L277.179 1056.04 L259.35 1056.04 L259.35 1052.84 Q261.513 1050.61 265.237 1046.85 Q268.979 1043.06 269.938 1041.97 Q271.763 1039.92 272.478 1038.51 Q273.211 1037.08 273.211 1035.71 Q273.211 1033.47 271.631 1032.06 Q270.07 1030.65 267.55 1030.65 Q265.763 1030.65 263.77 1031.27 Q261.795 1031.89 259.538 1033.15 L259.538 1029.32 Q261.832 1028.39 263.826 1027.92 Q265.82 1027.45 267.475 1027.45 Q271.838 1027.45 274.434 1029.64 Q277.029 1031.82 277.029 1035.47 Q277.029 1037.2 276.371 1038.76 Q275.731 1040.3 274.02 1042.41 Q273.55 1042.95 271.029 1045.57 Q268.509 1048.16 263.92 1052.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M281.336 1027.96 L296.251 1027.96 L296.251 1031.16 L284.815 1031.16 L284.815 1038.04 Q285.643 1037.76 286.471 1037.63 Q287.298 1037.48 288.126 1037.48 Q292.828 1037.48 295.573 1040.06 Q298.319 1042.63 298.319 1047.03 Q298.319 1051.57 295.498 1054.09 Q292.677 1056.59 287.543 1056.59 Q285.775 1056.59 283.931 1056.29 Q282.107 1055.99 280.151 1055.38 L280.151 1051.57 Q281.844 1052.49 283.649 1052.94 Q285.455 1053.39 287.467 1053.39 Q290.721 1053.39 292.621 1051.68 Q294.52 1049.97 294.52 1047.03 Q294.52 1044.1 292.621 1042.39 Q290.721 1040.68 287.467 1040.68 Q285.944 1040.68 284.42 1041.01 Q282.916 1041.35 281.336 1042.07 L281.336 1027.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M183.502 755.004 L191.141 755.004 L191.141 728.639 L182.831 730.305 L182.831 726.046 L191.095 724.38 L195.771 724.38 L195.771 755.004 L203.41 755.004 L203.41 758.94 L183.502 758.94 L183.502 755.004 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M218.479 727.458 Q214.868 727.458 213.039 731.023 Q211.234 734.565 211.234 741.694 Q211.234 748.801 213.039 752.366 Q214.868 755.907 218.479 755.907 Q222.113 755.907 223.919 752.366 Q225.747 748.801 225.747 741.694 Q225.747 734.565 223.919 731.023 Q222.113 727.458 218.479 727.458 M218.479 723.755 Q224.289 723.755 227.345 728.361 Q230.423 732.944 230.423 741.694 Q230.423 750.421 227.345 755.028 Q224.289 759.611 218.479 759.611 Q212.669 759.611 209.59 755.028 Q206.535 750.421 206.535 741.694 Q206.535 732.944 209.59 728.361 Q212.669 723.755 218.479 723.755 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M230.969 728.332 L237.175 728.332 L237.175 706.91 L230.423 708.264 L230.423 704.803 L237.138 703.449 L240.937 703.449 L240.937 728.332 L247.143 728.332 L247.143 731.529 L230.969 731.529 L230.969 728.332 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M251.262 726.752 L255.231 726.752 L255.231 731.529 L251.262 731.529 L251.262 726.752 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M259.387 703.449 L274.302 703.449 L274.302 706.646 L262.867 706.646 L262.867 713.53 Q263.694 713.248 264.522 713.116 Q265.349 712.966 266.177 712.966 Q270.879 712.966 273.625 715.543 Q276.371 718.119 276.371 722.52 Q276.371 727.053 273.55 729.573 Q270.728 732.075 265.594 732.075 Q263.826 732.075 261.983 731.774 Q260.158 731.473 258.202 730.871 L258.202 727.053 Q259.895 727.975 261.701 728.426 Q263.506 728.877 265.519 728.877 Q268.772 728.877 270.672 727.166 Q272.572 725.454 272.572 722.52 Q272.572 719.586 270.672 717.875 Q268.772 716.163 265.519 716.163 Q263.995 716.163 262.472 716.502 Q260.967 716.84 259.387 717.555 L259.387 703.449 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M288.615 705.951 Q285.681 705.951 284.195 708.847 Q282.728 711.725 282.728 717.517 Q282.728 723.291 284.195 726.188 Q285.681 729.065 288.615 729.065 Q291.567 729.065 293.034 726.188 Q294.52 723.291 294.52 717.517 Q294.52 711.725 293.034 708.847 Q291.567 705.951 288.615 705.951 M288.615 702.941 Q293.335 702.941 295.818 706.684 Q298.319 710.408 298.319 717.517 Q298.319 724.608 295.818 728.351 Q293.335 732.075 288.615 732.075 Q283.894 732.075 281.392 728.351 Q278.91 724.608 278.91 717.517 Q278.91 710.408 281.392 706.684 Q283.894 702.941 288.615 702.941 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M184.236 430.491 L191.875 430.491 L191.875 404.126 L183.565 405.792 L183.565 401.533 L191.828 399.866 L196.504 399.866 L196.504 430.491 L204.143 430.491 L204.143 434.426 L184.236 434.426 L184.236 430.491 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M219.212 402.945 Q215.601 402.945 213.773 406.51 Q211.967 410.052 211.967 417.181 Q211.967 424.288 213.773 427.852 Q215.601 431.394 219.212 431.394 Q222.847 431.394 224.652 427.852 Q226.481 424.288 226.481 417.181 Q226.481 410.052 224.652 406.51 Q222.847 402.945 219.212 402.945 M219.212 399.241 Q225.023 399.241 228.078 403.848 Q231.157 408.431 231.157 417.181 Q231.157 425.908 228.078 430.514 Q225.023 435.098 219.212 435.098 Q213.402 435.098 210.324 430.514 Q207.268 425.908 207.268 417.181 Q207.268 408.431 210.324 403.848 Q213.402 399.241 219.212 399.241 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M231.702 403.819 L237.909 403.819 L237.909 382.397 L231.157 383.751 L231.157 380.29 L237.871 378.936 L241.67 378.936 L241.67 403.819 L247.877 403.819 L247.877 407.016 L231.702 407.016 L231.702 403.819 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M251.996 402.239 L255.964 402.239 L255.964 407.016 L251.996 407.016 L251.996 402.239 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M259.124 378.936 L277.179 378.936 L277.179 380.553 L266.986 407.016 L263.017 407.016 L272.609 382.133 L259.124 382.133 L259.124 378.936 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M281.336 378.936 L296.251 378.936 L296.251 382.133 L284.815 382.133 L284.815 389.017 Q285.643 388.735 286.471 388.603 Q287.298 388.453 288.126 388.453 Q292.828 388.453 295.573 391.029 Q298.319 393.606 298.319 398.007 Q298.319 402.54 295.498 405.06 Q292.677 407.561 287.543 407.561 Q285.775 407.561 283.931 407.261 Q282.107 406.96 280.151 406.358 L280.151 402.54 Q281.844 403.461 283.649 403.913 Q285.455 404.364 287.467 404.364 Q290.721 404.364 292.621 402.653 Q294.52 400.941 294.52 398.007 Q294.52 395.073 292.621 393.362 Q290.721 391.65 287.467 391.65 Q285.944 391.65 284.42 391.989 Q282.916 392.327 281.336 393.042 L281.336 378.936 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M181.584 105.978 L189.223 105.978 L189.223 79.6125 L180.913 81.2792 L180.913 77.0199 L189.176 75.3533 L193.852 75.3533 L193.852 105.978 L201.491 105.978 L201.491 109.913 L181.584 109.913 L181.584 105.978 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M216.561 78.432 Q212.95 78.432 211.121 81.9968 Q209.315 85.5384 209.315 92.668 Q209.315 99.7744 211.121 103.339 Q212.95 106.881 216.561 106.881 Q220.195 106.881 222 103.339 Q223.829 99.7744 223.829 92.668 Q223.829 85.5384 222 81.9968 Q220.195 78.432 216.561 78.432 M216.561 74.7283 Q222.371 74.7283 225.426 79.3347 Q228.505 83.918 228.505 92.668 Q228.505 101.395 225.426 106.001 Q222.371 110.585 216.561 110.585 Q210.75 110.585 207.672 106.001 Q204.616 101.395 204.616 92.668 Q204.616 83.918 207.672 79.3347 Q210.75 74.7283 216.561 74.7283 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M233.075 79.3056 L246.335 79.3056 L246.335 82.5029 L228.505 82.5029 L228.505 79.3056 Q230.668 77.0674 234.392 73.3059 Q238.135 69.5255 239.094 68.4347 Q240.918 66.3846 241.633 64.974 Q242.366 63.5446 242.366 62.1717 Q242.366 59.9335 240.786 58.523 Q239.225 57.1124 236.705 57.1124 Q234.918 57.1124 232.925 57.733 Q230.95 58.3537 228.693 59.6138 L228.693 55.777 Q230.988 54.8555 232.981 54.3853 Q234.975 53.9151 236.63 53.9151 Q240.993 53.9151 243.589 56.0968 Q246.184 58.2785 246.184 61.9272 Q246.184 63.6575 245.526 65.2185 Q244.887 66.7608 243.175 68.8672 Q242.705 69.4127 240.185 72.0269 Q237.664 74.6224 233.075 79.3056 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M250.454 77.7257 L254.422 77.7257 L254.422 82.5029 L250.454 82.5029 L250.454 77.7257 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M266.666 56.9243 Q263.732 56.9243 262.246 59.8207 Q260.779 62.6983 260.779 68.4911 Q260.779 74.2651 262.246 77.1615 Q263.732 80.0391 266.666 80.0391 Q269.619 80.0391 271.086 77.1615 Q272.572 74.2651 272.572 68.4911 Q272.572 62.6983 271.086 59.8207 Q269.619 56.9243 266.666 56.9243 M266.666 53.9151 Q271.387 53.9151 273.869 57.6578 Q276.371 61.3817 276.371 68.4911 Q276.371 75.5816 273.869 79.3244 Q271.387 83.0483 266.666 83.0483 Q261.945 83.0483 259.444 79.3244 Q256.961 75.5816 256.961 68.4911 Q256.961 61.3817 259.444 57.6578 Q261.945 53.9151 266.666 53.9151 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M288.615 56.9243 Q285.681 56.9243 284.195 59.8207 Q282.728 62.6983 282.728 68.4911 Q282.728 74.2651 284.195 77.1615 Q285.681 80.0391 288.615 80.0391 Q291.567 80.0391 293.034 77.1615 Q294.52 74.2651 294.52 68.4911 Q294.52 62.6983 293.034 59.8207 Q291.567 56.9243 288.615 56.9243 M288.615 53.9151 Q293.335 53.9151 295.818 57.6578 Q298.319 61.3817 298.319 68.4911 Q298.319 75.5816 295.818 79.3244 Q293.335 83.0483 288.615 83.0483 Q283.894 83.0483 281.392 79.3244 Q278.91 75.5816 278.91 68.4911 Q278.91 61.3817 281.392 57.6578 Q283.894 53.9151 288.615 53.9151 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1029.61 1506.52 L1035.46 1506.52 L1035.46 1556.04 L1029.61 1556.04 L1029.61 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1072.1 1536.76 L1072.1 1539.62 L1045.17 1539.62 Q1045.55 1545.67 1048.8 1548.85 Q1052.08 1552 1057.9 1552 Q1061.28 1552 1064.43 1551.17 Q1067.61 1550.35 1070.73 1548.69 L1070.73 1554.23 Q1067.58 1555.57 1064.27 1556.27 Q1060.96 1556.97 1057.55 1556.97 Q1049.02 1556.97 1044.02 1552 Q1039.06 1547.04 1039.06 1538.57 Q1039.06 1529.82 1043.77 1524.69 Q1048.51 1519.54 1056.53 1519.54 Q1063.73 1519.54 1067.9 1524.18 Q1072.1 1528.8 1072.1 1536.76 M1066.24 1535.04 Q1066.18 1530.23 1063.54 1527.37 Q1060.93 1524.5 1056.6 1524.5 Q1051.7 1524.5 1048.74 1527.27 Q1045.81 1530.04 1045.36 1535.07 L1066.24 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1094.44 1538.12 Q1087.34 1538.12 1084.61 1539.75 Q1081.87 1541.37 1081.87 1545.29 Q1081.87 1548.4 1083.91 1550.25 Q1085.97 1552.07 1089.51 1552.07 Q1094.38 1552.07 1097.31 1548.63 Q1100.27 1545.16 1100.27 1539.43 L1100.27 1538.12 L1094.44 1538.12 M1106.12 1535.71 L1106.12 1556.04 L1100.27 1556.04 L1100.27 1550.63 Q1098.26 1553.88 1095.27 1555.44 Q1092.28 1556.97 1087.95 1556.97 Q1082.47 1556.97 1079.23 1553.91 Q1076.01 1550.82 1076.01 1545.67 Q1076.01 1539.65 1080.02 1536.6 Q1084.06 1533.54 1092.05 1533.54 L1100.27 1533.54 L1100.27 1532.97 Q1100.27 1528.93 1097.59 1526.73 Q1094.95 1524.5 1090.14 1524.5 Q1087.09 1524.5 1084.19 1525.23 Q1081.3 1525.97 1078.62 1527.43 L1078.62 1522.02 Q1081.84 1520.78 1084.86 1520.17 Q1087.88 1519.54 1090.75 1519.54 Q1098.48 1519.54 1102.3 1523.55 Q1106.12 1527.56 1106.12 1535.71 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1132.92 1525.87 Q1131.93 1525.3 1130.76 1525.04 Q1129.61 1524.76 1128.21 1524.76 Q1123.25 1524.76 1120.57 1528 Q1117.93 1531.22 1117.93 1537.27 L1117.93 1556.04 L1112.04 1556.04 L1112.04 1520.4 L1117.93 1520.4 L1117.93 1525.93 Q1119.78 1522.69 1122.74 1521.13 Q1125.7 1519.54 1129.93 1519.54 Q1130.53 1519.54 1131.27 1519.63 Q1132 1519.7 1132.89 1519.85 L1132.92 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1167.55 1534.53 L1167.55 1556.04 L1161.69 1556.04 L1161.69 1534.72 Q1161.69 1529.66 1159.72 1527.14 Q1157.75 1524.63 1153.8 1524.63 Q1149.06 1524.63 1146.32 1527.65 Q1143.58 1530.68 1143.58 1535.9 L1143.58 1556.04 L1137.7 1556.04 L1137.7 1520.4 L1143.58 1520.4 L1143.58 1525.93 Q1145.68 1522.72 1148.52 1521.13 Q1151.38 1519.54 1155.11 1519.54 Q1161.25 1519.54 1164.4 1523.36 Q1167.55 1527.14 1167.55 1534.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1173.69 1520.4 L1179.55 1520.4 L1179.55 1556.04 L1173.69 1556.04 L1173.69 1520.4 M1173.69 1506.52 L1179.55 1506.52 L1179.55 1513.93 L1173.69 1513.93 L1173.69 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1215.33 1534.53 L1215.33 1556.04 L1209.47 1556.04 L1209.47 1534.72 Q1209.47 1529.66 1207.5 1527.14 Q1205.52 1524.63 1201.58 1524.63 Q1196.83 1524.63 1194.1 1527.65 Q1191.36 1530.68 1191.36 1535.9 L1191.36 1556.04 L1185.47 1556.04 L1185.47 1520.4 L1191.36 1520.4 L1191.36 1525.93 Q1193.46 1522.72 1196.29 1521.13 Q1199.16 1519.54 1202.88 1519.54 Q1209.02 1519.54 1212.17 1523.36 Q1215.33 1527.14 1215.33 1534.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1244.93 1537.81 Q1244.93 1531.44 1242.28 1527.94 Q1239.67 1524.44 1234.93 1524.44 Q1230.22 1524.44 1227.58 1527.94 Q1224.97 1531.44 1224.97 1537.81 Q1224.97 1544.14 1227.58 1547.64 Q1230.22 1551.14 1234.93 1551.14 Q1239.67 1551.14 1242.28 1547.64 Q1244.93 1544.14 1244.93 1537.81 M1250.78 1551.62 Q1250.78 1560.72 1246.74 1565.15 Q1242.7 1569.6 1234.36 1569.6 Q1231.27 1569.6 1228.53 1569.13 Q1225.8 1568.68 1223.22 1567.72 L1223.22 1562.03 Q1225.8 1563.43 1228.31 1564.1 Q1230.83 1564.76 1233.44 1564.76 Q1239.2 1564.76 1242.06 1561.74 Q1244.93 1558.75 1244.93 1552.67 L1244.93 1549.77 Q1243.11 1552.92 1240.28 1554.48 Q1237.45 1556.04 1233.5 1556.04 Q1226.94 1556.04 1222.93 1551.05 Q1218.92 1546.05 1218.92 1537.81 Q1218.92 1529.53 1222.93 1524.53 Q1226.94 1519.54 1233.5 1519.54 Q1237.45 1519.54 1240.28 1521.1 Q1243.11 1522.66 1244.93 1525.81 L1244.93 1520.4 L1250.78 1520.4 L1250.78 1551.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1284.01 1566.87 L1284.01 1571.42 L1250.15 1571.42 L1250.15 1566.87 L1284.01 1566.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1310.81 1525.87 Q1309.82 1525.3 1308.65 1525.04 Q1307.5 1524.76 1306.1 1524.76 Q1301.14 1524.76 1298.46 1528 Q1295.82 1531.22 1295.82 1537.27 L1295.82 1556.04 L1289.93 1556.04 L1289.93 1520.4 L1295.82 1520.4 L1295.82 1525.93 Q1297.67 1522.69 1300.63 1521.13 Q1303.59 1519.54 1307.82 1519.54 Q1308.42 1519.54 1309.16 1519.63 Q1309.89 1519.7 1310.78 1519.85 L1310.81 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1333.15 1538.12 Q1326.06 1538.12 1323.32 1539.75 Q1320.58 1541.37 1320.58 1545.29 Q1320.58 1548.4 1322.62 1550.25 Q1324.69 1552.07 1328.22 1552.07 Q1333.09 1552.07 1336.02 1548.63 Q1338.98 1545.16 1338.98 1539.43 L1338.98 1538.12 L1333.15 1538.12 M1344.84 1535.71 L1344.84 1556.04 L1338.98 1556.04 L1338.98 1550.63 Q1336.97 1553.88 1333.98 1555.44 Q1330.99 1556.97 1326.66 1556.97 Q1321.19 1556.97 1317.94 1553.91 Q1314.73 1550.82 1314.73 1545.67 Q1314.73 1539.65 1318.74 1536.6 Q1322.78 1533.54 1330.77 1533.54 L1338.98 1533.54 L1338.98 1532.97 Q1338.98 1528.93 1336.31 1526.73 Q1333.66 1524.5 1328.86 1524.5 Q1325.8 1524.5 1322.91 1525.23 Q1320.01 1525.97 1317.34 1527.43 L1317.34 1522.02 Q1320.55 1520.78 1323.57 1520.17 Q1326.6 1519.54 1329.46 1519.54 Q1337.2 1519.54 1341.02 1523.55 Q1344.84 1527.56 1344.84 1535.71 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1356.77 1510.27 L1356.77 1520.4 L1368.83 1520.4 L1368.83 1524.95 L1356.77 1524.95 L1356.77 1544.3 Q1356.77 1548.66 1357.95 1549.9 Q1359.16 1551.14 1362.82 1551.14 L1368.83 1551.14 L1368.83 1556.04 L1362.82 1556.04 Q1356.04 1556.04 1353.46 1553.53 Q1350.88 1550.98 1350.88 1544.3 L1350.88 1524.95 L1346.59 1524.95 L1346.59 1520.4 L1350.88 1520.4 L1350.88 1510.27 L1356.77 1510.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1405.47 1536.76 L1405.47 1539.62 L1378.54 1539.62 Q1378.92 1545.67 1382.17 1548.85 Q1385.45 1552 1391.27 1552 Q1394.65 1552 1397.8 1551.17 Q1400.98 1550.35 1404.1 1548.69 L1404.1 1554.23 Q1400.95 1555.57 1397.64 1556.27 Q1394.33 1556.97 1390.92 1556.97 Q1382.39 1556.97 1377.4 1552 Q1372.43 1547.04 1372.43 1538.57 Q1372.43 1529.82 1377.14 1524.69 Q1381.88 1519.54 1389.91 1519.54 Q1397.1 1519.54 1401.27 1524.18 Q1405.47 1528.8 1405.47 1536.76 M1399.61 1535.04 Q1399.55 1530.23 1396.91 1527.37 Q1394.3 1524.5 1389.97 1524.5 Q1385.07 1524.5 1382.11 1527.27 Q1379.18 1530.04 1378.73 1535.07 L1399.61 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M66.4881 893.543 L88.0042 893.543 L88.0042 899.399 L66.679 899.399 Q61.6183 899.399 59.1038 901.373 Q56.5894 903.346 56.5894 907.293 Q56.5894 912.035 59.6131 914.773 Q62.6368 917.51 67.8567 917.51 L88.0042 917.51 L88.0042 923.398 L52.3562 923.398 L52.3562 917.51 L57.8944 917.51 Q54.6797 915.409 53.0883 912.576 Q51.4968 909.712 51.4968 905.988 Q51.4968 899.845 55.3163 896.694 Q59.1038 893.543 66.4881 893.543 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M98.8259 860.314 L103.377 860.314 L103.377 894.18 L98.8259 894.18 L98.8259 860.314 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M68.7161 823.679 L71.5806 823.679 L71.5806 850.606 Q77.6281 850.224 80.8109 846.978 Q83.9619 843.7 83.9619 837.875 Q83.9619 834.501 83.1344 831.35 Q82.3069 828.167 80.6518 825.048 L86.1899 825.048 Q87.5267 828.199 88.227 831.509 Q88.9272 834.819 88.9272 838.225 Q88.9272 846.755 83.9619 851.752 Q78.9967 856.717 70.5303 856.717 Q61.7774 856.717 56.6531 852.007 Q51.4968 847.264 51.4968 839.244 Q51.4968 832.05 56.1438 827.881 Q60.7589 823.679 68.7161 823.679 M66.9973 829.536 Q62.1912 829.599 59.3266 832.241 Q56.4621 834.851 56.4621 839.18 Q56.4621 844.081 59.2312 847.042 Q62.0002 849.97 67.0292 850.415 L66.9973 829.536 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M53.4065 794.811 L58.9447 794.811 Q57.6716 797.294 57.035 799.967 Q56.3984 802.641 56.3984 805.505 Q56.3984 809.866 57.7352 812.062 Q59.072 814.226 61.7456 814.226 Q63.7826 814.226 64.9603 812.667 Q66.1061 811.107 67.1565 806.396 L67.6021 804.391 Q68.9389 798.153 71.3897 795.543 Q73.8086 792.901 78.1691 792.901 Q83.1344 792.901 86.0308 796.848 Q88.9272 800.763 88.9272 807.638 Q88.9272 810.502 88.3543 813.622 Q87.8132 816.709 86.6992 820.146 L80.6518 820.146 Q82.3387 816.9 83.198 813.749 Q84.0256 810.598 84.0256 807.51 Q84.0256 803.373 82.6251 801.145 Q81.1929 798.917 78.6147 798.917 Q76.2276 798.917 74.9545 800.54 Q73.6813 802.131 72.5037 807.574 L72.0262 809.611 Q70.8804 815.054 68.5251 817.473 Q66.138 819.892 62.0002 819.892 Q56.9713 819.892 54.2341 816.327 Q51.4968 812.762 51.4968 806.206 Q51.4968 802.959 51.9743 800.094 Q52.4517 797.23 53.4065 794.811 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M42.2347 780.965 L52.3562 780.965 L52.3562 768.902 L56.9077 768.902 L56.9077 780.965 L76.2594 780.965 Q80.6199 780.965 81.8613 779.788 Q83.1026 778.578 83.1026 774.918 L83.1026 768.902 L88.0042 768.902 L88.0042 774.918 Q88.0042 781.698 85.4897 784.276 Q82.9434 786.854 76.2594 786.854 L56.9077 786.854 L56.9077 791.151 L52.3562 791.151 L52.3562 786.854 L42.2347 786.854 L42.2347 780.965 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M52.3562 762.76 L52.3562 756.903 L88.0042 756.903 L88.0042 762.76 L52.3562 762.76 M38.479 762.76 L38.479 756.903 L45.895 756.903 L45.895 762.76 L38.479 762.76 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M59.1993 723.006 Q55.2526 720.81 53.3747 717.754 Q51.4968 714.698 51.4968 710.561 Q51.4968 704.991 55.4117 701.967 Q59.2948 698.943 66.4881 698.943 L88.0042 698.943 L88.0042 704.832 L66.679 704.832 Q61.5546 704.832 59.072 706.646 Q56.5894 708.46 56.5894 712.184 Q56.5894 716.736 59.6131 719.377 Q62.6368 722.019 67.8567 722.019 L88.0042 722.019 L88.0042 727.907 L66.679 727.907 Q61.5228 727.907 59.072 729.722 Q56.5894 731.536 56.5894 735.323 Q56.5894 739.811 59.6449 742.453 Q62.6686 745.095 67.8567 745.095 L88.0042 745.095 L88.0042 750.983 L52.3562 750.983 L52.3562 745.095 L57.8944 745.095 Q54.616 743.09 53.0564 740.289 Q51.4968 737.488 51.4968 733.636 Q51.4968 729.753 53.4702 727.048 Q55.4436 724.311 59.1993 723.006 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M70.0847 676.6 Q70.0847 683.697 71.7079 686.435 Q73.3312 689.172 77.2461 689.172 Q80.3653 689.172 82.2114 687.135 Q84.0256 685.066 84.0256 681.533 Q84.0256 676.663 80.5881 673.735 Q77.1188 670.775 71.3897 670.775 L70.0847 670.775 L70.0847 676.6 M67.6657 664.919 L88.0042 664.919 L88.0042 670.775 L82.5933 670.775 Q85.8398 672.78 87.3994 675.772 Q88.9272 678.764 88.9272 683.093 Q88.9272 688.567 85.8716 691.814 Q82.7843 695.028 77.6281 695.028 Q71.6125 695.028 68.5569 691.018 Q65.5014 686.976 65.5014 678.987 L65.5014 670.775 L64.9285 670.775 Q60.8862 670.775 58.6901 673.449 Q56.4621 676.09 56.4621 680.897 Q56.4621 683.952 57.1941 686.849 Q57.9262 689.745 59.3903 692.419 L53.9795 692.419 Q52.7381 689.204 52.1334 686.18 Q51.4968 683.156 51.4968 680.292 Q51.4968 672.558 55.5072 668.738 Q59.5176 664.919 67.6657 664.919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M42.2347 652.983 L52.3562 652.983 L52.3562 640.92 L56.9077 640.92 L56.9077 652.983 L76.2594 652.983 Q80.6199 652.983 81.8613 651.805 Q83.1026 650.596 83.1026 646.936 L83.1026 640.92 L88.0042 640.92 L88.0042 646.936 Q88.0042 653.715 85.4897 656.293 Q82.9434 658.871 76.2594 658.871 L56.9077 658.871 L56.9077 663.168 L52.3562 663.168 L52.3562 658.871 L42.2347 658.871 L42.2347 652.983 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M56.4621 620.963 Q56.4621 625.674 60.1542 628.411 Q63.8145 631.149 70.212 631.149 Q76.6095 631.149 80.3017 628.443 Q83.9619 625.706 83.9619 620.963 Q83.9619 616.285 80.2698 613.547 Q76.5777 610.81 70.212 610.81 Q63.8781 610.81 60.186 613.547 Q56.4621 616.285 56.4621 620.963 M51.4968 620.963 Q51.4968 613.325 56.4621 608.964 Q61.4273 604.604 70.212 604.604 Q78.9649 604.604 83.9619 608.964 Q88.9272 613.325 88.9272 620.963 Q88.9272 628.634 83.9619 632.995 Q78.9649 637.323 70.212 637.323 Q61.4273 637.323 56.4621 632.995 Q51.4968 628.634 51.4968 620.963 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M57.8307 577.804 Q57.2578 578.791 57.0032 579.968 Q56.7167 581.114 56.7167 582.515 Q56.7167 587.48 59.9632 590.153 Q63.1779 592.795 69.2253 592.795 L88.0042 592.795 L88.0042 598.683 L52.3562 598.683 L52.3562 592.795 L57.8944 592.795 Q54.6479 590.949 53.0883 587.989 Q51.4968 585.029 51.4968 580.796 Q51.4968 580.191 51.5923 579.459 Q51.656 578.727 51.8151 577.836 L57.8307 577.804 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M53.4065 548.935 L58.9447 548.935 Q57.6716 551.418 57.035 554.092 Q56.3984 556.765 56.3984 559.63 Q56.3984 563.99 57.7352 566.187 Q59.072 568.351 61.7456 568.351 Q63.7826 568.351 64.9603 566.791 Q66.1061 565.232 67.1565 560.521 L67.6021 558.516 Q68.9389 552.277 71.3897 549.668 Q73.8086 547.026 78.1691 547.026 Q83.1344 547.026 86.0308 550.973 Q88.9272 554.887 88.9272 561.762 Q88.9272 564.627 88.3543 567.746 Q87.8132 570.834 86.6992 574.271 L80.6518 574.271 Q82.3387 571.024 83.198 567.873 Q84.0256 564.722 84.0256 561.635 Q84.0256 557.497 82.6251 555.269 Q81.1929 553.041 78.6147 553.041 Q76.2276 553.041 74.9545 554.665 Q73.6813 556.256 72.5037 561.699 L72.0262 563.736 Q70.8804 569.178 68.5251 571.597 Q66.138 574.016 62.0002 574.016 Q56.9713 574.016 54.2341 570.452 Q51.4968 566.887 51.4968 560.33 Q51.4968 557.084 51.9743 554.219 Q52.4517 551.354 53.4065 548.935 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"523.944\" r=\"45\" fill=\"#f1711d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"523.944\" r=\"30\" fill=\"#9f2a62\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"803.804\" r=\"41\" fill=\"#e05535\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"1236.33\" r=\"33\" fill=\"#b0315a\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"86.1857\" r=\"60\" fill=\"#f4f78d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"803.804\" r=\"37\" fill=\"#cb4148\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"1085.1\" r=\"41\" fill=\"#e15634\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"662.128\" r=\"23\" fill=\"#6e186e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"233.527\" r=\"55\" fill=\"#f8cb35\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"662.128\" r=\"46\" fill=\"#f68012\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"939.756\" r=\"32\" fill=\"#ae305b\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"233.527\" r=\"33\" fill=\"#b33259\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"803.804\" r=\"29\" fill=\"#992765\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"1236.33\" r=\"18\" fill=\"#4b0c6b\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"1236.33\" r=\"35\" fill=\"#be3852\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"523.944\" r=\"49\" fill=\"#fa9706\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"1085.1\" r=\"20\" fill=\"#59106d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"803.804\" r=\"34\" fill=\"#ba3654\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"1236.33\" r=\"37\" fill=\"#d04445\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"1384.24\" r=\"14\" fill=\"#30095c\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"939.756\" r=\"27\" fill=\"#88216a\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"374.157\" r=\"56\" fill=\"#f5d746\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"662.128\" r=\"13\" fill=\"#2b0a56\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"803.804\" r=\"46\" fill=\"#f57e14\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"1085.1\" r=\"31\" fill=\"#a32b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"662.128\" r=\"49\" fill=\"#fa9506\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"86.1857\" r=\"59\" fill=\"#f1f17c\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"662.128\" r=\"36\" fill=\"#c83f4b\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"374.157\" r=\"51\" fill=\"#fbab0f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"86.1857\" r=\"55\" fill=\"#f7ce39\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"1384.24\" r=\"17\" fill=\"#480b6a\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"803.804\" r=\"40\" fill=\"#e05435\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"374.157\" r=\"50\" fill=\"#fba208\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"86.1857\" r=\"61\" fill=\"#fcfea4\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"1085.1\" r=\"17\" fill=\"#490b6a\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"662.128\" r=\"40\" fill=\"#dd5238\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"233.527\" r=\"53\" fill=\"#fac027\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"523.944\" r=\"43\" fill=\"#eb6626\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"1236.33\" r=\"37\" fill=\"#cf4445\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"939.756\" r=\"44\" fill=\"#f0711e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"523.944\" r=\"16\" fill=\"#430a68\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"1085.1\" r=\"36\" fill=\"#c53d4d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"523.944\" r=\"49\" fill=\"#fa9606\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"523.944\" r=\"48\" fill=\"#f99008\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"662.128\" r=\"44\" fill=\"#ef6e20\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"233.527\" r=\"57\" fill=\"#f3e055\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"662.128\" r=\"28\" fill=\"#912467\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"939.756\" r=\"43\" fill=\"#ea6428\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"374.157\" r=\"51\" fill=\"#fbaa0e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"803.804\" r=\"16\" fill=\"#430a68\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"1236.33\" r=\"13\" fill=\"#2b0a57\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"233.527\" r=\"44\" fill=\"#f0701f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"939.756\" r=\"22\" fill=\"#68166e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"374.157\" r=\"50\" fill=\"#fba409\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"233.527\" r=\"55\" fill=\"#f7d23e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"803.804\" r=\"44\" fill=\"#ed6a23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"523.944\" r=\"34\" fill=\"#bb3654\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"86.1857\" r=\"38\" fill=\"#d54940\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"1384.24\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"233.527\" r=\"55\" fill=\"#f8cb34\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"233.527\" r=\"54\" fill=\"#f9c830\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"1384.24\" r=\"14\" fill=\"#34095f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"1085.1\" r=\"39\" fill=\"#d74b3e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"1236.33\" r=\"4\" fill=\"#01010b\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"86.1857\" r=\"57\" fill=\"#f1e763\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"86.1857\" r=\"56\" fill=\"#f4dd4f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"374.157\" r=\"23\" fill=\"#70196e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"86.1857\" r=\"56\" fill=\"#f5d848\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"523.944\" r=\"51\" fill=\"#fba80d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"939.756\" r=\"39\" fill=\"#da4e3c\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"1236.33\" r=\"21\" fill=\"#64146e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"939.756\" r=\"17\" fill=\"#4a0b6a\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"1085.1\" r=\"7\" fill=\"#08061f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"374.157\" r=\"52\" fill=\"#fbb61a\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"233.527\" r=\"50\" fill=\"#fb9e07\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"374.157\" r=\"43\" fill=\"#ec6726\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"1085.1\" r=\"14\" fill=\"#33095e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"939.756\" r=\"39\" fill=\"#d74b3e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"374.157\" r=\"51\" fill=\"#fba70c\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"803.804\" r=\"19\" fill=\"#560f6d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"939.756\" r=\"12\" fill=\"#260b51\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"1384.24\" r=\"31\" fill=\"#a72d5f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"662.128\" r=\"43\" fill=\"#ed6825\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"939.756\" r=\"9\" fill=\"#110a30\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"1384.24\" r=\"9\" fill=\"#140a36\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"1384.24\" r=\"29\" fill=\"#952666\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"86.1857\" r=\"60\" fill=\"#f5f891\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"233.527\" r=\"54\" fill=\"#f9c72f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"662.128\" r=\"44\" fill=\"#ed6a24\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"1384.24\" r=\"36\" fill=\"#c83e4b\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"1085.1\" r=\"42\" fill=\"#e9622a\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"1384.24\" r=\"25\" fill=\"#7d1d6c\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"1384.24\" r=\"36\" fill=\"#c73e4c\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"1236.33\" r=\"21\" fill=\"#5f136e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"1236.33\" r=\"36\" fill=\"#c43c4e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"803.804\" r=\"13\" fill=\"#290b54\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"86.1857\" r=\"50\" fill=\"#fba007\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"1085.1\" r=\"22\" fill=\"#6b176e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"523.944\" r=\"51\" fill=\"#fba50b\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"374.157\" r=\"38\" fill=\"#d24643\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip763\">\n",
       "    <rect x=\"2160\" y=\"47\" width=\"73\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<g clip-path=\"url(#clip763)\">\n",
       "<image width=\"72\" height=\"1376\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAEgAAAVgCAYAAADsKhu7AAAL6UlEQVR4nO3dwZEjNxBFQVBR/lsh\n",
       "L6UFZIHqHclDpgUTL35wCXTP7Off+/c7/K+/vv0D/DqBgkBBoCBQmPv+/fbP8NMsKAgUBAoChXnv\n",
       "z7d/hp9mQUGgIFAQKAgUHDWCBQWBgkBBoDDPh/TKgoJAQaAgUJh3fUhvLCgIFAQKAgWBgqNGsKAg\n",
       "UBAoCBR8SAcLCgIFgYJAwX1QsKAgUBAoCBQECnMcNVYWFAQKAgWBgvugYEFBoCBQECgIFOa4MFtZ\n",
       "UBAoCBQECnPuP9/+GX6aBQWBgkBBoODSPlhQECgIFAQKAgX3QcGCgkBBoCBQ8CEdLCgIFAQKAgX3\n",
       "QcGCgkBBoCBQECjMx1FjZUFBoCBQECjMuf5XhI0FBYGCQEGgIFBw1AgWFAQKAgWBgqNGsKAgUBAo\n",
       "CBS8HxQsKAgUBAoCBYHCfBw1VhYUBAoCBYGCt1yDBQWBgkBBoOCbdLCgIFAQKAgUBAoePQcLCgIF\n",
       "gYJAwVEjWFAQKAgUBAoCBUeNYEFBoCBQECg4agQLCgIFgYJAYc693/4ZfpoFBYGCQEGgIFBw1AgW\n",
       "FAQKAgWBgkv7YEFBoCBQECj4kA4WFAQKAgWBgkBhPp5qrCwoCBQECgIFR41gQUGgIFAQKAgUvEAV\n",
       "LCgIFAQKAgX3QcGCgkBBoCBQcB8ULCgIFAQKAgWBgvugYEFBoCBQECj4kA4WFAQKAgWBgl/qDRYU\n",
       "BAoCBYGCQMFRI1hQECgIFAQKPqSDBQWBgkBBoCBQ8K9YsKAgUBAoCBR8SAcLCgIFgYJAYc7zIb2x\n",
       "oCBQECgIFAQKjhrBgoJAQaAgUPAhHSwoCBQECgIFH9LBgoJAQaAgUBAozLnv2z/DT7OgIFAQKAgU\n",
       "HDWCBQWBgkBBoCBQcNQIFhQECgIFgYKjRrCgIFAQKAgUfJMOFhQECgIFgYJAYY6TxsqCgkBBoCBQ\n",
       "mPMcNTYWFAQKAgWBgm/SwYKCQEGgIFAQKPhXLFhQECgIFAQK44/g7SwoCBQECgIFgYKjRrCgIFAQ\n",
       "KAgUfEgHCwoCBYGCQGHO/Xz7Z/hpFhQECgIFgYJAYd7zr9jGgoJAQaAgUHAfFCwoCBQECgIF90HB\n",
       "goJAQaAgUBAozLsabdQJAgWBgkDBUSNYUBAoCBQECgKFOR49rywoCBQECgKFeY4aKwsKAgWBgkBh\n",
       "jkv7lTpBoCBQECgIFBw1ggUFgYJAQaDg0j5YUBAoCBQECl7iDOoEgYJAQaAgUPCWa7CgIFAQKAgU\n",
       "XNoHCwoCBYGCQEGgMO9ptFEnCBQECgIF90HBgoJAQaAgUHAfFCwoCBQECgIFgcIc90ErdYJAQaAg\n",
       "UHDUCBYUBAoCBYGCD+lgQUGgIFAQKAgU/NZzsKAgUBAoCBT81nNQJwgUBAoCBYHCPEeNlQUFgYJA\n",
       "QaDgqUawoCBQECgIFPyvCEGdIFAQKAgUBArug4IFBYGCQEGg4D4oWFAQKAgUBAq+SQcLCgIFgYJA\n",
       "QaDgLdegThAoCBQECo4awYKCQEGgIFAQKHiqESwoCBQECgIFR41gQUGgIFAQKPiva4IFBYGCQEGg\n",
       "IFDwv2QGdYJAQaAgUHAfFCwoCBQECgIFT1aDBQWBgkBBoCBQcNQIFhQECgIFgcJcH9IrCwoCBYGC\n",
       "QEGg4LeegzpBoCBQECi4DwoWFAQKAgWBgg/pYEFBoCBQECgIFLxAFSwoCBQECgIFR41gQUGgIFAQ\n",
       "KPiQDhYUBAoCBYGCQMGfpgjqBIGCQEGgMNel/cqCgkBBoCBQECi4MAsWFAQKAgWBgg/pYEFBoCBQ\n",
       "ECj4kA4WFAQKAgWBgkDBH1gKFhQECgIFgYKjRrCgIFAQKAgUfEgHCwoCBYGCQEGg4FcRgjpBoCBQ\n",
       "ECj4VYRgQUGgIFAQKAgUXJgFCwoCBYGCQMGHdLCgIFAQKAgUvMQZLCgIFAQKAgWBgqNGsKAgUBAo\n",
       "CBTmHR/SGwsKAgWBgkDBfVCwoCBQECgIFAQK7oOCBQWBgkBBoOCoESwoCBQECgIFgYKjRrCgIFAQ\n",
       "KAgUHDWCBQWBgkBBoOCbdLCgIFAQKAgUBAr+FQsWFAQKAgWBwly/irCyoCBQECgIFHyTDhYUBAoC\n",
       "BYGCQMGj52BBQaAgUBAozHvf/hF+mwUFgYJAQaAgUHDUCBYUBAoCBYGCv4IXLCgIFAQKAgXfpIMF\n",
       "BYGCQEGgIFDwAlWwoCBQECgIFPwqQrCgIFAQKAgUvB8ULCgIFAQKAgWBgqcawYKCQEGgIFBwaR8s\n",
       "KAgUBAoCBYGCpxrBgoJAQaAgUHAfFCwoCBQECgIFj56DBQWBgkBBoCBQcB8ULCgIFAQKAgWPnoMF\n",
       "BYGCQEGgMPfbP8GPs6AgUBAoCBQECo4awYKCQEGgIFDwflCwoCBQECgIFAQKLsyCBQWBgkBBoOA+\n",
       "KFhQECgIFAQKvkkHCwoCBYGCQEGg4KgRLCgIFAQKAgWPnoMFBYGCQEGgMP4yxc6CgkBBoCBQECg4\n",
       "agQLCgIFgYJAwaPnYEFBoCBQECgIFDx6DhYUBAoCBYGCo0awoCBQECgIFHyTDhYUBAoCBYGCQGGu\n",
       "N6hWFhQECgIFgYL7oGBBQaAgUBAouA8KFhQECgIFgYJAwVEjWFAQKAgUBArzXNqvLCgIFAQKAgWB\n",
       "wtzjwmxjQUGgIFAQKHg/KFhQECgIFAQK/gpesKAgUBAoCBQECv4KXrCgIFAQKAgUPHoOFhQECgIF\n",
       "gYKXOIMFBYGCQEGgIFDwC3XBgoJAQaAgUHDUCBYUBAoCBYGCQMFbrsGCgkBBoCBQ8JZrsKAgUBAo\n",
       "CBR8kw4WFAQKAgWBgkDBW67BgoJAQaAgUPD3g4IFBYGCQEGg4D4oWFAQKAgUBAoCBY+egwUFgYJA\n",
       "QaDgqBEsKAgUBAoCBYGCo0awoCBQECgIFBw1ggUFgYJAQaDgQzpYUBAoCBQECgIF90HBgoJAQaAg\n",
       "UHDUCBYUBAoCBYGCb9LBgoJAQaAgUBAoOGoECwoCBYGCQMHfDwoWFAQKAgWBgkDB/+0TLCgIFAQK\n",
       "AgX3QcGCgkBBoCBQmHd8Sm8sKAgUBAoCBYGCo0awoCBQECgIFDx6DhYUBAoCBYGCJ6vBgoJAQaAg\n",
       "UBAouA8KFhQECgIFgYJHz8GCgkBBoCBQECg4agQLCgIFgYJAwR9YChYUBAoCBYHCXC8IrSwoCBQE\n",
       "CgIFgYIXqIIFBYGCQEGgMM9RY2VBQaAgUBAo+CYdLCgIFAQKAgWBgqcawYKCQEGgIFDwflCwoCBQ\n",
       "ECgIFAQKc/1C3cqCgkBBoCBQcB8ULCgIFAQKAgV/miJYUBAoCBQECgIFL1AFCwoCBYGCQMGlfbCg\n",
       "IFAQKAgU3AcFCwoCBYGCQEGgMPe4EdpYUBAoCBQECu6DggUFgYJAQaAgUPCvWLCgIFAQKAgUPNUI\n",
       "FhQECgIFgYJL+2BBQaAgUBAoCBTmfhw1NhYUBAoCBYGCo0awoCBQECgIFOb5kF5ZUBAoCBQECgIF\n",
       "R41gQUGgIFAQKMz9+JDeWFAQKAgUBAoCBUeNYEFBoCBQECh4qhEsKAgUBAoCBd+kgwUFgYJAQaAg\n",
       "UPCvWLCgIFAQKAgUPHoOFhQECgIFgcK88+fbP8NPs6AgUBAoCBQECu6DggUFgYJAQaDg/aBgQUGg\n",
       "IFAQKAgU5j4XZhsLCgIFgYJAwVEjWFAQKAgUBApzPXpeWVAQKAgUBAoCBUeNYEFBoCBQECg4agQL\n",
       "CgIFgYJAYd7zTXpjQUGgIFAQKAgU3AcFCwoCBYGCQMFLnMGCgkBBoCBQECg4agQLCgIFgYJAYZ6j\n",
       "xsqCgkBBoCBQ8E06WFAQKAgUBAoChbleoFpZUBAoCBQECv7gdrCgIFAQKAgU/CpCsKAgUBAoCBQE\n",
       "Cp5qBAsKAgWBgkDBUSNYUBAoCBQECgIF/4oFCwoCBYGCQMGj52BBQaAgUBAo+CYdLCgIFAQKAgWB\n",
       "whyPnlcWFAQKAgWBgqNGsKAgUBAoCBS8xBksKAgUBAoCBYHCHEeNlQUFgYJAQaDg/aBgQUGgIFAQ\n",
       "KAgUPHoOFhQECgIFgYL7oGBBQaAgUBAozDvv2z/DT7OgIFAQKAgUBArug4IFBYGCQEGgMMdRY2VB\n",
       "QaAgUBAo/AdhU+3PqlcPeQAAAABJRU5ErkJggg==\n",
       "\" transform=\"translate(2161, 47)\"/>\n",
       "</g>\n",
       "<path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 1271.65 Q2277.09 1271.65 2275.26 1275.21 Q2273.45 1278.75 2273.45 1285.88 Q2273.45 1292.99 2275.26 1296.55 Q2277.09 1300.1 2280.7 1300.1 Q2284.33 1300.1 2286.14 1296.55 Q2287.97 1292.99 2287.97 1285.88 Q2287.97 1278.75 2286.14 1275.21 Q2284.33 1271.65 2280.7 1271.65 M2280.7 1267.94 Q2286.51 1267.94 2289.57 1272.55 Q2292.64 1277.13 2292.64 1285.88 Q2292.64 1294.61 2289.57 1299.22 Q2286.51 1303.8 2280.7 1303.8 Q2274.89 1303.8 2271.81 1299.22 Q2268.76 1294.61 2268.76 1285.88 Q2268.76 1277.13 2271.81 1272.55 Q2274.89 1267.94 2280.7 1267.94 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 1297.25 L2302.6 1297.25 L2302.6 1303.13 L2297.71 1303.13 L2297.71 1297.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2311.7 1299.19 L2328.01 1299.19 L2328.01 1303.13 L2306.07 1303.13 L2306.07 1299.19 Q2308.73 1296.44 2313.32 1291.81 Q2317.92 1287.16 2319.1 1285.81 Q2321.35 1283.29 2322.23 1281.55 Q2323.13 1279.79 2323.13 1278.1 Q2323.13 1275.35 2321.19 1273.61 Q2319.26 1271.88 2316.16 1271.88 Q2313.96 1271.88 2311.51 1272.64 Q2309.08 1273.41 2306.3 1274.96 L2306.3 1270.23 Q2309.13 1269.1 2311.58 1268.52 Q2314.03 1267.94 2316.07 1267.94 Q2321.44 1267.94 2324.64 1270.63 Q2327.83 1273.31 2327.83 1277.8 Q2327.83 1279.93 2327.02 1281.85 Q2326.23 1283.75 2324.13 1286.35 Q2323.55 1287.02 2320.45 1290.23 Q2317.34 1293.43 2311.7 1299.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2331.9 1268.57 L2354.13 1268.57 L2354.13 1270.56 L2341.58 1303.13 L2336.7 1303.13 L2348.5 1272.5 L2331.9 1272.5 L2331.9 1268.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2359.24 1268.57 L2377.6 1268.57 L2377.6 1272.5 L2363.52 1272.5 L2363.52 1280.98 Q2364.54 1280.63 2365.56 1280.47 Q2366.58 1280.28 2367.6 1280.28 Q2373.38 1280.28 2376.76 1283.45 Q2380.14 1286.62 2380.14 1292.04 Q2380.14 1297.62 2376.67 1300.72 Q2373.2 1303.8 2366.88 1303.8 Q2364.7 1303.8 2362.44 1303.43 Q2360.19 1303.06 2357.78 1302.32 L2357.78 1297.62 Q2359.87 1298.75 2362.09 1299.31 Q2364.31 1299.86 2366.79 1299.86 Q2370.79 1299.86 2373.13 1297.76 Q2375.47 1295.65 2375.47 1292.04 Q2375.47 1288.43 2373.13 1286.32 Q2370.79 1284.22 2366.79 1284.22 Q2364.91 1284.22 2363.04 1284.63 Q2361.19 1285.05 2359.24 1285.93 L2359.24 1268.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 1111.76 Q2277.09 1111.76 2275.26 1115.33 Q2273.45 1118.87 2273.45 1126 Q2273.45 1133.11 2275.26 1136.67 Q2277.09 1140.21 2280.7 1140.21 Q2284.33 1140.21 2286.14 1136.67 Q2287.97 1133.11 2287.97 1126 Q2287.97 1118.87 2286.14 1115.33 Q2284.33 1111.76 2280.7 1111.76 M2280.7 1108.06 Q2286.51 1108.06 2289.57 1112.67 Q2292.64 1117.25 2292.64 1126 Q2292.64 1134.73 2289.57 1139.33 Q2286.51 1143.92 2280.7 1143.92 Q2274.89 1143.92 2271.81 1139.33 Q2268.76 1134.73 2268.76 1126 Q2268.76 1117.25 2271.81 1112.67 Q2274.89 1108.06 2280.7 1108.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 1137.37 L2302.6 1137.37 L2302.6 1143.25 L2297.71 1143.25 L2297.71 1137.37 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2321.83 1124.61 Q2325.19 1125.33 2327.07 1127.6 Q2328.96 1129.87 2328.96 1133.2 Q2328.96 1138.32 2325.45 1141.12 Q2321.93 1143.92 2315.45 1143.92 Q2313.27 1143.92 2310.95 1143.48 Q2308.66 1143.06 2306.21 1142.2 L2306.21 1137.69 Q2308.15 1138.82 2310.47 1139.4 Q2312.78 1139.98 2315.31 1139.98 Q2319.7 1139.98 2322 1138.25 Q2324.31 1136.51 2324.31 1133.2 Q2324.31 1130.14 2322.16 1128.43 Q2320.03 1126.69 2316.21 1126.69 L2312.18 1126.69 L2312.18 1122.85 L2316.39 1122.85 Q2319.84 1122.85 2321.67 1121.49 Q2323.5 1120.1 2323.5 1117.51 Q2323.5 1114.84 2321.6 1113.43 Q2319.73 1112 2316.21 1112 Q2314.29 1112 2312.09 1112.41 Q2309.89 1112.83 2307.25 1113.71 L2307.25 1109.54 Q2309.91 1108.8 2312.23 1108.43 Q2314.57 1108.06 2316.63 1108.06 Q2321.95 1108.06 2325.05 1110.49 Q2328.15 1112.9 2328.15 1117.02 Q2328.15 1119.89 2326.51 1121.88 Q2324.87 1123.85 2321.83 1124.61 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2344.03 1111.76 Q2340.42 1111.76 2338.59 1115.33 Q2336.79 1118.87 2336.79 1126 Q2336.79 1133.11 2338.59 1136.67 Q2340.42 1140.21 2344.03 1140.21 Q2347.67 1140.21 2349.47 1136.67 Q2351.3 1133.11 2351.3 1126 Q2351.3 1118.87 2349.47 1115.33 Q2347.67 1111.76 2344.03 1111.76 M2344.03 1108.06 Q2349.84 1108.06 2352.9 1112.67 Q2355.98 1117.25 2355.98 1126 Q2355.98 1134.73 2352.9 1139.33 Q2349.84 1143.92 2344.03 1143.92 Q2338.22 1143.92 2335.14 1139.33 Q2332.09 1134.73 2332.09 1126 Q2332.09 1117.25 2335.14 1112.67 Q2338.22 1108.06 2344.03 1108.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2371.05 1111.76 Q2367.44 1111.76 2365.61 1115.33 Q2363.8 1118.87 2363.8 1126 Q2363.8 1133.11 2365.61 1136.67 Q2367.44 1140.21 2371.05 1140.21 Q2374.68 1140.21 2376.49 1136.67 Q2378.32 1133.11 2378.32 1126 Q2378.32 1118.87 2376.49 1115.33 Q2374.68 1111.76 2371.05 1111.76 M2371.05 1108.06 Q2376.86 1108.06 2379.91 1112.67 Q2382.99 1117.25 2382.99 1126 Q2382.99 1134.73 2379.91 1139.33 Q2376.86 1143.92 2371.05 1143.92 Q2365.24 1143.92 2362.16 1139.33 Q2359.1 1134.73 2359.1 1126 Q2359.1 1117.25 2362.16 1112.67 Q2365.24 1108.06 2371.05 1108.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 951.883 Q2277.09 951.883 2275.26 955.447 Q2273.45 958.989 2273.45 966.119 Q2273.45 973.225 2275.26 976.79 Q2277.09 980.331 2280.7 980.331 Q2284.33 980.331 2286.14 976.79 Q2287.97 973.225 2287.97 966.119 Q2287.97 958.989 2286.14 955.447 Q2284.33 951.883 2280.7 951.883 M2280.7 948.179 Q2286.51 948.179 2289.57 952.785 Q2292.64 957.369 2292.64 966.119 Q2292.64 974.845 2289.57 979.452 Q2286.51 984.035 2280.7 984.035 Q2274.89 984.035 2271.81 979.452 Q2268.76 974.845 2268.76 966.119 Q2268.76 957.369 2271.81 952.785 Q2274.89 948.179 2280.7 948.179 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 977.484 L2302.6 977.484 L2302.6 983.364 L2297.71 983.364 L2297.71 977.484 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2321.83 964.73 Q2325.19 965.447 2327.07 967.716 Q2328.96 969.984 2328.96 973.318 Q2328.96 978.433 2325.45 981.234 Q2321.93 984.035 2315.45 984.035 Q2313.27 984.035 2310.95 983.595 Q2308.66 983.179 2306.21 982.322 L2306.21 977.808 Q2308.15 978.943 2310.47 979.521 Q2312.78 980.1 2315.31 980.1 Q2319.7 980.1 2322 978.364 Q2324.31 976.628 2324.31 973.318 Q2324.31 970.262 2322.16 968.549 Q2320.03 966.813 2316.21 966.813 L2312.18 966.813 L2312.18 962.97 L2316.39 962.97 Q2319.84 962.97 2321.67 961.605 Q2323.5 960.216 2323.5 957.623 Q2323.5 954.961 2321.6 953.549 Q2319.73 952.114 2316.21 952.114 Q2314.29 952.114 2312.09 952.531 Q2309.89 952.947 2307.25 953.827 L2307.25 949.66 Q2309.91 948.92 2312.23 948.549 Q2314.57 948.179 2316.63 948.179 Q2321.95 948.179 2325.05 950.609 Q2328.15 953.017 2328.15 957.137 Q2328.15 960.008 2326.51 961.998 Q2324.87 963.966 2321.83 964.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2338.06 979.429 L2354.38 979.429 L2354.38 983.364 L2332.44 983.364 L2332.44 979.429 Q2335.1 976.674 2339.68 972.044 Q2344.29 967.392 2345.47 966.049 Q2347.71 963.526 2348.59 961.79 Q2349.5 960.031 2349.5 958.341 Q2349.5 955.586 2347.55 953.85 Q2345.63 952.114 2342.53 952.114 Q2340.33 952.114 2337.88 952.878 Q2335.45 953.642 2332.67 955.193 L2332.67 950.471 Q2335.49 949.336 2337.95 948.758 Q2340.4 948.179 2342.44 948.179 Q2347.81 948.179 2351 950.864 Q2354.2 953.549 2354.2 958.04 Q2354.2 960.17 2353.39 962.091 Q2352.6 963.989 2350.49 966.582 Q2349.91 967.253 2346.81 970.47 Q2343.71 973.665 2338.06 979.429 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2359.5 948.804 L2377.85 948.804 L2377.85 952.739 L2363.78 952.739 L2363.78 961.211 Q2364.8 960.864 2365.82 960.702 Q2366.83 960.517 2367.85 960.517 Q2373.64 960.517 2377.02 963.688 Q2380.4 966.859 2380.4 972.276 Q2380.4 977.855 2376.93 980.956 Q2373.45 984.035 2367.14 984.035 Q2364.96 984.035 2362.69 983.665 Q2360.45 983.294 2358.04 982.554 L2358.04 977.855 Q2360.12 978.989 2362.34 979.544 Q2364.57 980.1 2367.04 980.1 Q2371.05 980.1 2373.38 977.994 Q2375.72 975.887 2375.72 972.276 Q2375.72 968.665 2373.38 966.558 Q2371.05 964.452 2367.04 964.452 Q2365.17 964.452 2363.29 964.869 Q2361.44 965.285 2359.5 966.165 L2359.5 948.804 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 792.001 Q2277.09 792.001 2275.26 795.565 Q2273.45 799.107 2273.45 806.237 Q2273.45 813.343 2275.26 816.908 Q2277.09 820.45 2280.7 820.45 Q2284.33 820.45 2286.14 816.908 Q2287.97 813.343 2287.97 806.237 Q2287.97 799.107 2286.14 795.565 Q2284.33 792.001 2280.7 792.001 M2280.7 788.297 Q2286.51 788.297 2289.57 792.903 Q2292.64 797.487 2292.64 806.237 Q2292.64 814.963 2289.57 819.57 Q2286.51 824.153 2280.7 824.153 Q2274.89 824.153 2271.81 819.57 Q2268.76 814.963 2268.76 806.237 Q2268.76 797.487 2271.81 792.903 Q2274.89 788.297 2280.7 788.297 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 817.602 L2302.6 817.602 L2302.6 823.482 L2297.71 823.482 L2297.71 817.602 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2321.83 804.848 Q2325.19 805.565 2327.07 807.834 Q2328.96 810.102 2328.96 813.436 Q2328.96 818.551 2325.45 821.352 Q2321.93 824.153 2315.45 824.153 Q2313.27 824.153 2310.95 823.713 Q2308.66 823.297 2306.21 822.44 L2306.21 817.926 Q2308.15 819.061 2310.47 819.639 Q2312.78 820.218 2315.31 820.218 Q2319.7 820.218 2322 818.482 Q2324.31 816.746 2324.31 813.436 Q2324.31 810.38 2322.16 808.667 Q2320.03 806.931 2316.21 806.931 L2312.18 806.931 L2312.18 803.089 L2316.39 803.089 Q2319.84 803.089 2321.67 801.723 Q2323.5 800.334 2323.5 797.741 Q2323.5 795.079 2321.6 793.667 Q2319.73 792.232 2316.21 792.232 Q2314.29 792.232 2312.09 792.649 Q2309.89 793.065 2307.25 793.945 L2307.25 789.778 Q2309.91 789.038 2312.23 788.667 Q2314.57 788.297 2316.63 788.297 Q2321.95 788.297 2325.05 790.727 Q2328.15 793.135 2328.15 797.255 Q2328.15 800.126 2326.51 802.116 Q2324.87 804.084 2321.83 804.848 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2334.08 788.922 L2352.44 788.922 L2352.44 792.857 L2338.36 792.857 L2338.36 801.329 Q2339.38 800.982 2340.4 800.82 Q2341.42 800.635 2342.44 800.635 Q2348.22 800.635 2351.6 803.806 Q2354.98 806.977 2354.98 812.394 Q2354.98 817.973 2351.51 821.075 Q2348.04 824.153 2341.72 824.153 Q2339.54 824.153 2337.27 823.783 Q2335.03 823.412 2332.62 822.672 L2332.62 817.973 Q2334.7 819.107 2336.93 819.663 Q2339.15 820.218 2341.63 820.218 Q2345.63 820.218 2347.97 818.112 Q2350.31 816.005 2350.31 812.394 Q2350.31 808.783 2347.97 806.676 Q2345.63 804.57 2341.63 804.57 Q2339.75 804.57 2337.88 804.987 Q2336.02 805.403 2334.08 806.283 L2334.08 788.922 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2370.05 792.001 Q2366.44 792.001 2364.61 795.565 Q2362.81 799.107 2362.81 806.237 Q2362.81 813.343 2364.61 816.908 Q2366.44 820.45 2370.05 820.45 Q2373.69 820.45 2375.49 816.908 Q2377.32 813.343 2377.32 806.237 Q2377.32 799.107 2375.49 795.565 Q2373.69 792.001 2370.05 792.001 M2370.05 788.297 Q2375.86 788.297 2378.92 792.903 Q2382 797.487 2382 806.237 Q2382 814.963 2378.92 819.57 Q2375.86 824.153 2370.05 824.153 Q2364.24 824.153 2361.16 819.57 Q2358.11 814.963 2358.11 806.237 Q2358.11 797.487 2361.16 792.903 Q2364.24 788.297 2370.05 788.297 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 632.119 Q2277.09 632.119 2275.26 635.683 Q2273.45 639.225 2273.45 646.355 Q2273.45 653.461 2275.26 657.026 Q2277.09 660.568 2280.7 660.568 Q2284.33 660.568 2286.14 657.026 Q2287.97 653.461 2287.97 646.355 Q2287.97 639.225 2286.14 635.683 Q2284.33 632.119 2280.7 632.119 M2280.7 628.415 Q2286.51 628.415 2289.57 633.021 Q2292.64 637.605 2292.64 646.355 Q2292.64 655.082 2289.57 659.688 Q2286.51 664.271 2280.7 664.271 Q2274.89 664.271 2271.81 659.688 Q2268.76 655.082 2268.76 646.355 Q2268.76 637.605 2271.81 633.021 Q2274.89 628.415 2280.7 628.415 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 657.72 L2302.6 657.72 L2302.6 663.6 L2297.71 663.6 L2297.71 657.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2321.83 644.966 Q2325.19 645.683 2327.07 647.952 Q2328.96 650.22 2328.96 653.554 Q2328.96 658.669 2325.45 661.47 Q2321.93 664.271 2315.45 664.271 Q2313.27 664.271 2310.95 663.831 Q2308.66 663.415 2306.21 662.558 L2306.21 658.044 Q2308.15 659.179 2310.47 659.757 Q2312.78 660.336 2315.31 660.336 Q2319.7 660.336 2322 658.6 Q2324.31 656.864 2324.31 653.554 Q2324.31 650.498 2322.16 648.785 Q2320.03 647.049 2316.21 647.049 L2312.18 647.049 L2312.18 643.207 L2316.39 643.207 Q2319.84 643.207 2321.67 641.841 Q2323.5 640.452 2323.5 637.859 Q2323.5 635.197 2321.6 633.785 Q2319.73 632.35 2316.21 632.35 Q2314.29 632.35 2312.09 632.767 Q2309.89 633.183 2307.25 634.063 L2307.25 629.896 Q2309.91 629.156 2312.23 628.785 Q2314.57 628.415 2316.63 628.415 Q2321.95 628.415 2325.05 630.846 Q2328.15 633.253 2328.15 637.373 Q2328.15 640.244 2326.51 642.234 Q2324.87 644.202 2321.83 644.966 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2332.85 629.04 L2355.07 629.04 L2355.07 631.031 L2342.53 663.6 L2337.64 663.6 L2349.45 632.975 L2332.85 632.975 L2332.85 629.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2360.19 629.04 L2378.55 629.04 L2378.55 632.975 L2364.47 632.975 L2364.47 641.447 Q2365.49 641.1 2366.51 640.938 Q2367.53 640.753 2368.55 640.753 Q2374.33 640.753 2377.71 643.924 Q2381.09 647.095 2381.09 652.512 Q2381.09 658.091 2377.62 661.193 Q2374.15 664.271 2367.83 664.271 Q2365.65 664.271 2363.39 663.901 Q2361.14 663.531 2358.73 662.79 L2358.73 658.091 Q2360.82 659.225 2363.04 659.781 Q2365.26 660.336 2367.74 660.336 Q2371.74 660.336 2374.08 658.23 Q2376.42 656.123 2376.42 652.512 Q2376.42 648.901 2374.08 646.795 Q2371.74 644.688 2367.74 644.688 Q2365.86 644.688 2363.99 645.105 Q2362.14 645.521 2360.19 646.401 L2360.19 629.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 472.237 Q2277.09 472.237 2275.26 475.802 Q2273.45 479.343 2273.45 486.473 Q2273.45 493.579 2275.26 497.144 Q2277.09 500.686 2280.7 500.686 Q2284.33 500.686 2286.14 497.144 Q2287.97 493.579 2287.97 486.473 Q2287.97 479.343 2286.14 475.802 Q2284.33 472.237 2280.7 472.237 M2280.7 468.533 Q2286.51 468.533 2289.57 473.14 Q2292.64 477.723 2292.64 486.473 Q2292.64 495.2 2289.57 499.806 Q2286.51 504.389 2280.7 504.389 Q2274.89 504.389 2271.81 499.806 Q2268.76 495.2 2268.76 486.473 Q2268.76 477.723 2271.81 473.14 Q2274.89 468.533 2280.7 468.533 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 497.838 L2302.6 497.838 L2302.6 503.718 L2297.71 503.718 L2297.71 497.838 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2320.51 473.232 L2308.71 491.681 L2320.51 491.681 L2320.51 473.232 M2319.29 469.158 L2325.17 469.158 L2325.17 491.681 L2330.1 491.681 L2330.1 495.57 L2325.17 495.57 L2325.17 503.718 L2320.51 503.718 L2320.51 495.57 L2304.91 495.57 L2304.91 491.056 L2319.29 469.158 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2345.17 472.237 Q2341.56 472.237 2339.73 475.802 Q2337.92 479.343 2337.92 486.473 Q2337.92 493.579 2339.73 497.144 Q2341.56 500.686 2345.17 500.686 Q2348.8 500.686 2350.61 497.144 Q2352.44 493.579 2352.44 486.473 Q2352.44 479.343 2350.61 475.802 Q2348.8 472.237 2345.17 472.237 M2345.17 468.533 Q2350.98 468.533 2354.03 473.14 Q2357.11 477.723 2357.11 486.473 Q2357.11 495.2 2354.03 499.806 Q2350.98 504.389 2345.17 504.389 Q2339.36 504.389 2336.28 499.806 Q2333.22 495.2 2333.22 486.473 Q2333.22 477.723 2336.28 473.14 Q2339.36 468.533 2345.17 468.533 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2372.18 472.237 Q2368.57 472.237 2366.74 475.802 Q2364.94 479.343 2364.94 486.473 Q2364.94 493.579 2366.74 497.144 Q2368.57 500.686 2372.18 500.686 Q2375.82 500.686 2377.62 497.144 Q2379.45 493.579 2379.45 486.473 Q2379.45 479.343 2377.62 475.802 Q2375.82 472.237 2372.18 472.237 M2372.18 468.533 Q2377.99 468.533 2381.05 473.14 Q2384.13 477.723 2384.13 486.473 Q2384.13 495.2 2381.05 499.806 Q2377.99 504.389 2372.18 504.389 Q2366.37 504.389 2363.29 499.806 Q2360.24 495.2 2360.24 486.473 Q2360.24 477.723 2363.29 473.14 Q2366.37 468.533 2372.18 468.533 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 312.355 Q2277.09 312.355 2275.26 315.92 Q2273.45 319.461 2273.45 326.591 Q2273.45 333.697 2275.26 337.262 Q2277.09 340.804 2280.7 340.804 Q2284.33 340.804 2286.14 337.262 Q2287.97 333.697 2287.97 326.591 Q2287.97 319.461 2286.14 315.92 Q2284.33 312.355 2280.7 312.355 M2280.7 308.651 Q2286.51 308.651 2289.57 313.258 Q2292.64 317.841 2292.64 326.591 Q2292.64 335.318 2289.57 339.924 Q2286.51 344.507 2280.7 344.507 Q2274.89 344.507 2271.81 339.924 Q2268.76 335.318 2268.76 326.591 Q2268.76 317.841 2271.81 313.258 Q2274.89 308.651 2280.7 308.651 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 337.957 L2302.6 337.957 L2302.6 343.836 L2297.71 343.836 L2297.71 337.957 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2320.51 313.35 L2308.71 331.799 L2320.51 331.799 L2320.51 313.35 M2319.29 309.276 L2325.17 309.276 L2325.17 331.799 L2330.1 331.799 L2330.1 335.688 L2325.17 335.688 L2325.17 343.836 L2320.51 343.836 L2320.51 335.688 L2304.91 335.688 L2304.91 331.174 L2319.29 309.276 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2339.2 339.901 L2355.51 339.901 L2355.51 343.836 L2333.57 343.836 L2333.57 339.901 Q2336.23 337.146 2340.82 332.517 Q2345.42 327.864 2346.6 326.521 Q2348.85 323.998 2349.73 322.262 Q2350.63 320.503 2350.63 318.813 Q2350.63 316.058 2348.69 314.322 Q2346.76 312.586 2343.66 312.586 Q2341.46 312.586 2339.01 313.35 Q2336.58 314.114 2333.8 315.665 L2333.8 310.943 Q2336.63 309.809 2339.08 309.23 Q2341.53 308.651 2343.57 308.651 Q2348.94 308.651 2352.14 311.336 Q2355.33 314.021 2355.33 318.512 Q2355.33 320.642 2354.52 322.563 Q2353.73 324.461 2351.63 327.054 Q2351.05 327.725 2347.95 330.943 Q2344.84 334.137 2339.2 339.901 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2360.63 309.276 L2378.99 309.276 L2378.99 313.211 L2364.91 313.211 L2364.91 321.683 Q2365.93 321.336 2366.95 321.174 Q2367.97 320.989 2368.99 320.989 Q2374.77 320.989 2378.15 324.16 Q2381.53 327.332 2381.53 332.748 Q2381.53 338.327 2378.06 341.429 Q2374.59 344.507 2368.27 344.507 Q2366.09 344.507 2363.82 344.137 Q2361.58 343.767 2359.17 343.026 L2359.17 338.327 Q2361.26 339.461 2363.48 340.017 Q2365.7 340.572 2368.18 340.572 Q2372.18 340.572 2374.52 338.466 Q2376.86 336.359 2376.86 332.748 Q2376.86 329.137 2374.52 327.031 Q2372.18 324.924 2368.18 324.924 Q2366.3 324.924 2364.43 325.341 Q2362.57 325.758 2360.63 326.637 L2360.63 309.276 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 152.473 Q2277.09 152.473 2275.26 156.038 Q2273.45 159.579 2273.45 166.709 Q2273.45 173.815 2275.26 177.38 Q2277.09 180.922 2280.7 180.922 Q2284.33 180.922 2286.14 177.38 Q2287.97 173.815 2287.97 166.709 Q2287.97 159.579 2286.14 156.038 Q2284.33 152.473 2280.7 152.473 M2280.7 148.769 Q2286.51 148.769 2289.57 153.376 Q2292.64 157.959 2292.64 166.709 Q2292.64 175.436 2289.57 180.042 Q2286.51 184.625 2280.7 184.625 Q2274.89 184.625 2271.81 180.042 Q2268.76 175.436 2268.76 166.709 Q2268.76 157.959 2271.81 153.376 Q2274.89 148.769 2280.7 148.769 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 178.075 L2302.6 178.075 L2302.6 183.954 L2297.71 183.954 L2297.71 178.075 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2320.51 153.468 L2308.71 171.917 L2320.51 171.917 L2320.51 153.468 M2319.29 149.394 L2325.17 149.394 L2325.17 171.917 L2330.1 171.917 L2330.1 175.806 L2325.17 175.806 L2325.17 183.954 L2320.51 183.954 L2320.51 175.806 L2304.91 175.806 L2304.91 171.292 L2319.29 149.394 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2335.21 149.394 L2353.57 149.394 L2353.57 153.329 L2339.5 153.329 L2339.5 161.802 Q2340.51 161.454 2341.53 161.292 Q2342.55 161.107 2343.57 161.107 Q2349.36 161.107 2352.74 164.278 Q2356.12 167.45 2356.12 172.866 Q2356.12 178.445 2352.64 181.547 Q2349.17 184.625 2342.85 184.625 Q2340.68 184.625 2338.41 184.255 Q2336.16 183.885 2333.76 183.144 L2333.76 178.445 Q2335.84 179.579 2338.06 180.135 Q2340.28 180.69 2342.76 180.69 Q2346.76 180.69 2349.1 178.584 Q2351.44 176.477 2351.44 172.866 Q2351.44 169.255 2349.1 167.149 Q2346.76 165.042 2342.76 165.042 Q2340.89 165.042 2339.01 165.459 Q2337.16 165.876 2335.21 166.755 L2335.21 149.394 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2371.19 152.473 Q2367.57 152.473 2365.75 156.038 Q2363.94 159.579 2363.94 166.709 Q2363.94 173.815 2365.75 177.38 Q2367.57 180.922 2371.19 180.922 Q2374.82 180.922 2376.63 177.38 Q2378.45 173.815 2378.45 166.709 Q2378.45 159.579 2376.63 156.038 Q2374.82 152.473 2371.19 152.473 M2371.19 148.769 Q2377 148.769 2380.05 153.376 Q2383.13 157.959 2383.13 166.709 Q2383.13 175.436 2380.05 180.042 Q2377 184.625 2371.19 184.625 Q2365.38 184.625 2362.3 180.042 Q2359.24 175.436 2359.24 166.709 Q2359.24 157.959 2362.3 153.376 Q2365.38 148.769 2371.19 148.769 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2232.76,1423.18 2232.76,1289.48 2256.76,1289.48 2232.76,1289.48 2232.76,1129.59 2256.76,1129.59 2232.76,1129.59 2232.76,969.713 2256.76,969.713 2232.76,969.713 \n",
       "  2232.76,809.831 2256.76,809.831 2232.76,809.831 2232.76,649.949 2256.76,649.949 2232.76,649.949 2232.76,490.067 2256.76,490.067 2232.76,490.067 2232.76,330.185 \n",
       "  2256.76,330.185 2232.76,330.185 2232.76,170.303 2256.76,170.303 2232.76,170.303 2232.76,47.2441 \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(self_tuning_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(\n",
       "    base_estimator = nothing,\n",
       "    n_estimators = 10,\n",
       "    learning_rate = 0.1,\n",
       "    algorithm = \"SAMME.R\",\n",
       "    random_state = nothing)\u001b[34m @665\u001b[39m"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = fitted_params(self_tuning_boost)\n",
    "best.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25409"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss = round(z.report.best_result.measurement[1],digits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_n = best.best_model.n_estimators\n",
    "best_lr = best.best_model.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"Figures/LearningCurve_Boost_nestimators:$(best_n)_lr:$(best_lr)_loss:$(best_loss)\"\n",
    "png(replace(fn,'.' => ','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(\n",
       "    base_estimator = nothing,\n",
       "    n_estimators = 10,\n",
       "    learning_rate = 0.1,\n",
       "    algorithm = \"SAMME.R\",\n",
       "    random_state = nothing)\u001b[34m @665\u001b[39m"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_boost_model = best.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{AdaBoostClassifier} @893\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @775\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @402\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_Boost = machine(final_boost_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{AdaBoostClassifier} @893\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/machines.jl:322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{AdaBoostClassifier} @893\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @775\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @402\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(Final_Boost, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: both StatsBase and MLJ export \"predict\"; uses of it in module Main must be qualified\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: predict not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: predict not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[32]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "y2 = predict(Final_Boost, X[test,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: 2 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: 2 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[33]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "cross_entropy(y2, y[test]) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: 2 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: 2 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[34]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "acc(y2, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(mode.(y), y[test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
