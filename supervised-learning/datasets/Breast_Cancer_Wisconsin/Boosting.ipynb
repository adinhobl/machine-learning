{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using CSV\n",
    "using MLJ\n",
    "using Plots\n",
    "using StatsBase\n",
    "\n",
    "include(\"../../lib.jl\")\n",
    "\n",
    "ENV[\"LINES\"]=50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 2. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 3. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 4. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 5. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 6. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 7. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 8. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 9. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 10. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 11. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 12. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 13. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 14. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 15. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 16. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 17. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 18. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 19. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 20. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 21. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 22. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 23. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 24. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 25. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 26. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 27. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 28. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 29. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 30. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 31. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 32. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 33. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 34. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 35. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 36. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 37. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 38. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 39. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 40. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 41. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 42. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 43. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 44. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 45. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 46. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 47. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 48. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 49. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 50. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 51. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 52. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 53. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 54. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 55. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 56. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 57. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 58. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 59. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 60. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 61. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 62. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 63. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 64. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 65. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 66. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 67. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 68. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 69. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 70. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 71. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 72. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 73. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 74. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 75. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 76. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 77. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 78. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 79. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 80. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 81. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 82. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 83. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 84. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 85. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 86. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 87. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 88. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 89. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 90. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 91. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 92. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 93. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 94. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 95. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 96. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 97. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 98. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 99. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 100. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 101. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 102. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 103. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 104. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 105. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 106. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 107. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 108. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 109. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 110. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 111. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 112. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 113. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 114. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 115. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 116. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 117. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 118. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 119. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 120. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 121. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 122. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 123. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 124. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 125. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 126. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 127. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 128. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 129. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 130. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 131. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 132. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 133. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 134. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 135. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 136. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 137. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 138. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 139. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 140. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 141. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 142. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 143. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 144. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 145. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 146. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 147. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 148. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 149. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 150. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 151. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 152. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 153. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 154. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 155. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 156. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 157. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 158. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 159. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 160. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 161. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 162. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 163. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 164. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 165. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 166. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 167. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 168. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 169. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 170. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 171. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 172. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 173. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 174. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 175. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 176. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 177. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 178. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 179. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 180. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 181. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 182. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 183. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 184. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 185. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 186. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 187. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 188. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 189. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 190. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 191. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 192. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 193. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 194. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 195. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 196. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 197. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 198. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 199. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 200. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 201. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 202. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 203. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 204. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 205. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 206. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 207. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 208. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 209. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 210. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 211. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 212. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 213. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 214. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 215. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 216. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 217. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 218. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 219. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 220. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 221. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 222. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 223. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 224. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 225. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 226. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 227. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 228. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 229. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 230. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 231. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 232. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 233. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 234. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 235. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 236. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 237. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 238. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 239. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 240. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 241. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 242. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 243. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 244. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 245. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 246. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 247. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 248. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 249. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 250. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 251. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 252. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 253. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 254. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 255. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 256. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 257. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 258. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 259. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 260. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 261. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 262. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 263. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 264. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 265. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 266. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 267. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 268. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 269. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 270. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 271. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 272. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 273. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 274. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 275. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 276. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 277. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 278. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 279. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 280. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 281. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 282. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 283. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 284. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 285. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 286. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 287. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 288. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 289. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 290. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 291. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 292. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 293. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 294. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 295. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 296. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 297. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 298. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 299. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 300. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 301. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 302. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 303. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 304. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 305. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 306. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 307. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 308. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 309. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 310. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 311. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 312. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 313. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 314. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 315. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 316. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 317. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 318. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 319. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 320. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 321. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 322. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 323. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 324. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 325. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 326. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 327. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 328. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 329. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 330. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 331. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 332. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 333. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 334. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 335. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 336. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 337. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 338. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 339. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 340. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 341. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 342. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 343. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 344. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 345. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 346. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 347. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 348. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 349. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 350. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 351. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 352. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 353. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 354. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 355. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 356. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 357. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 358. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 359. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 360. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 361. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 362. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 363. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 364. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 365. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 366. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 367. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 368. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 369. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 370. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 371. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 372. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 373. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 374. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 375. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 376. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 377. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 378. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 379. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 380. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 381. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 382. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 383. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 384. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 385. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 386. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 387. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 388. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 389. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 390. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 391. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 392. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 393. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 394. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 395. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 396. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 397. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 398. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 399. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 400. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 401. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 402. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 403. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 404. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 405. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 406. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 407. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 408. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 409. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 410. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 411. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 412. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 413. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 414. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 415. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 416. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 417. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 418. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 419. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 420. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 421. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 422. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 423. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 424. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 425. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 426. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 427. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 428. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 429. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 430. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 431. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 432. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 433. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 434. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 435. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 436. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 437. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 438. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 439. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 440. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 441. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 442. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 443. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 444. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 445. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 446. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 447. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 448. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 449. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 450. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 451. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 452. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 453. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 454. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 455. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 456. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 457. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 458. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 459. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 460. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 461. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 462. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 463. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 464. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 465. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 466. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 467. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 468. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 469. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 470. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 471. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 472. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 473. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 474. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 475. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 476. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 477. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 478. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 479. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 480. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 481. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 482. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 483. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 484. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 485. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 486. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 487. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 488. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 489. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 490. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 491. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 492. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 493. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 494. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 495. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 496. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 497. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 498. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 499. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 500. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 501. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 502. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 503. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 504. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 505. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 506. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 507. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 508. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 509. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 510. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 511. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 512. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 513. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 514. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 515. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 516. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 517. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 518. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 519. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 520. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 521. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 522. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 523. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 524. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 525. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 526. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 527. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 528. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 529. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 530. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 531. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 532. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 533. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 534. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 535. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 536. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 537. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 538. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 539. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 540. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 541. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 542. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 543. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 544. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 545. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 546. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 547. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 548. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 549. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 550. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 551. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 552. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 553. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 554. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 555. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 556. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 557. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 558. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 559. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 560. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 561. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 562. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 563. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 564. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 565. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 566. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 567. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 568. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 569. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 570. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>id</th><th>diagnosis</th><th>radius_mean</th><th>texture_mean</th><th>perimeter_mean</th><th>area_mean</th><th>smoothness_mean</th></tr><tr><th></th><th>Int64</th><th>String</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>569 rows  33 columns (omitted printing of 26 columns)</p><tr><th>1</th><td>842302</td><td>M</td><td>17.99</td><td>10.38</td><td>122.8</td><td>1001.0</td><td>0.1184</td></tr><tr><th>2</th><td>842517</td><td>M</td><td>20.57</td><td>17.77</td><td>132.9</td><td>1326.0</td><td>0.08474</td></tr><tr><th>3</th><td>84300903</td><td>M</td><td>19.69</td><td>21.25</td><td>130.0</td><td>1203.0</td><td>0.1096</td></tr><tr><th>4</th><td>84348301</td><td>M</td><td>11.42</td><td>20.38</td><td>77.58</td><td>386.1</td><td>0.1425</td></tr><tr><th>5</th><td>84358402</td><td>M</td><td>20.29</td><td>14.34</td><td>135.1</td><td>1297.0</td><td>0.1003</td></tr><tr><th>6</th><td>843786</td><td>M</td><td>12.45</td><td>15.7</td><td>82.57</td><td>477.1</td><td>0.1278</td></tr><tr><th>7</th><td>844359</td><td>M</td><td>18.25</td><td>19.98</td><td>119.6</td><td>1040.0</td><td>0.09463</td></tr><tr><th>8</th><td>84458202</td><td>M</td><td>13.71</td><td>20.83</td><td>90.2</td><td>577.9</td><td>0.1189</td></tr><tr><th>9</th><td>844981</td><td>M</td><td>13.0</td><td>21.82</td><td>87.5</td><td>519.8</td><td>0.1273</td></tr><tr><th>10</th><td>84501001</td><td>M</td><td>12.46</td><td>24.04</td><td>83.97</td><td>475.9</td><td>0.1186</td></tr><tr><th>11</th><td>845636</td><td>M</td><td>16.02</td><td>23.24</td><td>102.7</td><td>797.8</td><td>0.08206</td></tr><tr><th>12</th><td>84610002</td><td>M</td><td>15.78</td><td>17.89</td><td>103.6</td><td>781.0</td><td>0.0971</td></tr><tr><th>13</th><td>846226</td><td>M</td><td>19.17</td><td>24.8</td><td>132.4</td><td>1123.0</td><td>0.0974</td></tr><tr><th>14</th><td>846381</td><td>M</td><td>15.85</td><td>23.95</td><td>103.7</td><td>782.7</td><td>0.08401</td></tr><tr><th>15</th><td>84667401</td><td>M</td><td>13.73</td><td>22.61</td><td>93.6</td><td>578.3</td><td>0.1131</td></tr><tr><th>16</th><td>84799002</td><td>M</td><td>14.54</td><td>27.54</td><td>96.73</td><td>658.8</td><td>0.1139</td></tr><tr><th>17</th><td>848406</td><td>M</td><td>14.68</td><td>20.13</td><td>94.74</td><td>684.5</td><td>0.09867</td></tr><tr><th>18</th><td>84862001</td><td>M</td><td>16.13</td><td>20.68</td><td>108.1</td><td>798.8</td><td>0.117</td></tr><tr><th>19</th><td>849014</td><td>M</td><td>19.81</td><td>22.15</td><td>130.0</td><td>1260.0</td><td>0.09831</td></tr><tr><th>20</th><td>8510426</td><td>B</td><td>13.54</td><td>14.36</td><td>87.46</td><td>566.3</td><td>0.09779</td></tr><tr><th>21</th><td>8510653</td><td>B</td><td>13.08</td><td>15.71</td><td>85.63</td><td>520.0</td><td>0.1075</td></tr><tr><th>22</th><td>8510824</td><td>B</td><td>9.504</td><td>12.44</td><td>60.34</td><td>273.9</td><td>0.1024</td></tr><tr><th>23</th><td>8511133</td><td>M</td><td>15.34</td><td>14.26</td><td>102.5</td><td>704.4</td><td>0.1073</td></tr><tr><th>24</th><td>851509</td><td>M</td><td>21.16</td><td>23.04</td><td>137.2</td><td>1404.0</td><td>0.09428</td></tr><tr><th>25</th><td>852552</td><td>M</td><td>16.65</td><td>21.38</td><td>110.0</td><td>904.6</td><td>0.1121</td></tr><tr><th>26</th><td>852631</td><td>M</td><td>17.14</td><td>16.4</td><td>116.0</td><td>912.7</td><td>0.1186</td></tr><tr><th>27</th><td>852763</td><td>M</td><td>14.58</td><td>21.53</td><td>97.41</td><td>644.8</td><td>0.1054</td></tr><tr><th>28</th><td>852781</td><td>M</td><td>18.61</td><td>20.25</td><td>122.1</td><td>1094.0</td><td>0.0944</td></tr><tr><th>29</th><td>852973</td><td>M</td><td>15.3</td><td>25.27</td><td>102.4</td><td>732.4</td><td>0.1082</td></tr><tr><th>30</th><td>853201</td><td>M</td><td>17.57</td><td>15.05</td><td>115.0</td><td>955.1</td><td>0.09847</td></tr><tr><th>31</th><td>853401</td><td>M</td><td>18.63</td><td>25.11</td><td>124.8</td><td>1088.0</td><td>0.1064</td></tr><tr><th>32</th><td>853612</td><td>M</td><td>11.84</td><td>18.7</td><td>77.93</td><td>440.6</td><td>0.1109</td></tr><tr><th>33</th><td>85382601</td><td>M</td><td>17.02</td><td>23.98</td><td>112.8</td><td>899.3</td><td>0.1197</td></tr><tr><th>34</th><td>854002</td><td>M</td><td>19.27</td><td>26.47</td><td>127.9</td><td>1162.0</td><td>0.09401</td></tr><tr><th>35</th><td>854039</td><td>M</td><td>16.13</td><td>17.88</td><td>107.0</td><td>807.2</td><td>0.104</td></tr><tr><th>36</th><td>854253</td><td>M</td><td>16.74</td><td>21.59</td><td>110.1</td><td>869.5</td><td>0.0961</td></tr><tr><th>37</th><td>854268</td><td>M</td><td>14.25</td><td>21.72</td><td>93.63</td><td>633.0</td><td>0.09823</td></tr><tr><th>38</th><td>854941</td><td>B</td><td>13.03</td><td>18.42</td><td>82.61</td><td>523.8</td><td>0.08983</td></tr><tr><th>39</th><td>855133</td><td>M</td><td>14.99</td><td>25.2</td><td>95.54</td><td>698.8</td><td>0.09387</td></tr><tr><th>40</th><td>855138</td><td>M</td><td>13.48</td><td>20.82</td><td>88.4</td><td>559.2</td><td>0.1016</td></tr><tr><th>41</th><td>855167</td><td>M</td><td>13.44</td><td>21.58</td><td>86.18</td><td>563.0</td><td>0.08162</td></tr><tr><th>42</th><td>855563</td><td>M</td><td>10.95</td><td>21.35</td><td>71.9</td><td>371.1</td><td>0.1227</td></tr><tr><th>43</th><td>855625</td><td>M</td><td>19.07</td><td>24.81</td><td>128.3</td><td>1104.0</td><td>0.09081</td></tr><tr><th>44</th><td>856106</td><td>M</td><td>13.28</td><td>20.28</td><td>87.32</td><td>545.2</td><td>0.1041</td></tr><tr><th>45</th><td>85638502</td><td>M</td><td>13.17</td><td>21.81</td><td>85.42</td><td>531.5</td><td>0.09714</td></tr><tr><th>46</th><td>857010</td><td>M</td><td>18.65</td><td>17.6</td><td>123.7</td><td>1076.0</td><td>0.1099</td></tr><tr><th>47</th><td>85713702</td><td>B</td><td>8.196</td><td>16.84</td><td>51.71</td><td>201.9</td><td>0.086</td></tr><tr><th>48</th><td>85715</td><td>M</td><td>13.17</td><td>18.66</td><td>85.98</td><td>534.6</td><td>0.1158</td></tr><tr><th>49</th><td>857155</td><td>B</td><td>12.05</td><td>14.63</td><td>78.04</td><td>449.3</td><td>0.1031</td></tr><tr><th>50</th><td>857156</td><td>B</td><td>13.49</td><td>22.3</td><td>86.91</td><td>561.0</td><td>0.08752</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& id & diagnosis & radius\\_mean & texture\\_mean & perimeter\\_mean & area\\_mean & smoothness\\_mean & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & String & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 842302 & M & 17.99 & 10.38 & 122.8 & 1001.0 & 0.1184 & $\\dots$ \\\\\n",
       "\t2 & 842517 & M & 20.57 & 17.77 & 132.9 & 1326.0 & 0.08474 & $\\dots$ \\\\\n",
       "\t3 & 84300903 & M & 19.69 & 21.25 & 130.0 & 1203.0 & 0.1096 & $\\dots$ \\\\\n",
       "\t4 & 84348301 & M & 11.42 & 20.38 & 77.58 & 386.1 & 0.1425 & $\\dots$ \\\\\n",
       "\t5 & 84358402 & M & 20.29 & 14.34 & 135.1 & 1297.0 & 0.1003 & $\\dots$ \\\\\n",
       "\t6 & 843786 & M & 12.45 & 15.7 & 82.57 & 477.1 & 0.1278 & $\\dots$ \\\\\n",
       "\t7 & 844359 & M & 18.25 & 19.98 & 119.6 & 1040.0 & 0.09463 & $\\dots$ \\\\\n",
       "\t8 & 84458202 & M & 13.71 & 20.83 & 90.2 & 577.9 & 0.1189 & $\\dots$ \\\\\n",
       "\t9 & 844981 & M & 13.0 & 21.82 & 87.5 & 519.8 & 0.1273 & $\\dots$ \\\\\n",
       "\t10 & 84501001 & M & 12.46 & 24.04 & 83.97 & 475.9 & 0.1186 & $\\dots$ \\\\\n",
       "\t11 & 845636 & M & 16.02 & 23.24 & 102.7 & 797.8 & 0.08206 & $\\dots$ \\\\\n",
       "\t12 & 84610002 & M & 15.78 & 17.89 & 103.6 & 781.0 & 0.0971 & $\\dots$ \\\\\n",
       "\t13 & 846226 & M & 19.17 & 24.8 & 132.4 & 1123.0 & 0.0974 & $\\dots$ \\\\\n",
       "\t14 & 846381 & M & 15.85 & 23.95 & 103.7 & 782.7 & 0.08401 & $\\dots$ \\\\\n",
       "\t15 & 84667401 & M & 13.73 & 22.61 & 93.6 & 578.3 & 0.1131 & $\\dots$ \\\\\n",
       "\t16 & 84799002 & M & 14.54 & 27.54 & 96.73 & 658.8 & 0.1139 & $\\dots$ \\\\\n",
       "\t17 & 848406 & M & 14.68 & 20.13 & 94.74 & 684.5 & 0.09867 & $\\dots$ \\\\\n",
       "\t18 & 84862001 & M & 16.13 & 20.68 & 108.1 & 798.8 & 0.117 & $\\dots$ \\\\\n",
       "\t19 & 849014 & M & 19.81 & 22.15 & 130.0 & 1260.0 & 0.09831 & $\\dots$ \\\\\n",
       "\t20 & 8510426 & B & 13.54 & 14.36 & 87.46 & 566.3 & 0.09779 & $\\dots$ \\\\\n",
       "\t21 & 8510653 & B & 13.08 & 15.71 & 85.63 & 520.0 & 0.1075 & $\\dots$ \\\\\n",
       "\t22 & 8510824 & B & 9.504 & 12.44 & 60.34 & 273.9 & 0.1024 & $\\dots$ \\\\\n",
       "\t23 & 8511133 & M & 15.34 & 14.26 & 102.5 & 704.4 & 0.1073 & $\\dots$ \\\\\n",
       "\t24 & 851509 & M & 21.16 & 23.04 & 137.2 & 1404.0 & 0.09428 & $\\dots$ \\\\\n",
       "\t25 & 852552 & M & 16.65 & 21.38 & 110.0 & 904.6 & 0.1121 & $\\dots$ \\\\\n",
       "\t26 & 852631 & M & 17.14 & 16.4 & 116.0 & 912.7 & 0.1186 & $\\dots$ \\\\\n",
       "\t27 & 852763 & M & 14.58 & 21.53 & 97.41 & 644.8 & 0.1054 & $\\dots$ \\\\\n",
       "\t28 & 852781 & M & 18.61 & 20.25 & 122.1 & 1094.0 & 0.0944 & $\\dots$ \\\\\n",
       "\t29 & 852973 & M & 15.3 & 25.27 & 102.4 & 732.4 & 0.1082 & $\\dots$ \\\\\n",
       "\t30 & 853201 & M & 17.57 & 15.05 & 115.0 & 955.1 & 0.09847 & $\\dots$ \\\\\n",
       "\t31 & 853401 & M & 18.63 & 25.11 & 124.8 & 1088.0 & 0.1064 & $\\dots$ \\\\\n",
       "\t32 & 853612 & M & 11.84 & 18.7 & 77.93 & 440.6 & 0.1109 & $\\dots$ \\\\\n",
       "\t33 & 85382601 & M & 17.02 & 23.98 & 112.8 & 899.3 & 0.1197 & $\\dots$ \\\\\n",
       "\t34 & 854002 & M & 19.27 & 26.47 & 127.9 & 1162.0 & 0.09401 & $\\dots$ \\\\\n",
       "\t35 & 854039 & M & 16.13 & 17.88 & 107.0 & 807.2 & 0.104 & $\\dots$ \\\\\n",
       "\t36 & 854253 & M & 16.74 & 21.59 & 110.1 & 869.5 & 0.0961 & $\\dots$ \\\\\n",
       "\t37 & 854268 & M & 14.25 & 21.72 & 93.63 & 633.0 & 0.09823 & $\\dots$ \\\\\n",
       "\t38 & 854941 & B & 13.03 & 18.42 & 82.61 & 523.8 & 0.08983 & $\\dots$ \\\\\n",
       "\t39 & 855133 & M & 14.99 & 25.2 & 95.54 & 698.8 & 0.09387 & $\\dots$ \\\\\n",
       "\t40 & 855138 & M & 13.48 & 20.82 & 88.4 & 559.2 & 0.1016 & $\\dots$ \\\\\n",
       "\t41 & 855167 & M & 13.44 & 21.58 & 86.18 & 563.0 & 0.08162 & $\\dots$ \\\\\n",
       "\t42 & 855563 & M & 10.95 & 21.35 & 71.9 & 371.1 & 0.1227 & $\\dots$ \\\\\n",
       "\t43 & 855625 & M & 19.07 & 24.81 & 128.3 & 1104.0 & 0.09081 & $\\dots$ \\\\\n",
       "\t44 & 856106 & M & 13.28 & 20.28 & 87.32 & 545.2 & 0.1041 & $\\dots$ \\\\\n",
       "\t45 & 85638502 & M & 13.17 & 21.81 & 85.42 & 531.5 & 0.09714 & $\\dots$ \\\\\n",
       "\t46 & 857010 & M & 18.65 & 17.6 & 123.7 & 1076.0 & 0.1099 & $\\dots$ \\\\\n",
       "\t47 & 85713702 & B & 8.196 & 16.84 & 51.71 & 201.9 & 0.086 & $\\dots$ \\\\\n",
       "\t48 & 85715 & M & 13.17 & 18.66 & 85.98 & 534.6 & 0.1158 & $\\dots$ \\\\\n",
       "\t49 & 857155 & B & 12.05 & 14.63 & 78.04 & 449.3 & 0.1031 & $\\dots$ \\\\\n",
       "\t50 & 857156 & B & 13.49 & 22.3 & 86.91 & 561.0 & 0.08752 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "56933 DataFrame. Omitted printing of 28 columns\n",
       " Row  id        diagnosis  radius_mean  texture_mean  perimeter_mean \n",
       "      \u001b[90mInt64\u001b[39m     \u001b[90mString\u001b[39m     \u001b[90mFloat64\u001b[39m      \u001b[90mFloat64\u001b[39m       \u001b[90mFloat64\u001b[39m        \n",
       "\n",
       " 1    842302    M          17.99        10.38         122.8          \n",
       " 2    842517    M          20.57        17.77         132.9          \n",
       " 3    84300903  M          19.69        21.25         130.0          \n",
       " 4    84348301  M          11.42        20.38         77.58          \n",
       " 5    84358402  M          20.29        14.34         135.1          \n",
       " 6    843786    M          12.45        15.7          82.57          \n",
       " 7    844359    M          18.25        19.98         119.6          \n",
       " 8    84458202  M          13.71        20.83         90.2           \n",
       " 9    844981    M          13.0         21.82         87.5           \n",
       " 10   84501001  M          12.46        24.04         83.97          \n",
       " 11   845636    M          16.02        23.24         102.7          \n",
       " 12   84610002  M          15.78        17.89         103.6          \n",
       " 13   846226    M          19.17        24.8          132.4          \n",
       " 14   846381    M          15.85        23.95         103.7          \n",
       " 15   84667401  M          13.73        22.61         93.6           \n",
       " 16   84799002  M          14.54        27.54         96.73          \n",
       " 17   848406    M          14.68        20.13         94.74          \n",
       " 18   84862001  M          16.13        20.68         108.1          \n",
       " 19   849014    M          19.81        22.15         130.0          \n",
       " 20   8510426   B          13.54        14.36         87.46          \n",
       "\n",
       " 549  923169    B          9.683        19.34         61.05          \n",
       " 550  923465    B          10.82        24.21         68.89          \n",
       " 551  923748    B          10.86        21.48         68.51          \n",
       " 552  923780    B          11.13        22.44         71.49          \n",
       " 553  924084    B          12.77        29.43         81.35          \n",
       " 554  924342    B          9.333        21.94         59.01          \n",
       " 555  924632    B          12.88        28.92         82.5           \n",
       " 556  924934    B          10.29        27.61         65.67          \n",
       " 557  924964    B          10.16        19.59         64.73          \n",
       " 558  925236    B          9.423        27.88         59.26          \n",
       " 559  925277    B          14.59        22.68         96.39          \n",
       " 560  925291    B          11.51        23.93         74.52          \n",
       " 561  925292    B          14.05        27.15         91.38          \n",
       " 562  925311    B          11.2         29.37         70.67          \n",
       " 563  925622    M          15.22        30.62         103.4          \n",
       " 564  926125    M          20.92        25.09         143.0          \n",
       " 565  926424    M          21.56        22.39         142.0          \n",
       " 566  926682    M          20.13        28.25         131.2          \n",
       " 567  926954    M          16.6         28.08         108.3          \n",
       " 568  927241    M          20.6         29.33         140.1          \n",
       " 569  92751     B          7.76         24.54         47.92          "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CSV.read(\"./data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nunique</th><th>nmissing</th></tr><tr><th></th><th>Symbol</th><th>Union</th><th>Any</th><th>Union</th><th>Any</th><th>Union</th><th>Nothing</th></tr></thead><tbody><p>31 rows  8 columns (omitted printing of 1 columns)</p><tr><th>1</th><td>diagnosis</td><td></td><td>B</td><td></td><td>M</td><td>2</td><td></td></tr><tr><th>2</th><td>radius_mean</td><td>14.1273</td><td>6.981</td><td>13.37</td><td>28.11</td><td></td><td></td></tr><tr><th>3</th><td>texture_mean</td><td>19.2896</td><td>9.71</td><td>18.84</td><td>39.28</td><td></td><td></td></tr><tr><th>4</th><td>perimeter_mean</td><td>91.969</td><td>43.79</td><td>86.24</td><td>188.5</td><td></td><td></td></tr><tr><th>5</th><td>area_mean</td><td>654.889</td><td>143.5</td><td>551.1</td><td>2501.0</td><td></td><td></td></tr><tr><th>6</th><td>smoothness_mean</td><td>0.0963603</td><td>0.05263</td><td>0.09587</td><td>0.1634</td><td></td><td></td></tr><tr><th>7</th><td>compactness_mean</td><td>0.104341</td><td>0.01938</td><td>0.09263</td><td>0.3454</td><td></td><td></td></tr><tr><th>8</th><td>concavity_mean</td><td>0.0887993</td><td>0.0</td><td>0.06154</td><td>0.4268</td><td></td><td></td></tr><tr><th>9</th><td>concave points_mean</td><td>0.0489191</td><td>0.0</td><td>0.0335</td><td>0.2012</td><td></td><td></td></tr><tr><th>10</th><td>symmetry_mean</td><td>0.181162</td><td>0.106</td><td>0.1792</td><td>0.304</td><td></td><td></td></tr><tr><th>11</th><td>fractal_dimension_mean</td><td>0.0627976</td><td>0.04996</td><td>0.06154</td><td>0.09744</td><td></td><td></td></tr><tr><th>12</th><td>radius_se</td><td>0.405172</td><td>0.1115</td><td>0.3242</td><td>2.873</td><td></td><td></td></tr><tr><th>13</th><td>texture_se</td><td>1.21685</td><td>0.3602</td><td>1.108</td><td>4.885</td><td></td><td></td></tr><tr><th>14</th><td>perimeter_se</td><td>2.86606</td><td>0.757</td><td>2.287</td><td>21.98</td><td></td><td></td></tr><tr><th>15</th><td>area_se</td><td>40.3371</td><td>6.802</td><td>24.53</td><td>542.2</td><td></td><td></td></tr><tr><th>16</th><td>smoothness_se</td><td>0.00704098</td><td>0.001713</td><td>0.00638</td><td>0.03113</td><td></td><td></td></tr><tr><th>17</th><td>compactness_se</td><td>0.0254781</td><td>0.002252</td><td>0.02045</td><td>0.1354</td><td></td><td></td></tr><tr><th>18</th><td>concavity_se</td><td>0.0318937</td><td>0.0</td><td>0.02589</td><td>0.396</td><td></td><td></td></tr><tr><th>19</th><td>concave points_se</td><td>0.0117961</td><td>0.0</td><td>0.01093</td><td>0.05279</td><td></td><td></td></tr><tr><th>20</th><td>symmetry_se</td><td>0.0205423</td><td>0.007882</td><td>0.01873</td><td>0.07895</td><td></td><td></td></tr><tr><th>21</th><td>fractal_dimension_se</td><td>0.0037949</td><td>0.0008948</td><td>0.003187</td><td>0.02984</td><td></td><td></td></tr><tr><th>22</th><td>radius_worst</td><td>16.2692</td><td>7.93</td><td>14.97</td><td>36.04</td><td></td><td></td></tr><tr><th>23</th><td>texture_worst</td><td>25.6772</td><td>12.02</td><td>25.41</td><td>49.54</td><td></td><td></td></tr><tr><th>24</th><td>perimeter_worst</td><td>107.261</td><td>50.41</td><td>97.66</td><td>251.2</td><td></td><td></td></tr><tr><th>25</th><td>area_worst</td><td>880.583</td><td>185.2</td><td>686.5</td><td>4254.0</td><td></td><td></td></tr><tr><th>26</th><td>smoothness_worst</td><td>0.132369</td><td>0.07117</td><td>0.1313</td><td>0.2226</td><td></td><td></td></tr><tr><th>27</th><td>compactness_worst</td><td>0.254265</td><td>0.02729</td><td>0.2119</td><td>1.058</td><td></td><td></td></tr><tr><th>28</th><td>concavity_worst</td><td>0.272188</td><td>0.0</td><td>0.2267</td><td>1.252</td><td></td><td></td></tr><tr><th>29</th><td>concave points_worst</td><td>0.114606</td><td>0.0</td><td>0.09993</td><td>0.291</td><td></td><td></td></tr><tr><th>30</th><td>symmetry_worst</td><td>0.290076</td><td>0.1565</td><td>0.2822</td><td>0.6638</td><td></td><td></td></tr><tr><th>31</th><td>fractal_dimension_worst</td><td>0.0839458</td><td>0.05504</td><td>0.08004</td><td>0.2075</td><td></td><td></td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& variable & mean & min & median & max & nunique & nmissing & \\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union & Any & Union & Any & Union & Nothing & \\\\\n",
       "\t\\hline\n",
       "\t1 & diagnosis &  & B &  & M & 2 &  & $\\dots$ \\\\\n",
       "\t2 & radius\\_mean & 14.1273 & 6.981 & 13.37 & 28.11 &  &  & $\\dots$ \\\\\n",
       "\t3 & texture\\_mean & 19.2896 & 9.71 & 18.84 & 39.28 &  &  & $\\dots$ \\\\\n",
       "\t4 & perimeter\\_mean & 91.969 & 43.79 & 86.24 & 188.5 &  &  & $\\dots$ \\\\\n",
       "\t5 & area\\_mean & 654.889 & 143.5 & 551.1 & 2501.0 &  &  & $\\dots$ \\\\\n",
       "\t6 & smoothness\\_mean & 0.0963603 & 0.05263 & 0.09587 & 0.1634 &  &  & $\\dots$ \\\\\n",
       "\t7 & compactness\\_mean & 0.104341 & 0.01938 & 0.09263 & 0.3454 &  &  & $\\dots$ \\\\\n",
       "\t8 & concavity\\_mean & 0.0887993 & 0.0 & 0.06154 & 0.4268 &  &  & $\\dots$ \\\\\n",
       "\t9 & concave points\\_mean & 0.0489191 & 0.0 & 0.0335 & 0.2012 &  &  & $\\dots$ \\\\\n",
       "\t10 & symmetry\\_mean & 0.181162 & 0.106 & 0.1792 & 0.304 &  &  & $\\dots$ \\\\\n",
       "\t11 & fractal\\_dimension\\_mean & 0.0627976 & 0.04996 & 0.06154 & 0.09744 &  &  & $\\dots$ \\\\\n",
       "\t12 & radius\\_se & 0.405172 & 0.1115 & 0.3242 & 2.873 &  &  & $\\dots$ \\\\\n",
       "\t13 & texture\\_se & 1.21685 & 0.3602 & 1.108 & 4.885 &  &  & $\\dots$ \\\\\n",
       "\t14 & perimeter\\_se & 2.86606 & 0.757 & 2.287 & 21.98 &  &  & $\\dots$ \\\\\n",
       "\t15 & area\\_se & 40.3371 & 6.802 & 24.53 & 542.2 &  &  & $\\dots$ \\\\\n",
       "\t16 & smoothness\\_se & 0.00704098 & 0.001713 & 0.00638 & 0.03113 &  &  & $\\dots$ \\\\\n",
       "\t17 & compactness\\_se & 0.0254781 & 0.002252 & 0.02045 & 0.1354 &  &  & $\\dots$ \\\\\n",
       "\t18 & concavity\\_se & 0.0318937 & 0.0 & 0.02589 & 0.396 &  &  & $\\dots$ \\\\\n",
       "\t19 & concave points\\_se & 0.0117961 & 0.0 & 0.01093 & 0.05279 &  &  & $\\dots$ \\\\\n",
       "\t20 & symmetry\\_se & 0.0205423 & 0.007882 & 0.01873 & 0.07895 &  &  & $\\dots$ \\\\\n",
       "\t21 & fractal\\_dimension\\_se & 0.0037949 & 0.0008948 & 0.003187 & 0.02984 &  &  & $\\dots$ \\\\\n",
       "\t22 & radius\\_worst & 16.2692 & 7.93 & 14.97 & 36.04 &  &  & $\\dots$ \\\\\n",
       "\t23 & texture\\_worst & 25.6772 & 12.02 & 25.41 & 49.54 &  &  & $\\dots$ \\\\\n",
       "\t24 & perimeter\\_worst & 107.261 & 50.41 & 97.66 & 251.2 &  &  & $\\dots$ \\\\\n",
       "\t25 & area\\_worst & 880.583 & 185.2 & 686.5 & 4254.0 &  &  & $\\dots$ \\\\\n",
       "\t26 & smoothness\\_worst & 0.132369 & 0.07117 & 0.1313 & 0.2226 &  &  & $\\dots$ \\\\\n",
       "\t27 & compactness\\_worst & 0.254265 & 0.02729 & 0.2119 & 1.058 &  &  & $\\dots$ \\\\\n",
       "\t28 & concavity\\_worst & 0.272188 & 0.0 & 0.2267 & 1.252 &  &  & $\\dots$ \\\\\n",
       "\t29 & concave points\\_worst & 0.114606 & 0.0 & 0.09993 & 0.291 &  &  & $\\dots$ \\\\\n",
       "\t30 & symmetry\\_worst & 0.290076 & 0.1565 & 0.2822 & 0.6638 &  &  & $\\dots$ \\\\\n",
       "\t31 & fractal\\_dimension\\_worst & 0.0839458 & 0.05504 & 0.08004 & 0.2075 &  &  & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "318 DataFrame. Omitted printing of 3 columns\n",
       " Row  variable                 mean        min        median    max     \n",
       "      \u001b[90mSymbol\u001b[39m                   \u001b[90mUnion\u001b[39m      \u001b[90mAny\u001b[39m        \u001b[90mUnion\u001b[39m    \u001b[90mAny\u001b[39m     \n",
       "\n",
       " 1    diagnosis                            B                    M       \n",
       " 2    radius_mean              14.1273     6.981      13.37     28.11   \n",
       " 3    texture_mean             19.2896     9.71       18.84     39.28   \n",
       " 4    perimeter_mean           91.969      43.79      86.24     188.5   \n",
       " 5    area_mean                654.889     143.5      551.1     2501.0  \n",
       " 6    smoothness_mean          0.0963603   0.05263    0.09587   0.1634  \n",
       " 7    compactness_mean         0.104341    0.01938    0.09263   0.3454  \n",
       " 8    concavity_mean           0.0887993   0.0        0.06154   0.4268  \n",
       " 9    concave points_mean      0.0489191   0.0        0.0335    0.2012  \n",
       " 10   symmetry_mean            0.181162    0.106      0.1792    0.304   \n",
       " 11   fractal_dimension_mean   0.0627976   0.04996    0.06154   0.09744 \n",
       " 12   radius_se                0.405172    0.1115     0.3242    2.873   \n",
       " 13   texture_se               1.21685     0.3602     1.108     4.885   \n",
       " 14   perimeter_se             2.86606     0.757      2.287     21.98   \n",
       " 15   area_se                  40.3371     6.802      24.53     542.2   \n",
       " 16   smoothness_se            0.00704098  0.001713   0.00638   0.03113 \n",
       " 17   compactness_se           0.0254781   0.002252   0.02045   0.1354  \n",
       " 18   concavity_se             0.0318937   0.0        0.02589   0.396   \n",
       " 19   concave points_se        0.0117961   0.0        0.01093   0.05279 \n",
       " 20   symmetry_se              0.0205423   0.007882   0.01873   0.07895 \n",
       " 21   fractal_dimension_se     0.0037949   0.0008948  0.003187  0.02984 \n",
       " 22   radius_worst             16.2692     7.93       14.97     36.04   \n",
       " 23   texture_worst            25.6772     12.02      25.41     49.54   \n",
       " 24   perimeter_worst          107.261     50.41      97.66     251.2   \n",
       " 25   area_worst               880.583     185.2      686.5     4254.0  \n",
       " 26   smoothness_worst         0.132369    0.07117    0.1313    0.2226  \n",
       " 27   compactness_worst        0.254265    0.02729    0.2119    1.058   \n",
       " 28   concavity_worst          0.272188    0.0        0.2267    1.252   \n",
       " 29   concave points_worst     0.114606    0.0        0.09993   0.291   \n",
       " 30   symmetry_worst           0.290076    0.1565     0.2822    0.6638  \n",
       " 31   fractal_dimension_worst  0.0839458   0.05504    0.08004   0.2075  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[:, Not([33, 1])]\n",
    "describe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at class labels to see if dataset is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Int64} with 2 entries:\n",
       "  \"B\" => 357\n",
       "  \"M\" => 212"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = countmap(data[:diagnosis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.6274165202108963\n",
       " 0.37258347978910367"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(label_counts[i] / size(data)[1] for i in keys(label_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[22m _.names                 \u001b[0m\u001b[0m\u001b[22m _.types                         \u001b[0m\u001b[0m\u001b[22m _.scitypes    \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m diagnosis               \u001b[0m\u001b[0m CategoricalValue{String,UInt32} \u001b[0m\u001b[0m Multiclass{2} \u001b[0m\u001b[0m\n",
       "\u001b[0m radius_mean             \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m texture_mean            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m perimeter_mean          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m area_mean               \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m smoothness_mean         \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m compactness_mean        \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concavity_mean          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concave points_mean     \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m symmetry_mean           \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m fractal_dimension_mean  \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m radius_se               \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m texture_se              \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m perimeter_se            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m area_se                 \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m smoothness_se           \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m compactness_se          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concavity_se            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concave points_se       \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m symmetry_se             \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m fractal_dimension_se    \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m radius_worst            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m texture_worst           \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m perimeter_worst         \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m area_worst              \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m smoothness_worst        \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m compactness_worst       \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concavity_worst         \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concave points_worst    \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m symmetry_worst          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m fractal_dimension_worst \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "_.nrows = 569\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coerce!(data, :diagnosis=>Multiclass)\n",
    "schema(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CategoricalValue{String,UInt32}[\"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\"    \"B\", \"B\", \"B\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"B\"], 56930 DataFrame. Omitted printing of 26 columns\n",
       " Row  radius_mean  texture_mean  perimeter_mean  area_mean \n",
       "      \u001b[90mFloat64\u001b[39m      \u001b[90mFloat64\u001b[39m       \u001b[90mFloat64\u001b[39m         \u001b[90mFloat64\u001b[39m   \n",
       "\n",
       " 1    17.99        10.38         122.8           1001.0    \n",
       " 2    20.57        17.77         132.9           1326.0    \n",
       " 3    19.69        21.25         130.0           1203.0    \n",
       " 4    11.42        20.38         77.58           386.1     \n",
       " 5    20.29        14.34         135.1           1297.0    \n",
       " 6    12.45        15.7          82.57           477.1     \n",
       " 7    18.25        19.98         119.6           1040.0    \n",
       " 8    13.71        20.83         90.2            577.9     \n",
       " 9    13.0         21.82         87.5            519.8     \n",
       " 10   12.46        24.04         83.97           475.9     \n",
       " 11   16.02        23.24         102.7           797.8     \n",
       " 12   15.78        17.89         103.6           781.0     \n",
       " 13   19.17        24.8          132.4           1123.0    \n",
       " 14   15.85        23.95         103.7           782.7     \n",
       " 15   13.73        22.61         93.6            578.3     \n",
       " 16   14.54        27.54         96.73           658.8     \n",
       " 17   14.68        20.13         94.74           684.5     \n",
       " 18   16.13        20.68         108.1           798.8     \n",
       " 19   19.81        22.15         130.0           1260.0    \n",
       " 20   13.54        14.36         87.46           566.3     \n",
       "\n",
       " 549  9.683        19.34         61.05           285.7     \n",
       " 550  10.82        24.21         68.89           361.6     \n",
       " 551  10.86        21.48         68.51           360.5     \n",
       " 552  11.13        22.44         71.49           378.4     \n",
       " 553  12.77        29.43         81.35           507.9     \n",
       " 554  9.333        21.94         59.01           264.0     \n",
       " 555  12.88        28.92         82.5            514.3     \n",
       " 556  10.29        27.61         65.67           321.4     \n",
       " 557  10.16        19.59         64.73           311.7     \n",
       " 558  9.423        27.88         59.26           271.3     \n",
       " 559  14.59        22.68         96.39           657.1     \n",
       " 560  11.51        23.93         74.52           403.5     \n",
       " 561  14.05        27.15         91.38           600.4     \n",
       " 562  11.2         29.37         70.67           386.0     \n",
       " 563  15.22        30.62         103.4           716.9     \n",
       " 564  20.92        25.09         143.0           1347.0    \n",
       " 565  21.56        22.39         142.0           1479.0    \n",
       " 566  20.13        28.25         131.2           1261.0    \n",
       " 567  16.6         28.08         108.3           858.1     \n",
       " 568  20.6         29.33         140.1           1265.0    \n",
       " 569  7.76         24.54         47.92           181.0     )"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, X = unpack(data, ==(:diagnosis), colname->true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition train and test data accoring to class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([483, 534, 159, 31, 170, 416, 231, 43, 161, 286    134, 500, 395, 533, 112, 396, 297, 106, 303, 261], [392, 390, 320, 27, 328, 477, 19, 356, 518, 444    136, 559, 505, 274, 508, 358, 90, 296, 79, 415])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data to use when trying to fit a single validation set\n",
    "train, test = partition(eachindex(y), 0.7, shuffle=true, rng=123, stratify=values(data[:diagnosis])) # gives 70:30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.628140703517588\n",
       " 0.37185929648241206"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts = countmap(data[train,:diagnosis])\n",
    "collect(train_counts[i] / size(train)[1] for i in keys(train_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.6257309941520468\n",
       " 0.3742690058479532"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_counts = countmap(data[test,:diagnosis])\n",
    "collect(test_counts[i] / size(test)[1] for i in keys(test_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five Learning Algorithms\n",
    "\n",
    "* Decision trees with some form of pruning\n",
    "* Neural networks\n",
    "* Boosting\n",
    "* Support Vector Machines\n",
    "* k-nearest neighbors\n",
    "\n",
    "\n",
    "##### Testing\n",
    "* Implement the algorithms\n",
    "* Design two *interesting* classification problems. For the purposes of this assignment, a classification problem is just a set of training examples and a set of test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43-element Array{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype),T} where T<:Tuple,1}:\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianLDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianQDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = ScikitLearn, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " (name = ExtraTreesClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n",
       " (name = GaussianNBClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GaussianProcessClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GradientBoostingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = KNNClassifier, package_name = NearestNeighbors, ... )\n",
       " (name = KNeighborsClassifier, package_name = ScikitLearn, ... )\n",
       " (name = LDA, package_name = MultivariateStats, ... )\n",
       " (name = LGBMClassifier, package_name = LightGBM, ... )\n",
       " (name = LinearBinaryClassifier, package_name = GLM, ... )\n",
       " (name = LinearSVC, package_name = LIBSVM, ... )\n",
       " (name = LogisticCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = LogisticClassifier, package_name = ScikitLearn, ... )\n",
       " (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n",
       " (name = NuSVC, package_name = LIBSVM, ... )\n",
       " (name = PassiveAggressiveClassifier, package_name = ScikitLearn, ... )\n",
       " (name = PerceptronClassifier, package_name = ScikitLearn, ... )\n",
       " (name = ProbabilisticSGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVC, package_name = LIBSVM, ... )\n",
       " (name = SVMClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(matching(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJScikitLearnInterface \n",
      "import MLJScikitLearnInterface \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Loading into module \"Main\": \n",
      " @ MLJModels /home/andrew/.julia/packages/MLJModels/5DFoi/src/loading.jl:70\n",
      " Warning: New model type being bound to `AdaBoostClassifier2` to avoid conflict with an existing name. \n",
      " @ MLJModels /home/andrew/.julia/packages/MLJModels/5DFoi/src/loading.jl:78\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(\n",
       "    base_estimator = nothing,\n",
       "    n_estimators = 50,\n",
       "    learning_rate = 1.0,\n",
       "    algorithm = \"SAMME.R\",\n",
       "    random_state = nothing)\u001b[34m @766\u001b[39m"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load AdaBoostClassifier verbosity=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boosting\n",
    "* Implement or steal a boosted version of your decision trees. \n",
    "* As before, you will want to use some form of pruning, but presumably because you're using boosting you can afford to be much more aggressive about your pruning.\n",
    "\n",
    "**Chooses the hardest examples** talk about in write-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(\n",
       "    base_estimator = nothing,\n",
       "    n_estimators = 50,\n",
       "    learning_rate = 1.0,\n",
       "    algorithm = \"SAMME.R\",\n",
       "    random_state = nothing)\u001b[34m @589\u001b[39m"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_model = AdaBoostClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{AdaBoostClassifier} @874\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @200\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @474\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_mach = machine(boost_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{AdaBoostClassifier} @874\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/machines.jl:322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{AdaBoostClassifier} @874\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @200\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @474\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(boost_mach, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[22m _.measure     \u001b[0m\u001b[0m\u001b[22m _.measurement \u001b[0m\u001b[0m\u001b[22m _.per_fold                                 \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m cross_entropy \u001b[0m\u001b[0m 0.455         \u001b[0m\u001b[0m [0.441, 0.442, 0.443, 0.498, 0.417, 0.491] \u001b[0m\u001b[0m\n",
       "\u001b[0m acc           \u001b[0m\u001b[0m 0.954         \u001b[0m\u001b[0m [0.937, 0.968, 0.947, 0.947, 0.968, 0.957] \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "_.per_observation = [[[0.24, 0.347, ..., 0.253], [0.559, 0.651, ..., 0.36], [0.616, 0.191, ..., 0.565], [0.528, 0.553, ..., 0.436], [0.655, 0.304, ..., 1.1], [0.532, 0.423, ..., 0.56]], missing]\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boost_acc = evaluate!(boost_mach, resampling=CV(shuffle=true), measure=[cross_entropy, acc], \n",
    "                      verbosity=1, check_measure=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  17%[====>                    ]  ETA: 0:00:02\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:01\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:01\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:01\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[22m _.measure           \u001b[0m\u001b[0m\u001b[22m _.measurement \u001b[0m\u001b[0m\u001b[22m _.per_fold                                 \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m true_negative_rate  \u001b[0m\u001b[0m 0.988         \u001b[0m\u001b[0m [1.0, 1.0, 1.0, 0.964, 0.982, 0.984]       \u001b[0m\u001b[0m\n",
       "\u001b[0m true_positive_rate  \u001b[0m\u001b[0m 0.93          \u001b[0m\u001b[0m [0.933, 0.946, 0.857, 0.975, 1.0, 0.871]   \u001b[0m\u001b[0m\n",
       "\u001b[0m false_negative_rate \u001b[0m\u001b[0m 0.0696        \u001b[0m\u001b[0m [0.0667, 0.0541, 0.143, 0.025, 0.0, 0.129] \u001b[0m\u001b[0m\n",
       "\u001b[0m false_positive_rate \u001b[0m\u001b[0m 0.0117        \u001b[0m\u001b[0m [0.0, 0.0, 0.0, 0.0364, 0.0179, 0.0159]    \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "_.per_observation = [missing, missing, missing, missing]\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate!(boost_mach, resampling=CV(shuffle=true), measure=[tnr,tpr,fnr,fpr], verbosity=1, operation=predict_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_params(boost_mach);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch \n",
    "number of estimators vs learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJBase.NumericRange(Int64, :n_estimators, ... )"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param1 = :learning_rate\n",
    "param2 = :n_estimators\n",
    "\n",
    "r1 = range(boost_model, param1, lower=0.1, upper=1, scale=:linear)\n",
    "r2 = range(boost_model, param2, lower=10, upper=100, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "    model = AdaBoostClassifier(\n",
       "            base_estimator = nothing,\n",
       "            n_estimators = 50,\n",
       "            learning_rate = 1.0,\n",
       "            algorithm = \"SAMME.R\",\n",
       "            random_state = nothing),\n",
       "    tuning = Grid(\n",
       "            goal = nothing,\n",
       "            resolution = 10,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = CV(\n",
       "            nfolds = 6,\n",
       "            shuffle = false,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = cross_entropy(\n",
       "            eps = 2.220446049250313e-16),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = MLJBase.NumericRange{T,MLJBase.Bounded,Symbol} where T[\u001b[34mNumericRange{Float64,} @023\u001b[39m, \u001b[34mNumericRange{Int64,} @909\u001b[39m],\n",
       "    train_best = true,\n",
       "    repeats = 1,\n",
       "    n = nothing,\n",
       "    acceleration = CPUThreads{Int64}(1),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true)\u001b[34m @830\u001b[39m"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_boost_model = TunedModel(model=boost_model,\n",
    "                                    tuning=Grid(),\n",
    "                                    resampling=CV(), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1, r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,}} @973\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @979\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @117\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_boost = machine(self_tuning_boost_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,}} @973\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/machines.jl:322\n",
      " Info: Attempting to evaluate 100 models.\n",
      " @ MLJTuning /home/andrew/.julia/packages/MLJTuning/nuvTc/src/tuned_models.jl:501\n",
      "\u001b[33mEvaluating over 100 metamodels: 100%[=========================] Time: 0:00:47\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,}} @973\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @979\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @117\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = fit!(self_tuning_boost, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip760\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip760)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip761\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip760)\" d=\"\n",
       "M322.319 1423.18 L2112.76 1423.18 L2112.76 47.2441 L322.319 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip762\">\n",
       "    <rect x=\"322\" y=\"47\" width=\"1791\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  560.669,1423.18 560.669,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  936.022,1423.18 936.022,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1311.38,1423.18 1311.38,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1686.73,1423.18 1686.73,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2062.08,1423.18 2062.08,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  322.319,1384.24 2112.76,1384.24 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  322.319,1059.73 2112.76,1059.73 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  322.319,735.212 2112.76,735.212 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  322.319,410.699 2112.76,410.699 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip762)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  322.319,86.1857 2112.76,86.1857 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  322.319,1423.18 2112.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  322.319,1423.18 322.319,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  560.669,1423.18 560.669,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  936.022,1423.18 936.022,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1311.38,1423.18 1311.38,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1686.73,1423.18 1686.73,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2062.08,1423.18 2062.08,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  322.319,1384.24 343.805,1384.24 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  322.319,1059.73 343.805,1059.73 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  322.319,735.212 343.805,735.212 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  322.319,410.699 343.805,410.699 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  322.319,86.1857 343.805,86.1857 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip760)\" d=\"M 0 0 M542.984 1445.17 Q539.373 1445.17 537.544 1448.74 Q535.739 1452.28 535.739 1459.41 Q535.739 1466.51 537.544 1470.08 Q539.373 1473.62 542.984 1473.62 Q546.618 1473.62 548.424 1470.08 Q550.252 1466.51 550.252 1459.41 Q550.252 1452.28 548.424 1448.74 Q546.618 1445.17 542.984 1445.17 M542.984 1441.47 Q548.794 1441.47 551.85 1446.07 Q554.928 1450.66 554.928 1459.41 Q554.928 1468.13 551.85 1472.74 Q548.794 1477.32 542.984 1477.32 Q537.174 1477.32 534.095 1472.74 Q531.039 1468.13 531.039 1459.41 Q531.039 1450.66 534.095 1446.07 Q537.174 1441.47 542.984 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M559.998 1470.77 L564.882 1470.77 L564.882 1476.65 L559.998 1476.65 L559.998 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M573.979 1472.72 L590.298 1472.72 L590.298 1476.65 L568.354 1476.65 L568.354 1472.72 Q571.016 1469.96 575.599 1465.33 Q580.206 1460.68 581.386 1459.34 Q583.632 1456.81 584.511 1455.08 Q585.414 1453.32 585.414 1451.63 Q585.414 1448.87 583.47 1447.14 Q581.548 1445.4 578.447 1445.4 Q576.248 1445.4 573.794 1446.17 Q571.363 1446.93 568.586 1448.48 L568.586 1443.76 Q571.41 1442.62 573.863 1442.05 Q576.317 1441.47 578.354 1441.47 Q583.724 1441.47 586.919 1444.15 Q590.113 1446.84 590.113 1451.33 Q590.113 1453.46 589.303 1455.38 Q588.516 1457.28 586.41 1459.87 Q585.831 1460.54 582.729 1463.76 Q579.627 1466.95 573.979 1472.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M917.296 1445.17 Q913.685 1445.17 911.856 1448.74 Q910.05 1452.28 910.05 1459.41 Q910.05 1466.51 911.856 1470.08 Q913.685 1473.62 917.296 1473.62 Q920.93 1473.62 922.736 1470.08 Q924.564 1466.51 924.564 1459.41 Q924.564 1452.28 922.736 1448.74 Q920.93 1445.17 917.296 1445.17 M917.296 1441.47 Q923.106 1441.47 926.161 1446.07 Q929.24 1450.66 929.24 1459.41 Q929.24 1468.13 926.161 1472.74 Q923.106 1477.32 917.296 1477.32 Q911.486 1477.32 908.407 1472.74 Q905.351 1468.13 905.351 1459.41 Q905.351 1450.66 908.407 1446.07 Q911.486 1441.47 917.296 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M934.31 1470.77 L939.194 1470.77 L939.194 1476.65 L934.31 1476.65 L934.31 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M957.11 1446.17 L945.305 1464.61 L957.11 1464.61 L957.11 1446.17 M955.883 1442.09 L961.763 1442.09 L961.763 1464.61 L966.694 1464.61 L966.694 1468.5 L961.763 1468.5 L961.763 1476.65 L957.11 1476.65 L957.11 1468.5 L941.509 1468.5 L941.509 1463.99 L955.883 1442.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1292.81 1445.17 Q1289.2 1445.17 1287.37 1448.74 Q1285.57 1452.28 1285.57 1459.41 Q1285.57 1466.51 1287.37 1470.08 Q1289.2 1473.62 1292.81 1473.62 Q1296.45 1473.62 1298.25 1470.08 Q1300.08 1466.51 1300.08 1459.41 Q1300.08 1452.28 1298.25 1448.74 Q1296.45 1445.17 1292.81 1445.17 M1292.81 1441.47 Q1298.62 1441.47 1301.68 1446.07 Q1304.76 1450.66 1304.76 1459.41 Q1304.76 1468.13 1301.68 1472.74 Q1298.62 1477.32 1292.81 1477.32 Q1287 1477.32 1283.92 1472.74 Q1280.87 1468.13 1280.87 1459.41 Q1280.87 1450.66 1283.92 1446.07 Q1287 1441.47 1292.81 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1309.83 1470.77 L1314.71 1470.77 L1314.71 1476.65 L1309.83 1476.65 L1309.83 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1330.36 1457.51 Q1327.21 1457.51 1325.36 1459.66 Q1323.53 1461.81 1323.53 1465.56 Q1323.53 1469.29 1325.36 1471.47 Q1327.21 1473.62 1330.36 1473.62 Q1333.51 1473.62 1335.33 1471.47 Q1337.19 1469.29 1337.19 1465.56 Q1337.19 1461.81 1335.33 1459.66 Q1333.51 1457.51 1330.36 1457.51 M1339.64 1442.86 L1339.64 1447.11 Q1337.88 1446.28 1336.07 1445.84 Q1334.29 1445.4 1332.53 1445.4 Q1327.9 1445.4 1325.45 1448.53 Q1323.02 1451.65 1322.67 1457.97 Q1324.04 1455.96 1326.1 1454.89 Q1328.16 1453.8 1330.64 1453.8 Q1335.84 1453.8 1338.85 1456.98 Q1341.89 1460.12 1341.89 1465.56 Q1341.89 1470.89 1338.74 1474.11 Q1335.59 1477.32 1330.36 1477.32 Q1324.36 1477.32 1321.19 1472.74 Q1318.02 1468.13 1318.02 1459.41 Q1318.02 1451.21 1321.91 1446.35 Q1325.8 1441.47 1332.35 1441.47 Q1334.11 1441.47 1335.89 1441.81 Q1337.7 1442.16 1339.64 1442.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1668.29 1445.17 Q1664.68 1445.17 1662.85 1448.74 Q1661.05 1452.28 1661.05 1459.41 Q1661.05 1466.51 1662.85 1470.08 Q1664.68 1473.62 1668.29 1473.62 Q1671.93 1473.62 1673.73 1470.08 Q1675.56 1466.51 1675.56 1459.41 Q1675.56 1452.28 1673.73 1448.74 Q1671.93 1445.17 1668.29 1445.17 M1668.29 1441.47 Q1674.1 1441.47 1677.16 1446.07 Q1680.24 1450.66 1680.24 1459.41 Q1680.24 1468.13 1677.16 1472.74 Q1674.1 1477.32 1668.29 1477.32 Q1662.48 1477.32 1659.4 1472.74 Q1656.35 1468.13 1656.35 1459.41 Q1656.35 1450.66 1659.4 1446.07 Q1662.48 1441.47 1668.29 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1685.31 1470.77 L1690.19 1470.77 L1690.19 1476.65 L1685.31 1476.65 L1685.31 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1705.26 1460.24 Q1701.93 1460.24 1700.01 1462.02 Q1698.11 1463.8 1698.11 1466.93 Q1698.11 1470.05 1700.01 1471.84 Q1701.93 1473.62 1705.26 1473.62 Q1708.59 1473.62 1710.51 1471.84 Q1712.44 1470.03 1712.44 1466.93 Q1712.44 1463.8 1710.51 1462.02 Q1708.62 1460.24 1705.26 1460.24 M1700.58 1458.25 Q1697.57 1457.51 1695.88 1455.45 Q1694.22 1453.39 1694.22 1450.43 Q1694.22 1446.28 1697.16 1443.87 Q1700.12 1441.47 1705.26 1441.47 Q1710.42 1441.47 1713.36 1443.87 Q1716.3 1446.28 1716.3 1450.43 Q1716.3 1453.39 1714.61 1455.45 Q1712.94 1457.51 1709.96 1458.25 Q1713.34 1459.04 1715.21 1461.33 Q1717.11 1463.62 1717.11 1466.93 Q1717.11 1471.95 1714.03 1474.64 Q1710.98 1477.32 1705.26 1477.32 Q1699.54 1477.32 1696.46 1474.64 Q1693.41 1471.95 1693.41 1466.93 Q1693.41 1463.62 1695.31 1461.33 Q1697.2 1459.04 1700.58 1458.25 M1698.87 1450.86 Q1698.87 1453.55 1700.54 1455.05 Q1702.23 1456.56 1705.26 1456.56 Q1708.27 1456.56 1709.96 1455.05 Q1711.67 1453.55 1711.67 1450.86 Q1711.67 1448.18 1709.96 1446.68 Q1708.27 1445.17 1705.26 1445.17 Q1702.23 1445.17 1700.54 1446.68 Q1698.87 1448.18 1698.87 1450.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2033.98 1472.72 L2041.62 1472.72 L2041.62 1446.35 L2033.31 1448.02 L2033.31 1443.76 L2041.57 1442.09 L2046.25 1442.09 L2046.25 1472.72 L2053.89 1472.72 L2053.89 1476.65 L2033.98 1476.65 L2033.98 1472.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2058.96 1470.77 L2063.84 1470.77 L2063.84 1476.65 L2058.96 1476.65 L2058.96 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2078.91 1445.17 Q2075.3 1445.17 2073.47 1448.74 Q2071.67 1452.28 2071.67 1459.41 Q2071.67 1466.51 2073.47 1470.08 Q2075.3 1473.62 2078.91 1473.62 Q2082.55 1473.62 2084.35 1470.08 Q2086.18 1466.51 2086.18 1459.41 Q2086.18 1452.28 2084.35 1448.74 Q2082.55 1445.17 2078.91 1445.17 M2078.91 1441.47 Q2084.72 1441.47 2087.78 1446.07 Q2090.86 1450.66 2090.86 1459.41 Q2090.86 1468.13 2087.78 1472.74 Q2084.72 1477.32 2078.91 1477.32 Q2073.1 1477.32 2070.02 1472.74 Q2066.97 1468.13 2066.97 1459.41 Q2066.97 1450.66 2070.02 1446.07 Q2073.1 1441.47 2078.91 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M182.694 1404.03 L190.332 1404.03 L190.332 1377.67 L182.022 1379.33 L182.022 1375.07 L190.286 1373.41 L194.962 1373.41 L194.962 1404.03 L202.601 1404.03 L202.601 1407.97 L182.694 1407.97 L182.694 1404.03 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M217.67 1376.48 Q214.059 1376.48 212.23 1380.05 Q210.425 1383.59 210.425 1390.72 Q210.425 1397.83 212.23 1401.39 Q214.059 1404.93 217.67 1404.93 Q221.304 1404.93 223.11 1401.39 Q224.939 1397.83 224.939 1390.72 Q224.939 1383.59 223.11 1380.05 Q221.304 1376.48 217.67 1376.48 M217.67 1372.78 Q223.48 1372.78 226.536 1377.39 Q229.615 1381.97 229.615 1390.72 Q229.615 1399.45 226.536 1404.05 Q223.48 1408.64 217.67 1408.64 Q211.86 1408.64 208.781 1404.05 Q205.726 1399.45 205.726 1390.72 Q205.726 1381.97 208.781 1377.39 Q211.86 1372.78 217.67 1372.78 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M230.16 1377.36 L236.367 1377.36 L236.367 1355.94 L229.615 1357.29 L229.615 1353.83 L236.329 1352.48 L240.128 1352.48 L240.128 1377.36 L246.335 1377.36 L246.335 1380.56 L230.16 1380.56 L230.16 1377.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M250.454 1375.78 L254.422 1375.78 L254.422 1380.56 L250.454 1380.56 L250.454 1375.78 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M266.666 1354.98 Q263.732 1354.98 262.246 1357.87 Q260.779 1360.75 260.779 1366.54 Q260.779 1372.32 262.246 1375.21 Q263.732 1378.09 266.666 1378.09 Q269.619 1378.09 271.086 1375.21 Q272.572 1372.32 272.572 1366.54 Q272.572 1360.75 271.086 1357.87 Q269.619 1354.98 266.666 1354.98 M266.666 1351.97 Q271.387 1351.97 273.869 1355.71 Q276.371 1359.43 276.371 1366.54 Q276.371 1373.63 273.869 1377.38 Q271.387 1381.1 266.666 1381.1 Q261.945 1381.1 259.444 1377.38 Q256.961 1373.63 256.961 1366.54 Q256.961 1359.43 259.444 1355.71 Q261.945 1351.97 266.666 1351.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M288.615 1354.98 Q285.681 1354.98 284.195 1357.87 Q282.728 1360.75 282.728 1366.54 Q282.728 1372.32 284.195 1375.21 Q285.681 1378.09 288.615 1378.09 Q291.567 1378.09 293.034 1375.21 Q294.52 1372.32 294.52 1366.54 Q294.52 1360.75 293.034 1357.87 Q291.567 1354.98 288.615 1354.98 M288.615 1351.97 Q293.335 1351.97 295.818 1355.71 Q298.319 1359.43 298.319 1366.54 Q298.319 1373.63 295.818 1377.38 Q293.335 1381.1 288.615 1381.1 Q283.894 1381.1 281.392 1377.38 Q278.91 1373.63 278.91 1366.54 Q278.91 1359.43 281.392 1355.71 Q283.894 1351.97 288.615 1351.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M184.8 1079.52 L192.439 1079.52 L192.439 1053.15 L184.129 1054.82 L184.129 1050.56 L192.393 1048.89 L197.069 1048.89 L197.069 1079.52 L204.707 1079.52 L204.707 1083.45 L184.8 1083.45 L184.8 1079.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M219.777 1051.97 Q216.166 1051.97 214.337 1055.54 Q212.531 1059.08 212.531 1066.21 Q212.531 1073.31 214.337 1076.88 Q216.166 1080.42 219.777 1080.42 Q223.411 1080.42 225.217 1076.88 Q227.045 1073.31 227.045 1066.21 Q227.045 1059.08 225.217 1055.54 Q223.411 1051.97 219.777 1051.97 M219.777 1048.27 Q225.587 1048.27 228.642 1052.87 Q231.721 1057.46 231.721 1066.21 Q231.721 1074.93 228.642 1079.54 Q225.587 1084.12 219.777 1084.12 Q213.967 1084.12 210.888 1079.54 Q207.832 1074.93 207.832 1066.21 Q207.832 1057.46 210.888 1052.87 Q213.967 1048.27 219.777 1048.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M232.267 1052.84 L238.473 1052.84 L238.473 1031.42 L231.721 1032.78 L231.721 1029.32 L238.435 1027.96 L242.235 1027.96 L242.235 1052.84 L248.441 1052.84 L248.441 1056.04 L232.267 1056.04 L232.267 1052.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M252.56 1051.27 L256.529 1051.27 L256.529 1056.04 L252.56 1056.04 L252.56 1051.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M263.92 1052.84 L277.179 1052.84 L277.179 1056.04 L259.35 1056.04 L259.35 1052.84 Q261.513 1050.61 265.237 1046.85 Q268.979 1043.06 269.938 1041.97 Q271.763 1039.92 272.478 1038.51 Q273.211 1037.08 273.211 1035.71 Q273.211 1033.47 271.631 1032.06 Q270.07 1030.65 267.55 1030.65 Q265.763 1030.65 263.77 1031.27 Q261.795 1031.89 259.538 1033.15 L259.538 1029.32 Q261.832 1028.39 263.826 1027.92 Q265.82 1027.45 267.475 1027.45 Q271.838 1027.45 274.434 1029.64 Q277.029 1031.82 277.029 1035.47 Q277.029 1037.2 276.371 1038.76 Q275.731 1040.3 274.02 1042.41 Q273.55 1042.95 271.029 1045.57 Q268.509 1048.16 263.92 1052.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M281.336 1027.96 L296.251 1027.96 L296.251 1031.16 L284.815 1031.16 L284.815 1038.04 Q285.643 1037.76 286.471 1037.63 Q287.298 1037.48 288.126 1037.48 Q292.828 1037.48 295.573 1040.06 Q298.319 1042.63 298.319 1047.03 Q298.319 1051.57 295.498 1054.09 Q292.677 1056.59 287.543 1056.59 Q285.775 1056.59 283.931 1056.29 Q282.107 1055.99 280.151 1055.38 L280.151 1051.57 Q281.844 1052.49 283.649 1052.94 Q285.455 1053.39 287.467 1053.39 Q290.721 1053.39 292.621 1051.68 Q294.52 1049.97 294.52 1047.03 Q294.52 1044.1 292.621 1042.39 Q290.721 1040.68 287.467 1040.68 Q285.944 1040.68 284.42 1041.01 Q282.916 1041.35 281.336 1042.07 L281.336 1027.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M183.502 755.004 L191.141 755.004 L191.141 728.639 L182.831 730.305 L182.831 726.046 L191.095 724.38 L195.771 724.38 L195.771 755.004 L203.41 755.004 L203.41 758.94 L183.502 758.94 L183.502 755.004 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M218.479 727.458 Q214.868 727.458 213.039 731.023 Q211.234 734.565 211.234 741.694 Q211.234 748.801 213.039 752.366 Q214.868 755.907 218.479 755.907 Q222.113 755.907 223.919 752.366 Q225.747 748.801 225.747 741.694 Q225.747 734.565 223.919 731.023 Q222.113 727.458 218.479 727.458 M218.479 723.755 Q224.289 723.755 227.345 728.361 Q230.423 732.944 230.423 741.694 Q230.423 750.421 227.345 755.028 Q224.289 759.611 218.479 759.611 Q212.669 759.611 209.59 755.028 Q206.535 750.421 206.535 741.694 Q206.535 732.944 209.59 728.361 Q212.669 723.755 218.479 723.755 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M230.969 728.332 L237.175 728.332 L237.175 706.91 L230.423 708.264 L230.423 704.803 L237.138 703.449 L240.937 703.449 L240.937 728.332 L247.143 728.332 L247.143 731.529 L230.969 731.529 L230.969 728.332 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M251.262 726.752 L255.231 726.752 L255.231 731.529 L251.262 731.529 L251.262 726.752 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M259.387 703.449 L274.302 703.449 L274.302 706.646 L262.867 706.646 L262.867 713.53 Q263.694 713.248 264.522 713.116 Q265.349 712.966 266.177 712.966 Q270.879 712.966 273.625 715.543 Q276.371 718.119 276.371 722.52 Q276.371 727.053 273.55 729.573 Q270.728 732.075 265.594 732.075 Q263.826 732.075 261.983 731.774 Q260.158 731.473 258.202 730.871 L258.202 727.053 Q259.895 727.975 261.701 728.426 Q263.506 728.877 265.519 728.877 Q268.772 728.877 270.672 727.166 Q272.572 725.454 272.572 722.52 Q272.572 719.586 270.672 717.875 Q268.772 716.163 265.519 716.163 Q263.995 716.163 262.472 716.502 Q260.967 716.84 259.387 717.555 L259.387 703.449 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M288.615 705.951 Q285.681 705.951 284.195 708.847 Q282.728 711.725 282.728 717.517 Q282.728 723.291 284.195 726.188 Q285.681 729.065 288.615 729.065 Q291.567 729.065 293.034 726.188 Q294.52 723.291 294.52 717.517 Q294.52 711.725 293.034 708.847 Q291.567 705.951 288.615 705.951 M288.615 702.941 Q293.335 702.941 295.818 706.684 Q298.319 710.408 298.319 717.517 Q298.319 724.608 295.818 728.351 Q293.335 732.075 288.615 732.075 Q283.894 732.075 281.392 728.351 Q278.91 724.608 278.91 717.517 Q278.91 710.408 281.392 706.684 Q283.894 702.941 288.615 702.941 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M184.236 430.491 L191.875 430.491 L191.875 404.126 L183.565 405.792 L183.565 401.533 L191.828 399.866 L196.504 399.866 L196.504 430.491 L204.143 430.491 L204.143 434.426 L184.236 434.426 L184.236 430.491 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M219.212 402.945 Q215.601 402.945 213.773 406.51 Q211.967 410.052 211.967 417.181 Q211.967 424.288 213.773 427.852 Q215.601 431.394 219.212 431.394 Q222.847 431.394 224.652 427.852 Q226.481 424.288 226.481 417.181 Q226.481 410.052 224.652 406.51 Q222.847 402.945 219.212 402.945 M219.212 399.241 Q225.023 399.241 228.078 403.848 Q231.157 408.431 231.157 417.181 Q231.157 425.908 228.078 430.514 Q225.023 435.098 219.212 435.098 Q213.402 435.098 210.324 430.514 Q207.268 425.908 207.268 417.181 Q207.268 408.431 210.324 403.848 Q213.402 399.241 219.212 399.241 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M231.702 403.819 L237.909 403.819 L237.909 382.397 L231.157 383.751 L231.157 380.29 L237.871 378.936 L241.67 378.936 L241.67 403.819 L247.877 403.819 L247.877 407.016 L231.702 407.016 L231.702 403.819 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M251.996 402.239 L255.964 402.239 L255.964 407.016 L251.996 407.016 L251.996 402.239 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M259.124 378.936 L277.179 378.936 L277.179 380.553 L266.986 407.016 L263.017 407.016 L272.609 382.133 L259.124 382.133 L259.124 378.936 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M281.336 378.936 L296.251 378.936 L296.251 382.133 L284.815 382.133 L284.815 389.017 Q285.643 388.735 286.471 388.603 Q287.298 388.453 288.126 388.453 Q292.828 388.453 295.573 391.029 Q298.319 393.606 298.319 398.007 Q298.319 402.54 295.498 405.06 Q292.677 407.561 287.543 407.561 Q285.775 407.561 283.931 407.261 Q282.107 406.96 280.151 406.358 L280.151 402.54 Q281.844 403.461 283.649 403.913 Q285.455 404.364 287.467 404.364 Q290.721 404.364 292.621 402.653 Q294.52 400.941 294.52 398.007 Q294.52 395.073 292.621 393.362 Q290.721 391.65 287.467 391.65 Q285.944 391.65 284.42 391.989 Q282.916 392.327 281.336 393.042 L281.336 378.936 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M181.584 105.978 L189.223 105.978 L189.223 79.6125 L180.913 81.2792 L180.913 77.0199 L189.176 75.3533 L193.852 75.3533 L193.852 105.978 L201.491 105.978 L201.491 109.913 L181.584 109.913 L181.584 105.978 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M216.561 78.432 Q212.95 78.432 211.121 81.9968 Q209.315 85.5384 209.315 92.668 Q209.315 99.7744 211.121 103.339 Q212.95 106.881 216.561 106.881 Q220.195 106.881 222 103.339 Q223.829 99.7744 223.829 92.668 Q223.829 85.5384 222 81.9968 Q220.195 78.432 216.561 78.432 M216.561 74.7283 Q222.371 74.7283 225.426 79.3347 Q228.505 83.918 228.505 92.668 Q228.505 101.395 225.426 106.001 Q222.371 110.585 216.561 110.585 Q210.75 110.585 207.672 106.001 Q204.616 101.395 204.616 92.668 Q204.616 83.918 207.672 79.3347 Q210.75 74.7283 216.561 74.7283 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M233.075 79.3056 L246.335 79.3056 L246.335 82.5029 L228.505 82.5029 L228.505 79.3056 Q230.668 77.0674 234.392 73.3059 Q238.135 69.5255 239.094 68.4347 Q240.918 66.3846 241.633 64.974 Q242.366 63.5446 242.366 62.1717 Q242.366 59.9335 240.786 58.523 Q239.225 57.1124 236.705 57.1124 Q234.918 57.1124 232.925 57.733 Q230.95 58.3537 228.693 59.6138 L228.693 55.777 Q230.988 54.8555 232.981 54.3853 Q234.975 53.9151 236.63 53.9151 Q240.993 53.9151 243.589 56.0968 Q246.184 58.2785 246.184 61.9272 Q246.184 63.6575 245.526 65.2185 Q244.887 66.7608 243.175 68.8672 Q242.705 69.4127 240.185 72.0269 Q237.664 74.6224 233.075 79.3056 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M250.454 77.7257 L254.422 77.7257 L254.422 82.5029 L250.454 82.5029 L250.454 77.7257 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M266.666 56.9243 Q263.732 56.9243 262.246 59.8207 Q260.779 62.6983 260.779 68.4911 Q260.779 74.2651 262.246 77.1615 Q263.732 80.0391 266.666 80.0391 Q269.619 80.0391 271.086 77.1615 Q272.572 74.2651 272.572 68.4911 Q272.572 62.6983 271.086 59.8207 Q269.619 56.9243 266.666 56.9243 M266.666 53.9151 Q271.387 53.9151 273.869 57.6578 Q276.371 61.3817 276.371 68.4911 Q276.371 75.5816 273.869 79.3244 Q271.387 83.0483 266.666 83.0483 Q261.945 83.0483 259.444 79.3244 Q256.961 75.5816 256.961 68.4911 Q256.961 61.3817 259.444 57.6578 Q261.945 53.9151 266.666 53.9151 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M288.615 56.9243 Q285.681 56.9243 284.195 59.8207 Q282.728 62.6983 282.728 68.4911 Q282.728 74.2651 284.195 77.1615 Q285.681 80.0391 288.615 80.0391 Q291.567 80.0391 293.034 77.1615 Q294.52 74.2651 294.52 68.4911 Q294.52 62.6983 293.034 59.8207 Q291.567 56.9243 288.615 56.9243 M288.615 53.9151 Q293.335 53.9151 295.818 57.6578 Q298.319 61.3817 298.319 68.4911 Q298.319 75.5816 295.818 79.3244 Q293.335 83.0483 288.615 83.0483 Q283.894 83.0483 281.392 79.3244 Q278.91 75.5816 278.91 68.4911 Q278.91 61.3817 281.392 57.6578 Q283.894 53.9151 288.615 53.9151 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1029.61 1506.52 L1035.46 1506.52 L1035.46 1556.04 L1029.61 1556.04 L1029.61 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1072.1 1536.76 L1072.1 1539.62 L1045.17 1539.62 Q1045.55 1545.67 1048.8 1548.85 Q1052.08 1552 1057.9 1552 Q1061.28 1552 1064.43 1551.17 Q1067.61 1550.35 1070.73 1548.69 L1070.73 1554.23 Q1067.58 1555.57 1064.27 1556.27 Q1060.96 1556.97 1057.55 1556.97 Q1049.02 1556.97 1044.02 1552 Q1039.06 1547.04 1039.06 1538.57 Q1039.06 1529.82 1043.77 1524.69 Q1048.51 1519.54 1056.53 1519.54 Q1063.73 1519.54 1067.9 1524.18 Q1072.1 1528.8 1072.1 1536.76 M1066.24 1535.04 Q1066.18 1530.23 1063.54 1527.37 Q1060.93 1524.5 1056.6 1524.5 Q1051.7 1524.5 1048.74 1527.27 Q1045.81 1530.04 1045.36 1535.07 L1066.24 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1094.44 1538.12 Q1087.34 1538.12 1084.61 1539.75 Q1081.87 1541.37 1081.87 1545.29 Q1081.87 1548.4 1083.91 1550.25 Q1085.97 1552.07 1089.51 1552.07 Q1094.38 1552.07 1097.31 1548.63 Q1100.27 1545.16 1100.27 1539.43 L1100.27 1538.12 L1094.44 1538.12 M1106.12 1535.71 L1106.12 1556.04 L1100.27 1556.04 L1100.27 1550.63 Q1098.26 1553.88 1095.27 1555.44 Q1092.28 1556.97 1087.95 1556.97 Q1082.47 1556.97 1079.23 1553.91 Q1076.01 1550.82 1076.01 1545.67 Q1076.01 1539.65 1080.02 1536.6 Q1084.06 1533.54 1092.05 1533.54 L1100.27 1533.54 L1100.27 1532.97 Q1100.27 1528.93 1097.59 1526.73 Q1094.95 1524.5 1090.14 1524.5 Q1087.09 1524.5 1084.19 1525.23 Q1081.3 1525.97 1078.62 1527.43 L1078.62 1522.02 Q1081.84 1520.78 1084.86 1520.17 Q1087.88 1519.54 1090.75 1519.54 Q1098.48 1519.54 1102.3 1523.55 Q1106.12 1527.56 1106.12 1535.71 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1132.92 1525.87 Q1131.93 1525.3 1130.76 1525.04 Q1129.61 1524.76 1128.21 1524.76 Q1123.25 1524.76 1120.57 1528 Q1117.93 1531.22 1117.93 1537.27 L1117.93 1556.04 L1112.04 1556.04 L1112.04 1520.4 L1117.93 1520.4 L1117.93 1525.93 Q1119.78 1522.69 1122.74 1521.13 Q1125.7 1519.54 1129.93 1519.54 Q1130.53 1519.54 1131.27 1519.63 Q1132 1519.7 1132.89 1519.85 L1132.92 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1167.55 1534.53 L1167.55 1556.04 L1161.69 1556.04 L1161.69 1534.72 Q1161.69 1529.66 1159.72 1527.14 Q1157.75 1524.63 1153.8 1524.63 Q1149.06 1524.63 1146.32 1527.65 Q1143.58 1530.68 1143.58 1535.9 L1143.58 1556.04 L1137.7 1556.04 L1137.7 1520.4 L1143.58 1520.4 L1143.58 1525.93 Q1145.68 1522.72 1148.52 1521.13 Q1151.38 1519.54 1155.11 1519.54 Q1161.25 1519.54 1164.4 1523.36 Q1167.55 1527.14 1167.55 1534.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1173.69 1520.4 L1179.55 1520.4 L1179.55 1556.04 L1173.69 1556.04 L1173.69 1520.4 M1173.69 1506.52 L1179.55 1506.52 L1179.55 1513.93 L1173.69 1513.93 L1173.69 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1215.33 1534.53 L1215.33 1556.04 L1209.47 1556.04 L1209.47 1534.72 Q1209.47 1529.66 1207.5 1527.14 Q1205.52 1524.63 1201.58 1524.63 Q1196.83 1524.63 1194.1 1527.65 Q1191.36 1530.68 1191.36 1535.9 L1191.36 1556.04 L1185.47 1556.04 L1185.47 1520.4 L1191.36 1520.4 L1191.36 1525.93 Q1193.46 1522.72 1196.29 1521.13 Q1199.16 1519.54 1202.88 1519.54 Q1209.02 1519.54 1212.17 1523.36 Q1215.33 1527.14 1215.33 1534.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1244.93 1537.81 Q1244.93 1531.44 1242.28 1527.94 Q1239.67 1524.44 1234.93 1524.44 Q1230.22 1524.44 1227.58 1527.94 Q1224.97 1531.44 1224.97 1537.81 Q1224.97 1544.14 1227.58 1547.64 Q1230.22 1551.14 1234.93 1551.14 Q1239.67 1551.14 1242.28 1547.64 Q1244.93 1544.14 1244.93 1537.81 M1250.78 1551.62 Q1250.78 1560.72 1246.74 1565.15 Q1242.7 1569.6 1234.36 1569.6 Q1231.27 1569.6 1228.53 1569.13 Q1225.8 1568.68 1223.22 1567.72 L1223.22 1562.03 Q1225.8 1563.43 1228.31 1564.1 Q1230.83 1564.76 1233.44 1564.76 Q1239.2 1564.76 1242.06 1561.74 Q1244.93 1558.75 1244.93 1552.67 L1244.93 1549.77 Q1243.11 1552.92 1240.28 1554.48 Q1237.45 1556.04 1233.5 1556.04 Q1226.94 1556.04 1222.93 1551.05 Q1218.92 1546.05 1218.92 1537.81 Q1218.92 1529.53 1222.93 1524.53 Q1226.94 1519.54 1233.5 1519.54 Q1237.45 1519.54 1240.28 1521.1 Q1243.11 1522.66 1244.93 1525.81 L1244.93 1520.4 L1250.78 1520.4 L1250.78 1551.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1284.01 1566.87 L1284.01 1571.42 L1250.15 1571.42 L1250.15 1566.87 L1284.01 1566.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1310.81 1525.87 Q1309.82 1525.3 1308.65 1525.04 Q1307.5 1524.76 1306.1 1524.76 Q1301.14 1524.76 1298.46 1528 Q1295.82 1531.22 1295.82 1537.27 L1295.82 1556.04 L1289.93 1556.04 L1289.93 1520.4 L1295.82 1520.4 L1295.82 1525.93 Q1297.67 1522.69 1300.63 1521.13 Q1303.59 1519.54 1307.82 1519.54 Q1308.42 1519.54 1309.16 1519.63 Q1309.89 1519.7 1310.78 1519.85 L1310.81 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1333.15 1538.12 Q1326.06 1538.12 1323.32 1539.75 Q1320.58 1541.37 1320.58 1545.29 Q1320.58 1548.4 1322.62 1550.25 Q1324.69 1552.07 1328.22 1552.07 Q1333.09 1552.07 1336.02 1548.63 Q1338.98 1545.16 1338.98 1539.43 L1338.98 1538.12 L1333.15 1538.12 M1344.84 1535.71 L1344.84 1556.04 L1338.98 1556.04 L1338.98 1550.63 Q1336.97 1553.88 1333.98 1555.44 Q1330.99 1556.97 1326.66 1556.97 Q1321.19 1556.97 1317.94 1553.91 Q1314.73 1550.82 1314.73 1545.67 Q1314.73 1539.65 1318.74 1536.6 Q1322.78 1533.54 1330.77 1533.54 L1338.98 1533.54 L1338.98 1532.97 Q1338.98 1528.93 1336.31 1526.73 Q1333.66 1524.5 1328.86 1524.5 Q1325.8 1524.5 1322.91 1525.23 Q1320.01 1525.97 1317.34 1527.43 L1317.34 1522.02 Q1320.55 1520.78 1323.57 1520.17 Q1326.6 1519.54 1329.46 1519.54 Q1337.2 1519.54 1341.02 1523.55 Q1344.84 1527.56 1344.84 1535.71 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1356.77 1510.27 L1356.77 1520.4 L1368.83 1520.4 L1368.83 1524.95 L1356.77 1524.95 L1356.77 1544.3 Q1356.77 1548.66 1357.95 1549.9 Q1359.16 1551.14 1362.82 1551.14 L1368.83 1551.14 L1368.83 1556.04 L1362.82 1556.04 Q1356.04 1556.04 1353.46 1553.53 Q1350.88 1550.98 1350.88 1544.3 L1350.88 1524.95 L1346.59 1524.95 L1346.59 1520.4 L1350.88 1520.4 L1350.88 1510.27 L1356.77 1510.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M1405.47 1536.76 L1405.47 1539.62 L1378.54 1539.62 Q1378.92 1545.67 1382.17 1548.85 Q1385.45 1552 1391.27 1552 Q1394.65 1552 1397.8 1551.17 Q1400.98 1550.35 1404.1 1548.69 L1404.1 1554.23 Q1400.95 1555.57 1397.64 1556.27 Q1394.33 1556.97 1390.92 1556.97 Q1382.39 1556.97 1377.4 1552 Q1372.43 1547.04 1372.43 1538.57 Q1372.43 1529.82 1377.14 1524.69 Q1381.88 1519.54 1389.91 1519.54 Q1397.1 1519.54 1401.27 1524.18 Q1405.47 1528.8 1405.47 1536.76 M1399.61 1535.04 Q1399.55 1530.23 1396.91 1527.37 Q1394.3 1524.5 1389.97 1524.5 Q1385.07 1524.5 1382.11 1527.27 Q1379.18 1530.04 1378.73 1535.07 L1399.61 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M66.4881 893.543 L88.0042 893.543 L88.0042 899.399 L66.679 899.399 Q61.6183 899.399 59.1038 901.373 Q56.5894 903.346 56.5894 907.293 Q56.5894 912.035 59.6131 914.773 Q62.6368 917.51 67.8567 917.51 L88.0042 917.51 L88.0042 923.398 L52.3562 923.398 L52.3562 917.51 L57.8944 917.51 Q54.6797 915.409 53.0883 912.576 Q51.4968 909.712 51.4968 905.988 Q51.4968 899.845 55.3163 896.694 Q59.1038 893.543 66.4881 893.543 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M98.8259 860.314 L103.377 860.314 L103.377 894.18 L98.8259 894.18 L98.8259 860.314 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M68.7161 823.679 L71.5806 823.679 L71.5806 850.606 Q77.6281 850.224 80.8109 846.978 Q83.9619 843.7 83.9619 837.875 Q83.9619 834.501 83.1344 831.35 Q82.3069 828.167 80.6518 825.048 L86.1899 825.048 Q87.5267 828.199 88.227 831.509 Q88.9272 834.819 88.9272 838.225 Q88.9272 846.755 83.9619 851.752 Q78.9967 856.717 70.5303 856.717 Q61.7774 856.717 56.6531 852.007 Q51.4968 847.264 51.4968 839.244 Q51.4968 832.05 56.1438 827.881 Q60.7589 823.679 68.7161 823.679 M66.9973 829.536 Q62.1912 829.599 59.3266 832.241 Q56.4621 834.851 56.4621 839.18 Q56.4621 844.081 59.2312 847.042 Q62.0002 849.97 67.0292 850.415 L66.9973 829.536 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M53.4065 794.811 L58.9447 794.811 Q57.6716 797.294 57.035 799.967 Q56.3984 802.641 56.3984 805.505 Q56.3984 809.866 57.7352 812.062 Q59.072 814.226 61.7456 814.226 Q63.7826 814.226 64.9603 812.667 Q66.1061 811.107 67.1565 806.396 L67.6021 804.391 Q68.9389 798.153 71.3897 795.543 Q73.8086 792.901 78.1691 792.901 Q83.1344 792.901 86.0308 796.848 Q88.9272 800.763 88.9272 807.638 Q88.9272 810.502 88.3543 813.622 Q87.8132 816.709 86.6992 820.146 L80.6518 820.146 Q82.3387 816.9 83.198 813.749 Q84.0256 810.598 84.0256 807.51 Q84.0256 803.373 82.6251 801.145 Q81.1929 798.917 78.6147 798.917 Q76.2276 798.917 74.9545 800.54 Q73.6813 802.131 72.5037 807.574 L72.0262 809.611 Q70.8804 815.054 68.5251 817.473 Q66.138 819.892 62.0002 819.892 Q56.9713 819.892 54.2341 816.327 Q51.4968 812.762 51.4968 806.206 Q51.4968 802.959 51.9743 800.094 Q52.4517 797.23 53.4065 794.811 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M42.2347 780.965 L52.3562 780.965 L52.3562 768.902 L56.9077 768.902 L56.9077 780.965 L76.2594 780.965 Q80.6199 780.965 81.8613 779.788 Q83.1026 778.578 83.1026 774.918 L83.1026 768.902 L88.0042 768.902 L88.0042 774.918 Q88.0042 781.698 85.4897 784.276 Q82.9434 786.854 76.2594 786.854 L56.9077 786.854 L56.9077 791.151 L52.3562 791.151 L52.3562 786.854 L42.2347 786.854 L42.2347 780.965 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M52.3562 762.76 L52.3562 756.903 L88.0042 756.903 L88.0042 762.76 L52.3562 762.76 M38.479 762.76 L38.479 756.903 L45.895 756.903 L45.895 762.76 L38.479 762.76 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M59.1993 723.006 Q55.2526 720.81 53.3747 717.754 Q51.4968 714.698 51.4968 710.561 Q51.4968 704.991 55.4117 701.967 Q59.2948 698.943 66.4881 698.943 L88.0042 698.943 L88.0042 704.832 L66.679 704.832 Q61.5546 704.832 59.072 706.646 Q56.5894 708.46 56.5894 712.184 Q56.5894 716.736 59.6131 719.377 Q62.6368 722.019 67.8567 722.019 L88.0042 722.019 L88.0042 727.907 L66.679 727.907 Q61.5228 727.907 59.072 729.722 Q56.5894 731.536 56.5894 735.323 Q56.5894 739.811 59.6449 742.453 Q62.6686 745.095 67.8567 745.095 L88.0042 745.095 L88.0042 750.983 L52.3562 750.983 L52.3562 745.095 L57.8944 745.095 Q54.616 743.09 53.0564 740.289 Q51.4968 737.488 51.4968 733.636 Q51.4968 729.753 53.4702 727.048 Q55.4436 724.311 59.1993 723.006 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M70.0847 676.6 Q70.0847 683.697 71.7079 686.435 Q73.3312 689.172 77.2461 689.172 Q80.3653 689.172 82.2114 687.135 Q84.0256 685.066 84.0256 681.533 Q84.0256 676.663 80.5881 673.735 Q77.1188 670.775 71.3897 670.775 L70.0847 670.775 L70.0847 676.6 M67.6657 664.919 L88.0042 664.919 L88.0042 670.775 L82.5933 670.775 Q85.8398 672.78 87.3994 675.772 Q88.9272 678.764 88.9272 683.093 Q88.9272 688.567 85.8716 691.814 Q82.7843 695.028 77.6281 695.028 Q71.6125 695.028 68.5569 691.018 Q65.5014 686.976 65.5014 678.987 L65.5014 670.775 L64.9285 670.775 Q60.8862 670.775 58.6901 673.449 Q56.4621 676.09 56.4621 680.897 Q56.4621 683.952 57.1941 686.849 Q57.9262 689.745 59.3903 692.419 L53.9795 692.419 Q52.7381 689.204 52.1334 686.18 Q51.4968 683.156 51.4968 680.292 Q51.4968 672.558 55.5072 668.738 Q59.5176 664.919 67.6657 664.919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M42.2347 652.983 L52.3562 652.983 L52.3562 640.92 L56.9077 640.92 L56.9077 652.983 L76.2594 652.983 Q80.6199 652.983 81.8613 651.805 Q83.1026 650.596 83.1026 646.936 L83.1026 640.92 L88.0042 640.92 L88.0042 646.936 Q88.0042 653.715 85.4897 656.293 Q82.9434 658.871 76.2594 658.871 L56.9077 658.871 L56.9077 663.168 L52.3562 663.168 L52.3562 658.871 L42.2347 658.871 L42.2347 652.983 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M56.4621 620.963 Q56.4621 625.674 60.1542 628.411 Q63.8145 631.149 70.212 631.149 Q76.6095 631.149 80.3017 628.443 Q83.9619 625.706 83.9619 620.963 Q83.9619 616.285 80.2698 613.547 Q76.5777 610.81 70.212 610.81 Q63.8781 610.81 60.186 613.547 Q56.4621 616.285 56.4621 620.963 M51.4968 620.963 Q51.4968 613.325 56.4621 608.964 Q61.4273 604.604 70.212 604.604 Q78.9649 604.604 83.9619 608.964 Q88.9272 613.325 88.9272 620.963 Q88.9272 628.634 83.9619 632.995 Q78.9649 637.323 70.212 637.323 Q61.4273 637.323 56.4621 632.995 Q51.4968 628.634 51.4968 620.963 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M57.8307 577.804 Q57.2578 578.791 57.0032 579.968 Q56.7167 581.114 56.7167 582.515 Q56.7167 587.48 59.9632 590.153 Q63.1779 592.795 69.2253 592.795 L88.0042 592.795 L88.0042 598.683 L52.3562 598.683 L52.3562 592.795 L57.8944 592.795 Q54.6479 590.949 53.0883 587.989 Q51.4968 585.029 51.4968 580.796 Q51.4968 580.191 51.5923 579.459 Q51.656 578.727 51.8151 577.836 L57.8307 577.804 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M53.4065 548.935 L58.9447 548.935 Q57.6716 551.418 57.035 554.092 Q56.3984 556.765 56.3984 559.63 Q56.3984 563.99 57.7352 566.187 Q59.072 568.351 61.7456 568.351 Q63.7826 568.351 64.9603 566.791 Q66.1061 565.232 67.1565 560.521 L67.6021 558.516 Q68.9389 552.277 71.3897 549.668 Q73.8086 547.026 78.1691 547.026 Q83.1344 547.026 86.0308 550.973 Q88.9272 554.887 88.9272 561.762 Q88.9272 564.627 88.3543 567.746 Q87.8132 570.834 86.6992 574.271 L80.6518 574.271 Q82.3387 571.024 83.198 567.873 Q84.0256 564.722 84.0256 561.635 Q84.0256 557.497 82.6251 555.269 Q81.1929 553.041 78.6147 553.041 Q76.2276 553.041 74.9545 554.665 Q73.6813 556.256 72.5037 561.699 L72.0262 563.736 Q70.8804 569.178 68.5251 571.597 Q66.138 574.016 62.0002 574.016 Q56.9713 574.016 54.2341 570.452 Q51.4968 566.887 51.4968 560.33 Q51.4968 557.084 51.9743 554.219 Q52.4517 551.354 53.4065 548.935 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"523.944\" r=\"45\" fill=\"#f1711d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"523.944\" r=\"30\" fill=\"#9f2a62\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"803.804\" r=\"41\" fill=\"#e05535\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"1236.33\" r=\"33\" fill=\"#b0315a\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"86.1857\" r=\"60\" fill=\"#f4f78d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"803.804\" r=\"37\" fill=\"#cb4148\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"1085.1\" r=\"41\" fill=\"#e15634\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"662.128\" r=\"23\" fill=\"#6e186e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"233.527\" r=\"55\" fill=\"#f8cb35\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"662.128\" r=\"46\" fill=\"#f68012\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"939.756\" r=\"32\" fill=\"#ae305b\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"233.527\" r=\"33\" fill=\"#b33259\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"803.804\" r=\"29\" fill=\"#992765\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"1236.33\" r=\"18\" fill=\"#4b0c6b\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"1236.33\" r=\"35\" fill=\"#be3852\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"523.944\" r=\"49\" fill=\"#fa9706\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"1085.1\" r=\"20\" fill=\"#59106d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"803.804\" r=\"34\" fill=\"#ba3654\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"1236.33\" r=\"37\" fill=\"#d04445\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"1384.24\" r=\"14\" fill=\"#30095c\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"939.756\" r=\"27\" fill=\"#88216a\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"374.157\" r=\"56\" fill=\"#f5d746\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"662.128\" r=\"13\" fill=\"#2b0a56\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"803.804\" r=\"46\" fill=\"#f57e14\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"1085.1\" r=\"31\" fill=\"#a32b61\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"662.128\" r=\"49\" fill=\"#fa9506\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"86.1857\" r=\"59\" fill=\"#f1f17c\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"662.128\" r=\"36\" fill=\"#c83f4b\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"374.157\" r=\"51\" fill=\"#fbab0f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"86.1857\" r=\"55\" fill=\"#f7ce39\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"1384.24\" r=\"17\" fill=\"#480b6a\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"803.804\" r=\"40\" fill=\"#e05435\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"374.157\" r=\"50\" fill=\"#fba208\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"86.1857\" r=\"61\" fill=\"#fcfea4\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"1085.1\" r=\"17\" fill=\"#490b6a\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"662.128\" r=\"40\" fill=\"#dd5238\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"233.527\" r=\"53\" fill=\"#fac027\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"523.944\" r=\"43\" fill=\"#eb6626\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"1236.33\" r=\"37\" fill=\"#cf4445\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"939.756\" r=\"44\" fill=\"#f0711e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"523.944\" r=\"16\" fill=\"#430a68\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"1085.1\" r=\"36\" fill=\"#c53d4d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"523.944\" r=\"49\" fill=\"#fa9606\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"523.944\" r=\"48\" fill=\"#f99008\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"662.128\" r=\"44\" fill=\"#ef6e20\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"233.527\" r=\"57\" fill=\"#f3e055\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"662.128\" r=\"28\" fill=\"#912467\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"939.756\" r=\"43\" fill=\"#ea6428\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"374.157\" r=\"51\" fill=\"#fbaa0e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"803.804\" r=\"16\" fill=\"#430a68\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"1236.33\" r=\"13\" fill=\"#2b0a57\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"233.527\" r=\"44\" fill=\"#f0701f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"939.756\" r=\"22\" fill=\"#68166e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"374.157\" r=\"50\" fill=\"#fba409\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"233.527\" r=\"55\" fill=\"#f7d23e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"803.804\" r=\"44\" fill=\"#ed6a23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"523.944\" r=\"34\" fill=\"#bb3654\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"86.1857\" r=\"38\" fill=\"#d54940\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"1384.24\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"233.527\" r=\"55\" fill=\"#f8cb34\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"233.527\" r=\"54\" fill=\"#f9c830\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"1384.24\" r=\"14\" fill=\"#34095f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"1085.1\" r=\"39\" fill=\"#d74b3e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"1236.33\" r=\"4\" fill=\"#01010b\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"86.1857\" r=\"57\" fill=\"#f1e763\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"86.1857\" r=\"56\" fill=\"#f4dd4f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"374.157\" r=\"23\" fill=\"#70196e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"86.1857\" r=\"56\" fill=\"#f5d848\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"523.944\" r=\"51\" fill=\"#fba80d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"939.756\" r=\"39\" fill=\"#da4e3c\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"936.022\" cy=\"1236.33\" r=\"21\" fill=\"#64146e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"939.756\" r=\"17\" fill=\"#4a0b6a\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"1085.1\" r=\"7\" fill=\"#08061f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"374.157\" r=\"52\" fill=\"#fbb61a\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"233.527\" r=\"50\" fill=\"#fb9e07\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"374.157\" r=\"43\" fill=\"#ec6726\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"1085.1\" r=\"14\" fill=\"#33095e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"939.756\" r=\"39\" fill=\"#d74b3e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"374.157\" r=\"51\" fill=\"#fba70c\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"748.346\" cy=\"803.804\" r=\"19\" fill=\"#560f6d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"939.756\" r=\"12\" fill=\"#260b51\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1499.05\" cy=\"1384.24\" r=\"31\" fill=\"#a72d5f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"662.128\" r=\"43\" fill=\"#ed6825\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"939.756\" r=\"9\" fill=\"#110a30\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"1384.24\" r=\"9\" fill=\"#140a36\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"1384.24\" r=\"29\" fill=\"#952666\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"86.1857\" r=\"60\" fill=\"#f5f891\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"233.527\" r=\"54\" fill=\"#f9c72f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"662.128\" r=\"44\" fill=\"#ed6a24\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"2062.08\" cy=\"1384.24\" r=\"36\" fill=\"#c83e4b\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"1085.1\" r=\"42\" fill=\"#e9622a\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1311.38\" cy=\"1384.24\" r=\"25\" fill=\"#7d1d6c\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"1384.24\" r=\"36\" fill=\"#c73e4c\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"1236.33\" r=\"21\" fill=\"#5f136e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1686.73\" cy=\"1236.33\" r=\"36\" fill=\"#c43c4e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"372.992\" cy=\"803.804\" r=\"13\" fill=\"#290b54\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"86.1857\" r=\"50\" fill=\"#fba007\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1123.7\" cy=\"1085.1\" r=\"22\" fill=\"#6b176e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"1874.41\" cy=\"523.944\" r=\"51\" fill=\"#fba50b\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip762)\" cx=\"560.669\" cy=\"374.157\" r=\"38\" fill=\"#d24643\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip763\">\n",
       "    <rect x=\"2160\" y=\"47\" width=\"73\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<g clip-path=\"url(#clip763)\">\n",
       "<image width=\"72\" height=\"1376\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAEgAAAVgCAYAAADsKhu7AAAL6UlEQVR4nO3dwZEjNxBFQVBR/lsh\n",
       "L6UFZIHqHclDpgUTL35wCXTP7Off+/c7/K+/vv0D/DqBgkBBoCBQmPv+/fbP8NMsKAgUBAoChXnv\n",
       "z7d/hp9mQUGgIFAQKAgUHDWCBQWBgkBBoDDPh/TKgoJAQaAgUJh3fUhvLCgIFAQKAgWBgqNGsKAg\n",
       "UBAoCBR8SAcLCgIFgYJAwX1QsKAgUBAoCBQECnMcNVYWFAQKAgWBgvugYEFBoCBQECgIFOa4MFtZ\n",
       "UBAoCBQECnPuP9/+GX6aBQWBgkBBoODSPlhQECgIFAQKAgX3QcGCgkBBoCBQ8CEdLCgIFAQKAgX3\n",
       "QcGCgkBBoCBQECjMx1FjZUFBoCBQECjMuf5XhI0FBYGCQEGgIFBw1AgWFAQKAgWBgqNGsKAgUBAo\n",
       "CBS8HxQsKAgUBAoCBYHCfBw1VhYUBAoCBYGCt1yDBQWBgkBBoOCbdLCgIFAQKAgUBAoePQcLCgIF\n",
       "gYJAwVEjWFAQKAgUBAoCBUeNYEFBoCBQECg4agQLCgIFgYJAYc693/4ZfpoFBYGCQEGgIFBw1AgW\n",
       "FAQKAgWBgkv7YEFBoCBQECj4kA4WFAQKAgWBgkBhPp5qrCwoCBQECgIFR41gQUGgIFAQKAgUvEAV\n",
       "LCgIFAQKAgX3QcGCgkBBoCBQcB8ULCgIFAQKAgWBgvugYEFBoCBQECj4kA4WFAQKAgWBgl/qDRYU\n",
       "BAoCBYGCQMFRI1hQECgIFAQKPqSDBQWBgkBBoCBQ8K9YsKAgUBAoCBR8SAcLCgIFgYJAYc7zIb2x\n",
       "oCBQECgIFAQKjhrBgoJAQaAgUPAhHSwoCBQECgIFH9LBgoJAQaAgUBAozLnv2z/DT7OgIFAQKAgU\n",
       "HDWCBQWBgkBBoCBQcNQIFhQECgIFgYKjRrCgIFAQKAgUfJMOFhQECgIFgYJAYY6TxsqCgkBBoCBQ\n",
       "mPMcNTYWFAQKAgWBgm/SwYKCQEGgIFAQKPhXLFhQECgIFAQK44/g7SwoCBQECgIFgYKjRrCgIFAQ\n",
       "KAgUfEgHCwoCBYGCQGHO/Xz7Z/hpFhQECgIFgYJAYd7zr9jGgoJAQaAgUHAfFCwoCBQECgIF90HB\n",
       "goJAQaAgUBAozLsabdQJAgWBgkDBUSNYUBAoCBQECgKFOR49rywoCBQECgKFeY4aKwsKAgWBgkBh\n",
       "jkv7lTpBoCBQECgIFBw1ggUFgYJAQaDg0j5YUBAoCBQECl7iDOoEgYJAQaAgUPCWa7CgIFAQKAgU\n",
       "XNoHCwoCBYGCQEGgMO9ptFEnCBQECgIF90HBgoJAQaAgUHAfFCwoCBQECgIFgcIc90ErdYJAQaAg\n",
       "UHDUCBYUBAoCBYGCD+lgQUGgIFAQKAgU/NZzsKAgUBAoCBT81nNQJwgUBAoCBYHCPEeNlQUFgYJA\n",
       "QaDgqUawoCBQECgIFPyvCEGdIFAQKAgUBArug4IFBYGCQEGg4D4oWFAQKAgUBAq+SQcLCgIFgYJA\n",
       "QaDgLdegThAoCBQECo4awYKCQEGgIFAQKHiqESwoCBQECgIFR41gQUGgIFAQKPiva4IFBYGCQEGg\n",
       "IFDwv2QGdYJAQaAgUHAfFCwoCBQECgIFT1aDBQWBgkBBoCBQcNQIFhQECgIFgcJcH9IrCwoCBYGC\n",
       "QEGg4LeegzpBoCBQECi4DwoWFAQKAgWBgg/pYEFBoCBQECgIFLxAFSwoCBQECgIFR41gQUGgIFAQ\n",
       "KPiQDhYUBAoCBYGCQMGfpgjqBIGCQEGgMNel/cqCgkBBoCBQECi4MAsWFAQKAgWBgg/pYEFBoCBQ\n",
       "ECj4kA4WFAQKAgWBgkDBH1gKFhQECgIFgYKjRrCgIFAQKAgUfEgHCwoCBYGCQEGg4FcRgjpBoCBQ\n",
       "ECj4VYRgQUGgIFAQKAgUXJgFCwoCBYGCQMGHdLCgIFAQKAgUvMQZLCgIFAQKAgWBgqNGsKAgUBAo\n",
       "CBTmHR/SGwsKAgWBgkDBfVCwoCBQECgIFAQK7oOCBQWBgkBBoOCoESwoCBQECgIFgYKjRrCgIFAQ\n",
       "KAgUHDWCBQWBgkBBoOCbdLCgIFAQKAgUBAr+FQsWFAQKAgWBwly/irCyoCBQECgIFHyTDhYUBAoC\n",
       "BYGCQMGj52BBQaAgUBAozHvf/hF+mwUFgYJAQaAgUHDUCBYUBAoCBYGCv4IXLCgIFAQKAgXfpIMF\n",
       "BYGCQEGgIFDwAlWwoCBQECgIFPwqQrCgIFAQKAgUvB8ULCgIFAQKAgWBgqcawYKCQEGgIFBwaR8s\n",
       "KAgUBAoCBYGCpxrBgoJAQaAgUHAfFCwoCBQECgIFj56DBQWBgkBBoCBQcB8ULCgIFAQKAgWPnoMF\n",
       "BYGCQEGgMPfbP8GPs6AgUBAoCBQECo4awYKCQEGgIFDwflCwoCBQECgIFAQKLsyCBQWBgkBBoOA+\n",
       "KFhQECgIFAQKvkkHCwoCBYGCQEGg4KgRLCgIFAQKAgWPnoMFBYGCQEGgMP4yxc6CgkBBoCBQECg4\n",
       "agQLCgIFgYJAwaPnYEFBoCBQECgIFDx6DhYUBAoCBYGCo0awoCBQECgIFHyTDhYUBAoCBYGCQGGu\n",
       "N6hWFhQECgIFgYL7oGBBQaAgUBAouA8KFhQECgIFgYJAwVEjWFAQKAgUBArzXNqvLCgIFAQKAgWB\n",
       "wtzjwmxjQUGgIFAQKHg/KFhQECgIFAQK/gpesKAgUBAoCBQECv4KXrCgIFAQKAgUPHoOFhQECgIF\n",
       "gYKXOIMFBYGCQEGgIFDwC3XBgoJAQaAgUHDUCBYUBAoCBYGCQMFbrsGCgkBBoCBQ8JZrsKAgUBAo\n",
       "CBR8kw4WFAQKAgWBgkDBW67BgoJAQaAgUPD3g4IFBYGCQEGg4D4oWFAQKAgUBAoCBY+egwUFgYJA\n",
       "QaDgqBEsKAgUBAoCBYGCo0awoCBQECgIFBw1ggUFgYJAQaDgQzpYUBAoCBQECgIF90HBgoJAQaAg\n",
       "UHDUCBYUBAoCBYGCb9LBgoJAQaAgUBAoOGoECwoCBYGCQMHfDwoWFAQKAgWBgkDB/+0TLCgIFAQK\n",
       "AgX3QcGCgkBBoCBQmHd8Sm8sKAgUBAoCBYGCo0awoCBQECgIFDx6DhYUBAoCBYGCJ6vBgoJAQaAg\n",
       "UBAouA8KFhQECgIFgYJHz8GCgkBBoCBQECg4agQLCgIFgYJAwR9YChYUBAoCBYHCXC8IrSwoCBQE\n",
       "CgIFgYIXqIIFBYGCQEGgMM9RY2VBQaAgUBAo+CYdLCgIFAQKAgWBgqcawYKCQEGgIFDwflCwoCBQ\n",
       "ECgIFAQKc/1C3cqCgkBBoCBQcB8ULCgIFAQKAgV/miJYUBAoCBQECgIFL1AFCwoCBYGCQMGlfbCg\n",
       "IFAQKAgU3AcFCwoCBYGCQEGgMPe4EdpYUBAoCBQECu6DggUFgYJAQaAgUPCvWLCgIFAQKAgUPNUI\n",
       "FhQECgIFgYJL+2BBQaAgUBAoCBTmfhw1NhYUBAoCBYGCo0awoCBQECgIFOb5kF5ZUBAoCBQECgIF\n",
       "R41gQUGgIFAQKMz9+JDeWFAQKAgUBAoCBUeNYEFBoCBQECh4qhEsKAgUBAoCBd+kgwUFgYJAQaAg\n",
       "UPCvWLCgIFAQKAgUPHoOFhQECgIFgcK88+fbP8NPs6AgUBAoCBQECu6DggUFgYJAQaDg/aBgQUGg\n",
       "IFAQKAgU5j4XZhsLCgIFgYJAwVEjWFAQKAgUBApzPXpeWVAQKAgUBAoCBUeNYEFBoCBQECg4agQL\n",
       "CgIFgYJAYd7zTXpjQUGgIFAQKAgU3AcFCwoCBYGCQMFLnMGCgkBBoCBQECg4agQLCgIFgYJAYZ6j\n",
       "xsqCgkBBoCBQ8E06WFAQKAgUBAoChbleoFpZUBAoCBQECv7gdrCgIFAQKAgU/CpCsKAgUBAoCBQE\n",
       "Cp5qBAsKAgWBgkDBUSNYUBAoCBQECgIF/4oFCwoCBYGCQMGj52BBQaAgUBAo+CYdLCgIFAQKAgWB\n",
       "whyPnlcWFAQKAgWBgqNGsKAgUBAoCBS8xBksKAgUBAoCBYHCHEeNlQUFgYJAQaDg/aBgQUGgIFAQ\n",
       "KAgUPHoOFhQECgIFgYL7oGBBQaAgUBAozDvv2z/DT7OgIFAQKAgUBArug4IFBYGCQEGgMMdRY2VB\n",
       "QaAgUBAo/AdhU+3PqlcPeQAAAABJRU5ErkJggg==\n",
       "\" transform=\"translate(2161, 47)\"/>\n",
       "</g>\n",
       "<path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 1271.65 Q2277.09 1271.65 2275.26 1275.21 Q2273.45 1278.75 2273.45 1285.88 Q2273.45 1292.99 2275.26 1296.55 Q2277.09 1300.1 2280.7 1300.1 Q2284.33 1300.1 2286.14 1296.55 Q2287.97 1292.99 2287.97 1285.88 Q2287.97 1278.75 2286.14 1275.21 Q2284.33 1271.65 2280.7 1271.65 M2280.7 1267.94 Q2286.51 1267.94 2289.57 1272.55 Q2292.64 1277.13 2292.64 1285.88 Q2292.64 1294.61 2289.57 1299.22 Q2286.51 1303.8 2280.7 1303.8 Q2274.89 1303.8 2271.81 1299.22 Q2268.76 1294.61 2268.76 1285.88 Q2268.76 1277.13 2271.81 1272.55 Q2274.89 1267.94 2280.7 1267.94 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 1297.25 L2302.6 1297.25 L2302.6 1303.13 L2297.71 1303.13 L2297.71 1297.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2311.7 1299.19 L2328.01 1299.19 L2328.01 1303.13 L2306.07 1303.13 L2306.07 1299.19 Q2308.73 1296.44 2313.32 1291.81 Q2317.92 1287.16 2319.1 1285.81 Q2321.35 1283.29 2322.23 1281.55 Q2323.13 1279.79 2323.13 1278.1 Q2323.13 1275.35 2321.19 1273.61 Q2319.26 1271.88 2316.16 1271.88 Q2313.96 1271.88 2311.51 1272.64 Q2309.08 1273.41 2306.3 1274.96 L2306.3 1270.23 Q2309.13 1269.1 2311.58 1268.52 Q2314.03 1267.94 2316.07 1267.94 Q2321.44 1267.94 2324.64 1270.63 Q2327.83 1273.31 2327.83 1277.8 Q2327.83 1279.93 2327.02 1281.85 Q2326.23 1283.75 2324.13 1286.35 Q2323.55 1287.02 2320.45 1290.23 Q2317.34 1293.43 2311.7 1299.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2331.9 1268.57 L2354.13 1268.57 L2354.13 1270.56 L2341.58 1303.13 L2336.7 1303.13 L2348.5 1272.5 L2331.9 1272.5 L2331.9 1268.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2359.24 1268.57 L2377.6 1268.57 L2377.6 1272.5 L2363.52 1272.5 L2363.52 1280.98 Q2364.54 1280.63 2365.56 1280.47 Q2366.58 1280.28 2367.6 1280.28 Q2373.38 1280.28 2376.76 1283.45 Q2380.14 1286.62 2380.14 1292.04 Q2380.14 1297.62 2376.67 1300.72 Q2373.2 1303.8 2366.88 1303.8 Q2364.7 1303.8 2362.44 1303.43 Q2360.19 1303.06 2357.78 1302.32 L2357.78 1297.62 Q2359.87 1298.75 2362.09 1299.31 Q2364.31 1299.86 2366.79 1299.86 Q2370.79 1299.86 2373.13 1297.76 Q2375.47 1295.65 2375.47 1292.04 Q2375.47 1288.43 2373.13 1286.32 Q2370.79 1284.22 2366.79 1284.22 Q2364.91 1284.22 2363.04 1284.63 Q2361.19 1285.05 2359.24 1285.93 L2359.24 1268.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 1111.76 Q2277.09 1111.76 2275.26 1115.33 Q2273.45 1118.87 2273.45 1126 Q2273.45 1133.11 2275.26 1136.67 Q2277.09 1140.21 2280.7 1140.21 Q2284.33 1140.21 2286.14 1136.67 Q2287.97 1133.11 2287.97 1126 Q2287.97 1118.87 2286.14 1115.33 Q2284.33 1111.76 2280.7 1111.76 M2280.7 1108.06 Q2286.51 1108.06 2289.57 1112.67 Q2292.64 1117.25 2292.64 1126 Q2292.64 1134.73 2289.57 1139.33 Q2286.51 1143.92 2280.7 1143.92 Q2274.89 1143.92 2271.81 1139.33 Q2268.76 1134.73 2268.76 1126 Q2268.76 1117.25 2271.81 1112.67 Q2274.89 1108.06 2280.7 1108.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 1137.37 L2302.6 1137.37 L2302.6 1143.25 L2297.71 1143.25 L2297.71 1137.37 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2321.83 1124.61 Q2325.19 1125.33 2327.07 1127.6 Q2328.96 1129.87 2328.96 1133.2 Q2328.96 1138.32 2325.45 1141.12 Q2321.93 1143.92 2315.45 1143.92 Q2313.27 1143.92 2310.95 1143.48 Q2308.66 1143.06 2306.21 1142.2 L2306.21 1137.69 Q2308.15 1138.82 2310.47 1139.4 Q2312.78 1139.98 2315.31 1139.98 Q2319.7 1139.98 2322 1138.25 Q2324.31 1136.51 2324.31 1133.2 Q2324.31 1130.14 2322.16 1128.43 Q2320.03 1126.69 2316.21 1126.69 L2312.18 1126.69 L2312.18 1122.85 L2316.39 1122.85 Q2319.84 1122.85 2321.67 1121.49 Q2323.5 1120.1 2323.5 1117.51 Q2323.5 1114.84 2321.6 1113.43 Q2319.73 1112 2316.21 1112 Q2314.29 1112 2312.09 1112.41 Q2309.89 1112.83 2307.25 1113.71 L2307.25 1109.54 Q2309.91 1108.8 2312.23 1108.43 Q2314.57 1108.06 2316.63 1108.06 Q2321.95 1108.06 2325.05 1110.49 Q2328.15 1112.9 2328.15 1117.02 Q2328.15 1119.89 2326.51 1121.88 Q2324.87 1123.85 2321.83 1124.61 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2344.03 1111.76 Q2340.42 1111.76 2338.59 1115.33 Q2336.79 1118.87 2336.79 1126 Q2336.79 1133.11 2338.59 1136.67 Q2340.42 1140.21 2344.03 1140.21 Q2347.67 1140.21 2349.47 1136.67 Q2351.3 1133.11 2351.3 1126 Q2351.3 1118.87 2349.47 1115.33 Q2347.67 1111.76 2344.03 1111.76 M2344.03 1108.06 Q2349.84 1108.06 2352.9 1112.67 Q2355.98 1117.25 2355.98 1126 Q2355.98 1134.73 2352.9 1139.33 Q2349.84 1143.92 2344.03 1143.92 Q2338.22 1143.92 2335.14 1139.33 Q2332.09 1134.73 2332.09 1126 Q2332.09 1117.25 2335.14 1112.67 Q2338.22 1108.06 2344.03 1108.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2371.05 1111.76 Q2367.44 1111.76 2365.61 1115.33 Q2363.8 1118.87 2363.8 1126 Q2363.8 1133.11 2365.61 1136.67 Q2367.44 1140.21 2371.05 1140.21 Q2374.68 1140.21 2376.49 1136.67 Q2378.32 1133.11 2378.32 1126 Q2378.32 1118.87 2376.49 1115.33 Q2374.68 1111.76 2371.05 1111.76 M2371.05 1108.06 Q2376.86 1108.06 2379.91 1112.67 Q2382.99 1117.25 2382.99 1126 Q2382.99 1134.73 2379.91 1139.33 Q2376.86 1143.92 2371.05 1143.92 Q2365.24 1143.92 2362.16 1139.33 Q2359.1 1134.73 2359.1 1126 Q2359.1 1117.25 2362.16 1112.67 Q2365.24 1108.06 2371.05 1108.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 951.883 Q2277.09 951.883 2275.26 955.447 Q2273.45 958.989 2273.45 966.119 Q2273.45 973.225 2275.26 976.79 Q2277.09 980.331 2280.7 980.331 Q2284.33 980.331 2286.14 976.79 Q2287.97 973.225 2287.97 966.119 Q2287.97 958.989 2286.14 955.447 Q2284.33 951.883 2280.7 951.883 M2280.7 948.179 Q2286.51 948.179 2289.57 952.785 Q2292.64 957.369 2292.64 966.119 Q2292.64 974.845 2289.57 979.452 Q2286.51 984.035 2280.7 984.035 Q2274.89 984.035 2271.81 979.452 Q2268.76 974.845 2268.76 966.119 Q2268.76 957.369 2271.81 952.785 Q2274.89 948.179 2280.7 948.179 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 977.484 L2302.6 977.484 L2302.6 983.364 L2297.71 983.364 L2297.71 977.484 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2321.83 964.73 Q2325.19 965.447 2327.07 967.716 Q2328.96 969.984 2328.96 973.318 Q2328.96 978.433 2325.45 981.234 Q2321.93 984.035 2315.45 984.035 Q2313.27 984.035 2310.95 983.595 Q2308.66 983.179 2306.21 982.322 L2306.21 977.808 Q2308.15 978.943 2310.47 979.521 Q2312.78 980.1 2315.31 980.1 Q2319.7 980.1 2322 978.364 Q2324.31 976.628 2324.31 973.318 Q2324.31 970.262 2322.16 968.549 Q2320.03 966.813 2316.21 966.813 L2312.18 966.813 L2312.18 962.97 L2316.39 962.97 Q2319.84 962.97 2321.67 961.605 Q2323.5 960.216 2323.5 957.623 Q2323.5 954.961 2321.6 953.549 Q2319.73 952.114 2316.21 952.114 Q2314.29 952.114 2312.09 952.531 Q2309.89 952.947 2307.25 953.827 L2307.25 949.66 Q2309.91 948.92 2312.23 948.549 Q2314.57 948.179 2316.63 948.179 Q2321.95 948.179 2325.05 950.609 Q2328.15 953.017 2328.15 957.137 Q2328.15 960.008 2326.51 961.998 Q2324.87 963.966 2321.83 964.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2338.06 979.429 L2354.38 979.429 L2354.38 983.364 L2332.44 983.364 L2332.44 979.429 Q2335.1 976.674 2339.68 972.044 Q2344.29 967.392 2345.47 966.049 Q2347.71 963.526 2348.59 961.79 Q2349.5 960.031 2349.5 958.341 Q2349.5 955.586 2347.55 953.85 Q2345.63 952.114 2342.53 952.114 Q2340.33 952.114 2337.88 952.878 Q2335.45 953.642 2332.67 955.193 L2332.67 950.471 Q2335.49 949.336 2337.95 948.758 Q2340.4 948.179 2342.44 948.179 Q2347.81 948.179 2351 950.864 Q2354.2 953.549 2354.2 958.04 Q2354.2 960.17 2353.39 962.091 Q2352.6 963.989 2350.49 966.582 Q2349.91 967.253 2346.81 970.47 Q2343.71 973.665 2338.06 979.429 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2359.5 948.804 L2377.85 948.804 L2377.85 952.739 L2363.78 952.739 L2363.78 961.211 Q2364.8 960.864 2365.82 960.702 Q2366.83 960.517 2367.85 960.517 Q2373.64 960.517 2377.02 963.688 Q2380.4 966.859 2380.4 972.276 Q2380.4 977.855 2376.93 980.956 Q2373.45 984.035 2367.14 984.035 Q2364.96 984.035 2362.69 983.665 Q2360.45 983.294 2358.04 982.554 L2358.04 977.855 Q2360.12 978.989 2362.34 979.544 Q2364.57 980.1 2367.04 980.1 Q2371.05 980.1 2373.38 977.994 Q2375.72 975.887 2375.72 972.276 Q2375.72 968.665 2373.38 966.558 Q2371.05 964.452 2367.04 964.452 Q2365.17 964.452 2363.29 964.869 Q2361.44 965.285 2359.5 966.165 L2359.5 948.804 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 792.001 Q2277.09 792.001 2275.26 795.565 Q2273.45 799.107 2273.45 806.237 Q2273.45 813.343 2275.26 816.908 Q2277.09 820.45 2280.7 820.45 Q2284.33 820.45 2286.14 816.908 Q2287.97 813.343 2287.97 806.237 Q2287.97 799.107 2286.14 795.565 Q2284.33 792.001 2280.7 792.001 M2280.7 788.297 Q2286.51 788.297 2289.57 792.903 Q2292.64 797.487 2292.64 806.237 Q2292.64 814.963 2289.57 819.57 Q2286.51 824.153 2280.7 824.153 Q2274.89 824.153 2271.81 819.57 Q2268.76 814.963 2268.76 806.237 Q2268.76 797.487 2271.81 792.903 Q2274.89 788.297 2280.7 788.297 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 817.602 L2302.6 817.602 L2302.6 823.482 L2297.71 823.482 L2297.71 817.602 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2321.83 804.848 Q2325.19 805.565 2327.07 807.834 Q2328.96 810.102 2328.96 813.436 Q2328.96 818.551 2325.45 821.352 Q2321.93 824.153 2315.45 824.153 Q2313.27 824.153 2310.95 823.713 Q2308.66 823.297 2306.21 822.44 L2306.21 817.926 Q2308.15 819.061 2310.47 819.639 Q2312.78 820.218 2315.31 820.218 Q2319.7 820.218 2322 818.482 Q2324.31 816.746 2324.31 813.436 Q2324.31 810.38 2322.16 808.667 Q2320.03 806.931 2316.21 806.931 L2312.18 806.931 L2312.18 803.089 L2316.39 803.089 Q2319.84 803.089 2321.67 801.723 Q2323.5 800.334 2323.5 797.741 Q2323.5 795.079 2321.6 793.667 Q2319.73 792.232 2316.21 792.232 Q2314.29 792.232 2312.09 792.649 Q2309.89 793.065 2307.25 793.945 L2307.25 789.778 Q2309.91 789.038 2312.23 788.667 Q2314.57 788.297 2316.63 788.297 Q2321.95 788.297 2325.05 790.727 Q2328.15 793.135 2328.15 797.255 Q2328.15 800.126 2326.51 802.116 Q2324.87 804.084 2321.83 804.848 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2334.08 788.922 L2352.44 788.922 L2352.44 792.857 L2338.36 792.857 L2338.36 801.329 Q2339.38 800.982 2340.4 800.82 Q2341.42 800.635 2342.44 800.635 Q2348.22 800.635 2351.6 803.806 Q2354.98 806.977 2354.98 812.394 Q2354.98 817.973 2351.51 821.075 Q2348.04 824.153 2341.72 824.153 Q2339.54 824.153 2337.27 823.783 Q2335.03 823.412 2332.62 822.672 L2332.62 817.973 Q2334.7 819.107 2336.93 819.663 Q2339.15 820.218 2341.63 820.218 Q2345.63 820.218 2347.97 818.112 Q2350.31 816.005 2350.31 812.394 Q2350.31 808.783 2347.97 806.676 Q2345.63 804.57 2341.63 804.57 Q2339.75 804.57 2337.88 804.987 Q2336.02 805.403 2334.08 806.283 L2334.08 788.922 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2370.05 792.001 Q2366.44 792.001 2364.61 795.565 Q2362.81 799.107 2362.81 806.237 Q2362.81 813.343 2364.61 816.908 Q2366.44 820.45 2370.05 820.45 Q2373.69 820.45 2375.49 816.908 Q2377.32 813.343 2377.32 806.237 Q2377.32 799.107 2375.49 795.565 Q2373.69 792.001 2370.05 792.001 M2370.05 788.297 Q2375.86 788.297 2378.92 792.903 Q2382 797.487 2382 806.237 Q2382 814.963 2378.92 819.57 Q2375.86 824.153 2370.05 824.153 Q2364.24 824.153 2361.16 819.57 Q2358.11 814.963 2358.11 806.237 Q2358.11 797.487 2361.16 792.903 Q2364.24 788.297 2370.05 788.297 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 632.119 Q2277.09 632.119 2275.26 635.683 Q2273.45 639.225 2273.45 646.355 Q2273.45 653.461 2275.26 657.026 Q2277.09 660.568 2280.7 660.568 Q2284.33 660.568 2286.14 657.026 Q2287.97 653.461 2287.97 646.355 Q2287.97 639.225 2286.14 635.683 Q2284.33 632.119 2280.7 632.119 M2280.7 628.415 Q2286.51 628.415 2289.57 633.021 Q2292.64 637.605 2292.64 646.355 Q2292.64 655.082 2289.57 659.688 Q2286.51 664.271 2280.7 664.271 Q2274.89 664.271 2271.81 659.688 Q2268.76 655.082 2268.76 646.355 Q2268.76 637.605 2271.81 633.021 Q2274.89 628.415 2280.7 628.415 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 657.72 L2302.6 657.72 L2302.6 663.6 L2297.71 663.6 L2297.71 657.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2321.83 644.966 Q2325.19 645.683 2327.07 647.952 Q2328.96 650.22 2328.96 653.554 Q2328.96 658.669 2325.45 661.47 Q2321.93 664.271 2315.45 664.271 Q2313.27 664.271 2310.95 663.831 Q2308.66 663.415 2306.21 662.558 L2306.21 658.044 Q2308.15 659.179 2310.47 659.757 Q2312.78 660.336 2315.31 660.336 Q2319.7 660.336 2322 658.6 Q2324.31 656.864 2324.31 653.554 Q2324.31 650.498 2322.16 648.785 Q2320.03 647.049 2316.21 647.049 L2312.18 647.049 L2312.18 643.207 L2316.39 643.207 Q2319.84 643.207 2321.67 641.841 Q2323.5 640.452 2323.5 637.859 Q2323.5 635.197 2321.6 633.785 Q2319.73 632.35 2316.21 632.35 Q2314.29 632.35 2312.09 632.767 Q2309.89 633.183 2307.25 634.063 L2307.25 629.896 Q2309.91 629.156 2312.23 628.785 Q2314.57 628.415 2316.63 628.415 Q2321.95 628.415 2325.05 630.846 Q2328.15 633.253 2328.15 637.373 Q2328.15 640.244 2326.51 642.234 Q2324.87 644.202 2321.83 644.966 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2332.85 629.04 L2355.07 629.04 L2355.07 631.031 L2342.53 663.6 L2337.64 663.6 L2349.45 632.975 L2332.85 632.975 L2332.85 629.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2360.19 629.04 L2378.55 629.04 L2378.55 632.975 L2364.47 632.975 L2364.47 641.447 Q2365.49 641.1 2366.51 640.938 Q2367.53 640.753 2368.55 640.753 Q2374.33 640.753 2377.71 643.924 Q2381.09 647.095 2381.09 652.512 Q2381.09 658.091 2377.62 661.193 Q2374.15 664.271 2367.83 664.271 Q2365.65 664.271 2363.39 663.901 Q2361.14 663.531 2358.73 662.79 L2358.73 658.091 Q2360.82 659.225 2363.04 659.781 Q2365.26 660.336 2367.74 660.336 Q2371.74 660.336 2374.08 658.23 Q2376.42 656.123 2376.42 652.512 Q2376.42 648.901 2374.08 646.795 Q2371.74 644.688 2367.74 644.688 Q2365.86 644.688 2363.99 645.105 Q2362.14 645.521 2360.19 646.401 L2360.19 629.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 472.237 Q2277.09 472.237 2275.26 475.802 Q2273.45 479.343 2273.45 486.473 Q2273.45 493.579 2275.26 497.144 Q2277.09 500.686 2280.7 500.686 Q2284.33 500.686 2286.14 497.144 Q2287.97 493.579 2287.97 486.473 Q2287.97 479.343 2286.14 475.802 Q2284.33 472.237 2280.7 472.237 M2280.7 468.533 Q2286.51 468.533 2289.57 473.14 Q2292.64 477.723 2292.64 486.473 Q2292.64 495.2 2289.57 499.806 Q2286.51 504.389 2280.7 504.389 Q2274.89 504.389 2271.81 499.806 Q2268.76 495.2 2268.76 486.473 Q2268.76 477.723 2271.81 473.14 Q2274.89 468.533 2280.7 468.533 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 497.838 L2302.6 497.838 L2302.6 503.718 L2297.71 503.718 L2297.71 497.838 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2320.51 473.232 L2308.71 491.681 L2320.51 491.681 L2320.51 473.232 M2319.29 469.158 L2325.17 469.158 L2325.17 491.681 L2330.1 491.681 L2330.1 495.57 L2325.17 495.57 L2325.17 503.718 L2320.51 503.718 L2320.51 495.57 L2304.91 495.57 L2304.91 491.056 L2319.29 469.158 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2345.17 472.237 Q2341.56 472.237 2339.73 475.802 Q2337.92 479.343 2337.92 486.473 Q2337.92 493.579 2339.73 497.144 Q2341.56 500.686 2345.17 500.686 Q2348.8 500.686 2350.61 497.144 Q2352.44 493.579 2352.44 486.473 Q2352.44 479.343 2350.61 475.802 Q2348.8 472.237 2345.17 472.237 M2345.17 468.533 Q2350.98 468.533 2354.03 473.14 Q2357.11 477.723 2357.11 486.473 Q2357.11 495.2 2354.03 499.806 Q2350.98 504.389 2345.17 504.389 Q2339.36 504.389 2336.28 499.806 Q2333.22 495.2 2333.22 486.473 Q2333.22 477.723 2336.28 473.14 Q2339.36 468.533 2345.17 468.533 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2372.18 472.237 Q2368.57 472.237 2366.74 475.802 Q2364.94 479.343 2364.94 486.473 Q2364.94 493.579 2366.74 497.144 Q2368.57 500.686 2372.18 500.686 Q2375.82 500.686 2377.62 497.144 Q2379.45 493.579 2379.45 486.473 Q2379.45 479.343 2377.62 475.802 Q2375.82 472.237 2372.18 472.237 M2372.18 468.533 Q2377.99 468.533 2381.05 473.14 Q2384.13 477.723 2384.13 486.473 Q2384.13 495.2 2381.05 499.806 Q2377.99 504.389 2372.18 504.389 Q2366.37 504.389 2363.29 499.806 Q2360.24 495.2 2360.24 486.473 Q2360.24 477.723 2363.29 473.14 Q2366.37 468.533 2372.18 468.533 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 312.355 Q2277.09 312.355 2275.26 315.92 Q2273.45 319.461 2273.45 326.591 Q2273.45 333.697 2275.26 337.262 Q2277.09 340.804 2280.7 340.804 Q2284.33 340.804 2286.14 337.262 Q2287.97 333.697 2287.97 326.591 Q2287.97 319.461 2286.14 315.92 Q2284.33 312.355 2280.7 312.355 M2280.7 308.651 Q2286.51 308.651 2289.57 313.258 Q2292.64 317.841 2292.64 326.591 Q2292.64 335.318 2289.57 339.924 Q2286.51 344.507 2280.7 344.507 Q2274.89 344.507 2271.81 339.924 Q2268.76 335.318 2268.76 326.591 Q2268.76 317.841 2271.81 313.258 Q2274.89 308.651 2280.7 308.651 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 337.957 L2302.6 337.957 L2302.6 343.836 L2297.71 343.836 L2297.71 337.957 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2320.51 313.35 L2308.71 331.799 L2320.51 331.799 L2320.51 313.35 M2319.29 309.276 L2325.17 309.276 L2325.17 331.799 L2330.1 331.799 L2330.1 335.688 L2325.17 335.688 L2325.17 343.836 L2320.51 343.836 L2320.51 335.688 L2304.91 335.688 L2304.91 331.174 L2319.29 309.276 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2339.2 339.901 L2355.51 339.901 L2355.51 343.836 L2333.57 343.836 L2333.57 339.901 Q2336.23 337.146 2340.82 332.517 Q2345.42 327.864 2346.6 326.521 Q2348.85 323.998 2349.73 322.262 Q2350.63 320.503 2350.63 318.813 Q2350.63 316.058 2348.69 314.322 Q2346.76 312.586 2343.66 312.586 Q2341.46 312.586 2339.01 313.35 Q2336.58 314.114 2333.8 315.665 L2333.8 310.943 Q2336.63 309.809 2339.08 309.23 Q2341.53 308.651 2343.57 308.651 Q2348.94 308.651 2352.14 311.336 Q2355.33 314.021 2355.33 318.512 Q2355.33 320.642 2354.52 322.563 Q2353.73 324.461 2351.63 327.054 Q2351.05 327.725 2347.95 330.943 Q2344.84 334.137 2339.2 339.901 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2360.63 309.276 L2378.99 309.276 L2378.99 313.211 L2364.91 313.211 L2364.91 321.683 Q2365.93 321.336 2366.95 321.174 Q2367.97 320.989 2368.99 320.989 Q2374.77 320.989 2378.15 324.16 Q2381.53 327.332 2381.53 332.748 Q2381.53 338.327 2378.06 341.429 Q2374.59 344.507 2368.27 344.507 Q2366.09 344.507 2363.82 344.137 Q2361.58 343.767 2359.17 343.026 L2359.17 338.327 Q2361.26 339.461 2363.48 340.017 Q2365.7 340.572 2368.18 340.572 Q2372.18 340.572 2374.52 338.466 Q2376.86 336.359 2376.86 332.748 Q2376.86 329.137 2374.52 327.031 Q2372.18 324.924 2368.18 324.924 Q2366.3 324.924 2364.43 325.341 Q2362.57 325.758 2360.63 326.637 L2360.63 309.276 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2280.7 152.473 Q2277.09 152.473 2275.26 156.038 Q2273.45 159.579 2273.45 166.709 Q2273.45 173.815 2275.26 177.38 Q2277.09 180.922 2280.7 180.922 Q2284.33 180.922 2286.14 177.38 Q2287.97 173.815 2287.97 166.709 Q2287.97 159.579 2286.14 156.038 Q2284.33 152.473 2280.7 152.473 M2280.7 148.769 Q2286.51 148.769 2289.57 153.376 Q2292.64 157.959 2292.64 166.709 Q2292.64 175.436 2289.57 180.042 Q2286.51 184.625 2280.7 184.625 Q2274.89 184.625 2271.81 180.042 Q2268.76 175.436 2268.76 166.709 Q2268.76 157.959 2271.81 153.376 Q2274.89 148.769 2280.7 148.769 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2297.71 178.075 L2302.6 178.075 L2302.6 183.954 L2297.71 183.954 L2297.71 178.075 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2320.51 153.468 L2308.71 171.917 L2320.51 171.917 L2320.51 153.468 M2319.29 149.394 L2325.17 149.394 L2325.17 171.917 L2330.1 171.917 L2330.1 175.806 L2325.17 175.806 L2325.17 183.954 L2320.51 183.954 L2320.51 175.806 L2304.91 175.806 L2304.91 171.292 L2319.29 149.394 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2335.21 149.394 L2353.57 149.394 L2353.57 153.329 L2339.5 153.329 L2339.5 161.802 Q2340.51 161.454 2341.53 161.292 Q2342.55 161.107 2343.57 161.107 Q2349.36 161.107 2352.74 164.278 Q2356.12 167.45 2356.12 172.866 Q2356.12 178.445 2352.64 181.547 Q2349.17 184.625 2342.85 184.625 Q2340.68 184.625 2338.41 184.255 Q2336.16 183.885 2333.76 183.144 L2333.76 178.445 Q2335.84 179.579 2338.06 180.135 Q2340.28 180.69 2342.76 180.69 Q2346.76 180.69 2349.1 178.584 Q2351.44 176.477 2351.44 172.866 Q2351.44 169.255 2349.1 167.149 Q2346.76 165.042 2342.76 165.042 Q2340.89 165.042 2339.01 165.459 Q2337.16 165.876 2335.21 166.755 L2335.21 149.394 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip760)\" d=\"M 0 0 M2371.19 152.473 Q2367.57 152.473 2365.75 156.038 Q2363.94 159.579 2363.94 166.709 Q2363.94 173.815 2365.75 177.38 Q2367.57 180.922 2371.19 180.922 Q2374.82 180.922 2376.63 177.38 Q2378.45 173.815 2378.45 166.709 Q2378.45 159.579 2376.63 156.038 Q2374.82 152.473 2371.19 152.473 M2371.19 148.769 Q2377 148.769 2380.05 153.376 Q2383.13 157.959 2383.13 166.709 Q2383.13 175.436 2380.05 180.042 Q2377 184.625 2371.19 184.625 Q2365.38 184.625 2362.3 180.042 Q2359.24 175.436 2359.24 166.709 Q2359.24 157.959 2362.3 153.376 Q2365.38 148.769 2371.19 148.769 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip760)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2232.76,1423.18 2232.76,1289.48 2256.76,1289.48 2232.76,1289.48 2232.76,1129.59 2256.76,1129.59 2232.76,1129.59 2232.76,969.713 2256.76,969.713 2232.76,969.713 \n",
       "  2232.76,809.831 2256.76,809.831 2232.76,809.831 2232.76,649.949 2256.76,649.949 2232.76,649.949 2232.76,490.067 2256.76,490.067 2232.76,490.067 2232.76,330.185 \n",
       "  2256.76,330.185 2232.76,330.185 2232.76,170.303 2256.76,170.303 2232.76,170.303 2232.76,47.2441 \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(self_tuning_boost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(\n",
       "    base_estimator = nothing,\n",
       "    n_estimators = 10,\n",
       "    learning_rate = 0.1,\n",
       "    algorithm = \"SAMME.R\",\n",
       "    random_state = nothing)\u001b[34m @081\u001b[39m"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = fitted_params(self_tuning_boost)\n",
    "best.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25409"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss = round(z.report.best_result.measurement[1],digits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_n = best.best_model.n_estimators\n",
    "best_lr = best.best_model.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"Figures/LearningCurve_Boost_nestimators:$(best_n)_lr:$(best_lr)_loss:$(best_loss)\"\n",
    "png(replace(fn,'.' => ','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (10, 1.0, 0.8875)\n",
      "(d, train_metric, valid_metric) = (11, 1.0, 0.9375)\n",
      "(d, train_metric, valid_metric) = (12, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (13, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (14, 1.0, 0.8875)\n",
      "(d, train_metric, valid_metric) = (15, 1.0, 0.8875)\n",
      "(d, train_metric, valid_metric) = (16, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (17, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (18, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (19, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (20, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (21, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (22, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (23, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (24, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (25, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (26, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (27, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (28, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (29, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (30, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (31, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (32, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (33, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (34, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (35, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (36, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (37, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (38, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (39, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (40, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (41, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (42, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (43, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (44, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (45, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (46, 1.0, 0.9375)\n",
      "(d, train_metric, valid_metric) = (47, 1.0, 0.9375)\n",
      "(d, train_metric, valid_metric) = (48, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (49, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (50, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (51, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (52, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (53, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (54, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (55, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (56, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (57, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (58, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (59, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (60, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (61, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (62, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (63, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (64, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (65, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (66, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (67, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (68, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (69, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (70, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (71, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (72, 1.0, 0.9375)\n",
      "(d, train_metric, valid_metric) = (73, 1.0, 0.9375)\n",
      "(d, train_metric, valid_metric) = (74, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (75, 1.0, 0.9375)\n",
      "(d, train_metric, valid_metric) = (76, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (77, 1.0, 0.9375)\n",
      "(d, train_metric, valid_metric) = (78, 1.0, 0.9375)\n",
      "(d, train_metric, valid_metric) = (79, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (80, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (81, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (82, 0.9878048780487805, 0.9375)\n",
      "(d, train_metric, valid_metric) = (83, 0.9879518072289156, 0.9375)\n",
      "(d, train_metric, valid_metric) = (84, 0.9761904761904762, 0.9125)\n",
      "(d, train_metric, valid_metric) = (85, 0.9764705882352941, 0.9125)\n",
      "(d, train_metric, valid_metric) = (86, 0.9767441860465116, 0.9125)\n",
      "(d, train_metric, valid_metric) = (87, 0.9885057471264368, 0.925)\n",
      "(d, train_metric, valid_metric) = (88, 0.9886363636363636, 0.925)\n",
      "(d, train_metric, valid_metric) = (89, 0.9887640449438202, 0.925)\n",
      "(d, train_metric, valid_metric) = (90, 0.9777777777777777, 0.9125)\n",
      "(d, train_metric, valid_metric) = (91, 0.978021978021978, 0.9125)\n",
      "(d, train_metric, valid_metric) = (92, 0.9782608695652174, 0.9125)\n",
      "(d, train_metric, valid_metric) = (93, 1.0, 0.9375)\n",
      "(d, train_metric, valid_metric) = (94, 1.0, 0.9375)\n",
      "(d, train_metric, valid_metric) = (95, 0.9789473684210527, 0.9125)\n",
      "(d, train_metric, valid_metric) = (96, 0.9791666666666666, 0.9125)\n",
      "(d, train_metric, valid_metric) = (97, 1.0, 0.9375)\n",
      "(d, train_metric, valid_metric) = (98, 0.9795918367346939, 0.9125)\n",
      "(d, train_metric, valid_metric) = (99, 0.9797979797979798, 0.9125)\n",
      "(d, train_metric, valid_metric) = (100, 0.98, 0.9125)\n",
      "(d, train_metric, valid_metric) = (101, 0.9801980198019802, 0.9125)\n",
      "(d, train_metric, valid_metric) = (102, 0.9803921568627451, 0.9125)\n",
      "(d, train_metric, valid_metric) = (103, 0.9805825242718447, 0.9125)\n",
      "(d, train_metric, valid_metric) = (104, 0.9807692307692307, 0.9125)\n",
      "(d, train_metric, valid_metric) = (105, 0.9809523809523809, 0.9125)\n",
      "(d, train_metric, valid_metric) = (106, 0.9811320754716981, 0.9125)\n",
      "(d, train_metric, valid_metric) = (107, 0.9813084112149533, 0.9125)\n",
      "(d, train_metric, valid_metric) = (108, 0.9814814814814815, 0.9125)\n",
      "(d, train_metric, valid_metric) = (109, 0.963302752293578, 0.9125)\n",
      "(d, train_metric, valid_metric) = (110, 0.9636363636363636, 0.925)\n",
      "(d, train_metric, valid_metric) = (111, 0.972972972972973, 0.9375)\n",
      "(d, train_metric, valid_metric) = (112, 0.9732142857142857, 0.9375)\n",
      "(d, train_metric, valid_metric) = (113, 0.9734513274336283, 0.9375)\n",
      "(d, train_metric, valid_metric) = (114, 0.9736842105263158, 0.9375)\n",
      "(d, train_metric, valid_metric) = (115, 0.9739130434782609, 0.9375)\n",
      "(d, train_metric, valid_metric) = (116, 0.9741379310344828, 0.9375)\n",
      "(d, train_metric, valid_metric) = (117, 0.9743589743589743, 0.9375)\n",
      "(d, train_metric, valid_metric) = (118, 0.9745762711864406, 0.9375)\n",
      "(d, train_metric, valid_metric) = (119, 0.9747899159663865, 0.9375)\n",
      "(d, train_metric, valid_metric) = (120, 0.975, 0.9375)\n",
      "(d, train_metric, valid_metric) = (121, 0.9752066115702479, 0.9375)\n",
      "(d, train_metric, valid_metric) = (122, 0.9754098360655737, 0.9375)\n",
      "(d, train_metric, valid_metric) = (123, 0.975609756097561, 0.9375)\n",
      "(d, train_metric, valid_metric) = (124, 0.9838709677419355, 0.925)\n",
      "(d, train_metric, valid_metric) = (125, 0.984, 0.9375)\n",
      "(d, train_metric, valid_metric) = (126, 0.9841269841269841, 0.9375)\n",
      "(d, train_metric, valid_metric) = (127, 0.984251968503937, 0.9375)\n",
      "(d, train_metric, valid_metric) = (128, 0.96875, 0.925)\n",
      "(d, train_metric, valid_metric) = (129, 0.9844961240310077, 0.925)\n",
      "(d, train_metric, valid_metric) = (130, 0.9846153846153847, 0.925)\n",
      "(d, train_metric, valid_metric) = (131, 0.9847328244274809, 0.925)\n",
      "(d, train_metric, valid_metric) = (132, 0.9848484848484849, 0.925)\n",
      "(d, train_metric, valid_metric) = (133, 0.9849624060150376, 0.925)\n",
      "(d, train_metric, valid_metric) = (134, 0.9850746268656716, 0.925)\n",
      "(d, train_metric, valid_metric) = (135, 0.9851851851851852, 0.925)\n",
      "(d, train_metric, valid_metric) = (136, 0.9779411764705882, 0.925)\n",
      "(d, train_metric, valid_metric) = (137, 0.9781021897810219, 0.9375)\n",
      "(d, train_metric, valid_metric) = (138, 0.9782608695652174, 0.925)\n",
      "(d, train_metric, valid_metric) = (139, 0.9784172661870504, 0.9375)\n",
      "(d, train_metric, valid_metric) = (140, 0.9785714285714285, 0.925)\n",
      "(d, train_metric, valid_metric) = (141, 0.9787234042553191, 0.9375)\n",
      "(d, train_metric, valid_metric) = (142, 0.9788732394366197, 0.925)\n",
      "(d, train_metric, valid_metric) = (143, 0.9790209790209791, 0.9375)\n",
      "(d, train_metric, valid_metric) = (144, 0.9791666666666666, 0.9375)\n",
      "(d, train_metric, valid_metric) = (145, 0.9793103448275862, 0.9375)\n",
      "(d, train_metric, valid_metric) = (146, 0.9794520547945206, 0.9375)\n",
      "(d, train_metric, valid_metric) = (147, 0.9727891156462585, 0.9375)\n",
      "(d, train_metric, valid_metric) = (148, 0.972972972972973, 0.9375)\n",
      "(d, train_metric, valid_metric) = (149, 0.9731543624161074, 0.9375)\n",
      "(d, train_metric, valid_metric) = (150, 0.9733333333333334, 0.9375)\n",
      "(d, train_metric, valid_metric) = (151, 0.9735099337748344, 0.9375)\n",
      "(d, train_metric, valid_metric) = (152, 0.9736842105263158, 0.9375)\n",
      "(d, train_metric, valid_metric) = (153, 0.9738562091503268, 0.9375)\n",
      "(d, train_metric, valid_metric) = (154, 0.974025974025974, 0.9375)\n",
      "(d, train_metric, valid_metric) = (155, 0.9741935483870968, 0.9375)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (156, 0.9743589743589743, 0.9375)\n",
      "(d, train_metric, valid_metric) = (157, 0.9745222929936306, 0.9375)\n",
      "(d, train_metric, valid_metric) = (158, 0.9746835443037974, 0.9375)\n",
      "(d, train_metric, valid_metric) = (159, 0.9748427672955975, 0.9375)\n",
      "(d, train_metric, valid_metric) = (160, 0.975, 0.9375)\n",
      "(d, train_metric, valid_metric) = (161, 0.9751552795031055, 0.9375)\n",
      "(d, train_metric, valid_metric) = (162, 0.9753086419753086, 0.9375)\n",
      "(d, train_metric, valid_metric) = (163, 0.9693251533742331, 0.9375)\n",
      "(d, train_metric, valid_metric) = (164, 0.9695121951219512, 0.9375)\n",
      "(d, train_metric, valid_metric) = (165, 0.9696969696969697, 0.9375)\n",
      "(d, train_metric, valid_metric) = (166, 0.9698795180722891, 0.9375)\n",
      "(d, train_metric, valid_metric) = (167, 0.9700598802395209, 0.9375)\n",
      "(d, train_metric, valid_metric) = (168, 0.9702380952380952, 0.9375)\n",
      "(d, train_metric, valid_metric) = (169, 0.9704142011834319, 0.9375)\n",
      "(d, train_metric, valid_metric) = (170, 0.9705882352941176, 0.9375)\n",
      "(d, train_metric, valid_metric) = (171, 0.9649122807017544, 0.9375)\n",
      "(d, train_metric, valid_metric) = (172, 0.9651162790697675, 0.9375)\n",
      "(d, train_metric, valid_metric) = (173, 0.9653179190751445, 0.9375)\n",
      "(d, train_metric, valid_metric) = (174, 0.9655172413793104, 0.9375)\n",
      "(d, train_metric, valid_metric) = (175, 0.96, 0.925)\n",
      "(d, train_metric, valid_metric) = (176, 0.9545454545454546, 0.9125)\n",
      "(d, train_metric, valid_metric) = (177, 0.9548022598870056, 0.9125)\n",
      "(d, train_metric, valid_metric) = (178, 0.9606741573033708, 0.9375)\n",
      "(d, train_metric, valid_metric) = (179, 0.9553072625698324, 0.9125)\n",
      "(d, train_metric, valid_metric) = (180, 0.9555555555555556, 0.9125)\n",
      "(d, train_metric, valid_metric) = (181, 0.9558011049723757, 0.9125)\n",
      "(d, train_metric, valid_metric) = (182, 0.9560439560439561, 0.9125)\n",
      "(d, train_metric, valid_metric) = (183, 0.9562841530054644, 0.9125)\n",
      "(d, train_metric, valid_metric) = (184, 0.9565217391304348, 0.9125)\n",
      "(d, train_metric, valid_metric) = (185, 0.9567567567567568, 0.9125)\n",
      "(d, train_metric, valid_metric) = (186, 0.956989247311828, 0.925)\n",
      "(d, train_metric, valid_metric) = (187, 0.9572192513368984, 0.925)\n",
      "(d, train_metric, valid_metric) = (188, 0.9574468085106383, 0.925)\n",
      "(d, train_metric, valid_metric) = (189, 0.9576719576719577, 0.925)\n",
      "(d, train_metric, valid_metric) = (190, 0.9578947368421052, 0.925)\n",
      "(d, train_metric, valid_metric) = (191, 0.9581151832460733, 0.925)\n",
      "(d, train_metric, valid_metric) = (192, 0.9583333333333334, 0.925)\n",
      "(d, train_metric, valid_metric) = (193, 0.9585492227979274, 0.925)\n",
      "(d, train_metric, valid_metric) = (194, 0.9587628865979382, 0.925)\n",
      "(d, train_metric, valid_metric) = (195, 0.958974358974359, 0.925)\n",
      "(d, train_metric, valid_metric) = (196, 0.9591836734693877, 0.925)\n",
      "(d, train_metric, valid_metric) = (197, 0.9593908629441624, 0.925)\n",
      "(d, train_metric, valid_metric) = (198, 0.9595959595959596, 0.925)\n",
      "(d, train_metric, valid_metric) = (199, 0.9597989949748744, 0.925)\n",
      "(d, train_metric, valid_metric) = (200, 0.96, 0.925)\n",
      "(d, train_metric, valid_metric) = (201, 0.9601990049751243, 0.925)\n",
      "(d, train_metric, valid_metric) = (202, 0.9603960396039604, 0.925)\n",
      "(d, train_metric, valid_metric) = (203, 0.9605911330049262, 0.925)\n",
      "(d, train_metric, valid_metric) = (204, 0.9607843137254902, 0.925)\n",
      "(d, train_metric, valid_metric) = (205, 0.9609756097560975, 0.925)\n",
      "(d, train_metric, valid_metric) = (206, 0.9611650485436893, 0.925)\n",
      "(d, train_metric, valid_metric) = (207, 0.961352657004831, 0.925)\n",
      "(d, train_metric, valid_metric) = (208, 0.9615384615384616, 0.925)\n",
      "(d, train_metric, valid_metric) = (209, 0.9617224880382775, 0.925)\n",
      "(d, train_metric, valid_metric) = (210, 0.9619047619047619, 0.925)\n",
      "(d, train_metric, valid_metric) = (211, 0.9620853080568721, 0.925)\n",
      "(d, train_metric, valid_metric) = (212, 0.9622641509433962, 0.925)\n",
      "(d, train_metric, valid_metric) = (213, 0.9624413145539906, 0.925)\n",
      "(d, train_metric, valid_metric) = (214, 0.9626168224299065, 0.925)\n",
      "(d, train_metric, valid_metric) = (215, 0.9627906976744186, 0.925)\n",
      "(d, train_metric, valid_metric) = (216, 0.9675925925925926, 0.9375)\n",
      "(d, train_metric, valid_metric) = (217, 0.967741935483871, 0.9375)\n",
      "(d, train_metric, valid_metric) = (218, 0.9678899082568807, 0.9375)\n",
      "(d, train_metric, valid_metric) = (219, 0.9680365296803652, 0.9375)\n",
      "(d, train_metric, valid_metric) = (220, 0.9681818181818181, 0.9375)\n",
      "(d, train_metric, valid_metric) = (221, 0.9683257918552036, 0.9375)\n",
      "(d, train_metric, valid_metric) = (222, 0.963963963963964, 0.9375)\n",
      "(d, train_metric, valid_metric) = (223, 0.968609865470852, 0.9375)\n",
      "(d, train_metric, valid_metric) = (224, 0.96875, 0.9375)\n",
      "(d, train_metric, valid_metric) = (225, 0.9688888888888889, 0.9375)\n",
      "(d, train_metric, valid_metric) = (226, 0.9690265486725663, 0.9125)\n",
      "(d, train_metric, valid_metric) = (227, 0.9691629955947136, 0.9375)\n",
      "(d, train_metric, valid_metric) = (228, 0.9649122807017544, 0.9125)\n",
      "(d, train_metric, valid_metric) = (229, 0.9650655021834061, 0.9125)\n",
      "(d, train_metric, valid_metric) = (230, 0.9652173913043478, 0.9125)\n",
      "(d, train_metric, valid_metric) = (231, 0.9653679653679653, 0.9125)\n",
      "(d, train_metric, valid_metric) = (232, 0.9612068965517241, 0.925)\n",
      "(d, train_metric, valid_metric) = (233, 0.9613733905579399, 0.925)\n",
      "(d, train_metric, valid_metric) = (234, 0.9615384615384616, 0.925)\n",
      "(d, train_metric, valid_metric) = (235, 0.9617021276595744, 0.925)\n",
      "(d, train_metric, valid_metric) = (236, 0.961864406779661, 0.925)\n",
      "(d, train_metric, valid_metric) = (237, 0.9620253164556962, 0.925)\n",
      "(d, train_metric, valid_metric) = (238, 0.9621848739495799, 0.9375)\n",
      "(d, train_metric, valid_metric) = (239, 0.9623430962343096, 0.9375)\n",
      "(d, train_metric, valid_metric) = (240, 0.9625, 0.9375)\n",
      "(d, train_metric, valid_metric) = (241, 0.9626556016597511, 0.9375)\n",
      "(d, train_metric, valid_metric) = (242, 0.9628099173553719, 0.9375)\n",
      "(d, train_metric, valid_metric) = (243, 0.9629629629629629, 0.9375)\n",
      "(d, train_metric, valid_metric) = (244, 0.9631147540983607, 0.9375)\n",
      "(d, train_metric, valid_metric) = (245, 0.963265306122449, 0.925)\n",
      "(d, train_metric, valid_metric) = (246, 0.9634146341463414, 0.925)\n",
      "(d, train_metric, valid_metric) = (247, 0.9635627530364372, 0.925)\n",
      "(d, train_metric, valid_metric) = (248, 0.9596774193548387, 0.9375)\n",
      "(d, train_metric, valid_metric) = (249, 0.9598393574297188, 0.9375)\n",
      "(d, train_metric, valid_metric) = (250, 0.96, 0.9375)\n",
      "(d, train_metric, valid_metric) = (251, 0.9601593625498008, 0.9375)\n",
      "(d, train_metric, valid_metric) = (252, 0.9603174603174603, 0.9375)\n",
      "(d, train_metric, valid_metric) = (253, 0.9644268774703557, 0.9375)\n",
      "(d, train_metric, valid_metric) = (254, 0.9645669291338582, 0.9375)\n",
      "(d, train_metric, valid_metric) = (255, 0.9647058823529412, 0.9375)\n",
      "(d, train_metric, valid_metric) = (256, 0.96484375, 0.9375)\n",
      "(d, train_metric, valid_metric) = (257, 0.9649805447470817, 0.9375)\n",
      "(d, train_metric, valid_metric) = (258, 0.9651162790697675, 0.9375)\n",
      "(d, train_metric, valid_metric) = (259, 0.9613899613899614, 0.9375)\n",
      "(d, train_metric, valid_metric) = (260, 0.9615384615384616, 0.9375)\n",
      "(d, train_metric, valid_metric) = (261, 0.9616858237547893, 0.9375)\n",
      "(d, train_metric, valid_metric) = (262, 0.9618320610687023, 0.9375)\n",
      "(d, train_metric, valid_metric) = (263, 0.9657794676806084, 0.9375)\n",
      "(d, train_metric, valid_metric) = (264, 0.9659090909090909, 0.925)\n",
      "(d, train_metric, valid_metric) = (265, 0.9660377358490566, 0.925)\n",
      "(d, train_metric, valid_metric) = (266, 0.9661654135338346, 0.925)\n",
      "(d, train_metric, valid_metric) = (267, 0.9662921348314607, 0.925)\n",
      "(d, train_metric, valid_metric) = (268, 0.9664179104477612, 0.925)\n",
      "(d, train_metric, valid_metric) = (269, 0.966542750929368, 0.925)\n",
      "(d, train_metric, valid_metric) = (270, 0.9666666666666667, 0.925)\n",
      "(d, train_metric, valid_metric) = (271, 0.959409594095941, 0.925)\n",
      "(d, train_metric, valid_metric) = (272, 0.9595588235294118, 0.925)\n",
      "(d, train_metric, valid_metric) = (273, 0.9597069597069597, 0.925)\n",
      "(d, train_metric, valid_metric) = (274, 0.9598540145985401, 0.925)\n",
      "(d, train_metric, valid_metric) = (275, 0.96, 0.925)\n",
      "(d, train_metric, valid_metric) = (276, 0.9601449275362319, 0.925)\n",
      "(d, train_metric, valid_metric) = (277, 0.9602888086642599, 0.925)\n",
      "(d, train_metric, valid_metric) = (278, 0.960431654676259, 0.925)\n",
      "(d, train_metric, valid_metric) = (279, 0.9605734767025089, 0.925)\n",
      "(d, train_metric, valid_metric) = (280, 0.9607142857142857, 0.925)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (281, 0.9608540925266904, 0.9375)\n",
      "(d, train_metric, valid_metric) = (282, 0.9609929078014184, 0.9375)\n",
      "(d, train_metric, valid_metric) = (283, 0.9611307420494699, 0.9375)\n",
      "(d, train_metric, valid_metric) = (284, 0.9612676056338029, 0.9375)\n",
      "(d, train_metric, valid_metric) = (285, 0.9614035087719298, 0.9375)\n",
      "(d, train_metric, valid_metric) = (286, 0.9615384615384616, 0.9375)\n",
      "(d, train_metric, valid_metric) = (287, 0.9616724738675958, 0.9375)\n",
      "(d, train_metric, valid_metric) = (288, 0.9618055555555556, 0.9375)\n",
      "(d, train_metric, valid_metric) = (289, 0.9515570934256056, 0.9375)\n",
      "(d, train_metric, valid_metric) = (290, 0.9517241379310345, 0.9375)\n",
      "(d, train_metric, valid_metric) = (291, 0.9518900343642611, 0.9375)\n",
      "(d, train_metric, valid_metric) = (292, 0.952054794520548, 0.9375)\n",
      "(d, train_metric, valid_metric) = (293, 0.9522184300341296, 0.9375)\n",
      "(d, train_metric, valid_metric) = (294, 0.9523809523809523, 0.9375)\n",
      "(d, train_metric, valid_metric) = (295, 0.9525423728813559, 0.9375)\n",
      "(d, train_metric, valid_metric) = (296, 0.9527027027027027, 0.9375)\n",
      "(d, train_metric, valid_metric) = (297, 0.9528619528619529, 0.9375)\n",
      "(d, train_metric, valid_metric) = (298, 0.9530201342281879, 0.9375)\n",
      "(d, train_metric, valid_metric) = (299, 0.9531772575250836, 0.925)\n",
      "(d, train_metric, valid_metric) = (300, 0.9533333333333334, 0.925)\n",
      "(d, train_metric, valid_metric) = (301, 0.9534883720930233, 0.925)\n",
      "(d, train_metric, valid_metric) = (302, 0.9536423841059603, 0.925)\n",
      "(d, train_metric, valid_metric) = (303, 0.9537953795379538, 0.925)\n",
      "(d, train_metric, valid_metric) = (304, 0.9506578947368421, 0.9375)\n",
      "(d, train_metric, valid_metric) = (305, 0.9508196721311475, 0.925)\n",
      "(d, train_metric, valid_metric) = (306, 0.9477124183006536, 0.925)\n",
      "(d, train_metric, valid_metric) = (307, 0.9478827361563518, 0.925)\n",
      "(d, train_metric, valid_metric) = (308, 0.948051948051948, 0.925)\n",
      "(d, train_metric, valid_metric) = (309, 0.948220064724919, 0.925)\n",
      "(d, train_metric, valid_metric) = (310, 0.9483870967741935, 0.925)\n",
      "(d, train_metric, valid_metric) = (311, 0.9485530546623794, 0.925)\n",
      "(d, train_metric, valid_metric) = (312, 0.9487179487179487, 0.925)\n",
      "(d, train_metric, valid_metric) = (313, 0.9488817891373802, 0.925)\n",
      "(d, train_metric, valid_metric) = (314, 0.9490445859872612, 0.925)\n",
      "(d, train_metric, valid_metric) = (315, 0.9492063492063492, 0.925)\n",
      "(d, train_metric, valid_metric) = (316, 0.9493670886075949, 0.925)\n",
      "(d, train_metric, valid_metric) = (317, 0.9463722397476341, 0.9375)\n",
      "(d, train_metric, valid_metric) = (318, 0.9465408805031447, 0.9375)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10:1:318, Any[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0    0.948220064724919, 0.9483870967741935, 0.9485530546623794, 0.9487179487179487, 0.9488817891373802, 0.9490445859872612, 0.9492063492063492, 0.9493670886075949, 0.9463722397476341, 0.9465408805031447], Any[0.8875, 0.9375, 0.925, 0.925, 0.8875, 0.8875, 0.925, 0.925, 0.925, 0.9125    0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.9375, 0.9375])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_schedule, training_losses, valid_losses = learn_curve(best.best_model, X[train,:], y[train], acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip930\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip930)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip931\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip930)\" d=\"\n",
       "M174.677 1486.45 L2352.76 1486.45 L2352.76 47.2441 L174.677 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip932\">\n",
       "    <rect x=\"174\" y=\"47\" width=\"2179\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  503.177,1486.45 503.177,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  836.747,1486.45 836.747,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1170.32,1486.45 1170.32,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1503.89,1486.45 1503.89,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1837.46,1486.45 1837.46,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2171.03,1486.45 2171.03,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.677,1251.75 2352.76,1251.75 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.677,863.827 2352.76,863.827 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.677,475.902 2352.76,475.902 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.677,87.9763 2352.76,87.9763 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.677,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.677,1486.45 174.677,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  503.177,1486.45 503.177,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  836.747,1486.45 836.747,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1170.32,1486.45 1170.32,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1503.89,1486.45 1503.89,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1837.46,1486.45 1837.46,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2171.03,1486.45 2171.03,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.677,1251.75 200.814,1251.75 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.677,863.827 200.814,863.827 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.677,475.902 200.814,475.902 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.677,87.9763 200.814,87.9763 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip930)\" d=\"M 0 0 M479.948 1505.36 L498.304 1505.36 L498.304 1509.3 L484.23 1509.3 L484.23 1517.77 Q485.248 1517.42 486.267 1517.26 Q487.286 1517.07 488.304 1517.07 Q494.091 1517.07 497.471 1520.24 Q500.85 1523.42 500.85 1528.83 Q500.85 1534.41 497.378 1537.51 Q493.906 1540.59 487.586 1540.59 Q485.411 1540.59 483.142 1540.22 Q480.897 1539.85 478.489 1539.11 L478.489 1534.41 Q480.573 1535.54 482.795 1536.1 Q485.017 1536.66 487.494 1536.66 Q491.498 1536.66 493.836 1534.55 Q496.174 1532.44 496.174 1528.83 Q496.174 1525.22 493.836 1523.11 Q491.498 1521.01 487.494 1521.01 Q485.619 1521.01 483.744 1521.42 Q481.892 1521.84 479.948 1522.72 L479.948 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M515.92 1508.44 Q512.309 1508.44 510.48 1512 Q508.674 1515.55 508.674 1522.67 Q508.674 1529.78 510.48 1533.35 Q512.309 1536.89 515.92 1536.89 Q519.554 1536.89 521.359 1533.35 Q523.188 1529.78 523.188 1522.67 Q523.188 1515.55 521.359 1512 Q519.554 1508.44 515.92 1508.44 M515.92 1504.73 Q521.73 1504.73 524.785 1509.34 Q527.864 1513.92 527.864 1522.67 Q527.864 1531.4 524.785 1536.01 Q521.73 1540.59 515.92 1540.59 Q510.109 1540.59 507.031 1536.01 Q503.975 1531.4 503.975 1522.67 Q503.975 1513.92 507.031 1509.34 Q510.109 1504.73 515.92 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M800.115 1535.98 L807.754 1535.98 L807.754 1509.62 L799.444 1511.29 L799.444 1507.03 L807.708 1505.36 L812.383 1505.36 L812.383 1535.98 L820.022 1535.98 L820.022 1539.92 L800.115 1539.92 L800.115 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M835.092 1508.44 Q831.481 1508.44 829.652 1512 Q827.846 1515.55 827.846 1522.67 Q827.846 1529.78 829.652 1533.35 Q831.481 1536.89 835.092 1536.89 Q838.726 1536.89 840.531 1533.35 Q842.36 1529.78 842.36 1522.67 Q842.36 1515.55 840.531 1512 Q838.726 1508.44 835.092 1508.44 M835.092 1504.73 Q840.902 1504.73 843.957 1509.34 Q847.036 1513.92 847.036 1522.67 Q847.036 1531.4 843.957 1536.01 Q840.902 1540.59 835.092 1540.59 Q829.281 1540.59 826.203 1536.01 Q823.147 1531.4 823.147 1522.67 Q823.147 1513.92 826.203 1509.34 Q829.281 1504.73 835.092 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M862.105 1508.44 Q858.494 1508.44 856.666 1512 Q854.86 1515.55 854.86 1522.67 Q854.86 1529.78 856.666 1533.35 Q858.494 1536.89 862.105 1536.89 Q865.74 1536.89 867.545 1533.35 Q869.374 1529.78 869.374 1522.67 Q869.374 1515.55 867.545 1512 Q865.74 1508.44 862.105 1508.44 M862.105 1504.73 Q867.916 1504.73 870.971 1509.34 Q874.05 1513.92 874.05 1522.67 Q874.05 1531.4 870.971 1536.01 Q867.916 1540.59 862.105 1540.59 Q856.295 1540.59 853.217 1536.01 Q850.161 1531.4 850.161 1522.67 Q850.161 1513.92 853.217 1509.34 Q856.295 1504.73 862.105 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M1134.18 1535.98 L1141.82 1535.98 L1141.82 1509.62 L1133.51 1511.29 L1133.51 1507.03 L1141.78 1505.36 L1146.45 1505.36 L1146.45 1535.98 L1154.09 1535.98 L1154.09 1539.92 L1134.18 1539.92 L1134.18 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M1159.21 1505.36 L1177.56 1505.36 L1177.56 1509.3 L1163.49 1509.3 L1163.49 1517.77 Q1164.51 1517.42 1165.53 1517.26 Q1166.54 1517.07 1167.56 1517.07 Q1173.35 1517.07 1176.73 1520.24 Q1180.11 1523.42 1180.11 1528.83 Q1180.11 1534.41 1176.64 1537.51 Q1173.16 1540.59 1166.84 1540.59 Q1164.67 1540.59 1162.4 1540.22 Q1160.15 1539.85 1157.75 1539.11 L1157.75 1534.41 Q1159.83 1535.54 1162.05 1536.1 Q1164.28 1536.66 1166.75 1536.66 Q1170.76 1536.66 1173.09 1534.55 Q1175.43 1532.44 1175.43 1528.83 Q1175.43 1525.22 1173.09 1523.11 Q1170.76 1521.01 1166.75 1521.01 Q1164.88 1521.01 1163 1521.42 Q1161.15 1521.84 1159.21 1522.72 L1159.21 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M1195.18 1508.44 Q1191.57 1508.44 1189.74 1512 Q1187.93 1515.55 1187.93 1522.67 Q1187.93 1529.78 1189.74 1533.35 Q1191.57 1536.89 1195.18 1536.89 Q1198.81 1536.89 1200.62 1533.35 Q1202.45 1529.78 1202.45 1522.67 Q1202.45 1515.55 1200.62 1512 Q1198.81 1508.44 1195.18 1508.44 M1195.18 1504.73 Q1200.99 1504.73 1204.04 1509.34 Q1207.12 1513.92 1207.12 1522.67 Q1207.12 1531.4 1204.04 1536.01 Q1200.99 1540.59 1195.18 1540.59 Q1189.37 1540.59 1186.29 1536.01 Q1183.23 1531.4 1183.23 1522.67 Q1183.23 1513.92 1186.29 1509.34 Q1189.37 1504.73 1195.18 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M1471.53 1535.98 L1487.85 1535.98 L1487.85 1539.92 L1465.9 1539.92 L1465.9 1535.98 Q1468.56 1533.23 1473.15 1528.6 Q1477.75 1523.95 1478.93 1522.61 Q1481.18 1520.08 1482.06 1518.35 Q1482.96 1516.59 1482.96 1514.9 Q1482.96 1512.14 1481.02 1510.41 Q1479.1 1508.67 1475.99 1508.67 Q1473.79 1508.67 1471.34 1509.43 Q1468.91 1510.2 1466.13 1511.75 L1466.13 1507.03 Q1468.96 1505.89 1471.41 1505.31 Q1473.86 1504.73 1475.9 1504.73 Q1481.27 1504.73 1484.47 1507.42 Q1487.66 1510.11 1487.66 1514.6 Q1487.66 1516.73 1486.85 1518.65 Q1486.06 1520.54 1483.96 1523.14 Q1483.38 1523.81 1480.28 1527.03 Q1477.17 1530.22 1471.53 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M1502.91 1508.44 Q1499.3 1508.44 1497.47 1512 Q1495.67 1515.55 1495.67 1522.67 Q1495.67 1529.78 1497.47 1533.35 Q1499.3 1536.89 1502.91 1536.89 Q1506.55 1536.89 1508.35 1533.35 Q1510.18 1529.78 1510.18 1522.67 Q1510.18 1515.55 1508.35 1512 Q1506.55 1508.44 1502.91 1508.44 M1502.91 1504.73 Q1508.72 1504.73 1511.78 1509.34 Q1514.86 1513.92 1514.86 1522.67 Q1514.86 1531.4 1511.78 1536.01 Q1508.72 1540.59 1502.91 1540.59 Q1497.1 1540.59 1494.03 1536.01 Q1490.97 1531.4 1490.97 1522.67 Q1490.97 1513.92 1494.03 1509.34 Q1497.1 1504.73 1502.91 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M1529.93 1508.44 Q1526.32 1508.44 1524.49 1512 Q1522.68 1515.55 1522.68 1522.67 Q1522.68 1529.78 1524.49 1533.35 Q1526.32 1536.89 1529.93 1536.89 Q1533.56 1536.89 1535.37 1533.35 Q1537.2 1529.78 1537.2 1522.67 Q1537.2 1515.55 1535.37 1512 Q1533.56 1508.44 1529.93 1508.44 M1529.93 1504.73 Q1535.74 1504.73 1538.79 1509.34 Q1541.87 1513.92 1541.87 1522.67 Q1541.87 1531.4 1538.79 1536.01 Q1535.74 1540.59 1529.93 1540.59 Q1524.12 1540.59 1521.04 1536.01 Q1517.98 1531.4 1517.98 1522.67 Q1517.98 1513.92 1521.04 1509.34 Q1524.12 1504.73 1529.93 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M1805.59 1535.98 L1821.91 1535.98 L1821.91 1539.92 L1799.97 1539.92 L1799.97 1535.98 Q1802.63 1533.23 1807.21 1528.6 Q1811.82 1523.95 1813 1522.61 Q1815.25 1520.08 1816.13 1518.35 Q1817.03 1516.59 1817.03 1514.9 Q1817.03 1512.14 1815.08 1510.41 Q1813.16 1508.67 1810.06 1508.67 Q1807.86 1508.67 1805.41 1509.43 Q1802.98 1510.2 1800.2 1511.75 L1800.2 1507.03 Q1803.02 1505.89 1805.48 1505.31 Q1807.93 1504.73 1809.97 1504.73 Q1815.34 1504.73 1818.53 1507.42 Q1821.73 1510.11 1821.73 1514.6 Q1821.73 1516.73 1820.92 1518.65 Q1820.13 1520.54 1818.02 1523.14 Q1817.45 1523.81 1814.34 1527.03 Q1811.24 1530.22 1805.59 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M1827.03 1505.36 L1845.39 1505.36 L1845.39 1509.3 L1831.31 1509.3 L1831.31 1517.77 Q1832.33 1517.42 1833.35 1517.26 Q1834.37 1517.07 1835.39 1517.07 Q1841.17 1517.07 1844.55 1520.24 Q1847.93 1523.42 1847.93 1528.83 Q1847.93 1534.41 1844.46 1537.51 Q1840.99 1540.59 1834.67 1540.59 Q1832.49 1540.59 1830.22 1540.22 Q1827.98 1539.85 1825.57 1539.11 L1825.57 1534.41 Q1827.65 1535.54 1829.88 1536.1 Q1832.1 1536.66 1834.57 1536.66 Q1838.58 1536.66 1840.92 1534.55 Q1843.26 1532.44 1843.26 1528.83 Q1843.26 1525.22 1840.92 1523.11 Q1838.58 1521.01 1834.57 1521.01 Q1832.7 1521.01 1830.82 1521.42 Q1828.97 1521.84 1827.03 1522.72 L1827.03 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M1863 1508.44 Q1859.39 1508.44 1857.56 1512 Q1855.76 1515.55 1855.76 1522.67 Q1855.76 1529.78 1857.56 1533.35 Q1859.39 1536.89 1863 1536.89 Q1866.63 1536.89 1868.44 1533.35 Q1870.27 1529.78 1870.27 1522.67 Q1870.27 1515.55 1868.44 1512 Q1866.63 1508.44 1863 1508.44 M1863 1504.73 Q1868.81 1504.73 1871.87 1509.34 Q1874.95 1513.92 1874.95 1522.67 Q1874.95 1531.4 1871.87 1536.01 Q1868.81 1540.59 1863 1540.59 Q1857.19 1540.59 1854.11 1536.01 Q1851.06 1531.4 1851.06 1522.67 Q1851.06 1513.92 1854.11 1509.34 Q1857.19 1504.73 1863 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M2148.26 1521.29 Q2151.62 1522 2153.49 1524.27 Q2155.39 1526.54 2155.39 1529.87 Q2155.39 1534.99 2151.87 1537.79 Q2148.35 1540.59 2141.87 1540.59 Q2139.7 1540.59 2137.38 1540.15 Q2135.09 1539.73 2132.64 1538.88 L2132.64 1534.36 Q2134.58 1535.5 2136.9 1536.08 Q2139.21 1536.66 2141.73 1536.66 Q2146.13 1536.66 2148.42 1534.92 Q2150.74 1533.18 2150.74 1529.87 Q2150.74 1526.82 2148.58 1525.11 Q2146.46 1523.37 2142.64 1523.37 L2138.61 1523.37 L2138.61 1519.53 L2142.82 1519.53 Q2146.27 1519.53 2148.1 1518.16 Q2149.93 1516.77 2149.93 1514.18 Q2149.93 1511.52 2148.03 1510.11 Q2146.15 1508.67 2142.64 1508.67 Q2140.71 1508.67 2138.52 1509.09 Q2136.32 1509.5 2133.68 1510.38 L2133.68 1506.22 Q2136.34 1505.48 2138.65 1505.11 Q2140.99 1504.73 2143.05 1504.73 Q2148.38 1504.73 2151.48 1507.17 Q2154.58 1509.57 2154.58 1513.69 Q2154.58 1516.56 2152.94 1518.55 Q2151.29 1520.52 2148.26 1521.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M2170.46 1508.44 Q2166.85 1508.44 2165.02 1512 Q2163.21 1515.55 2163.21 1522.67 Q2163.21 1529.78 2165.02 1533.35 Q2166.85 1536.89 2170.46 1536.89 Q2174.09 1536.89 2175.9 1533.35 Q2177.73 1529.78 2177.73 1522.67 Q2177.73 1515.55 2175.9 1512 Q2174.09 1508.44 2170.46 1508.44 M2170.46 1504.73 Q2176.27 1504.73 2179.33 1509.34 Q2182.4 1513.92 2182.4 1522.67 Q2182.4 1531.4 2179.33 1536.01 Q2176.27 1540.59 2170.46 1540.59 Q2164.65 1540.59 2161.57 1536.01 Q2158.52 1531.4 2158.52 1522.67 Q2158.52 1513.92 2161.57 1509.34 Q2164.65 1504.73 2170.46 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M2197.47 1508.44 Q2193.86 1508.44 2192.03 1512 Q2190.23 1515.55 2190.23 1522.67 Q2190.23 1529.78 2192.03 1533.35 Q2193.86 1536.89 2197.47 1536.89 Q2201.11 1536.89 2202.91 1533.35 Q2204.74 1529.78 2204.74 1522.67 Q2204.74 1515.55 2202.91 1512 Q2201.11 1508.44 2197.47 1508.44 M2197.47 1504.73 Q2203.28 1504.73 2206.34 1509.34 Q2209.42 1513.92 2209.42 1522.67 Q2209.42 1531.4 2206.34 1536.01 Q2203.28 1540.59 2197.47 1540.59 Q2191.66 1540.59 2188.58 1536.01 Q2185.53 1531.4 2185.53 1522.67 Q2185.53 1513.92 2188.58 1509.34 Q2191.66 1504.73 2197.47 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M75.8393 1237.55 Q72.2282 1237.55 70.3995 1241.12 Q68.5939 1244.66 68.5939 1251.79 Q68.5939 1258.89 70.3995 1262.46 Q72.2282 1266 75.8393 1266 Q79.4735 1266 81.2791 1262.46 Q83.1078 1258.89 83.1078 1251.79 Q83.1078 1244.66 81.2791 1241.12 Q79.4735 1237.55 75.8393 1237.55 M75.8393 1233.85 Q81.6494 1233.85 84.705 1238.45 Q87.7837 1243.04 87.7837 1251.79 Q87.7837 1260.51 84.705 1265.12 Q81.6494 1269.7 75.8393 1269.7 Q70.0291 1269.7 66.9504 1265.12 Q63.8949 1260.51 63.8949 1251.79 Q63.8949 1243.04 66.9504 1238.45 Q70.0291 1233.85 75.8393 1233.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M92.8531 1263.15 L97.7373 1263.15 L97.7373 1269.03 L92.8531 1269.03 L92.8531 1263.15 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M112.807 1252.62 Q109.473 1252.62 107.552 1254.4 Q105.654 1256.19 105.654 1259.31 Q105.654 1262.44 107.552 1264.22 Q109.473 1266 112.807 1266 Q116.14 1266 118.061 1264.22 Q119.983 1262.41 119.983 1259.31 Q119.983 1256.19 118.061 1254.4 Q116.163 1252.62 112.807 1252.62 M108.131 1250.63 Q105.122 1249.89 103.432 1247.83 Q101.765 1245.77 101.765 1242.81 Q101.765 1238.66 104.705 1236.26 Q107.668 1233.85 112.807 1233.85 Q117.969 1233.85 120.908 1236.26 Q123.848 1238.66 123.848 1242.81 Q123.848 1245.77 122.158 1247.83 Q120.492 1249.89 117.506 1250.63 Q120.885 1251.42 122.76 1253.71 Q124.658 1256 124.658 1259.31 Q124.658 1264.33 121.58 1267.02 Q118.524 1269.7 112.807 1269.7 Q107.089 1269.7 104.01 1267.02 Q100.955 1264.33 100.955 1259.31 Q100.955 1256 102.853 1253.71 Q104.751 1251.42 108.131 1250.63 M106.418 1243.25 Q106.418 1245.93 108.084 1247.44 Q109.774 1248.94 112.807 1248.94 Q115.816 1248.94 117.506 1247.44 Q119.219 1245.93 119.219 1243.25 Q119.219 1240.56 117.506 1239.06 Q115.816 1237.55 112.807 1237.55 Q109.774 1237.55 108.084 1239.06 Q106.418 1240.56 106.418 1243.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M129.774 1234.47 L148.131 1234.47 L148.131 1238.41 L134.057 1238.41 L134.057 1246.88 Q135.075 1246.53 136.094 1246.37 Q137.112 1246.19 138.131 1246.19 Q143.918 1246.19 147.297 1249.36 Q150.677 1252.53 150.677 1257.95 Q150.677 1263.52 147.205 1266.63 Q143.732 1269.7 137.413 1269.7 Q135.237 1269.7 132.969 1269.33 Q130.723 1268.96 128.316 1268.22 L128.316 1263.52 Q130.399 1264.66 132.621 1265.21 Q134.844 1265.77 137.32 1265.77 Q141.325 1265.77 143.663 1263.66 Q146.001 1261.56 146.001 1257.95 Q146.001 1254.33 143.663 1252.23 Q141.325 1250.12 137.32 1250.12 Q135.445 1250.12 133.57 1250.54 Q131.719 1250.95 129.774 1251.83 L129.774 1234.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M74.9365 849.626 Q71.3254 849.626 69.4967 853.191 Q67.6912 856.733 67.6912 863.862 Q67.6912 870.969 69.4967 874.533 Q71.3254 878.075 74.9365 878.075 Q78.5707 878.075 80.3763 874.533 Q82.205 870.969 82.205 863.862 Q82.205 856.733 80.3763 853.191 Q78.5707 849.626 74.9365 849.626 M74.9365 845.922 Q80.7467 845.922 83.8022 850.529 Q86.8809 855.112 86.8809 863.862 Q86.8809 872.589 83.8022 877.195 Q80.7467 881.779 74.9365 881.779 Q69.1264 881.779 66.0477 877.195 Q62.9921 872.589 62.9921 863.862 Q62.9921 855.112 66.0477 850.529 Q69.1264 845.922 74.9365 845.922 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M91.9503 875.228 L96.8345 875.228 L96.8345 881.107 L91.9503 881.107 L91.9503 875.228 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M102.043 880.39 L102.043 876.131 Q103.802 876.964 105.608 877.404 Q107.413 877.843 109.149 877.843 Q113.779 877.843 116.209 874.742 Q118.663 871.617 119.01 865.274 Q117.668 867.265 115.608 868.33 Q113.547 869.394 111.047 869.394 Q105.862 869.394 102.83 866.269 Q99.8206 863.121 99.8206 857.682 Q99.8206 852.358 102.969 849.14 Q106.117 845.922 111.348 845.922 Q117.344 845.922 120.492 850.529 Q123.663 855.112 123.663 863.862 Q123.663 872.033 119.774 876.918 Q115.909 881.779 109.358 881.779 Q107.598 881.779 105.793 881.431 Q103.987 881.084 102.043 880.39 M111.348 865.737 Q114.496 865.737 116.325 863.584 Q118.177 861.432 118.177 857.682 Q118.177 853.955 116.325 851.802 Q114.496 849.626 111.348 849.626 Q108.2 849.626 106.348 851.802 Q104.52 853.955 104.52 857.682 Q104.52 861.432 106.348 863.584 Q108.2 865.737 111.348 865.737 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M138.732 849.626 Q135.121 849.626 133.293 853.191 Q131.487 856.733 131.487 863.862 Q131.487 870.969 133.293 874.533 Q135.121 878.075 138.732 878.075 Q142.367 878.075 144.172 874.533 Q146.001 870.969 146.001 863.862 Q146.001 856.733 144.172 853.191 Q142.367 849.626 138.732 849.626 M138.732 845.922 Q144.543 845.922 147.598 850.529 Q150.677 855.112 150.677 863.862 Q150.677 872.589 147.598 877.195 Q144.543 881.779 138.732 881.779 Q132.922 881.779 129.844 877.195 Q126.788 872.589 126.788 863.862 Q126.788 855.112 129.844 850.529 Q132.922 845.922 138.732 845.922 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M75.9319 461.701 Q72.3208 461.701 70.4921 465.265 Q68.6865 468.807 68.6865 475.937 Q68.6865 483.043 70.4921 486.608 Q72.3208 490.149 75.9319 490.149 Q79.5661 490.149 81.3717 486.608 Q83.2004 483.043 83.2004 475.937 Q83.2004 468.807 81.3717 465.265 Q79.5661 461.701 75.9319 461.701 M75.9319 457.997 Q81.742 457.997 84.7976 462.603 Q87.8763 467.187 87.8763 475.937 Q87.8763 484.663 84.7976 489.27 Q81.742 493.853 75.9319 493.853 Q70.1217 493.853 67.043 489.27 Q63.9875 484.663 63.9875 475.937 Q63.9875 467.187 67.043 462.603 Q70.1217 457.997 75.9319 457.997 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M92.9457 487.302 L97.8299 487.302 L97.8299 493.182 L92.9457 493.182 L92.9457 487.302 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M103.038 492.464 L103.038 488.205 Q104.797 489.038 106.603 489.478 Q108.409 489.918 110.145 489.918 Q114.774 489.918 117.205 486.816 Q119.658 483.691 120.006 477.349 Q118.663 479.339 116.603 480.404 Q114.543 481.469 112.043 481.469 Q106.858 481.469 103.825 478.344 Q100.816 475.196 100.816 469.756 Q100.816 464.432 103.964 461.214 Q107.112 457.997 112.344 457.997 Q118.339 457.997 121.487 462.603 Q124.658 467.187 124.658 475.937 Q124.658 484.108 120.77 488.992 Q116.904 493.853 110.353 493.853 Q108.594 493.853 106.788 493.506 Q104.983 493.159 103.038 492.464 M112.344 477.812 Q115.492 477.812 117.321 475.659 Q119.172 473.506 119.172 469.756 Q119.172 466.029 117.321 463.876 Q115.492 461.701 112.344 461.701 Q109.196 461.701 107.344 463.876 Q105.515 466.029 105.515 469.756 Q105.515 473.506 107.344 475.659 Q109.196 477.812 112.344 477.812 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M129.774 458.622 L148.131 458.622 L148.131 462.557 L134.057 462.557 L134.057 471.029 Q135.075 470.682 136.094 470.52 Q137.112 470.335 138.131 470.335 Q143.918 470.335 147.297 473.506 Q150.677 476.677 150.677 482.094 Q150.677 487.673 147.205 490.774 Q143.732 493.853 137.413 493.853 Q135.237 493.853 132.969 493.483 Q130.723 493.112 128.316 492.372 L128.316 487.673 Q130.399 488.807 132.621 489.362 Q134.844 489.918 137.32 489.918 Q141.325 489.918 143.663 487.811 Q146.001 485.705 146.001 482.094 Q146.001 478.483 143.663 476.376 Q141.325 474.27 137.32 474.27 Q135.445 474.27 133.57 474.687 Q131.719 475.103 129.774 475.983 L129.774 458.622 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M66.7884 101.321 L74.4272 101.321 L74.4272 74.9555 L66.1171 76.6222 L66.1171 72.3629 L74.381 70.6963 L79.0569 70.6963 L79.0569 101.321 L86.6957 101.321 L86.6957 105.256 L66.7884 105.256 L66.7884 101.321 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M91.7651 99.3767 L96.6494 99.3767 L96.6494 105.256 L91.7651 105.256 L91.7651 99.3767 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M111.719 73.775 Q108.108 73.775 106.279 77.3398 Q104.473 80.8814 104.473 88.011 Q104.473 95.1174 106.279 98.6822 Q108.108 102.224 111.719 102.224 Q115.353 102.224 117.159 98.6822 Q118.987 95.1174 118.987 88.011 Q118.987 80.8814 117.159 77.3398 Q115.353 73.775 111.719 73.775 M111.719 70.0713 Q117.529 70.0713 120.584 74.6777 Q123.663 79.261 123.663 88.011 Q123.663 96.7378 120.584 101.344 Q117.529 105.928 111.719 105.928 Q105.909 105.928 102.83 101.344 Q99.7743 96.7378 99.7743 88.011 Q99.7743 79.261 102.83 74.6777 Q105.909 70.0713 111.719 70.0713 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M138.732 73.775 Q135.121 73.775 133.293 77.3398 Q131.487 80.8814 131.487 88.011 Q131.487 95.1174 133.293 98.6822 Q135.121 102.224 138.732 102.224 Q142.367 102.224 144.172 98.6822 Q146.001 95.1174 146.001 88.011 Q146.001 80.8814 144.172 77.3398 Q142.367 73.775 138.732 73.775 M138.732 70.0713 Q144.543 70.0713 147.598 74.6777 Q150.677 79.261 150.677 88.011 Q150.677 96.7378 147.598 101.344 Q144.543 105.928 138.732 105.928 Q132.922 105.928 129.844 101.344 Q126.788 96.7378 126.788 88.011 Q126.788 79.261 129.844 74.6777 Q132.922 70.0713 138.732 70.0713 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip932)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  236.321,87.9763 242.992,87.9763 249.663,87.9763 256.335,87.9763 263.006,87.9763 269.678,87.9763 276.349,87.9763 283.02,87.9763 289.692,87.9763 296.363,87.9763 \n",
       "  303.035,87.9763 309.706,87.9763 316.377,87.9763 323.049,87.9763 329.72,87.9763 336.392,87.9763 343.063,87.9763 349.734,87.9763 356.406,87.9763 363.077,87.9763 \n",
       "  369.749,87.9763 376.42,87.9763 383.091,87.9763 389.763,87.9763 396.434,87.9763 403.106,87.9763 409.777,87.9763 416.448,87.9763 423.12,87.9763 429.791,87.9763 \n",
       "  436.463,87.9763 443.134,87.9763 449.805,87.9763 456.477,87.9763 463.148,87.9763 469.82,87.9763 476.491,87.9763 483.162,87.9763 489.834,87.9763 496.505,87.9763 \n",
       "  503.177,87.9763 509.848,87.9763 516.519,87.9763 523.191,87.9763 529.862,87.9763 536.534,87.9763 543.205,87.9763 549.876,87.9763 556.548,87.9763 563.219,87.9763 \n",
       "  569.891,87.9763 576.562,87.9763 583.233,87.9763 589.905,87.9763 596.576,87.9763 603.248,87.9763 609.919,87.9763 616.59,87.9763 623.262,87.9763 629.933,87.9763 \n",
       "  636.605,87.9763 643.276,87.9763 649.947,87.9763 656.619,87.9763 663.29,87.9763 669.962,87.9763 676.633,87.9763 683.304,87.9763 689.976,87.9763 696.647,87.9763 \n",
       "  703.319,87.9763 709.99,87.9763 716.661,182.592 723.333,181.452 730.004,272.703 736.676,270.529 743.347,268.407 750.018,177.155 756.69,176.141 763.361,175.151 \n",
       "  770.033,260.388 776.704,258.493 783.375,256.64 790.047,87.9763 796.718,87.9763 803.39,251.313 810.061,249.612 816.732,87.9763 823.404,246.313 830.075,244.714 \n",
       "  836.747,243.146 843.418,241.61 850.089,240.104 856.761,238.627 863.432,237.178 870.104,235.757 876.775,234.363 883.447,232.995 890.118,231.652 896.789,372.692 \n",
       "  903.461,370.104 910.132,297.666 916.804,295.794 923.475,293.954 930.146,292.148 936.818,290.372 943.489,288.627 950.161,286.912 956.832,285.227 963.503,283.569 \n",
       "  970.175,281.939 976.846,280.336 983.518,278.759 990.189,277.208 996.86,213.114 1003.53,212.112 1010.2,211.127 1016.87,210.158 1023.55,330.43 1030.22,208.263 \n",
       "  1036.89,207.338 1043.56,206.427 1050.23,205.529 1056.9,204.646 1063.57,203.775 1070.25,202.917 1076.92,259.12 1083.59,257.871 1090.26,256.64 1096.93,255.426 \n",
       "  1103.6,254.23 1110.27,253.051 1116.95,251.888 1123.62,250.742 1130.29,249.612 1136.96,248.497 1143.63,247.398 1150.3,299.092 1156.97,297.666 1163.65,296.258 \n",
       "  1170.32,294.87 1176.99,293.5 1183.66,292.148 1190.33,290.813 1197,289.496 1203.67,288.196 1210.35,286.912 1217.02,285.645 1223.69,284.394 1230.36,283.159 \n",
       "  1237.03,281.939 1243.7,280.734 1250.37,279.544 1257.04,325.967 1263.72,324.516 1270.39,323.083 1277.06,321.666 1283.73,320.267 1290.4,318.884 1297.07,317.518 \n",
       "  1303.74,316.168 1310.42,360.205 1317.09,358.622 1323.76,357.058 1330.43,355.511 1337.1,398.317 1343.77,440.636 1350.44,438.643 1357.12,393.086 1363.79,434.725 \n",
       "  1370.46,432.799 1377.13,430.894 1383.8,429.01 1390.47,427.146 1397.14,425.303 1403.82,423.479 1410.49,421.676 1417.16,419.891 1423.83,418.126 1430.5,416.379 \n",
       "  1437.17,414.65 1443.84,412.94 1450.52,411.248 1457.19,409.573 1463.86,407.915 1470.53,406.274 1477.2,404.65 1483.87,403.043 1490.54,401.451 1497.22,399.876 \n",
       "  1503.89,398.317 1510.56,396.773 1517.23,395.244 1523.9,393.73 1530.57,392.232 1537.24,390.747 1543.92,389.278 1550.59,387.822 1557.26,386.381 1563.93,384.953 \n",
       "  1570.6,383.539 1577.27,382.138 1583.94,380.75 1590.62,379.376 1597.29,378.014 1603.96,376.665 1610.63,339.41 1617.3,338.251 1623.97,337.103 1630.64,335.965 \n",
       "  1637.31,334.838 1643.99,333.721 1650.66,367.562 1657.33,331.517 1664,330.43 1670.67,329.352 1677.34,328.284 1684.01,327.226 1690.69,360.205 1697.36,359.016 \n",
       "  1704.03,357.838 1710.7,356.669 1717.37,388.953 1724.04,387.661 1730.71,386.381 1737.39,385.111 1744.06,383.852 1750.73,382.603 1757.4,381.365 1764.07,380.138 \n",
       "  1770.74,378.92 1777.41,377.713 1784.09,376.516 1790.76,375.329 1797.43,374.151 1804.1,372.983 1810.77,371.824 1817.44,370.675 1824.11,400.819 1830.79,399.563 \n",
       "  1837.46,398.317 1844.13,397.08 1850.8,395.854 1857.47,363.971 1864.14,362.884 1870.81,361.806 1877.49,360.736 1884.16,359.675 1890.83,358.622 1897.5,387.533 \n",
       "  1904.17,386.381 1910.84,385.237 1917.51,384.103 1924.19,353.477 1930.86,352.471 1937.53,351.473 1944.2,350.482 1950.87,349.499 1957.54,348.523 1964.21,347.555 \n",
       "  1970.88,346.593 1977.56,402.897 1984.23,401.74 1990.9,400.59 1997.57,399.449 2004.24,398.317 2010.91,397.192 2017.58,396.076 2024.26,394.968 2030.93,393.867 \n",
       "  2037.6,392.775 2044.27,391.69 2050.94,390.613 2057.61,389.544 2064.28,388.482 2070.96,387.428 2077.63,386.381 2084.3,385.341 2090.97,384.308 2097.64,463.821 \n",
       "  2104.31,462.525 2110.98,461.238 2117.66,459.96 2124.33,458.69 2131,457.429 2137.67,456.177 2144.34,454.933 2151.01,453.697 2157.68,452.47 2164.36,451.251 \n",
       "  2171.03,450.04 2177.7,448.837 2184.37,447.642 2191.04,446.455 2197.71,470.798 2204.38,469.542 2211.06,493.65 2217.73,492.329 2224.4,491.016 2231.07,489.711 \n",
       "  2237.74,488.416 2244.41,487.128 2251.08,485.849 2257.76,484.577 2264.43,483.314 2271.1,482.059 2277.77,480.812 2284.44,504.048 2291.11,502.739 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip932)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  236.321,766.846 242.992,766.846 249.663,960.809 256.335,960.809 263.006,669.865 269.678,1445.72 276.349,960.809 283.02,669.865 289.692,766.846 296.363,669.865 \n",
       "  303.035,766.846 309.706,669.865 316.377,669.865 323.049,766.846 329.72,669.865 336.392,669.865 343.063,669.865 349.734,766.846 356.406,669.865 363.077,669.865 \n",
       "  369.749,669.865 376.42,669.865 383.091,669.865 389.763,669.865 396.434,669.865 403.106,669.865 409.777,669.865 416.448,669.865 423.12,669.865 429.791,669.865 \n",
       "  436.463,669.865 443.134,669.865 449.805,669.865 456.477,669.865 463.148,669.865 469.82,669.865 476.491,572.883 483.162,572.883 489.834,669.865 496.505,669.865 \n",
       "  503.177,669.865 509.848,669.865 516.519,669.865 523.191,669.865 529.862,669.865 536.534,669.865 543.205,669.865 549.876,669.865 556.548,669.865 563.219,669.865 \n",
       "  569.891,669.865 576.562,669.865 583.233,669.865 589.905,766.846 596.576,669.865 603.248,669.865 609.919,669.865 616.59,766.846 623.262,766.846 629.933,669.865 \n",
       "  636.605,669.865 643.276,572.883 649.947,572.883 656.619,572.883 663.29,669.865 669.962,669.865 676.633,572.883 683.304,669.865 689.976,669.865 696.647,766.846 \n",
       "  703.319,766.846 709.99,766.846 716.661,572.883 723.333,572.883 730.004,766.846 736.676,766.846 743.347,766.846 750.018,669.865 756.69,669.865 763.361,669.865 \n",
       "  770.033,766.846 776.704,766.846 783.375,766.846 790.047,572.883 796.718,572.883 803.39,766.846 810.061,766.846 816.732,572.883 823.404,766.846 830.075,766.846 \n",
       "  836.747,766.846 843.418,766.846 850.089,766.846 856.761,766.846 863.432,766.846 870.104,766.846 876.775,766.846 883.447,766.846 890.118,766.846 896.789,766.846 \n",
       "  903.461,669.865 910.132,572.883 916.804,572.883 923.475,572.883 930.146,572.883 936.818,572.883 943.489,572.883 950.161,572.883 956.832,572.883 963.503,572.883 \n",
       "  970.175,572.883 976.846,572.883 983.518,572.883 990.189,572.883 996.86,669.865 1003.53,572.883 1010.2,572.883 1016.87,572.883 1023.55,669.865 1030.22,669.865 \n",
       "  1036.89,669.865 1043.56,669.865 1050.23,669.865 1056.9,669.865 1063.57,669.865 1070.25,669.865 1076.92,669.865 1083.59,572.883 1090.26,669.865 1096.93,572.883 \n",
       "  1103.6,669.865 1110.27,572.883 1116.95,669.865 1123.62,572.883 1130.29,572.883 1136.96,572.883 1143.63,572.883 1150.3,572.883 1156.97,572.883 1163.65,572.883 \n",
       "  1170.32,572.883 1176.99,572.883 1183.66,572.883 1190.33,572.883 1197,572.883 1203.67,572.883 1210.35,572.883 1217.02,572.883 1223.69,572.883 1230.36,572.883 \n",
       "  1237.03,572.883 1243.7,572.883 1250.37,572.883 1257.04,572.883 1263.72,572.883 1270.39,572.883 1277.06,572.883 1283.73,572.883 1290.4,572.883 1297.07,572.883 \n",
       "  1303.74,572.883 1310.42,572.883 1317.09,572.883 1323.76,572.883 1330.43,572.883 1337.1,669.865 1343.77,766.846 1350.44,766.846 1357.12,572.883 1363.79,766.846 \n",
       "  1370.46,766.846 1377.13,766.846 1383.8,766.846 1390.47,766.846 1397.14,766.846 1403.82,766.846 1410.49,669.865 1417.16,669.865 1423.83,669.865 1430.5,669.865 \n",
       "  1437.17,669.865 1443.84,669.865 1450.52,669.865 1457.19,669.865 1463.86,669.865 1470.53,669.865 1477.2,669.865 1483.87,669.865 1490.54,669.865 1497.22,669.865 \n",
       "  1503.89,669.865 1510.56,669.865 1517.23,669.865 1523.9,669.865 1530.57,669.865 1537.24,669.865 1543.92,669.865 1550.59,669.865 1557.26,669.865 1563.93,669.865 \n",
       "  1570.6,669.865 1577.27,669.865 1583.94,669.865 1590.62,669.865 1597.29,669.865 1603.96,669.865 1610.63,572.883 1617.3,572.883 1623.97,572.883 1630.64,572.883 \n",
       "  1637.31,572.883 1643.99,572.883 1650.66,572.883 1657.33,572.883 1664,572.883 1670.67,572.883 1677.34,766.846 1684.01,572.883 1690.69,766.846 1697.36,766.846 \n",
       "  1704.03,766.846 1710.7,766.846 1717.37,669.865 1724.04,669.865 1730.71,669.865 1737.39,669.865 1744.06,669.865 1750.73,669.865 1757.4,572.883 1764.07,572.883 \n",
       "  1770.74,572.883 1777.41,572.883 1784.09,572.883 1790.76,572.883 1797.43,572.883 1804.1,669.865 1810.77,669.865 1817.44,669.865 1824.11,572.883 1830.79,572.883 \n",
       "  1837.46,572.883 1844.13,572.883 1850.8,572.883 1857.47,572.883 1864.14,572.883 1870.81,572.883 1877.49,572.883 1884.16,572.883 1890.83,572.883 1897.5,572.883 \n",
       "  1904.17,572.883 1910.84,572.883 1917.51,572.883 1924.19,572.883 1930.86,669.865 1937.53,669.865 1944.2,669.865 1950.87,669.865 1957.54,669.865 1964.21,669.865 \n",
       "  1970.88,669.865 1977.56,669.865 1984.23,669.865 1990.9,669.865 1997.57,669.865 2004.24,669.865 2010.91,669.865 2017.58,669.865 2024.26,669.865 2030.93,669.865 \n",
       "  2037.6,669.865 2044.27,572.883 2050.94,572.883 2057.61,572.883 2064.28,572.883 2070.96,572.883 2077.63,572.883 2084.3,572.883 2090.97,572.883 2097.64,572.883 \n",
       "  2104.31,572.883 2110.98,572.883 2117.66,572.883 2124.33,572.883 2131,572.883 2137.67,572.883 2144.34,572.883 2151.01,572.883 2157.68,572.883 2164.36,669.865 \n",
       "  2171.03,669.865 2177.7,669.865 2184.37,669.865 2191.04,669.865 2197.71,572.883 2204.38,669.865 2211.06,669.865 2217.73,669.865 2224.4,669.865 2231.07,669.865 \n",
       "  2237.74,669.865 2244.41,669.865 2251.08,669.865 2257.76,669.865 2264.43,669.865 2271.1,669.865 2277.77,669.865 2284.44,572.883 2291.11,572.883 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip930)\" d=\"\n",
       "M1987.13 276.658 L2280.15 276.658 L2280.15 95.2176 L1987.13 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1987.13,276.658 2280.15,276.658 2280.15,95.2176 1987.13,95.2176 1987.13,276.658 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip930)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2011.33,155.698 2156.53,155.698 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip930)\" d=\"M 0 0 M2194.58 175.385 Q2192.77 180.015 2191.06 181.427 Q2189.34 182.839 2186.47 182.839 L2183.07 182.839 L2183.07 179.274 L2185.57 179.274 Q2187.33 179.274 2188.3 178.44 Q2189.27 177.607 2190.46 174.505 L2191.22 172.561 L2180.73 147.052 L2185.25 147.052 L2193.35 167.329 L2201.45 147.052 L2205.96 147.052 L2194.58 175.385 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M2211.84 169.042 L2219.48 169.042 L2219.48 142.677 L2211.17 144.343 L2211.17 140.084 L2219.44 138.418 L2224.11 138.418 L2224.11 169.042 L2231.75 169.042 L2231.75 172.978 L2211.84 172.978 L2211.84 169.042 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip930)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2011.33,216.178 2156.53,216.178 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip930)\" d=\"M 0 0 M2194.58 235.865 Q2192.77 240.495 2191.06 241.907 Q2189.34 243.319 2186.47 243.319 L2183.07 243.319 L2183.07 239.754 L2185.57 239.754 Q2187.33 239.754 2188.3 238.92 Q2189.27 238.087 2190.46 234.985 L2191.22 233.041 L2180.73 207.532 L2185.25 207.532 L2193.35 227.809 L2201.45 207.532 L2205.96 207.532 L2194.58 235.865 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip930)\" d=\"M 0 0 M2215.06 229.522 L2231.38 229.522 L2231.38 233.458 L2209.44 233.458 L2209.44 229.522 Q2212.1 226.768 2216.68 222.138 Q2221.29 217.485 2222.47 216.143 Q2224.71 213.62 2225.59 211.884 Q2226.5 210.124 2226.5 208.435 Q2226.5 205.68 2224.55 203.944 Q2222.63 202.208 2219.53 202.208 Q2217.33 202.208 2214.88 202.972 Q2212.45 203.735 2209.67 205.286 L2209.67 200.564 Q2212.49 199.43 2214.95 198.851 Q2217.4 198.273 2219.44 198.273 Q2224.81 198.273 2228 200.958 Q2231.2 203.643 2231.2 208.134 Q2231.2 210.263 2230.39 212.185 Q2229.6 214.083 2227.49 216.675 Q2226.91 217.347 2223.81 220.564 Q2220.71 223.759 2215.06 229.522 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(data_schedule, training_losses)\n",
    "plot!(data_schedule, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(\n",
       "    base_estimator = nothing,\n",
       "    n_estimators = 10,\n",
       "    learning_rate = 0.1,\n",
       "    algorithm = \"SAMME.R\",\n",
       "    random_state = nothing)\u001b[34m @665\u001b[39m"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_boost_model = best.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{AdaBoostClassifier} @893\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @775\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @402\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_Boost = machine(final_boost_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{AdaBoostClassifier} @893\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/machines.jl:322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{AdaBoostClassifier} @893\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @775\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @402\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(Final_Boost, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: both StatsBase and MLJ export \"predict\"; uses of it in module Main must be qualified\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: predict not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: predict not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[32]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "y2 = predict(Final_Boost, X[test,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: 2 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: 2 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[33]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "cross_entropy(y2, y[test]) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: 2 not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: 2 not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[34]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "acc(y2, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(mode.(y), y[test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
