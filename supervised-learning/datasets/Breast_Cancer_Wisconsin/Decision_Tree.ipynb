{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using CSV\n",
    "using MLJ\n",
    "using DecisionTree: print_tree\n",
    "using Plots\n",
    "using StatsBase\n",
    "\n",
    "include(\"../../lib.jl\")\n",
    "\n",
    "ENV[\"LINES\"]=50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 2. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 3. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 4. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 5. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 6. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 7. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 8. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 9. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 10. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 11. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 12. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 13. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 14. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 15. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 16. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 17. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 18. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 19. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 20. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 21. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 22. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 23. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 24. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 25. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 26. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 27. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 28. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 29. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 30. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 31. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 32. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 33. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 34. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 35. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 36. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 37. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 38. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 39. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 40. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 41. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 42. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 43. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 44. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 45. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 46. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 47. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 48. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 49. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 50. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 51. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 52. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 53. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 54. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 55. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 56. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 57. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 58. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 59. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 60. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 61. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 62. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 63. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 64. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 65. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 66. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 67. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 68. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 69. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 70. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 71. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 72. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 73. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 74. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 75. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 76. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 77. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 78. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 79. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 80. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 81. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 82. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 83. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 84. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 85. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 86. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 87. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 88. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 89. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 90. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 91. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 92. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 93. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 94. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 95. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 96. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 97. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 98. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 99. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 100. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 101. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 102. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 103. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 104. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 105. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 106. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 107. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 108. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 109. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 110. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 111. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 112. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 113. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 114. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 115. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 116. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 117. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 118. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 119. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 120. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 121. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 122. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 123. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 124. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 125. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 126. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 127. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 128. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 129. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 130. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 131. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 132. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 133. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 134. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 135. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 136. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 137. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 138. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 139. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 140. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 141. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 142. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 143. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 144. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 145. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 146. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 147. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 148. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 149. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 150. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 151. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 152. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 153. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 154. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 155. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 156. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 157. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 158. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 159. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 160. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 161. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 162. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 163. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 164. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 165. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 166. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 167. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 168. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 169. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 170. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 171. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 172. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 173. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 174. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 175. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 176. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 177. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 178. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 179. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 180. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 181. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 182. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 183. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 184. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 185. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 186. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 187. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 188. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 189. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 190. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 191. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 192. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 193. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 194. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 195. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 196. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 197. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 198. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 199. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 200. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 201. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 202. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 203. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 204. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 205. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 206. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 207. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 208. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 209. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 210. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 211. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 212. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 213. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 214. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 215. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 216. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 217. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 218. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 219. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 220. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 221. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 222. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 223. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 224. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 225. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 226. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 227. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 228. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 229. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 230. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 231. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 232. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 233. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 234. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 235. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 236. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 237. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 238. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 239. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 240. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 241. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 242. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 243. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 244. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 245. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 246. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 247. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 248. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 249. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 250. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 251. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 252. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 253. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 254. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 255. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 256. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 257. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 258. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 259. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 260. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 261. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 262. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 263. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 264. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 265. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 266. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 267. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 268. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 269. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 270. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 271. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 272. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 273. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 274. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 275. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 276. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 277. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 278. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 279. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 280. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 281. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 282. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 283. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 284. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 285. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 286. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 287. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 288. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 289. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 290. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 291. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 292. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 293. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 294. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 295. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 296. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 297. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 298. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 299. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 300. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 301. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 302. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 303. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 304. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 305. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 306. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 307. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 308. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 309. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 310. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 311. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 312. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 313. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 314. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 315. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 316. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 317. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 318. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 319. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 320. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 321. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 322. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 323. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 324. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 325. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 326. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 327. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 328. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 329. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 330. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 331. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 332. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 333. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 334. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 335. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 336. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 337. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 338. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 339. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 340. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 341. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 342. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 343. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 344. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 345. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 346. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 347. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 348. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 349. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 350. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 351. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 352. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 353. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 354. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 355. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 356. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 357. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 358. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 359. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 360. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 361. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 362. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 363. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 364. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 365. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 366. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 367. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 368. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 369. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 370. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 371. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 372. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 373. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 374. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 375. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 376. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 377. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 378. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 379. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 380. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 381. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 382. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 383. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 384. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 385. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 386. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 387. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 388. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 389. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 390. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 391. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 392. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 393. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 394. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 395. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 396. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 397. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 398. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 399. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 400. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 401. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 402. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 403. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 404. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 405. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 406. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 407. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 408. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 409. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 410. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 411. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 412. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 413. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 414. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 415. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 416. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 417. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 418. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 419. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 420. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 421. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 422. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 423. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 424. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 425. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 426. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 427. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 428. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 429. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 430. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 431. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 432. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 433. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 434. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 435. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 436. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 437. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 438. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 439. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 440. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 441. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 442. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 443. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 444. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 445. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 446. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 447. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 448. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 449. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 450. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 451. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 452. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 453. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 454. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 455. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 456. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 457. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 458. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 459. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 460. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 461. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 462. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 463. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 464. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 465. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 466. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 467. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 468. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 469. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 470. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 471. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 472. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 473. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 474. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 475. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 476. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 477. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 478. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 479. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 480. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 481. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 482. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 483. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 484. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 485. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 486. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 487. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 488. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 489. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 490. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 491. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 492. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 493. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 494. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 495. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 496. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 497. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 498. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 499. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 500. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 501. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 502. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 503. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 504. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 505. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 506. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 507. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 508. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 509. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 510. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 511. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 512. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 513. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 514. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 515. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 516. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 517. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 518. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 519. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 520. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 521. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 522. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 523. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 524. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 525. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 526. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 527. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 528. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 529. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 530. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 531. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 532. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 533. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 534. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 535. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 536. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 537. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 538. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 539. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 540. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 541. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 542. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 543. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 544. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 545. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 546. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 547. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 548. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 549. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 550. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 551. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 552. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 553. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 554. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 555. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 556. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 557. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 558. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 559. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 560. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 561. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 562. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 563. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 564. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 565. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 566. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 567. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 568. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 569. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 570. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>id</th><th>diagnosis</th><th>radius_mean</th><th>texture_mean</th><th>perimeter_mean</th><th>area_mean</th><th>smoothness_mean</th></tr><tr><th></th><th>Int64</th><th>String</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>569 rows  33 columns (omitted printing of 26 columns)</p><tr><th>1</th><td>842302</td><td>M</td><td>17.99</td><td>10.38</td><td>122.8</td><td>1001.0</td><td>0.1184</td></tr><tr><th>2</th><td>842517</td><td>M</td><td>20.57</td><td>17.77</td><td>132.9</td><td>1326.0</td><td>0.08474</td></tr><tr><th>3</th><td>84300903</td><td>M</td><td>19.69</td><td>21.25</td><td>130.0</td><td>1203.0</td><td>0.1096</td></tr><tr><th>4</th><td>84348301</td><td>M</td><td>11.42</td><td>20.38</td><td>77.58</td><td>386.1</td><td>0.1425</td></tr><tr><th>5</th><td>84358402</td><td>M</td><td>20.29</td><td>14.34</td><td>135.1</td><td>1297.0</td><td>0.1003</td></tr><tr><th>6</th><td>843786</td><td>M</td><td>12.45</td><td>15.7</td><td>82.57</td><td>477.1</td><td>0.1278</td></tr><tr><th>7</th><td>844359</td><td>M</td><td>18.25</td><td>19.98</td><td>119.6</td><td>1040.0</td><td>0.09463</td></tr><tr><th>8</th><td>84458202</td><td>M</td><td>13.71</td><td>20.83</td><td>90.2</td><td>577.9</td><td>0.1189</td></tr><tr><th>9</th><td>844981</td><td>M</td><td>13.0</td><td>21.82</td><td>87.5</td><td>519.8</td><td>0.1273</td></tr><tr><th>10</th><td>84501001</td><td>M</td><td>12.46</td><td>24.04</td><td>83.97</td><td>475.9</td><td>0.1186</td></tr><tr><th>11</th><td>845636</td><td>M</td><td>16.02</td><td>23.24</td><td>102.7</td><td>797.8</td><td>0.08206</td></tr><tr><th>12</th><td>84610002</td><td>M</td><td>15.78</td><td>17.89</td><td>103.6</td><td>781.0</td><td>0.0971</td></tr><tr><th>13</th><td>846226</td><td>M</td><td>19.17</td><td>24.8</td><td>132.4</td><td>1123.0</td><td>0.0974</td></tr><tr><th>14</th><td>846381</td><td>M</td><td>15.85</td><td>23.95</td><td>103.7</td><td>782.7</td><td>0.08401</td></tr><tr><th>15</th><td>84667401</td><td>M</td><td>13.73</td><td>22.61</td><td>93.6</td><td>578.3</td><td>0.1131</td></tr><tr><th>16</th><td>84799002</td><td>M</td><td>14.54</td><td>27.54</td><td>96.73</td><td>658.8</td><td>0.1139</td></tr><tr><th>17</th><td>848406</td><td>M</td><td>14.68</td><td>20.13</td><td>94.74</td><td>684.5</td><td>0.09867</td></tr><tr><th>18</th><td>84862001</td><td>M</td><td>16.13</td><td>20.68</td><td>108.1</td><td>798.8</td><td>0.117</td></tr><tr><th>19</th><td>849014</td><td>M</td><td>19.81</td><td>22.15</td><td>130.0</td><td>1260.0</td><td>0.09831</td></tr><tr><th>20</th><td>8510426</td><td>B</td><td>13.54</td><td>14.36</td><td>87.46</td><td>566.3</td><td>0.09779</td></tr><tr><th>21</th><td>8510653</td><td>B</td><td>13.08</td><td>15.71</td><td>85.63</td><td>520.0</td><td>0.1075</td></tr><tr><th>22</th><td>8510824</td><td>B</td><td>9.504</td><td>12.44</td><td>60.34</td><td>273.9</td><td>0.1024</td></tr><tr><th>23</th><td>8511133</td><td>M</td><td>15.34</td><td>14.26</td><td>102.5</td><td>704.4</td><td>0.1073</td></tr><tr><th>24</th><td>851509</td><td>M</td><td>21.16</td><td>23.04</td><td>137.2</td><td>1404.0</td><td>0.09428</td></tr><tr><th>25</th><td>852552</td><td>M</td><td>16.65</td><td>21.38</td><td>110.0</td><td>904.6</td><td>0.1121</td></tr><tr><th>26</th><td>852631</td><td>M</td><td>17.14</td><td>16.4</td><td>116.0</td><td>912.7</td><td>0.1186</td></tr><tr><th>27</th><td>852763</td><td>M</td><td>14.58</td><td>21.53</td><td>97.41</td><td>644.8</td><td>0.1054</td></tr><tr><th>28</th><td>852781</td><td>M</td><td>18.61</td><td>20.25</td><td>122.1</td><td>1094.0</td><td>0.0944</td></tr><tr><th>29</th><td>852973</td><td>M</td><td>15.3</td><td>25.27</td><td>102.4</td><td>732.4</td><td>0.1082</td></tr><tr><th>30</th><td>853201</td><td>M</td><td>17.57</td><td>15.05</td><td>115.0</td><td>955.1</td><td>0.09847</td></tr><tr><th>31</th><td>853401</td><td>M</td><td>18.63</td><td>25.11</td><td>124.8</td><td>1088.0</td><td>0.1064</td></tr><tr><th>32</th><td>853612</td><td>M</td><td>11.84</td><td>18.7</td><td>77.93</td><td>440.6</td><td>0.1109</td></tr><tr><th>33</th><td>85382601</td><td>M</td><td>17.02</td><td>23.98</td><td>112.8</td><td>899.3</td><td>0.1197</td></tr><tr><th>34</th><td>854002</td><td>M</td><td>19.27</td><td>26.47</td><td>127.9</td><td>1162.0</td><td>0.09401</td></tr><tr><th>35</th><td>854039</td><td>M</td><td>16.13</td><td>17.88</td><td>107.0</td><td>807.2</td><td>0.104</td></tr><tr><th>36</th><td>854253</td><td>M</td><td>16.74</td><td>21.59</td><td>110.1</td><td>869.5</td><td>0.0961</td></tr><tr><th>37</th><td>854268</td><td>M</td><td>14.25</td><td>21.72</td><td>93.63</td><td>633.0</td><td>0.09823</td></tr><tr><th>38</th><td>854941</td><td>B</td><td>13.03</td><td>18.42</td><td>82.61</td><td>523.8</td><td>0.08983</td></tr><tr><th>39</th><td>855133</td><td>M</td><td>14.99</td><td>25.2</td><td>95.54</td><td>698.8</td><td>0.09387</td></tr><tr><th>40</th><td>855138</td><td>M</td><td>13.48</td><td>20.82</td><td>88.4</td><td>559.2</td><td>0.1016</td></tr><tr><th>41</th><td>855167</td><td>M</td><td>13.44</td><td>21.58</td><td>86.18</td><td>563.0</td><td>0.08162</td></tr><tr><th>42</th><td>855563</td><td>M</td><td>10.95</td><td>21.35</td><td>71.9</td><td>371.1</td><td>0.1227</td></tr><tr><th>43</th><td>855625</td><td>M</td><td>19.07</td><td>24.81</td><td>128.3</td><td>1104.0</td><td>0.09081</td></tr><tr><th>44</th><td>856106</td><td>M</td><td>13.28</td><td>20.28</td><td>87.32</td><td>545.2</td><td>0.1041</td></tr><tr><th>45</th><td>85638502</td><td>M</td><td>13.17</td><td>21.81</td><td>85.42</td><td>531.5</td><td>0.09714</td></tr><tr><th>46</th><td>857010</td><td>M</td><td>18.65</td><td>17.6</td><td>123.7</td><td>1076.0</td><td>0.1099</td></tr><tr><th>47</th><td>85713702</td><td>B</td><td>8.196</td><td>16.84</td><td>51.71</td><td>201.9</td><td>0.086</td></tr><tr><th>48</th><td>85715</td><td>M</td><td>13.17</td><td>18.66</td><td>85.98</td><td>534.6</td><td>0.1158</td></tr><tr><th>49</th><td>857155</td><td>B</td><td>12.05</td><td>14.63</td><td>78.04</td><td>449.3</td><td>0.1031</td></tr><tr><th>50</th><td>857156</td><td>B</td><td>13.49</td><td>22.3</td><td>86.91</td><td>561.0</td><td>0.08752</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& id & diagnosis & radius\\_mean & texture\\_mean & perimeter\\_mean & area\\_mean & smoothness\\_mean & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & String & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 842302 & M & 17.99 & 10.38 & 122.8 & 1001.0 & 0.1184 & $\\dots$ \\\\\n",
       "\t2 & 842517 & M & 20.57 & 17.77 & 132.9 & 1326.0 & 0.08474 & $\\dots$ \\\\\n",
       "\t3 & 84300903 & M & 19.69 & 21.25 & 130.0 & 1203.0 & 0.1096 & $\\dots$ \\\\\n",
       "\t4 & 84348301 & M & 11.42 & 20.38 & 77.58 & 386.1 & 0.1425 & $\\dots$ \\\\\n",
       "\t5 & 84358402 & M & 20.29 & 14.34 & 135.1 & 1297.0 & 0.1003 & $\\dots$ \\\\\n",
       "\t6 & 843786 & M & 12.45 & 15.7 & 82.57 & 477.1 & 0.1278 & $\\dots$ \\\\\n",
       "\t7 & 844359 & M & 18.25 & 19.98 & 119.6 & 1040.0 & 0.09463 & $\\dots$ \\\\\n",
       "\t8 & 84458202 & M & 13.71 & 20.83 & 90.2 & 577.9 & 0.1189 & $\\dots$ \\\\\n",
       "\t9 & 844981 & M & 13.0 & 21.82 & 87.5 & 519.8 & 0.1273 & $\\dots$ \\\\\n",
       "\t10 & 84501001 & M & 12.46 & 24.04 & 83.97 & 475.9 & 0.1186 & $\\dots$ \\\\\n",
       "\t11 & 845636 & M & 16.02 & 23.24 & 102.7 & 797.8 & 0.08206 & $\\dots$ \\\\\n",
       "\t12 & 84610002 & M & 15.78 & 17.89 & 103.6 & 781.0 & 0.0971 & $\\dots$ \\\\\n",
       "\t13 & 846226 & M & 19.17 & 24.8 & 132.4 & 1123.0 & 0.0974 & $\\dots$ \\\\\n",
       "\t14 & 846381 & M & 15.85 & 23.95 & 103.7 & 782.7 & 0.08401 & $\\dots$ \\\\\n",
       "\t15 & 84667401 & M & 13.73 & 22.61 & 93.6 & 578.3 & 0.1131 & $\\dots$ \\\\\n",
       "\t16 & 84799002 & M & 14.54 & 27.54 & 96.73 & 658.8 & 0.1139 & $\\dots$ \\\\\n",
       "\t17 & 848406 & M & 14.68 & 20.13 & 94.74 & 684.5 & 0.09867 & $\\dots$ \\\\\n",
       "\t18 & 84862001 & M & 16.13 & 20.68 & 108.1 & 798.8 & 0.117 & $\\dots$ \\\\\n",
       "\t19 & 849014 & M & 19.81 & 22.15 & 130.0 & 1260.0 & 0.09831 & $\\dots$ \\\\\n",
       "\t20 & 8510426 & B & 13.54 & 14.36 & 87.46 & 566.3 & 0.09779 & $\\dots$ \\\\\n",
       "\t21 & 8510653 & B & 13.08 & 15.71 & 85.63 & 520.0 & 0.1075 & $\\dots$ \\\\\n",
       "\t22 & 8510824 & B & 9.504 & 12.44 & 60.34 & 273.9 & 0.1024 & $\\dots$ \\\\\n",
       "\t23 & 8511133 & M & 15.34 & 14.26 & 102.5 & 704.4 & 0.1073 & $\\dots$ \\\\\n",
       "\t24 & 851509 & M & 21.16 & 23.04 & 137.2 & 1404.0 & 0.09428 & $\\dots$ \\\\\n",
       "\t25 & 852552 & M & 16.65 & 21.38 & 110.0 & 904.6 & 0.1121 & $\\dots$ \\\\\n",
       "\t26 & 852631 & M & 17.14 & 16.4 & 116.0 & 912.7 & 0.1186 & $\\dots$ \\\\\n",
       "\t27 & 852763 & M & 14.58 & 21.53 & 97.41 & 644.8 & 0.1054 & $\\dots$ \\\\\n",
       "\t28 & 852781 & M & 18.61 & 20.25 & 122.1 & 1094.0 & 0.0944 & $\\dots$ \\\\\n",
       "\t29 & 852973 & M & 15.3 & 25.27 & 102.4 & 732.4 & 0.1082 & $\\dots$ \\\\\n",
       "\t30 & 853201 & M & 17.57 & 15.05 & 115.0 & 955.1 & 0.09847 & $\\dots$ \\\\\n",
       "\t31 & 853401 & M & 18.63 & 25.11 & 124.8 & 1088.0 & 0.1064 & $\\dots$ \\\\\n",
       "\t32 & 853612 & M & 11.84 & 18.7 & 77.93 & 440.6 & 0.1109 & $\\dots$ \\\\\n",
       "\t33 & 85382601 & M & 17.02 & 23.98 & 112.8 & 899.3 & 0.1197 & $\\dots$ \\\\\n",
       "\t34 & 854002 & M & 19.27 & 26.47 & 127.9 & 1162.0 & 0.09401 & $\\dots$ \\\\\n",
       "\t35 & 854039 & M & 16.13 & 17.88 & 107.0 & 807.2 & 0.104 & $\\dots$ \\\\\n",
       "\t36 & 854253 & M & 16.74 & 21.59 & 110.1 & 869.5 & 0.0961 & $\\dots$ \\\\\n",
       "\t37 & 854268 & M & 14.25 & 21.72 & 93.63 & 633.0 & 0.09823 & $\\dots$ \\\\\n",
       "\t38 & 854941 & B & 13.03 & 18.42 & 82.61 & 523.8 & 0.08983 & $\\dots$ \\\\\n",
       "\t39 & 855133 & M & 14.99 & 25.2 & 95.54 & 698.8 & 0.09387 & $\\dots$ \\\\\n",
       "\t40 & 855138 & M & 13.48 & 20.82 & 88.4 & 559.2 & 0.1016 & $\\dots$ \\\\\n",
       "\t41 & 855167 & M & 13.44 & 21.58 & 86.18 & 563.0 & 0.08162 & $\\dots$ \\\\\n",
       "\t42 & 855563 & M & 10.95 & 21.35 & 71.9 & 371.1 & 0.1227 & $\\dots$ \\\\\n",
       "\t43 & 855625 & M & 19.07 & 24.81 & 128.3 & 1104.0 & 0.09081 & $\\dots$ \\\\\n",
       "\t44 & 856106 & M & 13.28 & 20.28 & 87.32 & 545.2 & 0.1041 & $\\dots$ \\\\\n",
       "\t45 & 85638502 & M & 13.17 & 21.81 & 85.42 & 531.5 & 0.09714 & $\\dots$ \\\\\n",
       "\t46 & 857010 & M & 18.65 & 17.6 & 123.7 & 1076.0 & 0.1099 & $\\dots$ \\\\\n",
       "\t47 & 85713702 & B & 8.196 & 16.84 & 51.71 & 201.9 & 0.086 & $\\dots$ \\\\\n",
       "\t48 & 85715 & M & 13.17 & 18.66 & 85.98 & 534.6 & 0.1158 & $\\dots$ \\\\\n",
       "\t49 & 857155 & B & 12.05 & 14.63 & 78.04 & 449.3 & 0.1031 & $\\dots$ \\\\\n",
       "\t50 & 857156 & B & 13.49 & 22.3 & 86.91 & 561.0 & 0.08752 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "56933 DataFrame. Omitted printing of 28 columns\n",
       " Row  id        diagnosis  radius_mean  texture_mean  perimeter_mean \n",
       "      \u001b[90mInt64\u001b[39m     \u001b[90mString\u001b[39m     \u001b[90mFloat64\u001b[39m      \u001b[90mFloat64\u001b[39m       \u001b[90mFloat64\u001b[39m        \n",
       "\n",
       " 1    842302    M          17.99        10.38         122.8          \n",
       " 2    842517    M          20.57        17.77         132.9          \n",
       " 3    84300903  M          19.69        21.25         130.0          \n",
       " 4    84348301  M          11.42        20.38         77.58          \n",
       " 5    84358402  M          20.29        14.34         135.1          \n",
       " 6    843786    M          12.45        15.7          82.57          \n",
       " 7    844359    M          18.25        19.98         119.6          \n",
       " 8    84458202  M          13.71        20.83         90.2           \n",
       " 9    844981    M          13.0         21.82         87.5           \n",
       " 10   84501001  M          12.46        24.04         83.97          \n",
       " 11   845636    M          16.02        23.24         102.7          \n",
       " 12   84610002  M          15.78        17.89         103.6          \n",
       " 13   846226    M          19.17        24.8          132.4          \n",
       " 14   846381    M          15.85        23.95         103.7          \n",
       " 15   84667401  M          13.73        22.61         93.6           \n",
       " 16   84799002  M          14.54        27.54         96.73          \n",
       " 17   848406    M          14.68        20.13         94.74          \n",
       " 18   84862001  M          16.13        20.68         108.1          \n",
       " 19   849014    M          19.81        22.15         130.0          \n",
       " 20   8510426   B          13.54        14.36         87.46          \n",
       "\n",
       " 549  923169    B          9.683        19.34         61.05          \n",
       " 550  923465    B          10.82        24.21         68.89          \n",
       " 551  923748    B          10.86        21.48         68.51          \n",
       " 552  923780    B          11.13        22.44         71.49          \n",
       " 553  924084    B          12.77        29.43         81.35          \n",
       " 554  924342    B          9.333        21.94         59.01          \n",
       " 555  924632    B          12.88        28.92         82.5           \n",
       " 556  924934    B          10.29        27.61         65.67          \n",
       " 557  924964    B          10.16        19.59         64.73          \n",
       " 558  925236    B          9.423        27.88         59.26          \n",
       " 559  925277    B          14.59        22.68         96.39          \n",
       " 560  925291    B          11.51        23.93         74.52          \n",
       " 561  925292    B          14.05        27.15         91.38          \n",
       " 562  925311    B          11.2         29.37         70.67          \n",
       " 563  925622    M          15.22        30.62         103.4          \n",
       " 564  926125    M          20.92        25.09         143.0          \n",
       " 565  926424    M          21.56        22.39         142.0          \n",
       " 566  926682    M          20.13        28.25         131.2          \n",
       " 567  926954    M          16.6         28.08         108.3          \n",
       " 568  927241    M          20.6         29.33         140.1          \n",
       " 569  92751     B          7.76         24.54         47.92          "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CSV.read(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nunique</th><th>nmissing</th></tr><tr><th></th><th>Symbol</th><th>Union</th><th>Any</th><th>Union</th><th>Any</th><th>Union</th><th>Nothing</th></tr></thead><tbody><p>31 rows  8 columns (omitted printing of 1 columns)</p><tr><th>1</th><td>diagnosis</td><td></td><td>B</td><td></td><td>M</td><td>2</td><td></td></tr><tr><th>2</th><td>radius_mean</td><td>14.1273</td><td>6.981</td><td>13.37</td><td>28.11</td><td></td><td></td></tr><tr><th>3</th><td>texture_mean</td><td>19.2896</td><td>9.71</td><td>18.84</td><td>39.28</td><td></td><td></td></tr><tr><th>4</th><td>perimeter_mean</td><td>91.969</td><td>43.79</td><td>86.24</td><td>188.5</td><td></td><td></td></tr><tr><th>5</th><td>area_mean</td><td>654.889</td><td>143.5</td><td>551.1</td><td>2501.0</td><td></td><td></td></tr><tr><th>6</th><td>smoothness_mean</td><td>0.0963603</td><td>0.05263</td><td>0.09587</td><td>0.1634</td><td></td><td></td></tr><tr><th>7</th><td>compactness_mean</td><td>0.104341</td><td>0.01938</td><td>0.09263</td><td>0.3454</td><td></td><td></td></tr><tr><th>8</th><td>concavity_mean</td><td>0.0887993</td><td>0.0</td><td>0.06154</td><td>0.4268</td><td></td><td></td></tr><tr><th>9</th><td>concave points_mean</td><td>0.0489191</td><td>0.0</td><td>0.0335</td><td>0.2012</td><td></td><td></td></tr><tr><th>10</th><td>symmetry_mean</td><td>0.181162</td><td>0.106</td><td>0.1792</td><td>0.304</td><td></td><td></td></tr><tr><th>11</th><td>fractal_dimension_mean</td><td>0.0627976</td><td>0.04996</td><td>0.06154</td><td>0.09744</td><td></td><td></td></tr><tr><th>12</th><td>radius_se</td><td>0.405172</td><td>0.1115</td><td>0.3242</td><td>2.873</td><td></td><td></td></tr><tr><th>13</th><td>texture_se</td><td>1.21685</td><td>0.3602</td><td>1.108</td><td>4.885</td><td></td><td></td></tr><tr><th>14</th><td>perimeter_se</td><td>2.86606</td><td>0.757</td><td>2.287</td><td>21.98</td><td></td><td></td></tr><tr><th>15</th><td>area_se</td><td>40.3371</td><td>6.802</td><td>24.53</td><td>542.2</td><td></td><td></td></tr><tr><th>16</th><td>smoothness_se</td><td>0.00704098</td><td>0.001713</td><td>0.00638</td><td>0.03113</td><td></td><td></td></tr><tr><th>17</th><td>compactness_se</td><td>0.0254781</td><td>0.002252</td><td>0.02045</td><td>0.1354</td><td></td><td></td></tr><tr><th>18</th><td>concavity_se</td><td>0.0318937</td><td>0.0</td><td>0.02589</td><td>0.396</td><td></td><td></td></tr><tr><th>19</th><td>concave points_se</td><td>0.0117961</td><td>0.0</td><td>0.01093</td><td>0.05279</td><td></td><td></td></tr><tr><th>20</th><td>symmetry_se</td><td>0.0205423</td><td>0.007882</td><td>0.01873</td><td>0.07895</td><td></td><td></td></tr><tr><th>21</th><td>fractal_dimension_se</td><td>0.0037949</td><td>0.0008948</td><td>0.003187</td><td>0.02984</td><td></td><td></td></tr><tr><th>22</th><td>radius_worst</td><td>16.2692</td><td>7.93</td><td>14.97</td><td>36.04</td><td></td><td></td></tr><tr><th>23</th><td>texture_worst</td><td>25.6772</td><td>12.02</td><td>25.41</td><td>49.54</td><td></td><td></td></tr><tr><th>24</th><td>perimeter_worst</td><td>107.261</td><td>50.41</td><td>97.66</td><td>251.2</td><td></td><td></td></tr><tr><th>25</th><td>area_worst</td><td>880.583</td><td>185.2</td><td>686.5</td><td>4254.0</td><td></td><td></td></tr><tr><th>26</th><td>smoothness_worst</td><td>0.132369</td><td>0.07117</td><td>0.1313</td><td>0.2226</td><td></td><td></td></tr><tr><th>27</th><td>compactness_worst</td><td>0.254265</td><td>0.02729</td><td>0.2119</td><td>1.058</td><td></td><td></td></tr><tr><th>28</th><td>concavity_worst</td><td>0.272188</td><td>0.0</td><td>0.2267</td><td>1.252</td><td></td><td></td></tr><tr><th>29</th><td>concave points_worst</td><td>0.114606</td><td>0.0</td><td>0.09993</td><td>0.291</td><td></td><td></td></tr><tr><th>30</th><td>symmetry_worst</td><td>0.290076</td><td>0.1565</td><td>0.2822</td><td>0.6638</td><td></td><td></td></tr><tr><th>31</th><td>fractal_dimension_worst</td><td>0.0839458</td><td>0.05504</td><td>0.08004</td><td>0.2075</td><td></td><td></td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& variable & mean & min & median & max & nunique & nmissing & \\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union & Any & Union & Any & Union & Nothing & \\\\\n",
       "\t\\hline\n",
       "\t1 & diagnosis &  & B &  & M & 2 &  & $\\dots$ \\\\\n",
       "\t2 & radius\\_mean & 14.1273 & 6.981 & 13.37 & 28.11 &  &  & $\\dots$ \\\\\n",
       "\t3 & texture\\_mean & 19.2896 & 9.71 & 18.84 & 39.28 &  &  & $\\dots$ \\\\\n",
       "\t4 & perimeter\\_mean & 91.969 & 43.79 & 86.24 & 188.5 &  &  & $\\dots$ \\\\\n",
       "\t5 & area\\_mean & 654.889 & 143.5 & 551.1 & 2501.0 &  &  & $\\dots$ \\\\\n",
       "\t6 & smoothness\\_mean & 0.0963603 & 0.05263 & 0.09587 & 0.1634 &  &  & $\\dots$ \\\\\n",
       "\t7 & compactness\\_mean & 0.104341 & 0.01938 & 0.09263 & 0.3454 &  &  & $\\dots$ \\\\\n",
       "\t8 & concavity\\_mean & 0.0887993 & 0.0 & 0.06154 & 0.4268 &  &  & $\\dots$ \\\\\n",
       "\t9 & concave points\\_mean & 0.0489191 & 0.0 & 0.0335 & 0.2012 &  &  & $\\dots$ \\\\\n",
       "\t10 & symmetry\\_mean & 0.181162 & 0.106 & 0.1792 & 0.304 &  &  & $\\dots$ \\\\\n",
       "\t11 & fractal\\_dimension\\_mean & 0.0627976 & 0.04996 & 0.06154 & 0.09744 &  &  & $\\dots$ \\\\\n",
       "\t12 & radius\\_se & 0.405172 & 0.1115 & 0.3242 & 2.873 &  &  & $\\dots$ \\\\\n",
       "\t13 & texture\\_se & 1.21685 & 0.3602 & 1.108 & 4.885 &  &  & $\\dots$ \\\\\n",
       "\t14 & perimeter\\_se & 2.86606 & 0.757 & 2.287 & 21.98 &  &  & $\\dots$ \\\\\n",
       "\t15 & area\\_se & 40.3371 & 6.802 & 24.53 & 542.2 &  &  & $\\dots$ \\\\\n",
       "\t16 & smoothness\\_se & 0.00704098 & 0.001713 & 0.00638 & 0.03113 &  &  & $\\dots$ \\\\\n",
       "\t17 & compactness\\_se & 0.0254781 & 0.002252 & 0.02045 & 0.1354 &  &  & $\\dots$ \\\\\n",
       "\t18 & concavity\\_se & 0.0318937 & 0.0 & 0.02589 & 0.396 &  &  & $\\dots$ \\\\\n",
       "\t19 & concave points\\_se & 0.0117961 & 0.0 & 0.01093 & 0.05279 &  &  & $\\dots$ \\\\\n",
       "\t20 & symmetry\\_se & 0.0205423 & 0.007882 & 0.01873 & 0.07895 &  &  & $\\dots$ \\\\\n",
       "\t21 & fractal\\_dimension\\_se & 0.0037949 & 0.0008948 & 0.003187 & 0.02984 &  &  & $\\dots$ \\\\\n",
       "\t22 & radius\\_worst & 16.2692 & 7.93 & 14.97 & 36.04 &  &  & $\\dots$ \\\\\n",
       "\t23 & texture\\_worst & 25.6772 & 12.02 & 25.41 & 49.54 &  &  & $\\dots$ \\\\\n",
       "\t24 & perimeter\\_worst & 107.261 & 50.41 & 97.66 & 251.2 &  &  & $\\dots$ \\\\\n",
       "\t25 & area\\_worst & 880.583 & 185.2 & 686.5 & 4254.0 &  &  & $\\dots$ \\\\\n",
       "\t26 & smoothness\\_worst & 0.132369 & 0.07117 & 0.1313 & 0.2226 &  &  & $\\dots$ \\\\\n",
       "\t27 & compactness\\_worst & 0.254265 & 0.02729 & 0.2119 & 1.058 &  &  & $\\dots$ \\\\\n",
       "\t28 & concavity\\_worst & 0.272188 & 0.0 & 0.2267 & 1.252 &  &  & $\\dots$ \\\\\n",
       "\t29 & concave points\\_worst & 0.114606 & 0.0 & 0.09993 & 0.291 &  &  & $\\dots$ \\\\\n",
       "\t30 & symmetry\\_worst & 0.290076 & 0.1565 & 0.2822 & 0.6638 &  &  & $\\dots$ \\\\\n",
       "\t31 & fractal\\_dimension\\_worst & 0.0839458 & 0.05504 & 0.08004 & 0.2075 &  &  & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "318 DataFrame. Omitted printing of 3 columns\n",
       " Row  variable                 mean        min        median    max     \n",
       "      \u001b[90mSymbol\u001b[39m                   \u001b[90mUnion\u001b[39m      \u001b[90mAny\u001b[39m        \u001b[90mUnion\u001b[39m    \u001b[90mAny\u001b[39m     \n",
       "\n",
       " 1    diagnosis                            B                    M       \n",
       " 2    radius_mean              14.1273     6.981      13.37     28.11   \n",
       " 3    texture_mean             19.2896     9.71       18.84     39.28   \n",
       " 4    perimeter_mean           91.969      43.79      86.24     188.5   \n",
       " 5    area_mean                654.889     143.5      551.1     2501.0  \n",
       " 6    smoothness_mean          0.0963603   0.05263    0.09587   0.1634  \n",
       " 7    compactness_mean         0.104341    0.01938    0.09263   0.3454  \n",
       " 8    concavity_mean           0.0887993   0.0        0.06154   0.4268  \n",
       " 9    concave points_mean      0.0489191   0.0        0.0335    0.2012  \n",
       " 10   symmetry_mean            0.181162    0.106      0.1792    0.304   \n",
       " 11   fractal_dimension_mean   0.0627976   0.04996    0.06154   0.09744 \n",
       " 12   radius_se                0.405172    0.1115     0.3242    2.873   \n",
       " 13   texture_se               1.21685     0.3602     1.108     4.885   \n",
       " 14   perimeter_se             2.86606     0.757      2.287     21.98   \n",
       " 15   area_se                  40.3371     6.802      24.53     542.2   \n",
       " 16   smoothness_se            0.00704098  0.001713   0.00638   0.03113 \n",
       " 17   compactness_se           0.0254781   0.002252   0.02045   0.1354  \n",
       " 18   concavity_se             0.0318937   0.0        0.02589   0.396   \n",
       " 19   concave points_se        0.0117961   0.0        0.01093   0.05279 \n",
       " 20   symmetry_se              0.0205423   0.007882   0.01873   0.07895 \n",
       " 21   fractal_dimension_se     0.0037949   0.0008948  0.003187  0.02984 \n",
       " 22   radius_worst             16.2692     7.93       14.97     36.04   \n",
       " 23   texture_worst            25.6772     12.02      25.41     49.54   \n",
       " 24   perimeter_worst          107.261     50.41      97.66     251.2   \n",
       " 25   area_worst               880.583     185.2      686.5     4254.0  \n",
       " 26   smoothness_worst         0.132369    0.07117    0.1313    0.2226  \n",
       " 27   compactness_worst        0.254265    0.02729    0.2119    1.058   \n",
       " 28   concavity_worst          0.272188    0.0        0.2267    1.252   \n",
       " 29   concave points_worst     0.114606    0.0        0.09993   0.291   \n",
       " 30   symmetry_worst           0.290076    0.1565     0.2822    0.6638  \n",
       " 31   fractal_dimension_worst  0.0839458   0.05504    0.08004   0.2075  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[:, Not([33, 1])]\n",
    "describe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at class labels to see if dataset is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Int64} with 2 entries:\n",
       "  \"B\" => 357\n",
       "  \"M\" => 212"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = countmap(data[:diagnosis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.6274165202108963\n",
       " 0.37258347978910367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(label_counts[i] / size(data)[1] for i in keys(label_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[22m _.names                 \u001b[0m\u001b[0m\u001b[22m _.types                         \u001b[0m\u001b[0m\u001b[22m _.scitypes    \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m diagnosis               \u001b[0m\u001b[0m CategoricalValue{String,UInt32} \u001b[0m\u001b[0m Multiclass{2} \u001b[0m\u001b[0m\n",
       "\u001b[0m radius_mean             \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m texture_mean            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m perimeter_mean          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m area_mean               \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m smoothness_mean         \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m compactness_mean        \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concavity_mean          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concave points_mean     \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m symmetry_mean           \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m fractal_dimension_mean  \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m radius_se               \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m texture_se              \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m perimeter_se            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m area_se                 \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m smoothness_se           \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m compactness_se          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concavity_se            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concave points_se       \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m symmetry_se             \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m fractal_dimension_se    \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m radius_worst            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m texture_worst           \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m perimeter_worst         \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m area_worst              \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m smoothness_worst        \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m compactness_worst       \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concavity_worst         \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concave points_worst    \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m symmetry_worst          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m fractal_dimension_worst \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "_.nrows = 569\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coerce!(data, :diagnosis=>Multiclass)\n",
    "schema(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CategoricalValue{String,UInt32}[\"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\"    \"B\", \"B\", \"B\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"B\"], 56930 DataFrame. Omitted printing of 26 columns\n",
       " Row  radius_mean  texture_mean  perimeter_mean  area_mean \n",
       "      \u001b[90mFloat64\u001b[39m      \u001b[90mFloat64\u001b[39m       \u001b[90mFloat64\u001b[39m         \u001b[90mFloat64\u001b[39m   \n",
       "\n",
       " 1    17.99        10.38         122.8           1001.0    \n",
       " 2    20.57        17.77         132.9           1326.0    \n",
       " 3    19.69        21.25         130.0           1203.0    \n",
       " 4    11.42        20.38         77.58           386.1     \n",
       " 5    20.29        14.34         135.1           1297.0    \n",
       " 6    12.45        15.7          82.57           477.1     \n",
       " 7    18.25        19.98         119.6           1040.0    \n",
       " 8    13.71        20.83         90.2            577.9     \n",
       " 9    13.0         21.82         87.5            519.8     \n",
       " 10   12.46        24.04         83.97           475.9     \n",
       " 11   16.02        23.24         102.7           797.8     \n",
       " 12   15.78        17.89         103.6           781.0     \n",
       " 13   19.17        24.8          132.4           1123.0    \n",
       " 14   15.85        23.95         103.7           782.7     \n",
       " 15   13.73        22.61         93.6            578.3     \n",
       " 16   14.54        27.54         96.73           658.8     \n",
       " 17   14.68        20.13         94.74           684.5     \n",
       " 18   16.13        20.68         108.1           798.8     \n",
       " 19   19.81        22.15         130.0           1260.0    \n",
       " 20   13.54        14.36         87.46           566.3     \n",
       "\n",
       " 549  9.683        19.34         61.05           285.7     \n",
       " 550  10.82        24.21         68.89           361.6     \n",
       " 551  10.86        21.48         68.51           360.5     \n",
       " 552  11.13        22.44         71.49           378.4     \n",
       " 553  12.77        29.43         81.35           507.9     \n",
       " 554  9.333        21.94         59.01           264.0     \n",
       " 555  12.88        28.92         82.5            514.3     \n",
       " 556  10.29        27.61         65.67           321.4     \n",
       " 557  10.16        19.59         64.73           311.7     \n",
       " 558  9.423        27.88         59.26           271.3     \n",
       " 559  14.59        22.68         96.39           657.1     \n",
       " 560  11.51        23.93         74.52           403.5     \n",
       " 561  14.05        27.15         91.38           600.4     \n",
       " 562  11.2         29.37         70.67           386.0     \n",
       " 563  15.22        30.62         103.4           716.9     \n",
       " 564  20.92        25.09         143.0           1347.0    \n",
       " 565  21.56        22.39         142.0           1479.0    \n",
       " 566  20.13        28.25         131.2           1261.0    \n",
       " 567  16.6         28.08         108.3           858.1     \n",
       " 568  20.6         29.33         140.1           1265.0    \n",
       " 569  7.76         24.54         47.92           181.0     )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, X = unpack(data, ==(:diagnosis), colname->true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition train and test data accoring to class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([483, 534, 159, 31, 170, 416, 231, 43, 161, 286    134, 500, 395, 533, 112, 396, 297, 106, 303, 261], [392, 390, 320, 27, 328, 477, 19, 356, 518, 444    136, 559, 505, 274, 508, 358, 90, 296, 79, 415])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data to use when trying to fit a single validation set\n",
    "train, test = partition(eachindex(y), 0.7, shuffle=true, rng=123, stratify=values(data[:diagnosis])) # gives 70:30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.628140703517588\n",
       " 0.37185929648241206"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts = countmap(data[train,:diagnosis])\n",
    "collect(train_counts[i] / size(train)[1] for i in keys(train_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.6257309941520468\n",
       " 0.3742690058479532"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_counts = countmap(data[test,:diagnosis])\n",
    "collect(test_counts[i] / size(test)[1] for i in keys(test_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five Learning Algorithms\n",
    "\n",
    "* Decision trees with some form of pruning\n",
    "* Neural networks\n",
    "* Boosting\n",
    "* Support Vector Machines\n",
    "* k-nearest neighbors\n",
    "\n",
    "\n",
    "##### Testing\n",
    "* Implement the algorithms\n",
    "* Design two *interesting* classification problems. For the purposes of this assignment, a classification problem is just a set of training examples and a set of test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43-element Array{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype),T} where T<:Tuple,1}:\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianLDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianQDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = ScikitLearn, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " (name = ExtraTreesClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n",
       " (name = GaussianNBClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GaussianProcessClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GradientBoostingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = KNNClassifier, package_name = NearestNeighbors, ... )\n",
       " (name = KNeighborsClassifier, package_name = ScikitLearn, ... )\n",
       " (name = LDA, package_name = MultivariateStats, ... )\n",
       " (name = LGBMClassifier, package_name = LightGBM, ... )\n",
       " (name = LinearBinaryClassifier, package_name = GLM, ... )\n",
       " (name = LinearSVC, package_name = LIBSVM, ... )\n",
       " (name = LogisticCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = LogisticClassifier, package_name = ScikitLearn, ... )\n",
       " (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n",
       " (name = NuSVC, package_name = LIBSVM, ... )\n",
       " (name = PassiveAggressiveClassifier, package_name = ScikitLearn, ... )\n",
       " (name = PerceptronClassifier, package_name = ScikitLearn, ... )\n",
       " (name = ProbabilisticSGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVC, package_name = LIBSVM, ... )\n",
       " (name = SVMClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(matching(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJModels \n",
      "import DecisionTree \n",
      "import MLJModels.DecisionTree_ \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Loading into module \"Main\": \n",
      " @ MLJModels /home/andrew/.julia/packages/MLJModels/mUBFt/src/loading.jl:70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = false,\n",
       "    merge_purity_threshold = 1.0,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 5)\u001b[34m @021\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load DecisionTreeClassifier verbosity=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees\n",
    "* Be sure to use some form of pruning. \n",
    "* You are not required to use information gain (for example, there is something called the GINI index that is sometimes used) to split attributes, but you should describe whatever it is that you do use.\n",
    "\n",
    "1. https://alan-turing-institute.github.io/MLJ.jl/dev/transformers/#MLJModels.UnivariateDiscretizer\n",
    "1. https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/#Getting-Started-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No post-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = false,\n",
       "    merge_purity_threshold = 1.0,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 8)\u001b[34m @045\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(post_prune=false, display_depth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @553\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @557\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @912\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tree = machine(dt, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{DecisionTreeClassifier} @553\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 23, Threshold 105.95\n",
      "L-> Feature 27, Threshold 0.3967\n",
      "    L-> Feature 14, Threshold 48.975\n",
      "        L-> 1 : 215/215\n",
      "        R-> Feature 5, Threshold 0.09072\n",
      "            L-> 1 : 2/2\n",
      "            R-> 2 : 2/2\n",
      "    R-> Feature 22, Threshold 24.89\n",
      "        L-> 1 : 8/8\n",
      "        R-> Feature 21, Threshold 12.6\n",
      "            L-> 1 : 2/2\n",
      "            R-> 2 : 9/9\n",
      "R-> Feature 23, Threshold 117.45\n",
      "    L-> Feature 25, Threshold 0.1361\n",
      "        L-> Feature 22, Threshold 26.58\n",
      "            L-> 1 : 17/17\n",
      "            R-> Feature 24, Threshold 871.8\n",
      "                L-> Feature 12, Threshold 1.8025000000000002\n",
      "                    L-> 1 : 4/4\n",
      "                    R-> 2 : 1/1\n",
      "                R-> 2 : 5/5\n",
      "        R-> Feature 2, Threshold 13.42\n",
      "            L-> 1 : 1/1\n",
      "            R-> 2 : 19/19\n",
      "    R-> Feature 16, Threshold 0.0087275\n",
      "        L-> 1 : 1/1\n",
      "        R-> 2 : 112/112\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @553\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @557\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @912\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(Tree, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:04\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[22m _.measure     \u001b[0m\u001b[0m\u001b[22m _.measurement \u001b[0m\u001b[0m\u001b[22m _.per_fold                                 \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m cross_entropy \u001b[0m\u001b[0m 2.09          \u001b[0m\u001b[0m [2.28, 1.52, 3.04, 2.28, 1.52, 1.92]       \u001b[0m\u001b[0m\n",
       "\u001b[0m acc           \u001b[0m\u001b[0m 0.942         \u001b[0m\u001b[0m [0.937, 0.958, 0.916, 0.937, 0.958, 0.947] \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "_.per_observation = [[[2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [36.0, 2.22e-16, ..., 2.22e-16]], missing]\n",
       "_.fitted_params_per_fold = [  ]\n",
       "_.report_per_fold = [  ]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_acc = evaluate!(Tree, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tree = Decision Tree\n",
       "Leaves: 15\n",
       "Depth:  5,\n",
       " encoding = Dict{CategoricalValue{String,UInt32},UInt32}(\"B\" => 0x00000001,\"M\" => 0x00000002),)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_params(Tree) \n",
    "# print_tree(Tree.fitresult[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(classes_seen = CategoricalValue{String,UInt32}[\"B\", \"M\"],\n",
       " print_tree = TreePrinter object (call with display depth),)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Post-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = true,\n",
       "    merge_purity_threshold = 1.0,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 5)\u001b[34m @835\u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt2 = DecisionTreeClassifier(post_prune=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @350\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @187\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @730\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tree2 = machine(dt2, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 23, Threshold 105.95\n",
      "L-> Feature 27, Threshold 0.3967\n",
      "    L-> Feature 14, Threshold 48.975\n",
      "        L-> 1 : 215/215\n",
      "        R-> Feature 12, Threshold 1.9375\n",
      "            L-> 1 : 2/2\n",
      "            R-> 2 : 2/2\n",
      "    R-> Feature 22, Threshold 24.89\n",
      "        L-> 1 : 8/8\n",
      "        R-> Feature 21, Threshold 12.6\n",
      "            L-> 1 : 2/2\n",
      "            R-> 2 : 9/9\n",
      "R-> Feature 23, Threshold 117.45\n",
      "    L-> Feature 25, Threshold 0.1361\n",
      "        L-> Feature 22, Threshold 26.58\n",
      "            L-> 1 : 17/17\n",
      "            R-> Feature 24, Threshold 871.8\n",
      "                L-> \n",
      "                R-> 2 : 5/5\n",
      "        R-> Feature 29, Threshold 0.2573\n",
      "            L-> 1 : 1/1\n",
      "            R-> 2 : 19/19\n",
      "    R-> Feature 27, Threshold 0.11295\n",
      "        L-> 1 : 1/1\n",
      "        R-> 2 : 112/112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{DecisionTreeClassifier} @350\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @350\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @187\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @730\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(Tree2, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[22m _.measure     \u001b[0m\u001b[0m\u001b[22m _.measurement \u001b[0m\u001b[0m\u001b[22m _.per_fold                                 \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m cross_entropy \u001b[0m\u001b[0m 2.41          \u001b[0m\u001b[0m [3.79, 4.55, 0.759, 1.52, 2.28, 1.53]      \u001b[0m\u001b[0m\n",
       "\u001b[0m acc           \u001b[0m\u001b[0m 0.933         \u001b[0m\u001b[0m [0.895, 0.874, 0.979, 0.958, 0.937, 0.957] \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "_.per_observation = [[[2.22e-16, 36.0, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16]], missing]\n",
       "_.fitted_params_per_fold = [  ]\n",
       "_.report_per_fold = [  ]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_acc = evaluate!(Tree2, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  17%[====>                    ]  ETA: 0:00:02\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:01\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[22m _.measure           \u001b[0m\u001b[0m\u001b[22m _.measurement \u001b[0m\u001b[0m\u001b[22m _.per_fold                                      \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m true_negative_rate  \u001b[0m\u001b[0m 0.93          \u001b[0m\u001b[0m [0.961, 0.929, 0.944, 0.942, 0.919, 0.887]      \u001b[0m\u001b[0m\n",
       "\u001b[0m true_positive_rate  \u001b[0m\u001b[0m 0.93          \u001b[0m\u001b[0m [0.955, 0.897, 0.951, 0.923, 0.952, 0.902]      \u001b[0m\u001b[0m\n",
       "\u001b[0m false_negative_rate \u001b[0m\u001b[0m 0.0698        \u001b[0m\u001b[0m [0.0455, 0.103, 0.0488, 0.0769, 0.0476, 0.0976] \u001b[0m\u001b[0m\n",
       "\u001b[0m false_positive_rate \u001b[0m\u001b[0m 0.0697        \u001b[0m\u001b[0m [0.0392, 0.0714, 0.0556, 0.058, 0.0811, 0.113]  \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "_.per_observation = [missing, missing, missing, missing]\n",
       "_.fitted_params_per_fold = [  ]\n",
       "_.report_per_fold = [  ]\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate!(Tree2, resampling=CV(shuffle=true), measure=[tnr,tpr,fnr,fpr], verbosity=1, operation=predict_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tree = Decision Tree\n",
       "Leaves: 17\n",
       "Depth:  6,\n",
       " encoding = Dict{CategoricalValue{String,UInt32},UInt32}(\"B\" => 0x00000001,\"M\" => 0x00000002),)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_params(Tree2) \n",
    "# print_tree(Tree.fitresult[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(classes_seen = CategoricalValue{String,UInt32}[\"B\", \"M\"],\n",
       " print_tree = TreePrinter object (call with display depth),)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(Tree2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch / RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,}} @502\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      " Info: Attempting to evaluate 101 models.\n",
      " @ MLJTuning /home/andrew/.julia/packages/MLJTuning/Bbgvk/src/tuned_models.jl:494\n",
      "\u001b[33mEvaluating over 101 metamodels: 100%[=========================] Time: 0:00:03\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(parameter_name = \"merge_purity_threshold\",\n",
       " parameter_scale = :none,\n",
       " parameter_values = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09    0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0],\n",
       " measurements = [0.6856475609566427, 0.6856475609566427, 0.6856475609566427, 0.6856475609566427, 0.6856475609566427, 0.6856475609566427, 0.6856475609566427, 0.6856475609566427, 0.6856475609566427, 0.6856475609566427    1.7669378711773989, 2.0198757896975192, 2.1376135189928767, 2.073033624902633, 2.126218065719009, 1.9997491064589488, 2.0629835860889787, 2.506970357959403, 2.0629835860889787, 2.3437119896917533],)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = collect(0:.01:1)\n",
    "r = range(dt2, :merge_purity_threshold, values=vals)\n",
    "# r = range(nn2, :epochs, lower=0, upper=max_epochs)\n",
    "curve = learning_curve(Tree2, \n",
    "                        range=r, \n",
    "#                         resampling=Holdout(fraction_train=0.7), \n",
    "                        resampling=CV(), \n",
    "                        measure=cross_entropy, \n",
    "                        acceleration=CPUThreads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip080\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip080)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip081\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip080)\" d=\"\n",
       "M210.121 1423.18 L2352.76 1423.18 L2352.76 47.2441 L210.121 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip082\">\n",
       "    <rect x=\"210\" y=\"47\" width=\"2144\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  270.762,1423.18 270.762,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  776.1,1423.18 776.1,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1281.44,1423.18 1281.44,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1786.78,1423.18 1786.78,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2292.12,1423.18 2292.12,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  210.121,1313.33 2352.76,1313.33 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  210.121,1007.61 2352.76,1007.61 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  210.121,701.89 2352.76,701.89 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  210.121,396.169 2352.76,396.169 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  210.121,90.4476 2352.76,90.4476 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  210.121,1423.18 2352.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  210.121,1423.18 210.121,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  270.762,1423.18 270.762,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  776.1,1423.18 776.1,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1281.44,1423.18 1281.44,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1786.78,1423.18 1786.78,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2292.12,1423.18 2292.12,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  210.121,1313.33 235.833,1313.33 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  210.121,1007.61 235.833,1007.61 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  210.121,701.89 235.833,701.89 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  210.121,396.169 235.833,396.169 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  210.121,90.4476 235.833,90.4476 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M238.771 1445.17 Q235.16 1445.17 233.331 1448.74 Q231.526 1452.28 231.526 1459.41 Q231.526 1466.51 233.331 1470.08 Q235.16 1473.62 238.771 1473.62 Q242.405 1473.62 244.211 1470.08 Q246.039 1466.51 246.039 1459.41 Q246.039 1452.28 244.211 1448.74 Q242.405 1445.17 238.771 1445.17 M238.771 1441.47 Q244.581 1441.47 247.637 1446.07 Q250.715 1450.66 250.715 1459.41 Q250.715 1468.13 247.637 1472.74 Q244.581 1477.32 238.771 1477.32 Q232.961 1477.32 229.882 1472.74 Q226.827 1468.13 226.827 1459.41 Q226.827 1450.66 229.882 1446.07 Q232.961 1441.47 238.771 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M255.785 1470.77 L260.669 1470.77 L260.669 1476.65 L255.785 1476.65 L255.785 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M275.738 1445.17 Q272.127 1445.17 270.299 1448.74 Q268.493 1452.28 268.493 1459.41 Q268.493 1466.51 270.299 1470.08 Q272.127 1473.62 275.738 1473.62 Q279.373 1473.62 281.178 1470.08 Q283.007 1466.51 283.007 1459.41 Q283.007 1452.28 281.178 1448.74 Q279.373 1445.17 275.738 1445.17 M275.738 1441.47 Q281.549 1441.47 284.604 1446.07 Q287.683 1450.66 287.683 1459.41 Q287.683 1468.13 284.604 1472.74 Q281.549 1477.32 275.738 1477.32 Q269.928 1477.32 266.85 1472.74 Q263.794 1468.13 263.794 1459.41 Q263.794 1450.66 266.85 1446.07 Q269.928 1441.47 275.738 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M302.752 1445.17 Q299.141 1445.17 297.312 1448.74 Q295.507 1452.28 295.507 1459.41 Q295.507 1466.51 297.312 1470.08 Q299.141 1473.62 302.752 1473.62 Q306.386 1473.62 308.192 1470.08 Q310.021 1466.51 310.021 1459.41 Q310.021 1452.28 308.192 1448.74 Q306.386 1445.17 302.752 1445.17 M302.752 1441.47 Q308.562 1441.47 311.618 1446.07 Q314.697 1450.66 314.697 1459.41 Q314.697 1468.13 311.618 1472.74 Q308.562 1477.32 302.752 1477.32 Q296.942 1477.32 293.863 1472.74 Q290.808 1468.13 290.808 1459.41 Q290.808 1450.66 293.863 1446.07 Q296.942 1441.47 302.752 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M745.406 1445.17 Q741.795 1445.17 739.966 1448.74 Q738.16 1452.28 738.16 1459.41 Q738.16 1466.51 739.966 1470.08 Q741.795 1473.62 745.406 1473.62 Q749.04 1473.62 750.845 1470.08 Q752.674 1466.51 752.674 1459.41 Q752.674 1452.28 750.845 1448.74 Q749.04 1445.17 745.406 1445.17 M745.406 1441.47 Q751.216 1441.47 754.271 1446.07 Q757.35 1450.66 757.35 1459.41 Q757.35 1468.13 754.271 1472.74 Q751.216 1477.32 745.406 1477.32 Q739.596 1477.32 736.517 1472.74 Q733.461 1468.13 733.461 1459.41 Q733.461 1450.66 736.517 1446.07 Q739.596 1441.47 745.406 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M762.42 1470.77 L767.304 1470.77 L767.304 1476.65 L762.42 1476.65 L762.42 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M776.401 1472.72 L792.72 1472.72 L792.72 1476.65 L770.776 1476.65 L770.776 1472.72 Q773.438 1469.96 778.021 1465.33 Q782.628 1460.68 783.808 1459.34 Q786.054 1456.81 786.933 1455.08 Q787.836 1453.32 787.836 1451.63 Q787.836 1448.87 785.892 1447.14 Q783.97 1445.4 780.868 1445.4 Q778.669 1445.4 776.216 1446.17 Q773.785 1446.93 771.007 1448.48 L771.007 1443.76 Q773.831 1442.62 776.285 1442.05 Q778.739 1441.47 780.776 1441.47 Q786.146 1441.47 789.341 1444.15 Q792.535 1446.84 792.535 1451.33 Q792.535 1453.46 791.725 1455.38 Q790.938 1457.28 788.831 1459.87 Q788.253 1460.54 785.151 1463.76 Q782.049 1466.95 776.401 1472.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M797.836 1442.09 L816.192 1442.09 L816.192 1446.03 L802.118 1446.03 L802.118 1454.5 Q803.137 1454.15 804.155 1453.99 Q805.174 1453.8 806.192 1453.8 Q811.979 1453.8 815.359 1456.98 Q818.739 1460.15 818.739 1465.56 Q818.739 1471.14 815.266 1474.24 Q811.794 1477.32 805.475 1477.32 Q803.299 1477.32 801.03 1476.95 Q798.785 1476.58 796.378 1475.84 L796.378 1471.14 Q798.461 1472.28 800.683 1472.83 Q802.905 1473.39 805.382 1473.39 Q809.387 1473.39 811.725 1471.28 Q814.063 1469.18 814.063 1465.56 Q814.063 1461.95 811.725 1459.85 Q809.387 1457.74 805.382 1457.74 Q803.507 1457.74 801.632 1458.16 Q799.78 1458.57 797.836 1459.45 L797.836 1442.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1249.95 1445.17 Q1246.33 1445.17 1244.51 1448.74 Q1242.7 1452.28 1242.7 1459.41 Q1242.7 1466.51 1244.51 1470.08 Q1246.33 1473.62 1249.95 1473.62 Q1253.58 1473.62 1255.39 1470.08 Q1257.21 1466.51 1257.21 1459.41 Q1257.21 1452.28 1255.39 1448.74 Q1253.58 1445.17 1249.95 1445.17 M1249.95 1441.47 Q1255.76 1441.47 1258.81 1446.07 Q1261.89 1450.66 1261.89 1459.41 Q1261.89 1468.13 1258.81 1472.74 Q1255.76 1477.32 1249.95 1477.32 Q1244.14 1477.32 1241.06 1472.74 Q1238 1468.13 1238 1459.41 Q1238 1450.66 1241.06 1446.07 Q1244.14 1441.47 1249.95 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1266.96 1470.77 L1271.84 1470.77 L1271.84 1476.65 L1266.96 1476.65 L1266.96 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1276.96 1442.09 L1295.32 1442.09 L1295.32 1446.03 L1281.24 1446.03 L1281.24 1454.5 Q1282.26 1454.15 1283.28 1453.99 Q1284.3 1453.8 1285.32 1453.8 Q1291.1 1453.8 1294.48 1456.98 Q1297.86 1460.15 1297.86 1465.56 Q1297.86 1471.14 1294.39 1474.24 Q1290.92 1477.32 1284.6 1477.32 Q1282.42 1477.32 1280.15 1476.95 Q1277.91 1476.58 1275.5 1475.84 L1275.5 1471.14 Q1277.58 1472.28 1279.81 1472.83 Q1282.03 1473.39 1284.51 1473.39 Q1288.51 1473.39 1290.85 1471.28 Q1293.19 1469.18 1293.19 1465.56 Q1293.19 1461.95 1290.85 1459.85 Q1288.51 1457.74 1284.51 1457.74 Q1282.63 1457.74 1280.76 1458.16 Q1278.9 1458.57 1276.96 1459.45 L1276.96 1442.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1312.93 1445.17 Q1309.32 1445.17 1307.49 1448.74 Q1305.69 1452.28 1305.69 1459.41 Q1305.69 1466.51 1307.49 1470.08 Q1309.32 1473.62 1312.93 1473.62 Q1316.57 1473.62 1318.37 1470.08 Q1320.2 1466.51 1320.2 1459.41 Q1320.2 1452.28 1318.37 1448.74 Q1316.57 1445.17 1312.93 1445.17 M1312.93 1441.47 Q1318.74 1441.47 1321.8 1446.07 Q1324.88 1450.66 1324.88 1459.41 Q1324.88 1468.13 1321.8 1472.74 Q1318.74 1477.32 1312.93 1477.32 Q1307.12 1477.32 1304.04 1472.74 Q1300.99 1468.13 1300.99 1459.41 Q1300.99 1450.66 1304.04 1446.07 Q1307.12 1441.47 1312.93 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1755.74 1445.17 Q1752.12 1445.17 1750.3 1448.74 Q1748.49 1452.28 1748.49 1459.41 Q1748.49 1466.51 1750.3 1470.08 Q1752.12 1473.62 1755.74 1473.62 Q1759.37 1473.62 1761.18 1470.08 Q1763 1466.51 1763 1459.41 Q1763 1452.28 1761.18 1448.74 Q1759.37 1445.17 1755.74 1445.17 M1755.74 1441.47 Q1761.55 1441.47 1764.6 1446.07 Q1767.68 1450.66 1767.68 1459.41 Q1767.68 1468.13 1764.6 1472.74 Q1761.55 1477.32 1755.74 1477.32 Q1749.93 1477.32 1746.85 1472.74 Q1743.79 1468.13 1743.79 1459.41 Q1743.79 1450.66 1746.85 1446.07 Q1749.93 1441.47 1755.74 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1772.75 1470.77 L1777.63 1470.77 L1777.63 1476.65 L1772.75 1476.65 L1772.75 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1781.52 1442.09 L1803.74 1442.09 L1803.74 1444.08 L1791.2 1476.65 L1786.31 1476.65 L1798.12 1446.03 L1781.52 1446.03 L1781.52 1442.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1808.86 1442.09 L1827.22 1442.09 L1827.22 1446.03 L1813.14 1446.03 L1813.14 1454.5 Q1814.16 1454.15 1815.18 1453.99 Q1816.2 1453.8 1817.22 1453.8 Q1823 1453.8 1826.38 1456.98 Q1829.76 1460.15 1829.76 1465.56 Q1829.76 1471.14 1826.29 1474.24 Q1822.82 1477.32 1816.5 1477.32 Q1814.32 1477.32 1812.05 1476.95 Q1809.81 1476.58 1807.4 1475.84 L1807.4 1471.14 Q1809.49 1472.28 1811.71 1472.83 Q1813.93 1473.39 1816.41 1473.39 Q1820.41 1473.39 1822.75 1471.28 Q1825.09 1469.18 1825.09 1465.56 Q1825.09 1461.95 1822.75 1459.85 Q1820.41 1457.74 1816.41 1457.74 Q1814.53 1457.74 1812.66 1458.16 Q1810.8 1458.57 1808.86 1459.45 L1808.86 1442.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2250.51 1472.72 L2258.15 1472.72 L2258.15 1446.35 L2249.84 1448.02 L2249.84 1443.76 L2258.1 1442.09 L2262.78 1442.09 L2262.78 1472.72 L2270.41 1472.72 L2270.41 1476.65 L2250.51 1476.65 L2250.51 1472.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2275.48 1470.77 L2280.37 1470.77 L2280.37 1476.65 L2275.48 1476.65 L2275.48 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2295.44 1445.17 Q2291.83 1445.17 2290 1448.74 Q2288.19 1452.28 2288.19 1459.41 Q2288.19 1466.51 2290 1470.08 Q2291.83 1473.62 2295.44 1473.62 Q2299.07 1473.62 2300.88 1470.08 Q2302.71 1466.51 2302.71 1459.41 Q2302.71 1452.28 2300.88 1448.74 Q2299.07 1445.17 2295.44 1445.17 M2295.44 1441.47 Q2301.25 1441.47 2304.3 1446.07 Q2307.38 1450.66 2307.38 1459.41 Q2307.38 1468.13 2304.3 1472.74 Q2301.25 1477.32 2295.44 1477.32 Q2289.63 1477.32 2286.55 1472.74 Q2283.49 1468.13 2283.49 1459.41 Q2283.49 1450.66 2286.55 1446.07 Q2289.63 1441.47 2295.44 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2322.45 1445.17 Q2318.84 1445.17 2317.01 1448.74 Q2315.21 1452.28 2315.21 1459.41 Q2315.21 1466.51 2317.01 1470.08 Q2318.84 1473.62 2322.45 1473.62 Q2326.09 1473.62 2327.89 1470.08 Q2329.72 1466.51 2329.72 1459.41 Q2329.72 1452.28 2327.89 1448.74 Q2326.09 1445.17 2322.45 1445.17 M2322.45 1441.47 Q2328.26 1441.47 2331.32 1446.07 Q2334.4 1450.66 2334.4 1459.41 Q2334.4 1468.13 2331.32 1472.74 Q2328.26 1477.32 2322.45 1477.32 Q2316.64 1477.32 2313.56 1472.74 Q2310.51 1468.13 2310.51 1459.41 Q2310.51 1450.66 2313.56 1446.07 Q2316.64 1441.47 2322.45 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M138.205 1299.13 Q134.593 1299.13 132.765 1302.7 Q130.959 1306.24 130.959 1313.37 Q130.959 1320.47 132.765 1324.04 Q134.593 1327.58 138.205 1327.58 Q141.839 1327.58 143.644 1324.04 Q145.473 1320.47 145.473 1313.37 Q145.473 1306.24 143.644 1302.7 Q141.839 1299.13 138.205 1299.13 M138.205 1295.43 Q144.015 1295.43 147.07 1300.03 Q150.149 1304.62 150.149 1313.37 Q150.149 1322.09 147.07 1326.7 Q144.015 1331.28 138.205 1331.28 Q132.394 1331.28 129.316 1326.7 Q126.26 1322.09 126.26 1313.37 Q126.26 1304.62 129.316 1300.03 Q132.394 1295.43 138.205 1295.43 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M155.218 1324.73 L160.103 1324.73 L160.103 1330.61 L155.218 1330.61 L155.218 1324.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M165.218 1296.05 L183.575 1296.05 L183.575 1299.99 L169.501 1299.99 L169.501 1308.46 Q170.519 1308.11 171.538 1307.95 Q172.556 1307.77 173.575 1307.77 Q179.362 1307.77 182.741 1310.94 Q186.121 1314.11 186.121 1319.52 Q186.121 1325.1 182.649 1328.2 Q179.177 1331.28 172.857 1331.28 Q170.681 1331.28 168.413 1330.91 Q166.167 1330.54 163.76 1329.8 L163.76 1325.1 Q165.843 1326.24 168.065 1326.79 Q170.288 1327.35 172.765 1327.35 Q176.769 1327.35 179.107 1325.24 Q181.445 1323.14 181.445 1319.52 Q181.445 1315.91 179.107 1313.81 Q176.769 1311.7 172.765 1311.7 Q170.89 1311.7 169.015 1312.12 Q167.163 1312.53 165.218 1313.41 L165.218 1296.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M129.246 1020.96 L136.885 1020.96 L136.885 994.59 L128.575 996.257 L128.575 991.998 L136.839 990.331 L141.515 990.331 L141.515 1020.96 L149.154 1020.96 L149.154 1024.89 L129.246 1024.89 L129.246 1020.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M154.223 1019.01 L159.107 1019.01 L159.107 1024.89 L154.223 1024.89 L154.223 1019.01 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M174.177 993.41 Q170.565 993.41 168.737 996.975 Q166.931 1000.52 166.931 1007.65 Q166.931 1014.75 168.737 1018.32 Q170.565 1021.86 174.177 1021.86 Q177.811 1021.86 179.616 1018.32 Q181.445 1014.75 181.445 1007.65 Q181.445 1000.52 179.616 996.975 Q177.811 993.41 174.177 993.41 M174.177 989.706 Q179.987 989.706 183.042 994.313 Q186.121 998.896 186.121 1007.65 Q186.121 1016.37 183.042 1020.98 Q179.987 1025.56 174.177 1025.56 Q168.366 1025.56 165.288 1020.98 Q162.232 1016.37 162.232 1007.65 Q162.232 998.896 165.288 994.313 Q168.366 989.706 174.177 989.706 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M130.242 715.235 L137.88 715.235 L137.88 688.869 L129.57 690.536 L129.57 686.277 L137.834 684.61 L142.51 684.61 L142.51 715.235 L150.149 715.235 L150.149 719.17 L130.242 719.17 L130.242 715.235 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M155.218 713.29 L160.103 713.29 L160.103 719.17 L155.218 719.17 L155.218 713.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M165.218 684.61 L183.575 684.61 L183.575 688.545 L169.501 688.545 L169.501 697.017 Q170.519 696.67 171.538 696.508 Q172.556 696.323 173.575 696.323 Q179.362 696.323 182.741 699.494 Q186.121 702.665 186.121 708.082 Q186.121 713.661 182.649 716.763 Q179.177 719.841 172.857 719.841 Q170.681 719.841 168.413 719.471 Q166.167 719.1 163.76 718.36 L163.76 713.661 Q165.843 714.795 168.065 715.351 Q170.288 715.906 172.765 715.906 Q176.769 715.906 179.107 713.8 Q181.445 711.693 181.445 708.082 Q181.445 704.471 179.107 702.364 Q176.769 700.258 172.765 700.258 Q170.89 700.258 169.015 700.675 Q167.163 701.091 165.218 701.971 L165.218 684.61 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M132.834 409.514 L149.154 409.514 L149.154 413.449 L127.209 413.449 L127.209 409.514 Q129.871 406.759 134.455 402.129 Q139.061 397.477 140.242 396.134 Q142.487 393.611 143.367 391.875 Q144.269 390.116 144.269 388.426 Q144.269 385.671 142.325 383.935 Q140.404 382.199 137.302 382.199 Q135.103 382.199 132.649 382.963 Q130.218 383.727 127.441 385.278 L127.441 380.555 Q130.265 379.421 132.718 378.842 Q135.172 378.264 137.209 378.264 Q142.58 378.264 145.774 380.949 Q148.968 383.634 148.968 388.125 Q148.968 390.254 148.158 392.176 Q147.371 394.074 145.265 396.666 Q144.686 397.338 141.584 400.555 Q138.482 403.75 132.834 409.514 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M154.223 407.569 L159.107 407.569 L159.107 413.449 L154.223 413.449 L154.223 407.569 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M174.177 381.967 Q170.565 381.967 168.737 385.532 Q166.931 389.074 166.931 396.204 Q166.931 403.31 168.737 406.875 Q170.565 410.416 174.177 410.416 Q177.811 410.416 179.616 406.875 Q181.445 403.31 181.445 396.204 Q181.445 389.074 179.616 385.532 Q177.811 381.967 174.177 381.967 M174.177 378.264 Q179.987 378.264 183.042 382.87 Q186.121 387.454 186.121 396.204 Q186.121 404.93 183.042 409.537 Q179.987 414.12 174.177 414.12 Q168.366 414.12 165.288 409.537 Q162.232 404.93 162.232 396.204 Q162.232 387.454 165.288 382.87 Q168.366 378.264 174.177 378.264 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M133.83 103.792 L150.149 103.792 L150.149 107.728 L128.205 107.728 L128.205 103.792 Q130.867 101.038 135.45 96.4083 Q140.056 91.7555 141.237 90.4129 Q143.482 87.8898 144.362 86.1537 Q145.265 84.3944 145.265 82.7046 Q145.265 79.95 143.32 78.2139 Q141.399 76.4778 138.297 76.4778 Q136.098 76.4778 133.644 77.2417 Q131.214 78.0056 128.436 79.5565 L128.436 74.8343 Q131.26 73.7 133.714 73.1213 Q136.168 72.5426 138.205 72.5426 Q143.575 72.5426 146.769 75.2278 Q149.964 77.913 149.964 82.4037 Q149.964 84.5333 149.154 86.4546 Q148.367 88.3527 146.26 90.9453 Q145.681 91.6166 142.58 94.8342 Q139.478 98.0286 133.83 103.792 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M155.218 101.848 L160.103 101.848 L160.103 107.728 L155.218 107.728 L155.218 101.848 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M165.218 73.1676 L183.575 73.1676 L183.575 77.1028 L169.501 77.1028 L169.501 85.575 Q170.519 85.2278 171.538 85.0657 Q172.556 84.8805 173.575 84.8805 Q179.362 84.8805 182.741 88.0518 Q186.121 91.2231 186.121 96.6397 Q186.121 102.218 182.649 105.32 Q179.177 108.399 172.857 108.399 Q170.681 108.399 168.413 108.029 Q166.167 107.658 163.76 106.917 L163.76 102.218 Q165.843 103.353 168.065 103.908 Q170.288 104.464 172.765 104.464 Q176.769 104.464 179.107 102.357 Q181.445 100.251 181.445 96.6397 Q181.445 93.0286 179.107 90.9222 Q176.769 88.8157 172.765 88.8157 Q170.89 88.8157 169.015 89.2324 Q167.163 89.649 165.218 90.5287 L165.218 73.1676 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M960.607 1527.24 Q962.803 1523.29 965.859 1521.41 Q968.914 1519.54 973.052 1519.54 Q978.622 1519.54 981.645 1523.45 Q984.669 1527.33 984.669 1534.53 L984.669 1556.04 L978.781 1556.04 L978.781 1534.72 Q978.781 1529.59 976.967 1527.11 Q975.152 1524.63 971.429 1524.63 Q966.877 1524.63 964.235 1527.65 Q961.593 1530.68 961.593 1535.9 L961.593 1556.04 L955.705 1556.04 L955.705 1534.72 Q955.705 1529.56 953.891 1527.11 Q952.077 1524.63 948.289 1524.63 Q943.801 1524.63 941.16 1527.68 Q938.518 1530.71 938.518 1535.9 L938.518 1556.04 L932.63 1556.04 L932.63 1520.4 L938.518 1520.4 L938.518 1525.93 Q940.523 1522.66 943.324 1521.1 Q946.125 1519.54 949.976 1519.54 Q953.859 1519.54 956.565 1521.51 Q959.302 1523.48 960.607 1527.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1021.3 1536.76 L1021.3 1539.62 L994.377 1539.62 Q994.759 1545.67 998.005 1548.85 Q1001.28 1552 1007.11 1552 Q1010.48 1552 1013.63 1551.17 Q1016.82 1550.35 1019.94 1548.69 L1019.94 1554.23 Q1016.78 1555.57 1013.47 1556.27 Q1010.16 1556.97 1006.76 1556.97 Q998.228 1556.97 993.231 1552 Q988.266 1547.04 988.266 1538.57 Q988.266 1529.82 992.976 1524.69 Q997.719 1519.54 1005.74 1519.54 Q1012.93 1519.54 1017.1 1524.18 Q1021.3 1528.8 1021.3 1536.76 M1015.45 1535.04 Q1015.38 1530.23 1012.74 1527.37 Q1010.13 1524.5 1005.8 1524.5 Q1000.9 1524.5 997.942 1527.27 Q995.013 1530.04 994.568 1535.07 L1015.45 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1048.1 1525.87 Q1047.12 1525.3 1045.94 1525.04 Q1044.79 1524.76 1043.39 1524.76 Q1038.43 1524.76 1035.75 1528 Q1033.11 1531.22 1033.11 1537.27 L1033.11 1556.04 L1027.22 1556.04 L1027.22 1520.4 L1033.11 1520.4 L1033.11 1525.93 Q1034.96 1522.69 1037.92 1521.13 Q1040.88 1519.54 1045.11 1519.54 Q1045.72 1519.54 1046.45 1519.63 Q1047.18 1519.7 1048.07 1519.85 L1048.1 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1076.56 1537.81 Q1076.56 1531.44 1073.92 1527.94 Q1071.31 1524.44 1066.56 1524.44 Q1061.85 1524.44 1059.21 1527.94 Q1056.6 1531.44 1056.6 1537.81 Q1056.6 1544.14 1059.21 1547.64 Q1061.85 1551.14 1066.56 1551.14 Q1071.31 1551.14 1073.92 1547.64 Q1076.56 1544.14 1076.56 1537.81 M1082.41 1551.62 Q1082.41 1560.72 1078.37 1565.15 Q1074.33 1569.6 1065.99 1569.6 Q1062.9 1569.6 1060.17 1569.13 Q1057.43 1568.68 1054.85 1567.72 L1054.85 1562.03 Q1057.43 1563.43 1059.94 1564.1 Q1062.46 1564.76 1065.07 1564.76 Q1070.83 1564.76 1073.69 1561.74 Q1076.56 1558.75 1076.56 1552.67 L1076.56 1549.77 Q1074.74 1552.92 1071.91 1554.48 Q1069.08 1556.04 1065.13 1556.04 Q1058.58 1556.04 1054.56 1551.05 Q1050.55 1546.05 1050.55 1537.81 Q1050.55 1529.53 1054.56 1524.53 Q1058.58 1519.54 1065.13 1519.54 Q1069.08 1519.54 1071.91 1521.1 Q1074.74 1522.66 1076.56 1525.81 L1076.56 1520.4 L1082.41 1520.4 L1082.41 1551.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1119.05 1536.76 L1119.05 1539.62 L1092.12 1539.62 Q1092.5 1545.67 1095.75 1548.85 Q1099.03 1552 1104.85 1552 Q1108.23 1552 1111.38 1551.17 Q1114.56 1550.35 1117.68 1548.69 L1117.68 1554.23 Q1114.53 1555.57 1111.22 1556.27 Q1107.91 1556.97 1104.5 1556.97 Q1095.97 1556.97 1090.98 1552 Q1086.01 1547.04 1086.01 1538.57 Q1086.01 1529.82 1090.72 1524.69 Q1095.46 1519.54 1103.49 1519.54 Q1110.68 1519.54 1114.85 1524.18 Q1119.05 1528.8 1119.05 1536.76 M1113.19 1535.04 Q1113.13 1530.23 1110.49 1527.37 Q1107.88 1524.5 1103.55 1524.5 Q1098.65 1524.5 1095.69 1527.27 Q1092.76 1530.04 1092.31 1535.07 L1113.19 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1152.28 1566.87 L1152.28 1571.42 L1118.41 1571.42 L1118.41 1566.87 L1152.28 1566.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1164.09 1550.7 L1164.09 1569.6 L1158.2 1569.6 L1158.2 1520.4 L1164.09 1520.4 L1164.09 1525.81 Q1165.93 1522.62 1168.73 1521.1 Q1171.57 1519.54 1175.48 1519.54 Q1181.97 1519.54 1186.02 1524.69 Q1190.09 1529.85 1190.09 1538.25 Q1190.09 1546.65 1186.02 1551.81 Q1181.97 1556.97 1175.48 1556.97 Q1171.57 1556.97 1168.73 1555.44 Q1165.93 1553.88 1164.09 1550.7 M1184.01 1538.25 Q1184.01 1531.79 1181.34 1528.13 Q1178.7 1524.44 1174.05 1524.44 Q1169.4 1524.44 1166.73 1528.13 Q1164.09 1531.79 1164.09 1538.25 Q1164.09 1544.71 1166.73 1548.4 Q1169.4 1552.07 1174.05 1552.07 Q1178.7 1552.07 1181.34 1548.4 Q1184.01 1544.71 1184.01 1538.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1195.63 1541.98 L1195.63 1520.4 L1201.49 1520.4 L1201.49 1541.75 Q1201.49 1546.81 1203.46 1549.36 Q1205.43 1551.87 1209.38 1551.87 Q1214.12 1551.87 1216.86 1548.85 Q1219.63 1545.83 1219.63 1540.61 L1219.63 1520.4 L1225.48 1520.4 L1225.48 1556.04 L1219.63 1556.04 L1219.63 1550.57 Q1217.49 1553.82 1214.66 1555.41 Q1211.86 1556.97 1208.14 1556.97 Q1201.99 1556.97 1198.81 1553.15 Q1195.63 1549.33 1195.63 1541.98 M1210.37 1519.54 L1210.37 1519.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1252.28 1525.87 Q1251.3 1525.3 1250.12 1525.04 Q1248.97 1524.76 1247.57 1524.76 Q1242.61 1524.76 1239.93 1528 Q1237.29 1531.22 1237.29 1537.27 L1237.29 1556.04 L1231.4 1556.04 L1231.4 1520.4 L1237.29 1520.4 L1237.29 1525.93 Q1239.14 1522.69 1242.1 1521.13 Q1245.06 1519.54 1249.29 1519.54 Q1249.9 1519.54 1250.63 1519.63 Q1251.36 1519.7 1252.25 1519.85 L1252.28 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1258.43 1520.4 L1264.28 1520.4 L1264.28 1556.04 L1258.43 1556.04 L1258.43 1520.4 M1258.43 1506.52 L1264.28 1506.52 L1264.28 1513.93 L1258.43 1513.93 L1258.43 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1276.22 1510.27 L1276.22 1520.4 L1288.28 1520.4 L1288.28 1524.95 L1276.22 1524.95 L1276.22 1544.3 Q1276.22 1548.66 1277.4 1549.9 Q1278.61 1551.14 1282.27 1551.14 L1288.28 1551.14 L1288.28 1556.04 L1282.27 1556.04 Q1275.49 1556.04 1272.91 1553.53 Q1270.33 1550.98 1270.33 1544.3 L1270.33 1524.95 L1266.03 1524.95 L1266.03 1520.4 L1270.33 1520.4 L1270.33 1510.27 L1276.22 1510.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1309.26 1559.35 Q1306.77 1565.72 1304.42 1567.66 Q1302.06 1569.6 1298.12 1569.6 L1293.44 1569.6 L1293.44 1564.7 L1296.88 1564.7 Q1299.29 1564.7 1300.63 1563.56 Q1301.97 1562.41 1303.59 1558.14 L1304.64 1555.47 L1290.22 1520.4 L1296.43 1520.4 L1307.57 1548.28 L1318.71 1520.4 L1324.92 1520.4 L1309.26 1559.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1358.15 1566.87 L1358.15 1571.42 L1324.28 1571.42 L1324.28 1566.87 L1358.15 1566.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1370.08 1510.27 L1370.08 1520.4 L1382.14 1520.4 L1382.14 1524.95 L1370.08 1524.95 L1370.08 1544.3 Q1370.08 1548.66 1371.26 1549.9 Q1372.47 1551.14 1376.13 1551.14 L1382.14 1551.14 L1382.14 1556.04 L1376.13 1556.04 Q1369.35 1556.04 1366.77 1553.53 Q1364.19 1550.98 1364.19 1544.3 L1364.19 1524.95 L1359.9 1524.95 L1359.9 1520.4 L1364.19 1520.4 L1364.19 1510.27 L1370.08 1510.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1417.92 1534.53 L1417.92 1556.04 L1412.06 1556.04 L1412.06 1534.72 Q1412.06 1529.66 1410.09 1527.14 Q1408.12 1524.63 1404.17 1524.63 Q1399.43 1524.63 1396.69 1527.65 Q1393.95 1530.68 1393.95 1535.9 L1393.95 1556.04 L1388.06 1556.04 L1388.06 1506.52 L1393.95 1506.52 L1393.95 1525.93 Q1396.05 1522.72 1398.89 1521.13 Q1401.75 1519.54 1405.47 1519.54 Q1411.62 1519.54 1414.77 1523.36 Q1417.92 1527.14 1417.92 1534.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1444.72 1525.87 Q1443.73 1525.3 1442.55 1525.04 Q1441.41 1524.76 1440.01 1524.76 Q1435.04 1524.76 1432.37 1528 Q1429.73 1531.22 1429.73 1537.27 L1429.73 1556.04 L1423.84 1556.04 L1423.84 1520.4 L1429.73 1520.4 L1429.73 1525.93 Q1431.57 1522.69 1434.53 1521.13 Q1437.49 1519.54 1441.73 1519.54 Q1442.33 1519.54 1443.06 1519.63 Q1443.8 1519.7 1444.69 1519.85 L1444.72 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1479.92 1536.76 L1479.92 1539.62 L1452.99 1539.62 Q1453.38 1545.67 1456.62 1548.85 Q1459.9 1552 1465.73 1552 Q1469.1 1552 1472.25 1551.17 Q1475.43 1550.35 1478.55 1548.69 L1478.55 1554.23 Q1475.4 1555.57 1472.09 1556.27 Q1468.78 1556.97 1465.38 1556.97 Q1456.85 1556.97 1451.85 1552 Q1446.88 1547.04 1446.88 1538.57 Q1446.88 1529.82 1451.59 1524.69 Q1456.34 1519.54 1464.36 1519.54 Q1471.55 1519.54 1475.72 1524.18 Q1479.92 1528.8 1479.92 1536.76 M1474.06 1535.04 Q1474 1530.23 1471.36 1527.37 Q1468.75 1524.5 1464.42 1524.5 Q1459.52 1524.5 1456.56 1527.27 Q1453.63 1530.04 1453.19 1535.07 L1474.06 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1508.79 1521.45 L1508.79 1526.98 Q1506.31 1525.71 1503.63 1525.07 Q1500.96 1524.44 1498.1 1524.44 Q1493.73 1524.44 1491.54 1525.77 Q1489.37 1527.11 1489.37 1529.79 Q1489.37 1531.82 1490.93 1533 Q1492.49 1534.15 1497.2 1535.2 L1499.21 1535.64 Q1505.45 1536.98 1508.06 1539.43 Q1510.7 1541.85 1510.7 1546.21 Q1510.7 1551.17 1506.75 1554.07 Q1502.84 1556.97 1495.96 1556.97 Q1493.1 1556.97 1489.98 1556.39 Q1486.89 1555.85 1483.45 1554.74 L1483.45 1548.69 Q1486.7 1550.38 1489.85 1551.24 Q1493 1552.07 1496.09 1552.07 Q1500.23 1552.07 1502.46 1550.66 Q1504.68 1549.23 1504.68 1546.65 Q1504.68 1544.27 1503.06 1542.99 Q1501.47 1541.72 1496.03 1540.54 L1493.99 1540.07 Q1488.55 1538.92 1486.13 1536.56 Q1483.71 1534.18 1483.71 1530.04 Q1483.71 1525.01 1487.27 1522.27 Q1490.84 1519.54 1497.4 1519.54 Q1500.64 1519.54 1503.51 1520.01 Q1506.37 1520.49 1508.79 1521.45 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1546.47 1534.53 L1546.47 1556.04 L1540.62 1556.04 L1540.62 1534.72 Q1540.62 1529.66 1538.64 1527.14 Q1536.67 1524.63 1532.72 1524.63 Q1527.98 1524.63 1525.24 1527.65 Q1522.51 1530.68 1522.51 1535.9 L1522.51 1556.04 L1516.62 1556.04 L1516.62 1506.52 L1522.51 1506.52 L1522.51 1525.93 Q1524.61 1522.72 1527.44 1521.13 Q1530.31 1519.54 1534.03 1519.54 Q1540.17 1519.54 1543.32 1523.36 Q1546.47 1527.14 1546.47 1534.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1566.43 1524.5 Q1561.72 1524.5 1558.98 1528.19 Q1556.25 1531.85 1556.25 1538.25 Q1556.25 1544.65 1558.95 1548.34 Q1561.69 1552 1566.43 1552 Q1571.11 1552 1573.85 1548.31 Q1576.58 1544.62 1576.58 1538.25 Q1576.58 1531.92 1573.85 1528.23 Q1571.11 1524.5 1566.43 1524.5 M1566.43 1519.54 Q1574.07 1519.54 1578.43 1524.5 Q1582.79 1529.47 1582.79 1538.25 Q1582.79 1547 1578.43 1552 Q1574.07 1556.97 1566.43 1556.97 Q1558.76 1556.97 1554.4 1552 Q1550.07 1547 1550.07 1538.25 Q1550.07 1529.47 1554.4 1524.5 Q1558.76 1519.54 1566.43 1519.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1588.93 1506.52 L1594.79 1506.52 L1594.79 1556.04 L1588.93 1556.04 L1588.93 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1624.39 1525.81 L1624.39 1506.52 L1630.25 1506.52 L1630.25 1556.04 L1624.39 1556.04 L1624.39 1550.7 Q1622.54 1553.88 1619.71 1555.44 Q1616.91 1556.97 1612.96 1556.97 Q1606.5 1556.97 1602.43 1551.81 Q1598.39 1546.65 1598.39 1538.25 Q1598.39 1529.85 1602.43 1524.69 Q1606.5 1519.54 1612.96 1519.54 Q1616.91 1519.54 1619.71 1521.1 Q1622.54 1522.62 1624.39 1525.81 M1604.43 1538.25 Q1604.43 1544.71 1607.08 1548.4 Q1609.75 1552.07 1614.4 1552.07 Q1619.04 1552.07 1621.72 1548.4 Q1624.39 1544.71 1624.39 1538.25 Q1624.39 1531.79 1621.72 1528.13 Q1619.04 1524.44 1614.4 1524.44 Q1609.75 1524.44 1607.08 1528.13 Q1604.43 1531.79 1604.43 1538.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M44.1444 904.492 L50.9239 904.492 Q47.9002 907.739 46.4043 911.431 Q44.9083 915.091 44.9083 919.229 Q44.9083 927.377 49.9054 931.705 Q54.8707 936.034 64.2919 936.034 Q73.6813 936.034 78.6784 931.705 Q83.6436 927.377 83.6436 919.229 Q83.6436 915.091 82.1477 911.431 Q80.6518 907.739 77.6281 904.492 L84.3439 904.492 Q86.6355 907.866 87.7814 911.653 Q88.9272 915.409 88.9272 919.611 Q88.9272 930.4 82.3387 936.607 Q75.7183 942.814 64.2919 942.814 Q52.8336 942.814 46.2451 936.607 Q39.6248 930.4 39.6248 919.611 Q39.6248 915.346 40.7706 911.59 Q41.8846 907.802 44.1444 904.492 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M57.8307 877.692 Q57.2578 878.679 57.0032 879.857 Q56.7167 881.003 56.7167 882.403 Q56.7167 887.368 59.9632 890.042 Q63.1779 892.684 69.2253 892.684 L88.0042 892.684 L88.0042 898.572 L52.3562 898.572 L52.3562 892.684 L57.8944 892.684 Q54.6479 890.838 53.0883 887.878 Q51.4968 884.917 51.4968 880.684 Q51.4968 880.08 51.5923 879.347 Q51.656 878.615 51.8151 877.724 L57.8307 877.692 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M56.4621 859.168 Q56.4621 863.879 60.1542 866.616 Q63.8145 869.353 70.212 869.353 Q76.6095 869.353 80.3017 866.648 Q83.9619 863.911 83.9619 859.168 Q83.9619 854.489 80.2698 851.752 Q76.5777 849.015 70.212 849.015 Q63.8781 849.015 60.186 851.752 Q56.4621 854.489 56.4621 859.168 M51.4968 859.168 Q51.4968 851.529 56.4621 847.169 Q61.4273 842.808 70.212 842.808 Q78.9649 842.808 83.9619 847.169 Q88.9272 851.529 88.9272 859.168 Q88.9272 866.839 83.9619 871.199 Q78.9649 875.528 70.212 875.528 Q61.4273 875.528 56.4621 871.199 Q51.4968 866.839 51.4968 859.168 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M53.4065 813.94 L58.9447 813.94 Q57.6716 816.422 57.035 819.096 Q56.3984 821.77 56.3984 824.634 Q56.3984 828.995 57.7352 831.191 Q59.072 833.355 61.7456 833.355 Q63.7826 833.355 64.9603 831.796 Q66.1061 830.236 67.1565 825.525 L67.6021 823.52 Q68.9389 817.282 71.3897 814.672 Q73.8086 812.03 78.1691 812.03 Q83.1344 812.03 86.0308 815.977 Q88.9272 819.892 88.9272 826.767 Q88.9272 829.631 88.3543 832.751 Q87.8132 835.838 86.6992 839.275 L80.6518 839.275 Q82.3387 836.029 83.198 832.878 Q84.0256 829.727 84.0256 826.639 Q84.0256 822.502 82.6251 820.274 Q81.1929 818.046 78.6147 818.046 Q76.2276 818.046 74.9545 819.669 Q73.6813 821.26 72.5037 826.703 L72.0262 828.74 Q70.8804 834.183 68.5251 836.602 Q66.138 839.021 62.0002 839.021 Q56.9713 839.021 54.2341 835.456 Q51.4968 831.891 51.4968 825.334 Q51.4968 822.088 51.9743 819.223 Q52.4517 816.359 53.4065 813.94 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M53.4065 783.162 L58.9447 783.162 Q57.6716 785.644 57.035 788.318 Q56.3984 790.991 56.3984 793.856 Q56.3984 798.217 57.7352 800.413 Q59.072 802.577 61.7456 802.577 Q63.7826 802.577 64.9603 801.017 Q66.1061 799.458 67.1565 794.747 L67.6021 792.742 Q68.9389 786.504 71.3897 783.894 Q73.8086 781.252 78.1691 781.252 Q83.1344 781.252 86.0308 785.199 Q88.9272 789.114 88.9272 795.989 Q88.9272 798.853 88.3543 801.972 Q87.8132 805.06 86.6992 808.497 L80.6518 808.497 Q82.3387 805.251 83.198 802.1 Q84.0256 798.949 84.0256 795.861 Q84.0256 791.724 82.6251 789.496 Q81.1929 787.268 78.6147 787.268 Q76.2276 787.268 74.9545 788.891 Q73.6813 790.482 72.5037 795.925 L72.0262 797.962 Q70.8804 803.405 68.5251 805.824 Q66.138 808.243 62.0002 808.243 Q56.9713 808.243 54.2341 804.678 Q51.4968 801.113 51.4968 794.556 Q51.4968 791.31 51.9743 788.445 Q52.4517 785.581 53.4065 783.162 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M40.4842 754.134 L40.4842 724.088 L45.895 724.088 L45.895 747.705 L59.9632 747.705 L59.9632 725.075 L65.3741 725.075 L65.3741 747.705 L82.5933 747.705 L82.5933 723.515 L88.0042 723.515 L88.0042 754.134 L40.4842 754.134 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M66.4881 687.74 L88.0042 687.74 L88.0042 693.596 L66.679 693.596 Q61.6183 693.596 59.1038 695.57 Q56.5894 697.543 56.5894 701.49 Q56.5894 706.232 59.6131 708.969 Q62.6368 711.707 67.8567 711.707 L88.0042 711.707 L88.0042 717.595 L52.3562 717.595 L52.3562 711.707 L57.8944 711.707 Q54.6797 709.606 53.0883 706.773 Q51.4968 703.909 51.4968 700.185 Q51.4968 694.042 55.3163 690.891 Q59.1038 687.74 66.4881 687.74 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M42.2347 675.804 L52.3562 675.804 L52.3562 663.741 L56.9077 663.741 L56.9077 675.804 L76.2594 675.804 Q80.6199 675.804 81.8613 674.626 Q83.1026 673.417 83.1026 669.757 L83.1026 663.741 L88.0042 663.741 L88.0042 669.757 Q88.0042 676.536 85.4897 679.114 Q82.9434 681.692 76.2594 681.692 L56.9077 681.692 L56.9077 685.989 L52.3562 685.989 L52.3562 681.692 L42.2347 681.692 L42.2347 675.804 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M57.8307 636.941 Q57.2578 637.928 57.0032 639.106 Q56.7167 640.252 56.7167 641.652 Q56.7167 646.617 59.9632 649.291 Q63.1779 651.933 69.2253 651.933 L88.0042 651.933 L88.0042 657.821 L52.3562 657.821 L52.3562 651.933 L57.8944 651.933 Q54.6479 650.087 53.0883 647.127 Q51.4968 644.166 51.4968 639.933 Q51.4968 639.329 51.5923 638.596 Q51.656 637.864 51.8151 636.973 L57.8307 636.941 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M56.4621 618.417 Q56.4621 623.128 60.1542 625.865 Q63.8145 628.602 70.212 628.602 Q76.6095 628.602 80.3017 625.897 Q83.9619 623.16 83.9619 618.417 Q83.9619 613.738 80.2698 611.001 Q76.5777 608.264 70.212 608.264 Q63.8781 608.264 60.186 611.001 Q56.4621 613.738 56.4621 618.417 M51.4968 618.417 Q51.4968 610.778 56.4621 606.418 Q61.4273 602.057 70.212 602.057 Q78.9649 602.057 83.9619 606.418 Q88.9272 610.778 88.9272 618.417 Q88.9272 626.088 83.9619 630.448 Q78.9649 634.777 70.212 634.777 Q61.4273 634.777 56.4621 630.448 Q51.4968 626.088 51.4968 618.417 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M82.657 590.249 L101.563 590.249 L101.563 596.137 L52.3562 596.137 L52.3562 590.249 L57.7671 590.249 Q54.5842 588.403 53.0564 585.602 Q51.4968 582.769 51.4968 578.854 Q51.4968 572.361 56.6531 568.319 Q61.8093 564.245 70.212 564.245 Q78.6147 564.245 83.771 568.319 Q88.9272 572.361 88.9272 578.854 Q88.9272 582.769 87.3994 585.602 Q85.8398 588.403 82.657 590.249 M70.212 570.324 Q63.7508 570.324 60.0905 572.998 Q56.3984 575.64 56.3984 580.287 Q56.3984 584.934 60.0905 587.607 Q63.7508 590.249 70.212 590.249 Q76.6732 590.249 80.3653 587.607 Q84.0256 584.934 84.0256 580.287 Q84.0256 575.64 80.3653 572.998 Q76.6732 570.324 70.212 570.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M91.3143 543.27 Q97.68 545.753 99.6216 548.108 Q101.563 550.463 101.563 554.41 L101.563 559.089 L96.6615 559.089 L96.6615 555.651 Q96.6615 553.232 95.5157 551.896 Q94.3699 550.559 90.1048 548.935 L87.4312 547.885 L52.3562 562.303 L52.3562 556.097 L80.238 544.957 L52.3562 533.817 L52.3562 527.61 L91.3143 543.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip082)\" style=\"stroke:#009af9; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  270.762,1199.82 290.975,1199.82 311.189,1199.82 331.402,1199.82 351.616,1199.82 371.829,1199.82 392.043,1199.82 412.256,1199.82 432.47,1199.82 452.683,1199.82 \n",
       "  472.897,1199.82 493.11,1199.82 513.324,1199.82 533.538,1199.82 553.751,1199.82 573.965,1199.82 594.178,1199.82 614.392,1199.82 634.605,1199.82 654.819,1199.82 \n",
       "  675.032,1199.82 695.246,1199.82 715.459,1199.82 735.673,1199.82 755.886,1199.82 776.1,1199.82 796.314,1199.82 816.527,1199.82 836.741,1199.82 856.954,1199.82 \n",
       "  877.168,1199.82 897.381,1199.82 917.595,1199.82 937.808,1199.82 958.022,1199.82 978.235,1199.82 998.449,1199.82 1018.66,1199.82 1038.88,1199.82 1059.09,1199.82 \n",
       "  1079.3,1199.82 1099.52,1199.82 1119.73,1199.82 1139.94,1199.82 1160.16,1199.82 1180.37,1199.82 1200.58,1199.82 1220.8,1199.82 1241.01,1199.82 1261.22,1199.82 \n",
       "  1281.44,1199.82 1301.65,1260.39 1321.87,1323.39 1342.08,1365.57 1362.29,1375.3 1382.51,1338.34 1402.72,1344.49 1422.93,1384.24 1443.15,1336.72 1463.36,1376.47 \n",
       "  1483.57,1336.72 1503.79,1231.35 1524,1231.35 1544.21,1195.84 1564.43,1023.97 1584.64,1023.97 1604.86,1063.72 1625.07,702.007 1645.28,701.596 1665.5,741.345 \n",
       "  1685.71,744.008 1705.92,747.795 1726.14,630.989 1746.35,591.913 1766.56,593.474 1786.78,593.503 1806.99,637.349 1827.2,529.177 1847.42,606.745 1867.63,487.287 \n",
       "  1887.84,526.946 1908.06,565.85 1928.27,527.186 1948.49,565.85 1968.7,527.186 1988.91,371.295 2009.13,378.589 2029.34,223.52 2049.55,461.344 2069.77,306.687 \n",
       "  2089.98,421.857 2110.19,538.673 2130.41,384.016 2150.62,312.026 2170.83,351.513 2191.05,318.994 2211.26,396.322 2231.47,357.658 2251.69,86.1857 2271.9,357.658 \n",
       "  2292.12,186.009 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"\n",
       "M1843.94 214.069 L2281.33 214.069 L2281.33 93.1086 L1843.94 93.1086  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1843.94,214.069 2281.33,214.069 2281.33,93.1086 1843.94,93.1086 1843.94,214.069 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1867.74,153.589 2010.59,153.589 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M2047.59 170.869 L2034.39 136.309 L2039.28 136.309 L2050.23 165.406 L2061.2 136.309 L2066.06 136.309 L2052.89 170.869 L2047.59 170.869 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2078.63 157.836 Q2073.47 157.836 2071.48 159.017 Q2069.49 160.197 2069.49 163.045 Q2069.49 165.313 2070.97 166.656 Q2072.47 167.975 2075.04 167.975 Q2078.58 167.975 2080.71 165.475 Q2082.86 162.952 2082.86 158.785 L2082.86 157.836 L2078.63 157.836 M2087.12 156.077 L2087.12 170.869 L2082.86 170.869 L2082.86 166.933 Q2081.41 169.295 2079.23 170.429 Q2077.05 171.54 2073.91 171.54 Q2069.93 171.54 2067.56 169.318 Q2065.23 167.072 2065.23 163.322 Q2065.23 158.947 2068.14 156.725 Q2071.08 154.503 2076.89 154.503 L2082.86 154.503 L2082.86 154.086 Q2082.86 151.147 2080.92 149.549 Q2079 147.929 2075.5 147.929 Q2073.28 147.929 2071.18 148.461 Q2069.07 148.994 2067.12 150.059 L2067.12 146.123 Q2069.46 145.221 2071.66 144.781 Q2073.86 144.318 2075.94 144.318 Q2081.57 144.318 2084.35 147.234 Q2087.12 150.151 2087.12 156.077 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2091.59 134.85 L2095.85 134.85 L2095.85 170.869 L2091.59 170.869 L2091.59 134.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2100.32 144.943 L2104.58 144.943 L2104.58 170.869 L2100.32 170.869 L2100.32 144.943 M2100.32 134.85 L2104.58 134.85 L2104.58 140.244 L2100.32 140.244 L2100.32 134.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2126.11 148.878 L2126.11 134.85 L2130.36 134.85 L2130.36 170.869 L2126.11 170.869 L2126.11 166.98 Q2124.76 169.295 2122.7 170.429 Q2120.67 171.54 2117.8 171.54 Q2113.1 171.54 2110.13 167.79 Q2107.19 164.04 2107.19 157.929 Q2107.19 151.818 2110.13 148.068 Q2113.1 144.318 2117.8 144.318 Q2120.67 144.318 2122.7 145.452 Q2124.76 146.563 2126.11 148.878 M2111.59 157.929 Q2111.59 162.628 2113.51 165.313 Q2115.46 167.975 2118.84 167.975 Q2122.22 167.975 2124.16 165.313 Q2126.11 162.628 2126.11 157.929 Q2126.11 153.23 2124.16 150.568 Q2122.22 147.883 2118.84 147.883 Q2115.46 147.883 2113.51 150.568 Q2111.59 153.23 2111.59 157.929 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2146.61 157.836 Q2141.45 157.836 2139.46 159.017 Q2137.47 160.197 2137.47 163.045 Q2137.47 165.313 2138.95 166.656 Q2140.46 167.975 2143.03 167.975 Q2146.57 167.975 2148.7 165.475 Q2150.85 162.952 2150.85 158.785 L2150.85 157.836 L2146.61 157.836 M2155.11 156.077 L2155.11 170.869 L2150.85 170.869 L2150.85 166.933 Q2149.39 169.295 2147.22 170.429 Q2145.04 171.54 2141.89 171.54 Q2137.91 171.54 2135.55 169.318 Q2133.21 167.072 2133.21 163.322 Q2133.21 158.947 2136.13 156.725 Q2139.07 154.503 2144.88 154.503 L2150.85 154.503 L2150.85 154.086 Q2150.85 151.147 2148.91 149.549 Q2146.98 147.929 2143.49 147.929 Q2141.27 147.929 2139.16 148.461 Q2137.05 148.994 2135.11 150.059 L2135.11 146.123 Q2137.45 145.221 2139.65 144.781 Q2141.85 144.318 2143.93 144.318 Q2149.55 144.318 2152.33 147.234 Q2155.11 150.151 2155.11 156.077 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2163.79 137.582 L2163.79 144.943 L2172.56 144.943 L2172.56 148.253 L2163.79 148.253 L2163.79 162.327 Q2163.79 165.498 2164.65 166.401 Q2165.53 167.304 2168.19 167.304 L2172.56 167.304 L2172.56 170.869 L2168.19 170.869 Q2163.26 170.869 2161.38 169.04 Q2159.51 167.188 2159.51 162.327 L2159.51 148.253 L2156.38 148.253 L2156.38 144.943 L2159.51 144.943 L2159.51 137.582 L2163.79 137.582 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2177.03 144.943 L2181.29 144.943 L2181.29 170.869 L2177.03 170.869 L2177.03 144.943 M2177.03 134.85 L2181.29 134.85 L2181.29 140.244 L2177.03 140.244 L2177.03 134.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2195.8 147.929 Q2192.38 147.929 2190.39 150.614 Q2188.4 153.276 2188.4 157.929 Q2188.4 162.582 2190.36 165.267 Q2192.36 167.929 2195.8 167.929 Q2199.21 167.929 2201.2 165.244 Q2203.19 162.558 2203.19 157.929 Q2203.19 153.322 2201.2 150.637 Q2199.21 147.929 2195.8 147.929 M2195.8 144.318 Q2201.36 144.318 2204.53 147.929 Q2207.7 151.54 2207.7 157.929 Q2207.7 164.295 2204.53 167.929 Q2201.36 171.54 2195.8 171.54 Q2190.23 171.54 2187.05 167.929 Q2183.91 164.295 2183.91 157.929 Q2183.91 151.54 2187.05 147.929 Q2190.23 144.318 2195.8 144.318 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2233.72 155.221 L2233.72 170.869 L2229.46 170.869 L2229.46 155.359 Q2229.46 151.679 2228.03 149.85 Q2226.59 148.022 2223.72 148.022 Q2220.27 148.022 2218.28 150.221 Q2216.29 152.42 2216.29 156.216 L2216.29 170.869 L2212.01 170.869 L2212.01 144.943 L2216.29 144.943 L2216.29 148.971 Q2217.82 146.633 2219.88 145.475 Q2221.96 144.318 2224.67 144.318 Q2229.14 144.318 2231.43 147.096 Q2233.72 149.85 2233.72 155.221 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(curve.parameter_values,\n",
    "     curve.measurements,\n",
    "     xlab=curve.parameter_name,\n",
    "     ylab=\"Cross Entropy\",\n",
    "     label=\"Validation\", lw=2)\n",
    "# plot!(Net2.report.training_losses, label=\"Training\", lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.38403"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = round(minimum(curve.measurements), digits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJBase.NumericRange(Float64, :merge_purity_threshold, ... )"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param1 = :merge_purity_threshold\n",
    "\n",
    "r1 = range(dt2, param1, lower=0, upper=1, scale=:linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "    model = DecisionTreeClassifier(\n",
       "            max_depth = -1,\n",
       "            min_samples_leaf = 1,\n",
       "            min_samples_split = 2,\n",
       "            min_purity_increase = 0.0,\n",
       "            n_subfeatures = 0,\n",
       "            post_prune = true,\n",
       "            merge_purity_threshold = 1.0,\n",
       "            pdf_smoothing = 0.0,\n",
       "            display_depth = 5),\n",
       "    tuning = Grid(\n",
       "            goal = 100,\n",
       "            resolution = 10,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = CV(\n",
       "            nfolds = 6,\n",
       "            shuffle = false,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = cross_entropy(\n",
       "            eps = 2.220446049250313e-16),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = MLJBase.NumericRange{Float64,MLJBase.Bounded,Symbol}[\u001b[34mNumericRange{Float64,} @027\u001b[39m],\n",
       "    train_best = true,\n",
       "    repeats = 1,\n",
       "    n = nothing,\n",
       "    acceleration = CPUThreads{Int64}(1),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true)\u001b[34m @807\u001b[39m"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_dt_model = TunedModel(model=dt2,\n",
    "                                    tuning=Grid(goal=100),\n",
    "                                    resampling=CV(), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,}} @113\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @988\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @700\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_dt = machine(self_tuning_dt_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,}} @113\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      " Info: Attempting to evaluate 100 models.\n",
      " @ MLJTuning /home/andrew/.julia/packages/MLJTuning/Bbgvk/src/tuned_models.jl:494\n",
      "\u001b[33mEvaluating over 100 metamodels: 100%[=========================] Time: 0:00:02\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,}} @113\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @988\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @700\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = fit!(self_tuning_dt, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(best_model = \u001b[34mDecisionTreeClassifier @840\u001b[39m,\n",
       " best_fitted_params = (tree = Decision Tree\n",
       "Leaves: 5\n",
       "Depth:  4,\n",
       "                       encoding = Dict{CategoricalValue{String,UInt32},UInt32}(\"B\" => 0x00000001,\"M\" => 0x00000002),),)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = fitted_params(self_tuning_dt)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = true,\n",
       "    merge_purity_threshold = 0.5050505050505051,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 5)\u001b[34m @840\u001b[39m"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.50505"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss = round(z.report.best_result.measurement[1],digits=5)\n",
    "best_mpt = round(best.best_model.merge_purity_threshold,digits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"Figures/LearningCurve_DT_merge_purity_thresh:$(best_mpt)_loss:$(best_loss)\"\n",
    "png(replace(fn,'.' => ','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (10, 0.7, 0.6625)\n",
      "(d, train_metric, valid_metric) = (11, 0.7272727272727273, 0.6625)\n",
      "(d, train_metric, valid_metric) = (12, 0.6666666666666666, 0.6625)\n",
      "(d, train_metric, valid_metric) = (13, 0.6923076923076923, 0.6625)\n",
      "(d, train_metric, valid_metric) = (14, 0.6428571428571429, 0.6625)\n",
      "(d, train_metric, valid_metric) = (15, 0.6, 0.6625)\n",
      "(d, train_metric, valid_metric) = (16, 0.5625, 0.6625)\n",
      "(d, train_metric, valid_metric) = (17, 0.5294117647058824, 0.6625)\n",
      "(d, train_metric, valid_metric) = (18, 0.5555555555555556, 0.6625)\n",
      "(d, train_metric, valid_metric) = (19, 0.5789473684210527, 0.6625)\n",
      "(d, train_metric, valid_metric) = (20, 0.6, 0.6625)\n",
      "(d, train_metric, valid_metric) = (21, 0.5714285714285714, 0.6625)\n",
      "(d, train_metric, valid_metric) = (22, 0.5454545454545454, 0.6625)\n",
      "(d, train_metric, valid_metric) = (23, 0.5652173913043478, 0.6625)\n",
      "(d, train_metric, valid_metric) = (24, 0.5833333333333334, 0.6625)\n",
      "(d, train_metric, valid_metric) = (25, 0.6, 0.6625)\n",
      "(d, train_metric, valid_metric) = (26, 0.6153846153846154, 0.6625)\n",
      "(d, train_metric, valid_metric) = (27, 0.5925925925925926, 0.6625)\n",
      "(d, train_metric, valid_metric) = (28, 0.6071428571428571, 0.6625)\n",
      "(d, train_metric, valid_metric) = (29, 0.6206896551724138, 0.6625)\n",
      "(d, train_metric, valid_metric) = (30, 0.6333333333333333, 0.6625)\n",
      "(d, train_metric, valid_metric) = (31, 0.6451612903225806, 0.6625)\n",
      "(d, train_metric, valid_metric) = (32, 0.625, 0.6625)\n",
      "(d, train_metric, valid_metric) = (33, 0.6060606060606061, 0.6625)\n",
      "(d, train_metric, valid_metric) = (34, 0.5882352941176471, 0.6625)\n",
      "(d, train_metric, valid_metric) = (35, 0.6, 0.6625)\n",
      "(d, train_metric, valid_metric) = (36, 0.6111111111111112, 0.6625)\n",
      "(d, train_metric, valid_metric) = (37, 0.6216216216216216, 0.6625)\n",
      "(d, train_metric, valid_metric) = (38, 0.6052631578947368, 0.6625)\n",
      "(d, train_metric, valid_metric) = (39, 0.6153846153846154, 0.6625)\n",
      "(d, train_metric, valid_metric) = (40, 0.625, 0.6625)\n",
      "(d, train_metric, valid_metric) = (41, 0.6341463414634146, 0.6625)\n",
      "(d, train_metric, valid_metric) = (42, 0.6428571428571429, 0.6625)\n",
      "(d, train_metric, valid_metric) = (43, 0.627906976744186, 0.6625)\n",
      "(d, train_metric, valid_metric) = (44, 0.6363636363636364, 0.6625)\n",
      "(d, train_metric, valid_metric) = (45, 0.6222222222222222, 0.6625)\n",
      "(d, train_metric, valid_metric) = (46, 0.6304347826086957, 0.6625)\n",
      "(d, train_metric, valid_metric) = (47, 0.6382978723404256, 0.6625)\n",
      "(d, train_metric, valid_metric) = (48, 0.6458333333333334, 0.6625)\n",
      "(d, train_metric, valid_metric) = (49, 0.6326530612244898, 0.6625)\n",
      "(d, train_metric, valid_metric) = (50, 0.64, 0.6625)\n",
      "(d, train_metric, valid_metric) = (51, 0.6470588235294118, 0.6625)\n",
      "(d, train_metric, valid_metric) = (52, 0.6538461538461539, 0.6625)\n",
      "(d, train_metric, valid_metric) = (53, 0.6415094339622641, 0.6625)\n",
      "(d, train_metric, valid_metric) = (54, 0.6481481481481481, 0.6625)\n",
      "(d, train_metric, valid_metric) = (55, 0.6545454545454545, 0.6625)\n",
      "(d, train_metric, valid_metric) = (56, 0.6607142857142857, 0.6625)\n",
      "(d, train_metric, valid_metric) = (57, 0.9824561403508771, 0.9)\n",
      "(d, train_metric, valid_metric) = (58, 0.9827586206896551, 0.8875)\n",
      "(d, train_metric, valid_metric) = (59, 0.9830508474576272, 0.8875)\n",
      "(d, train_metric, valid_metric) = (60, 0.9833333333333333, 0.8875)\n",
      "(d, train_metric, valid_metric) = (61, 0.9836065573770492, 0.9)\n",
      "(d, train_metric, valid_metric) = (62, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (63, 1.0, 0.8875)\n",
      "(d, train_metric, valid_metric) = (64, 1.0, 0.8875)\n",
      "(d, train_metric, valid_metric) = (65, 0.9538461538461539, 0.8875)\n",
      "(d, train_metric, valid_metric) = (66, 0.9545454545454546, 0.9)\n",
      "(d, train_metric, valid_metric) = (67, 0.9552238805970149, 0.9)\n",
      "(d, train_metric, valid_metric) = (68, 0.9558823529411765, 0.9)\n",
      "(d, train_metric, valid_metric) = (69, 0.9565217391304348, 0.925)\n",
      "(d, train_metric, valid_metric) = (70, 0.9571428571428572, 0.925)\n",
      "(d, train_metric, valid_metric) = (71, 0.9577464788732394, 0.9125)\n",
      "(d, train_metric, valid_metric) = (72, 0.9583333333333334, 0.8875)\n",
      "(d, train_metric, valid_metric) = (73, 0.958904109589041, 0.9125)\n",
      "(d, train_metric, valid_metric) = (74, 0.5945945945945946, 0.6625)\n",
      "(d, train_metric, valid_metric) = (75, 0.5866666666666667, 0.6625)\n",
      "(d, train_metric, valid_metric) = (76, 0.5921052631578947, 0.6625)\n",
      "(d, train_metric, valid_metric) = (77, 0.5974025974025974, 0.6625)\n",
      "(d, train_metric, valid_metric) = (78, 0.6025641025641025, 0.6625)\n",
      "(d, train_metric, valid_metric) = (79, 0.5949367088607594, 0.6625)\n",
      "(d, train_metric, valid_metric) = (80, 0.6, 0.6625)\n",
      "(d, train_metric, valid_metric) = (81, 0.5925925925925926, 0.6625)\n",
      "(d, train_metric, valid_metric) = (82, 0.5975609756097561, 0.6625)\n",
      "(d, train_metric, valid_metric) = (83, 0.6024096385542169, 0.6625)\n",
      "(d, train_metric, valid_metric) = (84, 0.5952380952380952, 0.6625)\n",
      "(d, train_metric, valid_metric) = (85, 0.6, 0.6625)\n",
      "(d, train_metric, valid_metric) = (86, 0.6046511627906976, 0.6625)\n",
      "(d, train_metric, valid_metric) = (87, 0.6091954022988506, 0.6625)\n",
      "(d, train_metric, valid_metric) = (88, 0.6136363636363636, 0.6625)\n",
      "(d, train_metric, valid_metric) = (89, 0.6067415730337079, 0.6625)\n",
      "(d, train_metric, valid_metric) = (90, 0.6, 0.6625)\n",
      "(d, train_metric, valid_metric) = (91, 0.6043956043956044, 0.6625)\n",
      "(d, train_metric, valid_metric) = (92, 0.5978260869565217, 0.6625)\n",
      "(d, train_metric, valid_metric) = (93, 0.6021505376344086, 0.6625)\n",
      "(d, train_metric, valid_metric) = (94, 0.5957446808510638, 0.6625)\n",
      "(d, train_metric, valid_metric) = (95, 0.5894736842105263, 0.6625)\n",
      "(d, train_metric, valid_metric) = (96, 0.59375, 0.6625)\n",
      "(d, train_metric, valid_metric) = (97, 0.5979381443298969, 0.6625)\n",
      "(d, train_metric, valid_metric) = (98, 0.6020408163265306, 0.6625)\n",
      "(d, train_metric, valid_metric) = (99, 0.6060606060606061, 0.6625)\n",
      "(d, train_metric, valid_metric) = (100, 0.6, 0.6625)\n",
      "(d, train_metric, valid_metric) = (101, 0.6039603960396039, 0.6625)\n",
      "(d, train_metric, valid_metric) = (102, 0.6078431372549019, 0.6625)\n",
      "(d, train_metric, valid_metric) = (103, 0.6116504854368932, 0.6625)\n",
      "(d, train_metric, valid_metric) = (104, 0.6153846153846154, 0.6625)\n",
      "(d, train_metric, valid_metric) = (105, 0.6190476190476191, 0.6625)\n",
      "(d, train_metric, valid_metric) = (106, 0.6132075471698113, 0.6625)\n",
      "(d, train_metric, valid_metric) = (107, 0.616822429906542, 0.6625)\n",
      "(d, train_metric, valid_metric) = (108, 0.6111111111111112, 0.6625)\n",
      "(d, train_metric, valid_metric) = (109, 0.6055045871559633, 0.6625)\n",
      "(d, train_metric, valid_metric) = (110, 0.6090909090909091, 0.6625)\n",
      "(d, train_metric, valid_metric) = (111, 0.6036036036036037, 0.6625)\n",
      "(d, train_metric, valid_metric) = (112, 0.5982142857142857, 0.6625)\n",
      "(d, train_metric, valid_metric) = (113, 0.6017699115044248, 0.6625)\n",
      "(d, train_metric, valid_metric) = (114, 0.5964912280701754, 0.6625)\n",
      "(d, train_metric, valid_metric) = (115, 0.6, 0.6625)\n",
      "(d, train_metric, valid_metric) = (116, 0.603448275862069, 0.6625)\n",
      "(d, train_metric, valid_metric) = (117, 0.6068376068376068, 0.6625)\n",
      "(d, train_metric, valid_metric) = (118, 0.6016949152542372, 0.6625)\n",
      "(d, train_metric, valid_metric) = (119, 0.6050420168067226, 0.6625)\n",
      "(d, train_metric, valid_metric) = (120, 0.6, 0.6625)\n",
      "(d, train_metric, valid_metric) = (121, 0.5950413223140496, 0.6625)\n",
      "(d, train_metric, valid_metric) = (122, 0.5983606557377049, 0.6625)\n",
      "(d, train_metric, valid_metric) = (123, 0.5934959349593496, 0.6625)\n",
      "(d, train_metric, valid_metric) = (124, 0.5967741935483871, 0.6625)\n",
      "(d, train_metric, valid_metric) = (125, 0.592, 0.6625)\n",
      "(d, train_metric, valid_metric) = (126, 0.5952380952380952, 0.6625)\n",
      "(d, train_metric, valid_metric) = (127, 0.5905511811023622, 0.6625)\n",
      "(d, train_metric, valid_metric) = (128, 0.5859375, 0.6625)\n",
      "(d, train_metric, valid_metric) = (129, 0.5813953488372093, 0.6625)\n",
      "(d, train_metric, valid_metric) = (130, 0.5846153846153846, 0.6625)\n",
      "(d, train_metric, valid_metric) = (131, 0.5801526717557252, 0.6625)\n",
      "(d, train_metric, valid_metric) = (132, 0.5833333333333334, 0.6625)\n",
      "(d, train_metric, valid_metric) = (133, 0.5864661654135338, 0.6625)\n",
      "(d, train_metric, valid_metric) = (134, 0.5895522388059702, 0.6625)\n",
      "(d, train_metric, valid_metric) = (135, 0.5851851851851851, 0.6625)\n",
      "(d, train_metric, valid_metric) = (136, 0.5882352941176471, 0.6625)\n",
      "(d, train_metric, valid_metric) = (137, 0.5912408759124088, 0.6625)\n",
      "(d, train_metric, valid_metric) = (138, 0.5942028985507246, 0.6625)\n",
      "(d, train_metric, valid_metric) = (139, 0.5899280575539568, 0.6625)\n",
      "(d, train_metric, valid_metric) = (140, 0.5928571428571429, 0.6625)\n",
      "(d, train_metric, valid_metric) = (141, 0.5957446808510638, 0.6625)\n",
      "(d, train_metric, valid_metric) = (142, 0.5915492957746479, 0.6625)\n",
      "(d, train_metric, valid_metric) = (143, 0.5944055944055944, 0.6625)\n",
      "(d, train_metric, valid_metric) = (144, 0.5972222222222222, 0.6625)\n",
      "(d, train_metric, valid_metric) = (145, 0.593103448275862, 0.6625)\n",
      "(d, train_metric, valid_metric) = (146, 0.5958904109589042, 0.6625)\n",
      "(d, train_metric, valid_metric) = (147, 0.5986394557823129, 0.6625)\n",
      "(d, train_metric, valid_metric) = (148, 0.6013513513513513, 0.6625)\n",
      "(d, train_metric, valid_metric) = (149, 0.5973154362416108, 0.6625)\n",
      "(d, train_metric, valid_metric) = (150, 0.6, 0.6625)\n",
      "(d, train_metric, valid_metric) = (151, 0.6026490066225165, 0.6625)\n",
      "(d, train_metric, valid_metric) = (152, 0.6052631578947368, 0.6625)\n",
      "(d, train_metric, valid_metric) = (153, 0.6078431372549019, 0.6625)\n",
      "(d, train_metric, valid_metric) = (154, 0.6103896103896104, 0.6625)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (155, 0.6064516129032258, 0.6625)\n",
      "(d, train_metric, valid_metric) = (156, 0.6089743589743589, 0.6625)\n",
      "(d, train_metric, valid_metric) = (157, 0.6050955414012739, 0.6625)\n",
      "(d, train_metric, valid_metric) = (158, 0.6012658227848101, 0.6625)\n",
      "(d, train_metric, valid_metric) = (159, 0.6037735849056604, 0.6625)\n",
      "(d, train_metric, valid_metric) = (160, 0.60625, 0.6625)\n",
      "(d, train_metric, valid_metric) = (161, 0.6086956521739131, 0.6625)\n",
      "(d, train_metric, valid_metric) = (162, 0.6049382716049383, 0.6625)\n",
      "(d, train_metric, valid_metric) = (163, 0.6073619631901841, 0.6625)\n",
      "(d, train_metric, valid_metric) = (164, 0.6036585365853658, 0.6625)\n",
      "(d, train_metric, valid_metric) = (165, 0.6, 0.6625)\n",
      "(d, train_metric, valid_metric) = (166, 0.5963855421686747, 0.6625)\n",
      "(d, train_metric, valid_metric) = (167, 0.5988023952095808, 0.6625)\n",
      "(d, train_metric, valid_metric) = (168, 0.6011904761904762, 0.6625)\n",
      "(d, train_metric, valid_metric) = (169, 0.5976331360946746, 0.6625)\n",
      "(d, train_metric, valid_metric) = (170, 0.6, 0.6625)\n",
      "(d, train_metric, valid_metric) = (171, 0.5964912280701754, 0.6625)\n",
      "(d, train_metric, valid_metric) = (172, 0.5930232558139535, 0.6625)\n",
      "(d, train_metric, valid_metric) = (173, 0.5895953757225434, 0.6625)\n",
      "(d, train_metric, valid_metric) = (174, 0.5862068965517241, 0.6625)\n",
      "(d, train_metric, valid_metric) = (175, 0.5885714285714285, 0.6625)\n",
      "(d, train_metric, valid_metric) = (176, 0.5909090909090909, 0.6625)\n",
      "(d, train_metric, valid_metric) = (177, 0.5932203389830508, 0.6625)\n",
      "(d, train_metric, valid_metric) = (178, 0.5955056179775281, 0.6625)\n",
      "(d, train_metric, valid_metric) = (179, 0.5921787709497207, 0.6625)\n",
      "(d, train_metric, valid_metric) = (180, 0.5888888888888889, 0.6625)\n",
      "(d, train_metric, valid_metric) = (181, 0.5911602209944752, 0.6625)\n",
      "(d, train_metric, valid_metric) = (182, 0.9505494505494505, 0.875)\n",
      "(d, train_metric, valid_metric) = (183, 0.9508196721311475, 0.9375)\n",
      "(d, train_metric, valid_metric) = (184, 0.9510869565217391, 0.875)\n",
      "(d, train_metric, valid_metric) = (185, 0.9513513513513514, 0.9375)\n",
      "(d, train_metric, valid_metric) = (186, 0.5967741935483871, 0.6625)\n",
      "(d, train_metric, valid_metric) = (187, 0.5989304812834224, 0.6625)\n",
      "(d, train_metric, valid_metric) = (188, 0.601063829787234, 0.6625)\n",
      "(d, train_metric, valid_metric) = (189, 0.5978835978835979, 0.6625)\n",
      "(d, train_metric, valid_metric) = (190, 0.6, 0.6625)\n",
      "(d, train_metric, valid_metric) = (191, 0.5968586387434555, 0.6625)\n",
      "(d, train_metric, valid_metric) = (192, 0.5989583333333334, 0.6625)\n",
      "(d, train_metric, valid_metric) = (193, 0.6010362694300518, 0.6625)\n",
      "(d, train_metric, valid_metric) = (194, 0.9484536082474226, 0.9375)\n",
      "(d, train_metric, valid_metric) = (195, 0.9487179487179487, 0.9125)\n",
      "(d, train_metric, valid_metric) = (196, 0.9489795918367347, 0.925)\n",
      "(d, train_metric, valid_metric) = (197, 0.949238578680203, 0.925)\n",
      "(d, train_metric, valid_metric) = (198, 0.9494949494949495, 0.9125)\n",
      "(d, train_metric, valid_metric) = (199, 0.949748743718593, 0.9375)\n",
      "(d, train_metric, valid_metric) = (200, 0.95, 0.925)\n",
      "(d, train_metric, valid_metric) = (201, 0.945273631840796, 0.925)\n",
      "(d, train_metric, valid_metric) = (202, 0.9405940594059405, 0.925)\n",
      "(d, train_metric, valid_metric) = (203, 0.9408866995073891, 0.9125)\n",
      "(d, train_metric, valid_metric) = (204, 0.9411764705882353, 0.9375)\n",
      "(d, train_metric, valid_metric) = (205, 0.9414634146341463, 0.9125)\n",
      "(d, train_metric, valid_metric) = (206, 0.941747572815534, 0.925)\n",
      "(d, train_metric, valid_metric) = (207, 0.9420289855072463, 0.925)\n",
      "(d, train_metric, valid_metric) = (208, 0.9423076923076923, 0.9125)\n",
      "(d, train_metric, valid_metric) = (209, 0.9425837320574163, 0.9125)\n",
      "(d, train_metric, valid_metric) = (210, 0.9428571428571428, 0.9125)\n",
      "(d, train_metric, valid_metric) = (211, 0.943127962085308, 0.9125)\n",
      "(d, train_metric, valid_metric) = (212, 0.9433962264150944, 0.925)\n",
      "(d, train_metric, valid_metric) = (213, 0.5915492957746479, 0.6625)\n",
      "(d, train_metric, valid_metric) = (214, 0.5887850467289719, 0.6625)\n",
      "(d, train_metric, valid_metric) = (215, 0.5906976744186047, 0.6625)\n",
      "(d, train_metric, valid_metric) = (216, 0.5925925925925926, 0.6625)\n",
      "(d, train_metric, valid_metric) = (217, 0.5944700460829493, 0.6625)\n",
      "(d, train_metric, valid_metric) = (218, 0.5963302752293578, 0.6625)\n",
      "(d, train_metric, valid_metric) = (219, 0.593607305936073, 0.6625)\n",
      "(d, train_metric, valid_metric) = (220, 0.5909090909090909, 0.6625)\n",
      "(d, train_metric, valid_metric) = (221, 0.5927601809954751, 0.6625)\n",
      "(d, train_metric, valid_metric) = (222, 0.5945945945945946, 0.6625)\n",
      "(d, train_metric, valid_metric) = (223, 0.5964125560538116, 0.6625)\n",
      "(d, train_metric, valid_metric) = (224, 0.5982142857142857, 0.6625)\n",
      "(d, train_metric, valid_metric) = (225, 0.6, 0.6625)\n",
      "(d, train_metric, valid_metric) = (226, 0.6017699115044248, 0.6625)\n",
      "(d, train_metric, valid_metric) = (227, 0.6035242290748899, 0.6625)\n",
      "(d, train_metric, valid_metric) = (228, 0.6052631578947368, 0.6625)\n",
      "(d, train_metric, valid_metric) = (229, 0.6026200873362445, 0.6625)\n",
      "(d, train_metric, valid_metric) = (230, 0.6043478260869565, 0.6625)\n",
      "(d, train_metric, valid_metric) = (231, 0.6060606060606061, 0.6625)\n",
      "(d, train_metric, valid_metric) = (232, 0.6077586206896551, 0.6625)\n",
      "(d, train_metric, valid_metric) = (233, 0.6094420600858369, 0.6625)\n",
      "(d, train_metric, valid_metric) = (234, 0.6068376068376068, 0.6625)\n",
      "(d, train_metric, valid_metric) = (235, 0.6085106382978723, 0.6625)\n",
      "(d, train_metric, valid_metric) = (236, 0.6101694915254238, 0.6625)\n",
      "(d, train_metric, valid_metric) = (237, 0.6075949367088608, 0.6625)\n",
      "(d, train_metric, valid_metric) = (238, 0.6050420168067226, 0.6625)\n",
      "(d, train_metric, valid_metric) = (239, 0.606694560669456, 0.6625)\n",
      "(d, train_metric, valid_metric) = (240, 0.6041666666666666, 0.6625)\n",
      "(d, train_metric, valid_metric) = (241, 0.6058091286307054, 0.6625)\n",
      "(d, train_metric, valid_metric) = (242, 0.6074380165289256, 0.6625)\n",
      "(d, train_metric, valid_metric) = (243, 0.6090534979423868, 0.6625)\n",
      "(d, train_metric, valid_metric) = (244, 0.610655737704918, 0.6625)\n",
      "(d, train_metric, valid_metric) = (245, 0.6081632653061224, 0.6625)\n",
      "(d, train_metric, valid_metric) = (246, 0.6097560975609756, 0.6625)\n",
      "(d, train_metric, valid_metric) = (247, 0.9676113360323887, 0.9)\n",
      "(d, train_metric, valid_metric) = (248, 0.967741935483871, 0.9)\n",
      "(d, train_metric, valid_metric) = (249, 0.9678714859437751, 0.9125)\n",
      "(d, train_metric, valid_metric) = (250, 0.608, 0.6625)\n",
      "(d, train_metric, valid_metric) = (251, 0.6095617529880478, 0.6625)\n",
      "(d, train_metric, valid_metric) = (252, 0.6111111111111112, 0.6625)\n",
      "(d, train_metric, valid_metric) = (253, 0.6126482213438735, 0.6625)\n",
      "(d, train_metric, valid_metric) = (254, 0.610236220472441, 0.6625)\n",
      "(d, train_metric, valid_metric) = (255, 0.611764705882353, 0.6625)\n",
      "(d, train_metric, valid_metric) = (256, 0.61328125, 0.6625)\n",
      "(d, train_metric, valid_metric) = (257, 0.6147859922178989, 0.6625)\n",
      "(d, train_metric, valid_metric) = (258, 0.6162790697674418, 0.6625)\n",
      "(d, train_metric, valid_metric) = (259, 0.6138996138996139, 0.6625)\n",
      "(d, train_metric, valid_metric) = (260, 0.6115384615384616, 0.6625)\n",
      "(d, train_metric, valid_metric) = (261, 0.6130268199233716, 0.6625)\n",
      "(d, train_metric, valid_metric) = (262, 0.6145038167938931, 0.6625)\n",
      "(d, train_metric, valid_metric) = (263, 0.6159695817490495, 0.6625)\n",
      "(d, train_metric, valid_metric) = (264, 0.6174242424242424, 0.6625)\n",
      "(d, train_metric, valid_metric) = (265, 0.6150943396226415, 0.6625)\n",
      "(d, train_metric, valid_metric) = (266, 0.6127819548872181, 0.6625)\n",
      "(d, train_metric, valid_metric) = (267, 0.6142322097378277, 0.6625)\n",
      "(d, train_metric, valid_metric) = (268, 0.6156716417910447, 0.6625)\n",
      "(d, train_metric, valid_metric) = (269, 0.6171003717472119, 0.6625)\n",
      "(d, train_metric, valid_metric) = (270, 0.6185185185185185, 0.6625)\n",
      "(d, train_metric, valid_metric) = (271, 0.6199261992619927, 0.6625)\n",
      "(d, train_metric, valid_metric) = (272, 0.6213235294117647, 0.6625)\n",
      "(d, train_metric, valid_metric) = (273, 0.6227106227106227, 0.6625)\n",
      "(d, train_metric, valid_metric) = (274, 0.6204379562043796, 0.6625)\n",
      "(d, train_metric, valid_metric) = (275, 0.6218181818181818, 0.6625)\n",
      "(d, train_metric, valid_metric) = (276, 0.6195652173913043, 0.6625)\n",
      "(d, train_metric, valid_metric) = (277, 0.6209386281588448, 0.6625)\n",
      "(d, train_metric, valid_metric) = (278, 0.6187050359712231, 0.6625)\n",
      "(d, train_metric, valid_metric) = (279, 0.6200716845878136, 0.6625)\n",
      "(d, train_metric, valid_metric) = (280, 0.6214285714285714, 0.6625)\n",
      "(d, train_metric, valid_metric) = (281, 0.6192170818505338, 0.6625)\n",
      "(d, train_metric, valid_metric) = (282, 0.6205673758865248, 0.6625)\n",
      "(d, train_metric, valid_metric) = (283, 0.6183745583038869, 0.6625)\n",
      "(d, train_metric, valid_metric) = (284, 0.6161971830985915, 0.6625)\n",
      "(d, train_metric, valid_metric) = (285, 0.6140350877192983, 0.6625)\n",
      "(d, train_metric, valid_metric) = (286, 0.6153846153846154, 0.6625)\n",
      "(d, train_metric, valid_metric) = (287, 0.6132404181184669, 0.6625)\n",
      "(d, train_metric, valid_metric) = (288, 0.6145833333333334, 0.6625)\n",
      "(d, train_metric, valid_metric) = (289, 0.615916955017301, 0.6625)\n",
      "(d, train_metric, valid_metric) = (290, 0.6137931034482759, 0.6625)\n",
      "(d, train_metric, valid_metric) = (291, 0.6151202749140894, 0.6625)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (292, 0.6164383561643836, 0.6625)\n",
      "(d, train_metric, valid_metric) = (293, 0.6143344709897611, 0.6625)\n",
      "(d, train_metric, valid_metric) = (294, 0.6122448979591837, 0.6625)\n",
      "(d, train_metric, valid_metric) = (295, 0.6135593220338983, 0.6625)\n",
      "(d, train_metric, valid_metric) = (296, 0.6148648648648649, 0.6625)\n",
      "(d, train_metric, valid_metric) = (297, 0.6161616161616161, 0.6625)\n",
      "(d, train_metric, valid_metric) = (298, 0.6140939597315436, 0.6625)\n",
      "(d, train_metric, valid_metric) = (299, 0.6153846153846154, 0.6625)\n",
      "(d, train_metric, valid_metric) = (300, 0.6166666666666667, 0.6625)\n",
      "(d, train_metric, valid_metric) = (301, 0.6146179401993356, 0.6625)\n",
      "(d, train_metric, valid_metric) = (302, 0.6158940397350994, 0.6625)\n",
      "(d, train_metric, valid_metric) = (303, 0.6138613861386139, 0.6625)\n",
      "(d, train_metric, valid_metric) = (304, 0.6151315789473685, 0.6625)\n",
      "(d, train_metric, valid_metric) = (305, 0.6163934426229508, 0.6625)\n",
      "(d, train_metric, valid_metric) = (306, 0.6143790849673203, 0.6625)\n",
      "(d, train_metric, valid_metric) = (307, 0.6156351791530945, 0.6625)\n",
      "(d, train_metric, valid_metric) = (308, 0.6168831168831169, 0.6625)\n",
      "(d, train_metric, valid_metric) = (309, 0.6148867313915858, 0.6625)\n",
      "(d, train_metric, valid_metric) = (310, 0.6161290322580645, 0.6625)\n",
      "(d, train_metric, valid_metric) = (311, 0.6141479099678456, 0.6625)\n",
      "(d, train_metric, valid_metric) = (312, 0.6121794871794872, 0.6625)\n",
      "(d, train_metric, valid_metric) = (313, 0.6134185303514377, 0.6625)\n",
      "(d, train_metric, valid_metric) = (314, 0.9713375796178344, 0.8875)\n",
      "(d, train_metric, valid_metric) = (315, 0.6158730158730159, 0.6625)\n",
      "(d, train_metric, valid_metric) = (316, 0.6170886075949367, 0.6625)\n",
      "(d, train_metric, valid_metric) = (317, 0.6182965299684543, 0.6625)\n",
      "(d, train_metric, valid_metric) = (318, 0.6194968553459119, 0.6625)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10:1:318, Any[0.7, 0.7272727272727273, 0.6666666666666666, 0.6923076923076923, 0.6428571428571429, 0.6, 0.5625, 0.5294117647058824, 0.5555555555555556, 0.5789473684210527    0.6148867313915858, 0.6161290322580645, 0.6141479099678456, 0.6121794871794872, 0.6134185303514377, 0.9713375796178344, 0.6158730158730159, 0.6170886075949367, 0.6182965299684543, 0.6194968553459119], Any[0.6625, 0.6625, 0.6625, 0.6625, 0.6625, 0.6625, 0.6625, 0.6625, 0.6625, 0.6625    0.6625, 0.6625, 0.6625, 0.6625, 0.6625, 0.8875, 0.6625, 0.6625, 0.6625, 0.6625])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_schedule, training_losses, valid_losses = learn_curve(best.best_model, X[train,:], y[train], acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip200\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip200)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip201\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip200)\" d=\"\n",
       "M148.01 1486.45 L2352.76 1486.45 L2352.76 47.2441 L148.01 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip202\">\n",
       "    <rect x=\"148\" y=\"47\" width=\"2206\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  480.532,1486.45 480.532,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  818.186,1486.45 818.186,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1155.84,1486.45 1155.84,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1493.49,1486.45 1493.49,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1831.15,1486.45 1831.15,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2168.8,1486.45 2168.8,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.01,1242.05 2352.76,1242.05 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.01,953.535 2352.76,953.535 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.01,665.016 2352.76,665.016 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.01,376.496 2352.76,376.496 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  148.01,87.9763 2352.76,87.9763 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.01,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.01,1486.45 148.01,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  480.532,1486.45 480.532,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  818.186,1486.45 818.186,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1155.84,1486.45 1155.84,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1493.49,1486.45 1493.49,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1831.15,1486.45 1831.15,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2168.8,1486.45 2168.8,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.01,1242.05 174.467,1242.05 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.01,953.535 174.467,953.535 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.01,665.016 174.467,665.016 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.01,376.496 174.467,376.496 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  148.01,87.9763 174.467,87.9763 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip200)\" d=\"M 0 0 M457.303 1505.36 L475.659 1505.36 L475.659 1509.3 L461.585 1509.3 L461.585 1517.77 Q462.604 1517.42 463.622 1517.26 Q464.641 1517.07 465.659 1517.07 Q471.446 1517.07 474.826 1520.24 Q478.206 1523.42 478.206 1528.83 Q478.206 1534.41 474.733 1537.51 Q471.261 1540.59 464.942 1540.59 Q462.766 1540.59 460.497 1540.22 Q458.252 1539.85 455.845 1539.11 L455.845 1534.41 Q457.928 1535.54 460.15 1536.1 Q462.372 1536.66 464.849 1536.66 Q468.854 1536.66 471.192 1534.55 Q473.53 1532.44 473.53 1528.83 Q473.53 1525.22 471.192 1523.11 Q468.854 1521.01 464.849 1521.01 Q462.974 1521.01 461.099 1521.42 Q459.247 1521.84 457.303 1522.72 L457.303 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M493.275 1508.44 Q489.664 1508.44 487.835 1512 Q486.03 1515.55 486.03 1522.67 Q486.03 1529.78 487.835 1533.35 Q489.664 1536.89 493.275 1536.89 Q496.909 1536.89 498.715 1533.35 Q500.543 1529.78 500.543 1522.67 Q500.543 1515.55 498.715 1512 Q496.909 1508.44 493.275 1508.44 M493.275 1504.73 Q499.085 1504.73 502.141 1509.34 Q505.219 1513.92 505.219 1522.67 Q505.219 1531.4 502.141 1536.01 Q499.085 1540.59 493.275 1540.59 Q487.465 1540.59 484.386 1536.01 Q481.331 1531.4 481.331 1522.67 Q481.331 1513.92 484.386 1509.34 Q487.465 1504.73 493.275 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M781.554 1535.98 L789.193 1535.98 L789.193 1509.62 L780.883 1511.29 L780.883 1507.03 L789.147 1505.36 L793.823 1505.36 L793.823 1535.98 L801.462 1535.98 L801.462 1539.92 L781.554 1539.92 L781.554 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M816.531 1508.44 Q812.92 1508.44 811.091 1512 Q809.286 1515.55 809.286 1522.67 Q809.286 1529.78 811.091 1533.35 Q812.92 1536.89 816.531 1536.89 Q820.165 1536.89 821.971 1533.35 Q823.799 1529.78 823.799 1522.67 Q823.799 1515.55 821.971 1512 Q820.165 1508.44 816.531 1508.44 M816.531 1504.73 Q822.341 1504.73 825.397 1509.34 Q828.475 1513.92 828.475 1522.67 Q828.475 1531.4 825.397 1536.01 Q822.341 1540.59 816.531 1540.59 Q810.721 1540.59 807.642 1536.01 Q804.587 1531.4 804.587 1522.67 Q804.587 1513.92 807.642 1509.34 Q810.721 1504.73 816.531 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M843.545 1508.44 Q839.934 1508.44 838.105 1512 Q836.299 1515.55 836.299 1522.67 Q836.299 1529.78 838.105 1533.35 Q839.934 1536.89 843.545 1536.89 Q847.179 1536.89 848.984 1533.35 Q850.813 1529.78 850.813 1522.67 Q850.813 1515.55 848.984 1512 Q847.179 1508.44 843.545 1508.44 M843.545 1504.73 Q849.355 1504.73 852.41 1509.34 Q855.489 1513.92 855.489 1522.67 Q855.489 1531.4 852.41 1536.01 Q849.355 1540.59 843.545 1540.59 Q837.734 1540.59 834.656 1536.01 Q831.6 1531.4 831.6 1522.67 Q831.6 1513.92 834.656 1509.34 Q837.734 1504.73 843.545 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M1119.71 1535.98 L1127.34 1535.98 L1127.34 1509.62 L1119.03 1511.29 L1119.03 1507.03 L1127.3 1505.36 L1131.97 1505.36 L1131.97 1535.98 L1139.61 1535.98 L1139.61 1539.92 L1119.71 1539.92 L1119.71 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M1144.73 1505.36 L1163.09 1505.36 L1163.09 1509.3 L1149.01 1509.3 L1149.01 1517.77 Q1150.03 1517.42 1151.05 1517.26 Q1152.07 1517.07 1153.09 1517.07 Q1158.87 1517.07 1162.25 1520.24 Q1165.63 1523.42 1165.63 1528.83 Q1165.63 1534.41 1162.16 1537.51 Q1158.69 1540.59 1152.37 1540.59 Q1150.19 1540.59 1147.92 1540.22 Q1145.68 1539.85 1143.27 1539.11 L1143.27 1534.41 Q1145.35 1535.54 1147.58 1536.1 Q1149.8 1536.66 1152.28 1536.66 Q1156.28 1536.66 1158.62 1534.55 Q1160.96 1532.44 1160.96 1528.83 Q1160.96 1525.22 1158.62 1523.11 Q1156.28 1521.01 1152.28 1521.01 Q1150.4 1521.01 1148.53 1521.42 Q1146.67 1521.84 1144.73 1522.72 L1144.73 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M1180.7 1508.44 Q1177.09 1508.44 1175.26 1512 Q1173.46 1515.55 1173.46 1522.67 Q1173.46 1529.78 1175.26 1533.35 Q1177.09 1536.89 1180.7 1536.89 Q1184.34 1536.89 1186.14 1533.35 Q1187.97 1529.78 1187.97 1522.67 Q1187.97 1515.55 1186.14 1512 Q1184.34 1508.44 1180.7 1508.44 M1180.7 1504.73 Q1186.51 1504.73 1189.57 1509.34 Q1192.65 1513.92 1192.65 1522.67 Q1192.65 1531.4 1189.57 1536.01 Q1186.51 1540.59 1180.7 1540.59 Q1174.89 1540.59 1171.81 1536.01 Q1168.76 1531.4 1168.76 1522.67 Q1168.76 1513.92 1171.81 1509.34 Q1174.89 1504.73 1180.7 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M1461.13 1535.98 L1477.45 1535.98 L1477.45 1539.92 L1455.51 1539.92 L1455.51 1535.98 Q1458.17 1533.23 1462.75 1528.6 Q1467.36 1523.95 1468.54 1522.61 Q1470.79 1520.08 1471.67 1518.35 Q1472.57 1516.59 1472.57 1514.9 Q1472.57 1512.14 1470.62 1510.41 Q1468.7 1508.67 1465.6 1508.67 Q1463.4 1508.67 1460.95 1509.43 Q1458.52 1510.2 1455.74 1511.75 L1455.74 1507.03 Q1458.56 1505.89 1461.02 1505.31 Q1463.47 1504.73 1465.51 1504.73 Q1470.88 1504.73 1474.07 1507.42 Q1477.27 1510.11 1477.27 1514.6 Q1477.27 1516.73 1476.46 1518.65 Q1475.67 1520.54 1473.56 1523.14 Q1472.98 1523.81 1469.88 1527.03 Q1466.78 1530.22 1461.13 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M1492.52 1508.44 Q1488.91 1508.44 1487.08 1512 Q1485.28 1515.55 1485.28 1522.67 Q1485.28 1529.78 1487.08 1533.35 Q1488.91 1536.89 1492.52 1536.89 Q1496.16 1536.89 1497.96 1533.35 Q1499.79 1529.78 1499.79 1522.67 Q1499.79 1515.55 1497.96 1512 Q1496.16 1508.44 1492.52 1508.44 M1492.52 1504.73 Q1498.33 1504.73 1501.39 1509.34 Q1504.47 1513.92 1504.47 1522.67 Q1504.47 1531.4 1501.39 1536.01 Q1498.33 1540.59 1492.52 1540.59 Q1486.71 1540.59 1483.63 1536.01 Q1480.58 1531.4 1480.58 1522.67 Q1480.58 1513.92 1483.63 1509.34 Q1486.71 1504.73 1492.52 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M1519.54 1508.44 Q1515.92 1508.44 1514.1 1512 Q1512.29 1515.55 1512.29 1522.67 Q1512.29 1529.78 1514.1 1533.35 Q1515.92 1536.89 1519.54 1536.89 Q1523.17 1536.89 1524.98 1533.35 Q1526.8 1529.78 1526.8 1522.67 Q1526.8 1515.55 1524.98 1512 Q1523.17 1508.44 1519.54 1508.44 M1519.54 1504.73 Q1525.35 1504.73 1528.4 1509.34 Q1531.48 1513.92 1531.48 1522.67 Q1531.48 1531.4 1528.4 1536.01 Q1525.35 1540.59 1519.54 1540.59 Q1513.73 1540.59 1510.65 1536.01 Q1507.59 1531.4 1507.59 1522.67 Q1507.59 1513.92 1510.65 1509.34 Q1513.73 1504.73 1519.54 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M1799.28 1535.98 L1815.6 1535.98 L1815.6 1539.92 L1793.66 1539.92 L1793.66 1535.98 Q1796.32 1533.23 1800.91 1528.6 Q1805.51 1523.95 1806.69 1522.61 Q1808.94 1520.08 1809.82 1518.35 Q1810.72 1516.59 1810.72 1514.9 Q1810.72 1512.14 1808.78 1510.41 Q1806.85 1508.67 1803.75 1508.67 Q1801.55 1508.67 1799.1 1509.43 Q1796.67 1510.2 1793.89 1511.75 L1793.89 1507.03 Q1796.72 1505.89 1799.17 1505.31 Q1801.62 1504.73 1803.66 1504.73 Q1809.03 1504.73 1812.22 1507.42 Q1815.42 1510.11 1815.42 1514.6 Q1815.42 1516.73 1814.61 1518.65 Q1813.82 1520.54 1811.72 1523.14 Q1811.14 1523.81 1808.03 1527.03 Q1804.93 1530.22 1799.28 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M1820.72 1505.36 L1839.08 1505.36 L1839.08 1509.3 L1825 1509.3 L1825 1517.77 Q1826.02 1517.42 1827.04 1517.26 Q1828.06 1517.07 1829.08 1517.07 Q1834.86 1517.07 1838.24 1520.24 Q1841.62 1523.42 1841.62 1528.83 Q1841.62 1534.41 1838.15 1537.51 Q1834.68 1540.59 1828.36 1540.59 Q1826.18 1540.59 1823.91 1540.22 Q1821.67 1539.85 1819.26 1539.11 L1819.26 1534.41 Q1821.34 1535.54 1823.57 1536.1 Q1825.79 1536.66 1828.27 1536.66 Q1832.27 1536.66 1834.61 1534.55 Q1836.95 1532.44 1836.95 1528.83 Q1836.95 1525.22 1834.61 1523.11 Q1832.27 1521.01 1828.27 1521.01 Q1826.39 1521.01 1824.52 1521.42 Q1822.66 1521.84 1820.72 1522.72 L1820.72 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M1856.69 1508.44 Q1853.08 1508.44 1851.25 1512 Q1849.45 1515.55 1849.45 1522.67 Q1849.45 1529.78 1851.25 1533.35 Q1853.08 1536.89 1856.69 1536.89 Q1860.33 1536.89 1862.13 1533.35 Q1863.96 1529.78 1863.96 1522.67 Q1863.96 1515.55 1862.13 1512 Q1860.33 1508.44 1856.69 1508.44 M1856.69 1504.73 Q1862.5 1504.73 1865.56 1509.34 Q1868.64 1513.92 1868.64 1522.67 Q1868.64 1531.4 1865.56 1536.01 Q1862.5 1540.59 1856.69 1540.59 Q1850.88 1540.59 1847.8 1536.01 Q1844.75 1531.4 1844.75 1522.67 Q1844.75 1513.92 1847.8 1509.34 Q1850.88 1504.73 1856.69 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M2146.04 1521.29 Q2149.39 1522 2151.27 1524.27 Q2153.17 1526.54 2153.17 1529.87 Q2153.17 1534.99 2149.65 1537.79 Q2146.13 1540.59 2139.65 1540.59 Q2137.47 1540.59 2135.16 1540.15 Q2132.86 1539.73 2130.41 1538.88 L2130.41 1534.36 Q2132.36 1535.5 2134.67 1536.08 Q2136.99 1536.66 2139.51 1536.66 Q2143.91 1536.66 2146.2 1534.92 Q2148.51 1533.18 2148.51 1529.87 Q2148.51 1526.82 2146.36 1525.11 Q2144.23 1523.37 2140.41 1523.37 L2136.38 1523.37 L2136.38 1519.53 L2140.6 1519.53 Q2144.05 1519.53 2145.87 1518.16 Q2147.7 1516.77 2147.7 1514.18 Q2147.7 1511.52 2145.8 1510.11 Q2143.93 1508.67 2140.41 1508.67 Q2138.49 1508.67 2136.29 1509.09 Q2134.09 1509.5 2131.45 1510.38 L2131.45 1506.22 Q2134.11 1505.48 2136.43 1505.11 Q2138.77 1504.73 2140.83 1504.73 Q2146.15 1504.73 2149.25 1507.17 Q2152.36 1509.57 2152.36 1513.69 Q2152.36 1516.56 2150.71 1518.55 Q2149.07 1520.52 2146.04 1521.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M2168.23 1508.44 Q2164.62 1508.44 2162.8 1512 Q2160.99 1515.55 2160.99 1522.67 Q2160.99 1529.78 2162.8 1533.35 Q2164.62 1536.89 2168.23 1536.89 Q2171.87 1536.89 2173.67 1533.35 Q2175.5 1529.78 2175.5 1522.67 Q2175.5 1515.55 2173.67 1512 Q2171.87 1508.44 2168.23 1508.44 M2168.23 1504.73 Q2174.05 1504.73 2177.1 1509.34 Q2180.18 1513.92 2180.18 1522.67 Q2180.18 1531.4 2177.1 1536.01 Q2174.05 1540.59 2168.23 1540.59 Q2162.42 1540.59 2159.35 1536.01 Q2156.29 1531.4 2156.29 1522.67 Q2156.29 1513.92 2159.35 1509.34 Q2162.42 1504.73 2168.23 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M2195.25 1508.44 Q2191.64 1508.44 2189.81 1512 Q2188 1515.55 2188 1522.67 Q2188 1529.78 2189.81 1533.35 Q2191.64 1536.89 2195.25 1536.89 Q2198.88 1536.89 2200.69 1533.35 Q2202.52 1529.78 2202.52 1522.67 Q2202.52 1515.55 2200.69 1512 Q2198.88 1508.44 2195.25 1508.44 M2195.25 1504.73 Q2201.06 1504.73 2204.11 1509.34 Q2207.19 1513.92 2207.19 1522.67 Q2207.19 1531.4 2204.11 1536.01 Q2201.06 1540.59 2195.25 1540.59 Q2189.44 1540.59 2186.36 1536.01 Q2183.3 1531.4 2183.3 1522.67 Q2183.3 1513.92 2186.36 1509.34 Q2189.44 1504.73 2195.25 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M74.9365 1227.85 Q71.3254 1227.85 69.4967 1231.42 Q67.6912 1234.96 67.6912 1242.09 Q67.6912 1249.2 69.4967 1252.76 Q71.3254 1256.3 74.9365 1256.3 Q78.5707 1256.3 80.3763 1252.76 Q82.205 1249.2 82.205 1242.09 Q82.205 1234.96 80.3763 1231.42 Q78.5707 1227.85 74.9365 1227.85 M74.9365 1224.15 Q80.7467 1224.15 83.8022 1228.76 Q86.8809 1233.34 86.8809 1242.09 Q86.8809 1250.82 83.8022 1255.42 Q80.7467 1260.01 74.9365 1260.01 Q69.1264 1260.01 66.0477 1255.42 Q62.9921 1250.82 62.9921 1242.09 Q62.9921 1233.34 66.0477 1228.76 Q69.1264 1224.15 74.9365 1224.15 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M91.9503 1253.46 L96.8345 1253.46 L96.8345 1259.33 L91.9503 1259.33 L91.9503 1253.46 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M112.483 1240.19 Q109.334 1240.19 107.483 1242.34 Q105.654 1244.5 105.654 1248.25 Q105.654 1251.97 107.483 1254.15 Q109.334 1256.3 112.483 1256.3 Q115.631 1256.3 117.459 1254.15 Q119.311 1251.97 119.311 1248.25 Q119.311 1244.5 117.459 1242.34 Q115.631 1240.19 112.483 1240.19 M121.765 1225.54 L121.765 1229.8 Q120.006 1228.96 118.2 1228.52 Q116.418 1228.08 114.659 1228.08 Q110.029 1228.08 107.575 1231.21 Q105.145 1234.33 104.797 1240.65 Q106.163 1238.64 108.223 1237.58 Q110.284 1236.49 112.76 1236.49 Q117.969 1236.49 120.978 1239.66 Q124.01 1242.81 124.01 1248.25 Q124.01 1253.57 120.862 1256.79 Q117.714 1260.01 112.483 1260.01 Q106.487 1260.01 103.316 1255.42 Q100.145 1250.82 100.145 1242.09 Q100.145 1233.9 104.034 1229.03 Q107.922 1224.15 114.473 1224.15 Q116.233 1224.15 118.015 1224.5 Q119.821 1224.84 121.765 1225.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M76.0013 939.334 Q72.3902 939.334 70.5615 942.899 Q68.756 946.44 68.756 953.57 Q68.756 960.676 70.5615 964.241 Q72.3902 967.783 76.0013 967.783 Q79.6356 967.783 81.4411 964.241 Q83.2698 960.676 83.2698 953.57 Q83.2698 946.44 81.4411 942.899 Q79.6356 939.334 76.0013 939.334 M76.0013 935.63 Q81.8115 935.63 84.867 940.237 Q87.9457 944.82 87.9457 953.57 Q87.9457 962.297 84.867 966.903 Q81.8115 971.486 76.0013 971.486 Q70.1912 971.486 67.1125 966.903 Q64.0569 962.297 64.0569 953.57 Q64.0569 944.82 67.1125 940.237 Q70.1912 935.63 76.0013 935.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M93.0151 964.936 L97.8993 964.936 L97.8993 970.815 L93.0151 970.815 L93.0151 964.936 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M101.788 936.255 L124.01 936.255 L124.01 938.246 L111.464 970.815 L106.58 970.815 L118.385 940.19 L101.788 940.19 L101.788 936.255 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M75.1911 650.814 Q71.58 650.814 69.7513 654.379 Q67.9458 657.921 67.9458 665.05 Q67.9458 672.157 69.7513 675.721 Q71.58 679.263 75.1911 679.263 Q78.8254 679.263 80.6309 675.721 Q82.4596 672.157 82.4596 665.05 Q82.4596 657.921 80.6309 654.379 Q78.8254 650.814 75.1911 650.814 M75.1911 647.111 Q81.0013 647.111 84.0568 651.717 Q87.1355 656.3 87.1355 665.05 Q87.1355 673.777 84.0568 678.384 Q81.0013 682.967 75.1911 682.967 Q69.381 682.967 66.3023 678.384 Q63.2468 673.777 63.2468 665.05 Q63.2468 656.3 66.3023 651.717 Q69.381 647.111 75.1911 647.111 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M92.2049 676.416 L97.0892 676.416 L97.0892 682.296 L92.2049 682.296 L92.2049 676.416 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M112.159 665.884 Q108.825 665.884 106.904 667.666 Q105.006 669.448 105.006 672.573 Q105.006 675.698 106.904 677.481 Q108.825 679.263 112.159 679.263 Q115.492 679.263 117.413 677.481 Q119.334 675.675 119.334 672.573 Q119.334 669.448 117.413 667.666 Q115.515 665.884 112.159 665.884 M107.483 663.893 Q104.473 663.152 102.784 661.092 Q101.117 659.032 101.117 656.069 Q101.117 651.925 104.057 649.518 Q107.02 647.111 112.159 647.111 Q117.321 647.111 120.26 649.518 Q123.2 651.925 123.2 656.069 Q123.2 659.032 121.51 661.092 Q119.844 663.152 116.858 663.893 Q120.237 664.68 122.112 666.972 Q124.01 669.263 124.01 672.573 Q124.01 677.596 120.932 680.282 Q117.876 682.967 112.159 682.967 Q106.441 682.967 103.362 680.282 Q100.307 677.596 100.307 672.573 Q100.307 669.263 102.205 666.972 Q104.103 664.68 107.483 663.893 M105.77 656.509 Q105.77 659.194 107.436 660.698 Q109.126 662.203 112.159 662.203 Q115.168 662.203 116.858 660.698 Q118.571 659.194 118.571 656.509 Q118.571 653.823 116.858 652.319 Q115.168 650.814 112.159 650.814 Q109.126 650.814 107.436 652.319 Q105.77 653.823 105.77 656.509 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M75.2837 362.295 Q71.6726 362.295 69.8439 365.859 Q68.0384 369.401 68.0384 376.531 Q68.0384 383.637 69.8439 387.202 Q71.6726 390.744 75.2837 390.744 Q78.918 390.744 80.7235 387.202 Q82.5522 383.637 82.5522 376.531 Q82.5522 369.401 80.7235 365.859 Q78.918 362.295 75.2837 362.295 M75.2837 358.591 Q81.0939 358.591 84.1494 363.197 Q87.2281 367.781 87.2281 376.531 Q87.2281 385.257 84.1494 389.864 Q81.0939 394.447 75.2837 394.447 Q69.4736 394.447 66.3949 389.864 Q63.3393 385.257 63.3393 376.531 Q63.3393 367.781 66.3949 363.197 Q69.4736 358.591 75.2837 358.591 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M92.2975 387.896 L97.1818 387.896 L97.1818 393.776 L92.2975 393.776 L92.2975 387.896 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M102.39 393.058 L102.39 388.799 Q104.149 389.632 105.955 390.072 Q107.76 390.512 109.497 390.512 Q114.126 390.512 116.557 387.41 Q119.01 384.285 119.358 377.943 Q118.015 379.933 115.955 380.998 Q113.895 382.063 111.395 382.063 Q106.209 382.063 103.177 378.938 Q100.168 375.79 100.168 370.35 Q100.168 365.026 103.316 361.808 Q106.464 358.591 111.696 358.591 Q117.691 358.591 120.839 363.197 Q124.01 367.781 124.01 376.531 Q124.01 384.702 120.121 389.586 Q116.256 394.447 109.705 394.447 Q107.946 394.447 106.14 394.1 Q104.334 393.753 102.39 393.058 M111.696 378.406 Q114.844 378.406 116.672 376.253 Q118.524 374.1 118.524 370.35 Q118.524 366.623 116.672 364.471 Q114.844 362.295 111.696 362.295 Q108.547 362.295 106.696 364.471 Q104.867 366.623 104.867 370.35 Q104.867 374.1 106.696 376.253 Q108.547 378.406 111.696 378.406 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M67.1356 101.321 L74.7745 101.321 L74.7745 74.9555 L66.4643 76.6222 L66.4643 72.3629 L74.7282 70.6963 L79.4041 70.6963 L79.4041 101.321 L87.0429 101.321 L87.0429 105.256 L67.1356 105.256 L67.1356 101.321 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M92.1123 99.3767 L96.9966 99.3767 L96.9966 105.256 L92.1123 105.256 L92.1123 99.3767 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M112.066 73.775 Q108.455 73.775 106.626 77.3398 Q104.821 80.8814 104.821 88.011 Q104.821 95.1174 106.626 98.6822 Q108.455 102.224 112.066 102.224 Q115.7 102.224 117.506 98.6822 Q119.334 95.1174 119.334 88.011 Q119.334 80.8814 117.506 77.3398 Q115.7 73.775 112.066 73.775 M112.066 70.0713 Q117.876 70.0713 120.932 74.6777 Q124.01 79.261 124.01 88.011 Q124.01 96.7378 120.932 101.344 Q117.876 105.928 112.066 105.928 Q106.256 105.928 103.177 101.344 Q100.122 96.7378 100.122 88.011 Q100.122 79.261 103.177 74.6777 Q106.256 70.0713 112.066 70.0713 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip202)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  210.409,953.535 217.162,874.848 223.915,1049.71 230.668,975.729 237.421,1118.4 244.174,1242.05 250.927,1350.25 257.68,1445.72 264.433,1370.29 271.186,1302.8 \n",
       "  277.94,1242.05 284.693,1324.49 291.446,1399.43 298.199,1342.41 304.952,1290.14 311.705,1242.05 318.458,1197.67 325.211,1263.43 331.964,1221.45 338.717,1182.36 \n",
       "  345.47,1145.88 352.223,1111.76 358.977,1169.92 365.73,1224.57 372.483,1276 379.236,1242.05 385.989,1210 392.742,1179.67 399.495,1226.87 406.248,1197.67 \n",
       "  413.001,1169.92 419.754,1143.54 426.507,1118.4 433.26,1161.54 440.014,1137.14 446.767,1177.94 453.52,1154.24 460.273,1131.56 467.026,1109.82 473.779,1147.84 \n",
       "  480.532,1126.65 487.285,1106.28 494.038,1086.7 500.791,1122.29 507.544,1103.14 514.297,1084.68 521.05,1066.88 527.804,138.594 534.557,137.721 541.31,136.878 \n",
       "  548.063,136.063 554.816,135.275 561.569,87.9763 568.322,87.9763 575.075,87.9763 581.828,221.139 588.581,219.122 595.334,217.164 602.087,215.264 608.841,213.42 \n",
       "  615.594,211.628 622.347,209.886 629.1,208.193 635.853,206.546 642.606,1257.65 649.359,1280.52 656.112,1264.83 662.865,1249.55 669.618,1234.66 676.371,1256.66 \n",
       "  683.124,1242.05 689.877,1263.43 696.631,1249.09 703.384,1235.1 710.137,1255.79 716.89,1242.05 723.643,1228.64 730.396,1215.52 737.149,1202.71 743.902,1222.6 \n",
       "  750.655,1242.05 757.408,1229.37 764.161,1248.33 770.914,1235.85 777.668,1254.33 784.421,1272.43 791.174,1260.09 797.927,1248 804.68,1236.17 811.433,1224.57 \n",
       "  818.186,1242.05 824.939,1230.63 831.692,1219.43 838.445,1208.44 845.198,1197.67 851.951,1187.1 858.704,1203.95 865.458,1193.52 872.211,1210 878.964,1226.17 \n",
       "  885.717,1215.83 892.47,1231.66 899.223,1247.21 905.976,1236.95 912.729,1252.18 919.482,1242.05 926.235,1232.11 932.988,1222.33 939.741,1237.16 946.495,1227.51 \n",
       "  953.248,1242.05 960.001,1256.36 966.754,1246.78 973.507,1260.82 980.26,1251.36 987.013,1265.14 993.766,1255.79 1000.52,1269.32 1007.27,1282.63 1014.03,1295.73 \n",
       "  1020.78,1286.44 1027.53,1299.32 1034.28,1290.14 1041.04,1281.1 1047.79,1272.2 1054.54,1284.8 1061.3,1276 1068.05,1267.33 1074.8,1258.78 1081.56,1271.11 \n",
       "  1088.31,1262.66 1095.06,1254.33 1101.82,1266.44 1108.57,1258.2 1115.32,1250.07 1122.07,1261.95 1128.83,1253.91 1135.58,1245.98 1142.33,1238.16 1149.09,1249.8 \n",
       "  1155.84,1242.05 1162.59,1234.41 1169.35,1226.87 1176.1,1219.43 1182.85,1212.08 1189.61,1223.44 1196.36,1216.16 1203.11,1227.35 1209.86,1238.4 1216.62,1231.17 \n",
       "  1223.37,1224.02 1230.12,1216.97 1236.88,1227.81 1243.63,1220.81 1250.38,1231.5 1257.14,1242.05 1263.89,1252.48 1270.64,1245.51 1277.4,1238.62 1284.15,1248.88 \n",
       "  1290.9,1242.05 1297.65,1252.18 1304.41,1262.18 1311.16,1272.07 1317.91,1281.85 1324.67,1275.03 1331.42,1268.28 1338.17,1261.62 1344.93,1255.02 1351.68,1264.62 \n",
       "  1358.43,1274.11 1365.19,1267.56 1371.94,230.651 1378.69,229.871 1385.44,229.1 1392.2,228.337 1398.95,1251.36 1405.7,1245.14 1412.46,1238.99 1419.21,1248.16 \n",
       "  1425.96,1242.05 1432.72,1251.12 1439.47,1245.06 1446.22,1239.06 1452.98,236.698 1459.73,235.935 1466.48,235.18 1473.23,234.433 1479.99,233.693 1486.74,232.961 \n",
       "  1493.49,232.236 1500.25,245.873 1507,259.374 1513.75,258.53 1520.51,257.694 1527.26,256.866 1534.01,256.046 1540.77,255.234 1547.52,254.43 1554.27,253.633 \n",
       "  1561.02,252.845 1567.78,252.063 1574.53,251.289 1581.28,1266.44 1588.04,1274.41 1594.79,1268.89 1601.54,1263.43 1608.3,1258.01 1615.05,1252.64 1621.8,1260.5 \n",
       "  1628.56,1268.28 1635.31,1262.94 1642.06,1257.65 1648.81,1252.41 1655.57,1247.21 1662.32,1242.05 1669.07,1236.95 1675.83,1231.89 1682.58,1226.87 1689.33,1234.5 \n",
       "  1696.09,1229.51 1702.84,1224.57 1709.59,1219.67 1716.35,1214.81 1723.1,1222.33 1729.85,1217.5 1736.6,1212.71 1743.36,1220.14 1750.11,1227.51 1756.86,1222.74 \n",
       "  1763.62,1230.03 1770.37,1225.29 1777.12,1220.59 1783.88,1215.93 1790.63,1211.31 1797.38,1218.5 1804.14,1213.91 1810.89,181.424 1817.64,181.047 1824.39,180.673 \n",
       "  1831.15,1218.97 1837.9,1214.47 1844.65,1210 1851.41,1205.56 1858.16,1212.52 1864.91,1208.11 1871.67,1203.74 1878.42,1199.39 1885.17,1195.09 1891.93,1201.95 \n",
       "  1898.68,1208.76 1905.43,1204.47 1912.18,1200.21 1918.94,1195.98 1925.69,1191.78 1932.44,1198.5 1939.2,1205.18 1945.95,1200.99 1952.7,1196.84 1959.46,1192.72 \n",
       "  1966.21,1188.63 1972.96,1184.56 1979.72,1180.53 1986.47,1176.53 1993.22,1183.09 1999.98,1179.11 2006.73,1185.61 2013.48,1181.64 2020.23,1188.09 2026.99,1184.14 \n",
       "  2033.74,1180.23 2040.49,1186.61 2047.25,1182.71 2054,1189.04 2060.75,1195.32 2067.51,1201.56 2074.26,1197.67 2081.01,1203.85 2087.77,1199.98 2094.52,1196.13 \n",
       "  2101.27,1202.26 2108.02,1198.43 2114.78,1194.63 2121.53,1200.7 2128.28,1206.73 2135.04,1202.93 2141.79,1199.17 2148.54,1195.43 2155.3,1201.39 2162.05,1197.67 \n",
       "  2168.8,1193.97 2175.56,1199.88 2182.31,1196.2 2189.06,1202.06 2195.81,1198.4 2202.57,1194.76 2209.32,1200.57 2216.07,1196.94 2222.83,1193.34 2229.58,1199.1 \n",
       "  2236.33,1195.52 2243.09,1201.24 2249.84,1206.91 2256.59,1203.34 2263.35,170.673 2270.1,1196.26 2276.85,1192.75 2283.6,1189.27 2290.36,1185.8 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip202)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  210.409,1061.73 217.162,1061.73 223.915,1061.73 230.668,1061.73 237.421,1061.73 244.174,1061.73 250.927,1061.73 257.68,1061.73 264.433,1061.73 271.186,1061.73 \n",
       "  277.94,1061.73 284.693,1061.73 291.446,1061.73 298.199,1061.73 304.952,1061.73 311.705,1061.73 318.458,1061.73 325.211,1061.73 331.964,1061.73 338.717,1061.73 \n",
       "  345.47,1061.73 352.223,1061.73 358.977,1061.73 365.73,1061.73 372.483,1061.73 379.236,1061.73 385.989,1061.73 392.742,1061.73 399.495,1061.73 406.248,1061.73 \n",
       "  413.001,1061.73 419.754,1061.73 426.507,1061.73 433.26,1061.73 440.014,1061.73 446.767,1061.73 453.52,1061.73 460.273,1061.73 467.026,1061.73 473.779,1061.73 \n",
       "  480.532,1061.73 487.285,1061.73 494.038,1061.73 500.791,1061.73 507.544,1061.73 514.297,1061.73 521.05,1061.73 527.804,376.496 534.557,412.561 541.31,412.561 \n",
       "  548.063,412.561 554.816,376.496 561.569,376.496 568.322,412.561 575.075,412.561 581.828,412.561 588.581,376.496 595.334,376.496 602.087,376.496 608.841,304.366 \n",
       "  615.594,304.366 622.347,340.431 629.1,412.561 635.853,340.431 642.606,1061.73 649.359,1061.73 656.112,1061.73 662.865,1061.73 669.618,1061.73 676.371,1061.73 \n",
       "  683.124,1061.73 689.877,1061.73 696.631,1061.73 703.384,1061.73 710.137,1061.73 716.89,1061.73 723.643,1061.73 730.396,1061.73 737.149,1061.73 743.902,1061.73 \n",
       "  750.655,1061.73 757.408,1061.73 764.161,1061.73 770.914,1061.73 777.668,1061.73 784.421,1061.73 791.174,1061.73 797.927,1061.73 804.68,1061.73 811.433,1061.73 \n",
       "  818.186,1061.73 824.939,1061.73 831.692,1061.73 838.445,1061.73 845.198,1061.73 851.951,1061.73 858.704,1061.73 865.458,1061.73 872.211,1061.73 878.964,1061.73 \n",
       "  885.717,1061.73 892.47,1061.73 899.223,1061.73 905.976,1061.73 912.729,1061.73 919.482,1061.73 926.235,1061.73 932.988,1061.73 939.741,1061.73 946.495,1061.73 \n",
       "  953.248,1061.73 960.001,1061.73 966.754,1061.73 973.507,1061.73 980.26,1061.73 987.013,1061.73 993.766,1061.73 1000.52,1061.73 1007.27,1061.73 1014.03,1061.73 \n",
       "  1020.78,1061.73 1027.53,1061.73 1034.28,1061.73 1041.04,1061.73 1047.79,1061.73 1054.54,1061.73 1061.3,1061.73 1068.05,1061.73 1074.8,1061.73 1081.56,1061.73 \n",
       "  1088.31,1061.73 1095.06,1061.73 1101.82,1061.73 1108.57,1061.73 1115.32,1061.73 1122.07,1061.73 1128.83,1061.73 1135.58,1061.73 1142.33,1061.73 1149.09,1061.73 \n",
       "  1155.84,1061.73 1162.59,1061.73 1169.35,1061.73 1176.1,1061.73 1182.85,1061.73 1189.61,1061.73 1196.36,1061.73 1203.11,1061.73 1209.86,1061.73 1216.62,1061.73 \n",
       "  1223.37,1061.73 1230.12,1061.73 1236.88,1061.73 1243.63,1061.73 1250.38,1061.73 1257.14,1061.73 1263.89,1061.73 1270.64,1061.73 1277.4,1061.73 1284.15,1061.73 \n",
       "  1290.9,1061.73 1297.65,1061.73 1304.41,1061.73 1311.16,1061.73 1317.91,1061.73 1324.67,1061.73 1331.42,1061.73 1338.17,1061.73 1344.93,1061.73 1351.68,1061.73 \n",
       "  1358.43,1061.73 1365.19,1061.73 1371.94,448.626 1378.69,268.301 1385.44,448.626 1392.2,268.301 1398.95,1061.73 1405.7,1061.73 1412.46,1061.73 1419.21,1061.73 \n",
       "  1425.96,1061.73 1432.72,1061.73 1439.47,1061.73 1446.22,1061.73 1452.98,268.301 1459.73,340.431 1466.48,304.366 1473.23,304.366 1479.99,340.431 1486.74,268.301 \n",
       "  1493.49,304.366 1500.25,304.366 1507,304.366 1513.75,340.431 1520.51,268.301 1527.26,340.431 1534.01,304.366 1540.77,304.366 1547.52,340.431 1554.27,340.431 \n",
       "  1561.02,340.431 1567.78,340.431 1574.53,304.366 1581.28,1061.73 1588.04,1061.73 1594.79,1061.73 1601.54,1061.73 1608.3,1061.73 1615.05,1061.73 1621.8,1061.73 \n",
       "  1628.56,1061.73 1635.31,1061.73 1642.06,1061.73 1648.81,1061.73 1655.57,1061.73 1662.32,1061.73 1669.07,1061.73 1675.83,1061.73 1682.58,1061.73 1689.33,1061.73 \n",
       "  1696.09,1061.73 1702.84,1061.73 1709.59,1061.73 1716.35,1061.73 1723.1,1061.73 1729.85,1061.73 1736.6,1061.73 1743.36,1061.73 1750.11,1061.73 1756.86,1061.73 \n",
       "  1763.62,1061.73 1770.37,1061.73 1777.12,1061.73 1783.88,1061.73 1790.63,1061.73 1797.38,1061.73 1804.14,1061.73 1810.89,376.496 1817.64,376.496 1824.39,340.431 \n",
       "  1831.15,1061.73 1837.9,1061.73 1844.65,1061.73 1851.41,1061.73 1858.16,1061.73 1864.91,1061.73 1871.67,1061.73 1878.42,1061.73 1885.17,1061.73 1891.93,1061.73 \n",
       "  1898.68,1061.73 1905.43,1061.73 1912.18,1061.73 1918.94,1061.73 1925.69,1061.73 1932.44,1061.73 1939.2,1061.73 1945.95,1061.73 1952.7,1061.73 1959.46,1061.73 \n",
       "  1966.21,1061.73 1972.96,1061.73 1979.72,1061.73 1986.47,1061.73 1993.22,1061.73 1999.98,1061.73 2006.73,1061.73 2013.48,1061.73 2020.23,1061.73 2026.99,1061.73 \n",
       "  2033.74,1061.73 2040.49,1061.73 2047.25,1061.73 2054,1061.73 2060.75,1061.73 2067.51,1061.73 2074.26,1061.73 2081.01,1061.73 2087.77,1061.73 2094.52,1061.73 \n",
       "  2101.27,1061.73 2108.02,1061.73 2114.78,1061.73 2121.53,1061.73 2128.28,1061.73 2135.04,1061.73 2141.79,1061.73 2148.54,1061.73 2155.3,1061.73 2162.05,1061.73 \n",
       "  2168.8,1061.73 2175.56,1061.73 2182.31,1061.73 2189.06,1061.73 2195.81,1061.73 2202.57,1061.73 2209.32,1061.73 2216.07,1061.73 2222.83,1061.73 2229.58,1061.73 \n",
       "  2236.33,1061.73 2243.09,1061.73 2249.84,1061.73 2256.59,1061.73 2263.35,412.561 2270.1,1061.73 2276.85,1061.73 2283.6,1061.73 2290.36,1061.73 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip200)\" d=\"\n",
       "M1983.27 276.658 L2279.26 276.658 L2279.26 95.2176 L1983.27 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1983.27,276.658 2279.26,276.658 2279.26,95.2176 1983.27,95.2176 1983.27,276.658 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip200)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2007.77,155.698 2154.75,155.698 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip200)\" d=\"M 0 0 M2193.09 175.385 Q2191.29 180.015 2189.58 181.427 Q2187.86 182.839 2184.99 182.839 L2181.59 182.839 L2181.59 179.274 L2184.09 179.274 Q2185.85 179.274 2186.82 178.44 Q2187.79 177.607 2188.97 174.505 L2189.74 172.561 L2179.25 147.052 L2183.77 147.052 L2191.87 167.329 L2199.97 147.052 L2204.48 147.052 L2193.09 175.385 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M2210.36 169.042 L2218 169.042 L2218 142.677 L2209.69 144.343 L2209.69 140.084 L2217.96 138.418 L2222.63 138.418 L2222.63 169.042 L2230.27 169.042 L2230.27 172.978 L2210.36 172.978 L2210.36 169.042 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip200)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2007.77,216.178 2154.75,216.178 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip200)\" d=\"M 0 0 M2193.09 235.865 Q2191.29 240.495 2189.58 241.907 Q2187.86 243.319 2184.99 243.319 L2181.59 243.319 L2181.59 239.754 L2184.09 239.754 Q2185.85 239.754 2186.82 238.92 Q2187.79 238.087 2188.97 234.985 L2189.74 233.041 L2179.25 207.532 L2183.77 207.532 L2191.87 227.809 L2199.97 207.532 L2204.48 207.532 L2193.09 235.865 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip200)\" d=\"M 0 0 M2213.58 229.522 L2229.9 229.522 L2229.9 233.458 L2207.96 233.458 L2207.96 229.522 Q2210.62 226.768 2215.2 222.138 Q2219.81 217.485 2220.99 216.143 Q2223.23 213.62 2224.11 211.884 Q2225.02 210.124 2225.02 208.435 Q2225.02 205.68 2223.07 203.944 Q2221.15 202.208 2218.05 202.208 Q2215.85 202.208 2213.4 202.972 Q2210.96 203.735 2208.19 205.286 L2208.19 200.564 Q2211.01 199.43 2213.46 198.851 Q2215.92 198.273 2217.96 198.273 Q2223.33 198.273 2226.52 200.958 Q2229.71 203.643 2229.71 208.134 Q2229.71 210.263 2228.9 212.185 Q2228.12 214.083 2226.01 216.675 Q2225.43 217.347 2222.33 220.564 Q2219.23 223.759 2213.58 229.522 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(data_schedule, training_losses)\n",
    "plot!(data_schedule, valid_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = true,\n",
       "    merge_purity_threshold = 0.5555555555555556,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 5)\u001b[34m @957\u001b[39m"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dt = best.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @272\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @211\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @922\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_Tree = machine(final_dt, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 23, Threshold 105.95\n",
      "L-> Feature 27, Threshold 0.3967\n",
      "    L-> Feature 14, Threshold 48.975\n",
      "        L-> 1 : 215/215\n",
      "        R-> Feature 5, Threshold 0.09072\n",
      "            L-> 1 : 2/2\n",
      "            R-> 2 : 2/2\n",
      "    R-> Feature 22, Threshold 24.89\n",
      "        L-> 1 : 8/8\n",
      "        R-> 2 : 9/11\n",
      "R-> Feature 23, Threshold 117.45\n",
      "    L-> Feature 25, Threshold 0.1361\n",
      "        L-> 1 : 21/27\n",
      "        R-> 2 : 19/20\n",
      "    R-> 2 : 112/113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{DecisionTreeClassifier} @272\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @272\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @211\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @922\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(Final_Tree, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = predict(Final_Tree, X[test,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8278474730377929"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(y2, y[test]) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9122807017543859"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(y2, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(mode.(y), y[test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
