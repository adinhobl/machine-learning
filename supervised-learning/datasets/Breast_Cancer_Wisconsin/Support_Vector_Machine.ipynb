{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using CSV\n",
    "using MLJ\n",
    "using Plots\n",
    "using StatsBase\n",
    "\n",
    "include(\"../../lib.jl\")\n",
    "\n",
    "ENV[\"LINES\"]=50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 2. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 3. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 4. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 5. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 6. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 7. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 8. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 9. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 10. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 11. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 12. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 13. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 14. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 15. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 16. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 17. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 18. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 19. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 20. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 21. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 22. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 23. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 24. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 25. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 26. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 27. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 28. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 29. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 30. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 31. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 32. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 33. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 34. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 35. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 36. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 37. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 38. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 39. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 40. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 41. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 42. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 43. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 44. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 45. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 46. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 47. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 48. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 49. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 50. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 51. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 52. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 53. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 54. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 55. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 56. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 57. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 58. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 59. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 60. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 61. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 62. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 63. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 64. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 65. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 66. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 67. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 68. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 69. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 70. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 71. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 72. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 73. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 74. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 75. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 76. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 77. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 78. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 79. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 80. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 81. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 82. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 83. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 84. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 85. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 86. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 87. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 88. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 89. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 90. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 91. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 92. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 93. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 94. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 95. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 96. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 97. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 98. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 99. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 100. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 101. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 102. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 103. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 104. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 105. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 106. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 107. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 108. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 109. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 110. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 111. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 112. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 113. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 114. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 115. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 116. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 117. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 118. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 119. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 120. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 121. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 122. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 123. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 124. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 125. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 126. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 127. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 128. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 129. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 130. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 131. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 132. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 133. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 134. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 135. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 136. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 137. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 138. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 139. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 140. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 141. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 142. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 143. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 144. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 145. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 146. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 147. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 148. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 149. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 150. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 151. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 152. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 153. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 154. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 155. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 156. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 157. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 158. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 159. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 160. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 161. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 162. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 163. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 164. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 165. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 166. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 167. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 168. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 169. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 170. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 171. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 172. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 173. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 174. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 175. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 176. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 177. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 178. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 179. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 180. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 181. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 182. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 183. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 184. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 185. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 186. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 187. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 188. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 189. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 190. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 191. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 192. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 193. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 194. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 195. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 196. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 197. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 198. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 199. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 200. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 201. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 202. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 203. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 204. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 205. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 206. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 207. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 208. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 209. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 210. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 211. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 212. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 213. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 214. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 215. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 216. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 217. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 218. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 219. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 220. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 221. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 222. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 223. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 224. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 225. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 226. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 227. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 228. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 229. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 230. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 231. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 232. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 233. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 234. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 235. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 236. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 237. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 238. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 239. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 240. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 241. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 242. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 243. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 244. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 245. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 246. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 247. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 248. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 249. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 250. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 251. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 252. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 253. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 254. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 255. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 256. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 257. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 258. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 259. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 260. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 261. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 262. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 263. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 264. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 265. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 266. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 267. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 268. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 269. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 270. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 271. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 272. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 273. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 274. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 275. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 276. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 277. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 278. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 279. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 280. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 281. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 282. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 283. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 284. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 285. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 286. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 287. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 288. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 289. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 290. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 291. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 292. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 293. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 294. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 295. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 296. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 297. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 298. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 299. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 300. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 301. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 302. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 303. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 304. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 305. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 306. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 307. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 308. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 309. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 310. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 311. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 312. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 313. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 314. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 315. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 316. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 317. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 318. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 319. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 320. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 321. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 322. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 323. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 324. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 325. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 326. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 327. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 328. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 329. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 330. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 331. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 332. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 333. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 334. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 335. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 336. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 337. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 338. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 339. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 340. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 341. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 342. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 343. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 344. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 345. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 346. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 347. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 348. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 349. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 350. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 351. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 352. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 353. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 354. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 355. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 356. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 357. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 358. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 359. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 360. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 361. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 362. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 363. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 364. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 365. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 366. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 367. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 368. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 369. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 370. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 371. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 372. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 373. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 374. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 375. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 376. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 377. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 378. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 379. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 380. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 381. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 382. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 383. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 384. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 385. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 386. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 387. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 388. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 389. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 390. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 391. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 392. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 393. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 394. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 395. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 396. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 397. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 398. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 399. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 400. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 401. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 402. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 403. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 404. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 405. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 406. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 407. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 408. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 409. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 410. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 411. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 412. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 413. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 414. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 415. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 416. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 417. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 418. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 419. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 420. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 421. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 422. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 423. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 424. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 425. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 426. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 427. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 428. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 429. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 430. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 431. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 432. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 433. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 434. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 435. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 436. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 437. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 438. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 439. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 440. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 441. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 442. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 443. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 444. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 445. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 446. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 447. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 448. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 449. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 450. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 451. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 452. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 453. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 454. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 455. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 456. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 457. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 458. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 459. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 460. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 461. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 462. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 463. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 464. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 465. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 466. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 467. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 468. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 469. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 470. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 471. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 472. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 473. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 474. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 475. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 476. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 477. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 478. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 479. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 480. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 481. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 482. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 483. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 484. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 485. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 486. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 487. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 488. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 489. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 490. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 491. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 492. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 493. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 494. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 495. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 496. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 497. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 498. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 499. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 500. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 501. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 502. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 503. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 504. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 505. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 506. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 507. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 508. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 509. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 510. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 511. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 512. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 513. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 514. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 515. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 516. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 517. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 518. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 519. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 520. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 521. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 522. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 523. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 524. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 525. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 526. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 527. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 528. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 529. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 530. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 531. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 532. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 533. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 534. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 535. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 536. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 537. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 538. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 539. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 540. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 541. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 542. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 543. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 544. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 545. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 546. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 547. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 548. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 549. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 550. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 551. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 552. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 553. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 554. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 555. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 556. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 557. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 558. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 559. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 560. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 561. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 562. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 563. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 564. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 565. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 566. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 567. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 568. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 569. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 570. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>id</th><th>diagnosis</th><th>radius_mean</th><th>texture_mean</th><th>perimeter_mean</th><th>area_mean</th><th>smoothness_mean</th></tr><tr><th></th><th>Int64</th><th>String</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>569 rows  33 columns (omitted printing of 26 columns)</p><tr><th>1</th><td>842302</td><td>M</td><td>17.99</td><td>10.38</td><td>122.8</td><td>1001.0</td><td>0.1184</td></tr><tr><th>2</th><td>842517</td><td>M</td><td>20.57</td><td>17.77</td><td>132.9</td><td>1326.0</td><td>0.08474</td></tr><tr><th>3</th><td>84300903</td><td>M</td><td>19.69</td><td>21.25</td><td>130.0</td><td>1203.0</td><td>0.1096</td></tr><tr><th>4</th><td>84348301</td><td>M</td><td>11.42</td><td>20.38</td><td>77.58</td><td>386.1</td><td>0.1425</td></tr><tr><th>5</th><td>84358402</td><td>M</td><td>20.29</td><td>14.34</td><td>135.1</td><td>1297.0</td><td>0.1003</td></tr><tr><th>6</th><td>843786</td><td>M</td><td>12.45</td><td>15.7</td><td>82.57</td><td>477.1</td><td>0.1278</td></tr><tr><th>7</th><td>844359</td><td>M</td><td>18.25</td><td>19.98</td><td>119.6</td><td>1040.0</td><td>0.09463</td></tr><tr><th>8</th><td>84458202</td><td>M</td><td>13.71</td><td>20.83</td><td>90.2</td><td>577.9</td><td>0.1189</td></tr><tr><th>9</th><td>844981</td><td>M</td><td>13.0</td><td>21.82</td><td>87.5</td><td>519.8</td><td>0.1273</td></tr><tr><th>10</th><td>84501001</td><td>M</td><td>12.46</td><td>24.04</td><td>83.97</td><td>475.9</td><td>0.1186</td></tr><tr><th>11</th><td>845636</td><td>M</td><td>16.02</td><td>23.24</td><td>102.7</td><td>797.8</td><td>0.08206</td></tr><tr><th>12</th><td>84610002</td><td>M</td><td>15.78</td><td>17.89</td><td>103.6</td><td>781.0</td><td>0.0971</td></tr><tr><th>13</th><td>846226</td><td>M</td><td>19.17</td><td>24.8</td><td>132.4</td><td>1123.0</td><td>0.0974</td></tr><tr><th>14</th><td>846381</td><td>M</td><td>15.85</td><td>23.95</td><td>103.7</td><td>782.7</td><td>0.08401</td></tr><tr><th>15</th><td>84667401</td><td>M</td><td>13.73</td><td>22.61</td><td>93.6</td><td>578.3</td><td>0.1131</td></tr><tr><th>16</th><td>84799002</td><td>M</td><td>14.54</td><td>27.54</td><td>96.73</td><td>658.8</td><td>0.1139</td></tr><tr><th>17</th><td>848406</td><td>M</td><td>14.68</td><td>20.13</td><td>94.74</td><td>684.5</td><td>0.09867</td></tr><tr><th>18</th><td>84862001</td><td>M</td><td>16.13</td><td>20.68</td><td>108.1</td><td>798.8</td><td>0.117</td></tr><tr><th>19</th><td>849014</td><td>M</td><td>19.81</td><td>22.15</td><td>130.0</td><td>1260.0</td><td>0.09831</td></tr><tr><th>20</th><td>8510426</td><td>B</td><td>13.54</td><td>14.36</td><td>87.46</td><td>566.3</td><td>0.09779</td></tr><tr><th>21</th><td>8510653</td><td>B</td><td>13.08</td><td>15.71</td><td>85.63</td><td>520.0</td><td>0.1075</td></tr><tr><th>22</th><td>8510824</td><td>B</td><td>9.504</td><td>12.44</td><td>60.34</td><td>273.9</td><td>0.1024</td></tr><tr><th>23</th><td>8511133</td><td>M</td><td>15.34</td><td>14.26</td><td>102.5</td><td>704.4</td><td>0.1073</td></tr><tr><th>24</th><td>851509</td><td>M</td><td>21.16</td><td>23.04</td><td>137.2</td><td>1404.0</td><td>0.09428</td></tr><tr><th>25</th><td>852552</td><td>M</td><td>16.65</td><td>21.38</td><td>110.0</td><td>904.6</td><td>0.1121</td></tr><tr><th>26</th><td>852631</td><td>M</td><td>17.14</td><td>16.4</td><td>116.0</td><td>912.7</td><td>0.1186</td></tr><tr><th>27</th><td>852763</td><td>M</td><td>14.58</td><td>21.53</td><td>97.41</td><td>644.8</td><td>0.1054</td></tr><tr><th>28</th><td>852781</td><td>M</td><td>18.61</td><td>20.25</td><td>122.1</td><td>1094.0</td><td>0.0944</td></tr><tr><th>29</th><td>852973</td><td>M</td><td>15.3</td><td>25.27</td><td>102.4</td><td>732.4</td><td>0.1082</td></tr><tr><th>30</th><td>853201</td><td>M</td><td>17.57</td><td>15.05</td><td>115.0</td><td>955.1</td><td>0.09847</td></tr><tr><th>31</th><td>853401</td><td>M</td><td>18.63</td><td>25.11</td><td>124.8</td><td>1088.0</td><td>0.1064</td></tr><tr><th>32</th><td>853612</td><td>M</td><td>11.84</td><td>18.7</td><td>77.93</td><td>440.6</td><td>0.1109</td></tr><tr><th>33</th><td>85382601</td><td>M</td><td>17.02</td><td>23.98</td><td>112.8</td><td>899.3</td><td>0.1197</td></tr><tr><th>34</th><td>854002</td><td>M</td><td>19.27</td><td>26.47</td><td>127.9</td><td>1162.0</td><td>0.09401</td></tr><tr><th>35</th><td>854039</td><td>M</td><td>16.13</td><td>17.88</td><td>107.0</td><td>807.2</td><td>0.104</td></tr><tr><th>36</th><td>854253</td><td>M</td><td>16.74</td><td>21.59</td><td>110.1</td><td>869.5</td><td>0.0961</td></tr><tr><th>37</th><td>854268</td><td>M</td><td>14.25</td><td>21.72</td><td>93.63</td><td>633.0</td><td>0.09823</td></tr><tr><th>38</th><td>854941</td><td>B</td><td>13.03</td><td>18.42</td><td>82.61</td><td>523.8</td><td>0.08983</td></tr><tr><th>39</th><td>855133</td><td>M</td><td>14.99</td><td>25.2</td><td>95.54</td><td>698.8</td><td>0.09387</td></tr><tr><th>40</th><td>855138</td><td>M</td><td>13.48</td><td>20.82</td><td>88.4</td><td>559.2</td><td>0.1016</td></tr><tr><th>41</th><td>855167</td><td>M</td><td>13.44</td><td>21.58</td><td>86.18</td><td>563.0</td><td>0.08162</td></tr><tr><th>42</th><td>855563</td><td>M</td><td>10.95</td><td>21.35</td><td>71.9</td><td>371.1</td><td>0.1227</td></tr><tr><th>43</th><td>855625</td><td>M</td><td>19.07</td><td>24.81</td><td>128.3</td><td>1104.0</td><td>0.09081</td></tr><tr><th>44</th><td>856106</td><td>M</td><td>13.28</td><td>20.28</td><td>87.32</td><td>545.2</td><td>0.1041</td></tr><tr><th>45</th><td>85638502</td><td>M</td><td>13.17</td><td>21.81</td><td>85.42</td><td>531.5</td><td>0.09714</td></tr><tr><th>46</th><td>857010</td><td>M</td><td>18.65</td><td>17.6</td><td>123.7</td><td>1076.0</td><td>0.1099</td></tr><tr><th>47</th><td>85713702</td><td>B</td><td>8.196</td><td>16.84</td><td>51.71</td><td>201.9</td><td>0.086</td></tr><tr><th>48</th><td>85715</td><td>M</td><td>13.17</td><td>18.66</td><td>85.98</td><td>534.6</td><td>0.1158</td></tr><tr><th>49</th><td>857155</td><td>B</td><td>12.05</td><td>14.63</td><td>78.04</td><td>449.3</td><td>0.1031</td></tr><tr><th>50</th><td>857156</td><td>B</td><td>13.49</td><td>22.3</td><td>86.91</td><td>561.0</td><td>0.08752</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& id & diagnosis & radius\\_mean & texture\\_mean & perimeter\\_mean & area\\_mean & smoothness\\_mean & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & String & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 842302 & M & 17.99 & 10.38 & 122.8 & 1001.0 & 0.1184 & $\\dots$ \\\\\n",
       "\t2 & 842517 & M & 20.57 & 17.77 & 132.9 & 1326.0 & 0.08474 & $\\dots$ \\\\\n",
       "\t3 & 84300903 & M & 19.69 & 21.25 & 130.0 & 1203.0 & 0.1096 & $\\dots$ \\\\\n",
       "\t4 & 84348301 & M & 11.42 & 20.38 & 77.58 & 386.1 & 0.1425 & $\\dots$ \\\\\n",
       "\t5 & 84358402 & M & 20.29 & 14.34 & 135.1 & 1297.0 & 0.1003 & $\\dots$ \\\\\n",
       "\t6 & 843786 & M & 12.45 & 15.7 & 82.57 & 477.1 & 0.1278 & $\\dots$ \\\\\n",
       "\t7 & 844359 & M & 18.25 & 19.98 & 119.6 & 1040.0 & 0.09463 & $\\dots$ \\\\\n",
       "\t8 & 84458202 & M & 13.71 & 20.83 & 90.2 & 577.9 & 0.1189 & $\\dots$ \\\\\n",
       "\t9 & 844981 & M & 13.0 & 21.82 & 87.5 & 519.8 & 0.1273 & $\\dots$ \\\\\n",
       "\t10 & 84501001 & M & 12.46 & 24.04 & 83.97 & 475.9 & 0.1186 & $\\dots$ \\\\\n",
       "\t11 & 845636 & M & 16.02 & 23.24 & 102.7 & 797.8 & 0.08206 & $\\dots$ \\\\\n",
       "\t12 & 84610002 & M & 15.78 & 17.89 & 103.6 & 781.0 & 0.0971 & $\\dots$ \\\\\n",
       "\t13 & 846226 & M & 19.17 & 24.8 & 132.4 & 1123.0 & 0.0974 & $\\dots$ \\\\\n",
       "\t14 & 846381 & M & 15.85 & 23.95 & 103.7 & 782.7 & 0.08401 & $\\dots$ \\\\\n",
       "\t15 & 84667401 & M & 13.73 & 22.61 & 93.6 & 578.3 & 0.1131 & $\\dots$ \\\\\n",
       "\t16 & 84799002 & M & 14.54 & 27.54 & 96.73 & 658.8 & 0.1139 & $\\dots$ \\\\\n",
       "\t17 & 848406 & M & 14.68 & 20.13 & 94.74 & 684.5 & 0.09867 & $\\dots$ \\\\\n",
       "\t18 & 84862001 & M & 16.13 & 20.68 & 108.1 & 798.8 & 0.117 & $\\dots$ \\\\\n",
       "\t19 & 849014 & M & 19.81 & 22.15 & 130.0 & 1260.0 & 0.09831 & $\\dots$ \\\\\n",
       "\t20 & 8510426 & B & 13.54 & 14.36 & 87.46 & 566.3 & 0.09779 & $\\dots$ \\\\\n",
       "\t21 & 8510653 & B & 13.08 & 15.71 & 85.63 & 520.0 & 0.1075 & $\\dots$ \\\\\n",
       "\t22 & 8510824 & B & 9.504 & 12.44 & 60.34 & 273.9 & 0.1024 & $\\dots$ \\\\\n",
       "\t23 & 8511133 & M & 15.34 & 14.26 & 102.5 & 704.4 & 0.1073 & $\\dots$ \\\\\n",
       "\t24 & 851509 & M & 21.16 & 23.04 & 137.2 & 1404.0 & 0.09428 & $\\dots$ \\\\\n",
       "\t25 & 852552 & M & 16.65 & 21.38 & 110.0 & 904.6 & 0.1121 & $\\dots$ \\\\\n",
       "\t26 & 852631 & M & 17.14 & 16.4 & 116.0 & 912.7 & 0.1186 & $\\dots$ \\\\\n",
       "\t27 & 852763 & M & 14.58 & 21.53 & 97.41 & 644.8 & 0.1054 & $\\dots$ \\\\\n",
       "\t28 & 852781 & M & 18.61 & 20.25 & 122.1 & 1094.0 & 0.0944 & $\\dots$ \\\\\n",
       "\t29 & 852973 & M & 15.3 & 25.27 & 102.4 & 732.4 & 0.1082 & $\\dots$ \\\\\n",
       "\t30 & 853201 & M & 17.57 & 15.05 & 115.0 & 955.1 & 0.09847 & $\\dots$ \\\\\n",
       "\t31 & 853401 & M & 18.63 & 25.11 & 124.8 & 1088.0 & 0.1064 & $\\dots$ \\\\\n",
       "\t32 & 853612 & M & 11.84 & 18.7 & 77.93 & 440.6 & 0.1109 & $\\dots$ \\\\\n",
       "\t33 & 85382601 & M & 17.02 & 23.98 & 112.8 & 899.3 & 0.1197 & $\\dots$ \\\\\n",
       "\t34 & 854002 & M & 19.27 & 26.47 & 127.9 & 1162.0 & 0.09401 & $\\dots$ \\\\\n",
       "\t35 & 854039 & M & 16.13 & 17.88 & 107.0 & 807.2 & 0.104 & $\\dots$ \\\\\n",
       "\t36 & 854253 & M & 16.74 & 21.59 & 110.1 & 869.5 & 0.0961 & $\\dots$ \\\\\n",
       "\t37 & 854268 & M & 14.25 & 21.72 & 93.63 & 633.0 & 0.09823 & $\\dots$ \\\\\n",
       "\t38 & 854941 & B & 13.03 & 18.42 & 82.61 & 523.8 & 0.08983 & $\\dots$ \\\\\n",
       "\t39 & 855133 & M & 14.99 & 25.2 & 95.54 & 698.8 & 0.09387 & $\\dots$ \\\\\n",
       "\t40 & 855138 & M & 13.48 & 20.82 & 88.4 & 559.2 & 0.1016 & $\\dots$ \\\\\n",
       "\t41 & 855167 & M & 13.44 & 21.58 & 86.18 & 563.0 & 0.08162 & $\\dots$ \\\\\n",
       "\t42 & 855563 & M & 10.95 & 21.35 & 71.9 & 371.1 & 0.1227 & $\\dots$ \\\\\n",
       "\t43 & 855625 & M & 19.07 & 24.81 & 128.3 & 1104.0 & 0.09081 & $\\dots$ \\\\\n",
       "\t44 & 856106 & M & 13.28 & 20.28 & 87.32 & 545.2 & 0.1041 & $\\dots$ \\\\\n",
       "\t45 & 85638502 & M & 13.17 & 21.81 & 85.42 & 531.5 & 0.09714 & $\\dots$ \\\\\n",
       "\t46 & 857010 & M & 18.65 & 17.6 & 123.7 & 1076.0 & 0.1099 & $\\dots$ \\\\\n",
       "\t47 & 85713702 & B & 8.196 & 16.84 & 51.71 & 201.9 & 0.086 & $\\dots$ \\\\\n",
       "\t48 & 85715 & M & 13.17 & 18.66 & 85.98 & 534.6 & 0.1158 & $\\dots$ \\\\\n",
       "\t49 & 857155 & B & 12.05 & 14.63 & 78.04 & 449.3 & 0.1031 & $\\dots$ \\\\\n",
       "\t50 & 857156 & B & 13.49 & 22.3 & 86.91 & 561.0 & 0.08752 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "56933 DataFrame. Omitted printing of 28 columns\n",
       " Row  id        diagnosis  radius_mean  texture_mean  perimeter_mean \n",
       "      \u001b[90mInt64\u001b[39m     \u001b[90mString\u001b[39m     \u001b[90mFloat64\u001b[39m      \u001b[90mFloat64\u001b[39m       \u001b[90mFloat64\u001b[39m        \n",
       "\n",
       " 1    842302    M          17.99        10.38         122.8          \n",
       " 2    842517    M          20.57        17.77         132.9          \n",
       " 3    84300903  M          19.69        21.25         130.0          \n",
       " 4    84348301  M          11.42        20.38         77.58          \n",
       " 5    84358402  M          20.29        14.34         135.1          \n",
       " 6    843786    M          12.45        15.7          82.57          \n",
       " 7    844359    M          18.25        19.98         119.6          \n",
       " 8    84458202  M          13.71        20.83         90.2           \n",
       " 9    844981    M          13.0         21.82         87.5           \n",
       " 10   84501001  M          12.46        24.04         83.97          \n",
       " 11   845636    M          16.02        23.24         102.7          \n",
       " 12   84610002  M          15.78        17.89         103.6          \n",
       " 13   846226    M          19.17        24.8          132.4          \n",
       " 14   846381    M          15.85        23.95         103.7          \n",
       " 15   84667401  M          13.73        22.61         93.6           \n",
       " 16   84799002  M          14.54        27.54         96.73          \n",
       " 17   848406    M          14.68        20.13         94.74          \n",
       " 18   84862001  M          16.13        20.68         108.1          \n",
       " 19   849014    M          19.81        22.15         130.0          \n",
       " 20   8510426   B          13.54        14.36         87.46          \n",
       "\n",
       " 549  923169    B          9.683        19.34         61.05          \n",
       " 550  923465    B          10.82        24.21         68.89          \n",
       " 551  923748    B          10.86        21.48         68.51          \n",
       " 552  923780    B          11.13        22.44         71.49          \n",
       " 553  924084    B          12.77        29.43         81.35          \n",
       " 554  924342    B          9.333        21.94         59.01          \n",
       " 555  924632    B          12.88        28.92         82.5           \n",
       " 556  924934    B          10.29        27.61         65.67          \n",
       " 557  924964    B          10.16        19.59         64.73          \n",
       " 558  925236    B          9.423        27.88         59.26          \n",
       " 559  925277    B          14.59        22.68         96.39          \n",
       " 560  925291    B          11.51        23.93         74.52          \n",
       " 561  925292    B          14.05        27.15         91.38          \n",
       " 562  925311    B          11.2         29.37         70.67          \n",
       " 563  925622    M          15.22        30.62         103.4          \n",
       " 564  926125    M          20.92        25.09         143.0          \n",
       " 565  926424    M          21.56        22.39         142.0          \n",
       " 566  926682    M          20.13        28.25         131.2          \n",
       " 567  926954    M          16.6         28.08         108.3          \n",
       " 568  927241    M          20.6         29.33         140.1          \n",
       " 569  92751     B          7.76         24.54         47.92          "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CSV.read(\"./data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nunique</th><th>nmissing</th></tr><tr><th></th><th>Symbol</th><th>Union</th><th>Any</th><th>Union</th><th>Any</th><th>Union</th><th>Nothing</th></tr></thead><tbody><p>31 rows  8 columns (omitted printing of 1 columns)</p><tr><th>1</th><td>diagnosis</td><td></td><td>B</td><td></td><td>M</td><td>2</td><td></td></tr><tr><th>2</th><td>radius_mean</td><td>14.1273</td><td>6.981</td><td>13.37</td><td>28.11</td><td></td><td></td></tr><tr><th>3</th><td>texture_mean</td><td>19.2896</td><td>9.71</td><td>18.84</td><td>39.28</td><td></td><td></td></tr><tr><th>4</th><td>perimeter_mean</td><td>91.969</td><td>43.79</td><td>86.24</td><td>188.5</td><td></td><td></td></tr><tr><th>5</th><td>area_mean</td><td>654.889</td><td>143.5</td><td>551.1</td><td>2501.0</td><td></td><td></td></tr><tr><th>6</th><td>smoothness_mean</td><td>0.0963603</td><td>0.05263</td><td>0.09587</td><td>0.1634</td><td></td><td></td></tr><tr><th>7</th><td>compactness_mean</td><td>0.104341</td><td>0.01938</td><td>0.09263</td><td>0.3454</td><td></td><td></td></tr><tr><th>8</th><td>concavity_mean</td><td>0.0887993</td><td>0.0</td><td>0.06154</td><td>0.4268</td><td></td><td></td></tr><tr><th>9</th><td>concave points_mean</td><td>0.0489191</td><td>0.0</td><td>0.0335</td><td>0.2012</td><td></td><td></td></tr><tr><th>10</th><td>symmetry_mean</td><td>0.181162</td><td>0.106</td><td>0.1792</td><td>0.304</td><td></td><td></td></tr><tr><th>11</th><td>fractal_dimension_mean</td><td>0.0627976</td><td>0.04996</td><td>0.06154</td><td>0.09744</td><td></td><td></td></tr><tr><th>12</th><td>radius_se</td><td>0.405172</td><td>0.1115</td><td>0.3242</td><td>2.873</td><td></td><td></td></tr><tr><th>13</th><td>texture_se</td><td>1.21685</td><td>0.3602</td><td>1.108</td><td>4.885</td><td></td><td></td></tr><tr><th>14</th><td>perimeter_se</td><td>2.86606</td><td>0.757</td><td>2.287</td><td>21.98</td><td></td><td></td></tr><tr><th>15</th><td>area_se</td><td>40.3371</td><td>6.802</td><td>24.53</td><td>542.2</td><td></td><td></td></tr><tr><th>16</th><td>smoothness_se</td><td>0.00704098</td><td>0.001713</td><td>0.00638</td><td>0.03113</td><td></td><td></td></tr><tr><th>17</th><td>compactness_se</td><td>0.0254781</td><td>0.002252</td><td>0.02045</td><td>0.1354</td><td></td><td></td></tr><tr><th>18</th><td>concavity_se</td><td>0.0318937</td><td>0.0</td><td>0.02589</td><td>0.396</td><td></td><td></td></tr><tr><th>19</th><td>concave points_se</td><td>0.0117961</td><td>0.0</td><td>0.01093</td><td>0.05279</td><td></td><td></td></tr><tr><th>20</th><td>symmetry_se</td><td>0.0205423</td><td>0.007882</td><td>0.01873</td><td>0.07895</td><td></td><td></td></tr><tr><th>21</th><td>fractal_dimension_se</td><td>0.0037949</td><td>0.0008948</td><td>0.003187</td><td>0.02984</td><td></td><td></td></tr><tr><th>22</th><td>radius_worst</td><td>16.2692</td><td>7.93</td><td>14.97</td><td>36.04</td><td></td><td></td></tr><tr><th>23</th><td>texture_worst</td><td>25.6772</td><td>12.02</td><td>25.41</td><td>49.54</td><td></td><td></td></tr><tr><th>24</th><td>perimeter_worst</td><td>107.261</td><td>50.41</td><td>97.66</td><td>251.2</td><td></td><td></td></tr><tr><th>25</th><td>area_worst</td><td>880.583</td><td>185.2</td><td>686.5</td><td>4254.0</td><td></td><td></td></tr><tr><th>26</th><td>smoothness_worst</td><td>0.132369</td><td>0.07117</td><td>0.1313</td><td>0.2226</td><td></td><td></td></tr><tr><th>27</th><td>compactness_worst</td><td>0.254265</td><td>0.02729</td><td>0.2119</td><td>1.058</td><td></td><td></td></tr><tr><th>28</th><td>concavity_worst</td><td>0.272188</td><td>0.0</td><td>0.2267</td><td>1.252</td><td></td><td></td></tr><tr><th>29</th><td>concave points_worst</td><td>0.114606</td><td>0.0</td><td>0.09993</td><td>0.291</td><td></td><td></td></tr><tr><th>30</th><td>symmetry_worst</td><td>0.290076</td><td>0.1565</td><td>0.2822</td><td>0.6638</td><td></td><td></td></tr><tr><th>31</th><td>fractal_dimension_worst</td><td>0.0839458</td><td>0.05504</td><td>0.08004</td><td>0.2075</td><td></td><td></td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& variable & mean & min & median & max & nunique & nmissing & \\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union & Any & Union & Any & Union & Nothing & \\\\\n",
       "\t\\hline\n",
       "\t1 & diagnosis &  & B &  & M & 2 &  & $\\dots$ \\\\\n",
       "\t2 & radius\\_mean & 14.1273 & 6.981 & 13.37 & 28.11 &  &  & $\\dots$ \\\\\n",
       "\t3 & texture\\_mean & 19.2896 & 9.71 & 18.84 & 39.28 &  &  & $\\dots$ \\\\\n",
       "\t4 & perimeter\\_mean & 91.969 & 43.79 & 86.24 & 188.5 &  &  & $\\dots$ \\\\\n",
       "\t5 & area\\_mean & 654.889 & 143.5 & 551.1 & 2501.0 &  &  & $\\dots$ \\\\\n",
       "\t6 & smoothness\\_mean & 0.0963603 & 0.05263 & 0.09587 & 0.1634 &  &  & $\\dots$ \\\\\n",
       "\t7 & compactness\\_mean & 0.104341 & 0.01938 & 0.09263 & 0.3454 &  &  & $\\dots$ \\\\\n",
       "\t8 & concavity\\_mean & 0.0887993 & 0.0 & 0.06154 & 0.4268 &  &  & $\\dots$ \\\\\n",
       "\t9 & concave points\\_mean & 0.0489191 & 0.0 & 0.0335 & 0.2012 &  &  & $\\dots$ \\\\\n",
       "\t10 & symmetry\\_mean & 0.181162 & 0.106 & 0.1792 & 0.304 &  &  & $\\dots$ \\\\\n",
       "\t11 & fractal\\_dimension\\_mean & 0.0627976 & 0.04996 & 0.06154 & 0.09744 &  &  & $\\dots$ \\\\\n",
       "\t12 & radius\\_se & 0.405172 & 0.1115 & 0.3242 & 2.873 &  &  & $\\dots$ \\\\\n",
       "\t13 & texture\\_se & 1.21685 & 0.3602 & 1.108 & 4.885 &  &  & $\\dots$ \\\\\n",
       "\t14 & perimeter\\_se & 2.86606 & 0.757 & 2.287 & 21.98 &  &  & $\\dots$ \\\\\n",
       "\t15 & area\\_se & 40.3371 & 6.802 & 24.53 & 542.2 &  &  & $\\dots$ \\\\\n",
       "\t16 & smoothness\\_se & 0.00704098 & 0.001713 & 0.00638 & 0.03113 &  &  & $\\dots$ \\\\\n",
       "\t17 & compactness\\_se & 0.0254781 & 0.002252 & 0.02045 & 0.1354 &  &  & $\\dots$ \\\\\n",
       "\t18 & concavity\\_se & 0.0318937 & 0.0 & 0.02589 & 0.396 &  &  & $\\dots$ \\\\\n",
       "\t19 & concave points\\_se & 0.0117961 & 0.0 & 0.01093 & 0.05279 &  &  & $\\dots$ \\\\\n",
       "\t20 & symmetry\\_se & 0.0205423 & 0.007882 & 0.01873 & 0.07895 &  &  & $\\dots$ \\\\\n",
       "\t21 & fractal\\_dimension\\_se & 0.0037949 & 0.0008948 & 0.003187 & 0.02984 &  &  & $\\dots$ \\\\\n",
       "\t22 & radius\\_worst & 16.2692 & 7.93 & 14.97 & 36.04 &  &  & $\\dots$ \\\\\n",
       "\t23 & texture\\_worst & 25.6772 & 12.02 & 25.41 & 49.54 &  &  & $\\dots$ \\\\\n",
       "\t24 & perimeter\\_worst & 107.261 & 50.41 & 97.66 & 251.2 &  &  & $\\dots$ \\\\\n",
       "\t25 & area\\_worst & 880.583 & 185.2 & 686.5 & 4254.0 &  &  & $\\dots$ \\\\\n",
       "\t26 & smoothness\\_worst & 0.132369 & 0.07117 & 0.1313 & 0.2226 &  &  & $\\dots$ \\\\\n",
       "\t27 & compactness\\_worst & 0.254265 & 0.02729 & 0.2119 & 1.058 &  &  & $\\dots$ \\\\\n",
       "\t28 & concavity\\_worst & 0.272188 & 0.0 & 0.2267 & 1.252 &  &  & $\\dots$ \\\\\n",
       "\t29 & concave points\\_worst & 0.114606 & 0.0 & 0.09993 & 0.291 &  &  & $\\dots$ \\\\\n",
       "\t30 & symmetry\\_worst & 0.290076 & 0.1565 & 0.2822 & 0.6638 &  &  & $\\dots$ \\\\\n",
       "\t31 & fractal\\_dimension\\_worst & 0.0839458 & 0.05504 & 0.08004 & 0.2075 &  &  & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "318 DataFrame. Omitted printing of 3 columns\n",
       " Row  variable                 mean        min        median    max     \n",
       "      \u001b[90mSymbol\u001b[39m                   \u001b[90mUnion\u001b[39m      \u001b[90mAny\u001b[39m        \u001b[90mUnion\u001b[39m    \u001b[90mAny\u001b[39m     \n",
       "\n",
       " 1    diagnosis                            B                    M       \n",
       " 2    radius_mean              14.1273     6.981      13.37     28.11   \n",
       " 3    texture_mean             19.2896     9.71       18.84     39.28   \n",
       " 4    perimeter_mean           91.969      43.79      86.24     188.5   \n",
       " 5    area_mean                654.889     143.5      551.1     2501.0  \n",
       " 6    smoothness_mean          0.0963603   0.05263    0.09587   0.1634  \n",
       " 7    compactness_mean         0.104341    0.01938    0.09263   0.3454  \n",
       " 8    concavity_mean           0.0887993   0.0        0.06154   0.4268  \n",
       " 9    concave points_mean      0.0489191   0.0        0.0335    0.2012  \n",
       " 10   symmetry_mean            0.181162    0.106      0.1792    0.304   \n",
       " 11   fractal_dimension_mean   0.0627976   0.04996    0.06154   0.09744 \n",
       " 12   radius_se                0.405172    0.1115     0.3242    2.873   \n",
       " 13   texture_se               1.21685     0.3602     1.108     4.885   \n",
       " 14   perimeter_se             2.86606     0.757      2.287     21.98   \n",
       " 15   area_se                  40.3371     6.802      24.53     542.2   \n",
       " 16   smoothness_se            0.00704098  0.001713   0.00638   0.03113 \n",
       " 17   compactness_se           0.0254781   0.002252   0.02045   0.1354  \n",
       " 18   concavity_se             0.0318937   0.0        0.02589   0.396   \n",
       " 19   concave points_se        0.0117961   0.0        0.01093   0.05279 \n",
       " 20   symmetry_se              0.0205423   0.007882   0.01873   0.07895 \n",
       " 21   fractal_dimension_se     0.0037949   0.0008948  0.003187  0.02984 \n",
       " 22   radius_worst             16.2692     7.93       14.97     36.04   \n",
       " 23   texture_worst            25.6772     12.02      25.41     49.54   \n",
       " 24   perimeter_worst          107.261     50.41      97.66     251.2   \n",
       " 25   area_worst               880.583     185.2      686.5     4254.0  \n",
       " 26   smoothness_worst         0.132369    0.07117    0.1313    0.2226  \n",
       " 27   compactness_worst        0.254265    0.02729    0.2119    1.058   \n",
       " 28   concavity_worst          0.272188    0.0        0.2267    1.252   \n",
       " 29   concave points_worst     0.114606    0.0        0.09993   0.291   \n",
       " 30   symmetry_worst           0.290076    0.1565     0.2822    0.6638  \n",
       " 31   fractal_dimension_worst  0.0839458   0.05504    0.08004   0.2075  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[:, Not([33, 1])]\n",
    "describe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at class labels to see if dataset is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Int64} with 2 entries:\n",
       "  \"B\" => 357\n",
       "  \"M\" => 212"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = countmap(data[:diagnosis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.6274165202108963\n",
       " 0.37258347978910367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(label_counts[i] / size(data)[1] for i in keys(label_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[22m _.names                 \u001b[0m\u001b[0m\u001b[22m _.types                         \u001b[0m\u001b[0m\u001b[22m _.scitypes    \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m diagnosis               \u001b[0m\u001b[0m CategoricalValue{String,UInt32} \u001b[0m\u001b[0m Multiclass{2} \u001b[0m\u001b[0m\n",
       "\u001b[0m radius_mean             \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m texture_mean            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m perimeter_mean          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m area_mean               \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m smoothness_mean         \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m compactness_mean        \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concavity_mean          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concave points_mean     \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m symmetry_mean           \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m fractal_dimension_mean  \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m radius_se               \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m texture_se              \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m perimeter_se            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m area_se                 \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m smoothness_se           \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m compactness_se          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concavity_se            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concave points_se       \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m symmetry_se             \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m fractal_dimension_se    \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m radius_worst            \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m texture_worst           \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m perimeter_worst         \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m area_worst              \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m smoothness_worst        \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m compactness_worst       \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concavity_worst         \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m concave points_worst    \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m symmetry_worst          \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m fractal_dimension_worst \u001b[0m\u001b[0m Float64                         \u001b[0m\u001b[0m Continuous    \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "_.nrows = 569\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coerce!(data, :diagnosis=>Multiclass)\n",
    "schema(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CategoricalValue{String,UInt32}[\"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\"    \"B\", \"B\", \"B\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"B\"], 56930 DataFrame. Omitted printing of 26 columns\n",
       " Row  radius_mean  texture_mean  perimeter_mean  area_mean \n",
       "      \u001b[90mFloat64\u001b[39m      \u001b[90mFloat64\u001b[39m       \u001b[90mFloat64\u001b[39m         \u001b[90mFloat64\u001b[39m   \n",
       "\n",
       " 1    17.99        10.38         122.8           1001.0    \n",
       " 2    20.57        17.77         132.9           1326.0    \n",
       " 3    19.69        21.25         130.0           1203.0    \n",
       " 4    11.42        20.38         77.58           386.1     \n",
       " 5    20.29        14.34         135.1           1297.0    \n",
       " 6    12.45        15.7          82.57           477.1     \n",
       " 7    18.25        19.98         119.6           1040.0    \n",
       " 8    13.71        20.83         90.2            577.9     \n",
       " 9    13.0         21.82         87.5            519.8     \n",
       " 10   12.46        24.04         83.97           475.9     \n",
       " 11   16.02        23.24         102.7           797.8     \n",
       " 12   15.78        17.89         103.6           781.0     \n",
       " 13   19.17        24.8          132.4           1123.0    \n",
       " 14   15.85        23.95         103.7           782.7     \n",
       " 15   13.73        22.61         93.6            578.3     \n",
       " 16   14.54        27.54         96.73           658.8     \n",
       " 17   14.68        20.13         94.74           684.5     \n",
       " 18   16.13        20.68         108.1           798.8     \n",
       " 19   19.81        22.15         130.0           1260.0    \n",
       " 20   13.54        14.36         87.46           566.3     \n",
       "\n",
       " 549  9.683        19.34         61.05           285.7     \n",
       " 550  10.82        24.21         68.89           361.6     \n",
       " 551  10.86        21.48         68.51           360.5     \n",
       " 552  11.13        22.44         71.49           378.4     \n",
       " 553  12.77        29.43         81.35           507.9     \n",
       " 554  9.333        21.94         59.01           264.0     \n",
       " 555  12.88        28.92         82.5            514.3     \n",
       " 556  10.29        27.61         65.67           321.4     \n",
       " 557  10.16        19.59         64.73           311.7     \n",
       " 558  9.423        27.88         59.26           271.3     \n",
       " 559  14.59        22.68         96.39           657.1     \n",
       " 560  11.51        23.93         74.52           403.5     \n",
       " 561  14.05        27.15         91.38           600.4     \n",
       " 562  11.2         29.37         70.67           386.0     \n",
       " 563  15.22        30.62         103.4           716.9     \n",
       " 564  20.92        25.09         143.0           1347.0    \n",
       " 565  21.56        22.39         142.0           1479.0    \n",
       " 566  20.13        28.25         131.2           1261.0    \n",
       " 567  16.6         28.08         108.3           858.1     \n",
       " 568  20.6         29.33         140.1           1265.0    \n",
       " 569  7.76         24.54         47.92           181.0     )"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, X = unpack(data, ==(:diagnosis), colname->true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition train and test data accoring to class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([483, 534, 392, 159, 390, 31, 320, 27, 170, 416    472, 339, 492, 524, 279, 7, 6, 178, 190, 76], [462, 24, 105, 98, 536, 223, 400, 382, 187, 361    500, 395, 533, 112, 396, 297, 106, 303, 415, 261])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data to use when trying to fit a single validation set\n",
    "train, test = partition(eachindex(y), 0.7, shuffle=true, rng=123) # gives 70:30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.628140703517588\n",
       " 0.37185929648241206"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts = countmap(data[train,:diagnosis])\n",
    "collect(train_counts[i] / size(train)[1] for i in keys(train_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.6257309941520468\n",
       " 0.3742690058479532"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_counts = countmap(data[test,:diagnosis])\n",
    "collect(test_counts[i] / size(test)[1] for i in keys(test_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five Learning Algorithms\n",
    "\n",
    "* Decision trees with some form of pruning\n",
    "* Neural networks\n",
    "* Boosting\n",
    "* Support Vector Machines\n",
    "* k-nearest neighbors\n",
    "\n",
    "\n",
    "##### Testing\n",
    "* Implement the algorithms\n",
    "* Design two *interesting* classification problems. For the purposes of this assignment, a classification problem is just a set of training examples and a set of test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43-element Array{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype),T} where T<:Tuple,1}:\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianLDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianQDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = ScikitLearn, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " (name = ExtraTreesClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n",
       " (name = GaussianNBClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GaussianProcessClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GradientBoostingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = KNNClassifier, package_name = NearestNeighbors, ... )\n",
       " (name = KNeighborsClassifier, package_name = ScikitLearn, ... )\n",
       " (name = LDA, package_name = MultivariateStats, ... )\n",
       " (name = LGBMClassifier, package_name = LightGBM, ... )\n",
       " (name = LinearBinaryClassifier, package_name = GLM, ... )\n",
       " (name = LinearSVC, package_name = LIBSVM, ... )\n",
       " (name = LogisticCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = LogisticClassifier, package_name = ScikitLearn, ... )\n",
       " (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n",
       " (name = NuSVC, package_name = LIBSVM, ... )\n",
       " (name = PassiveAggressiveClassifier, package_name = ScikitLearn, ... )\n",
       " (name = PerceptronClassifier, package_name = ScikitLearn, ... )\n",
       " (name = ProbabilisticSGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVC, package_name = LIBSVM, ... )\n",
       " (name = SVMClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(matching(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVMClassifier(\n",
       "    C = 1.0,\n",
       "    kernel = \"rbf\",\n",
       "    degree = 3,\n",
       "    gamma = \"auto\",\n",
       "    coef0 = 0.0,\n",
       "    shrinking = true,\n",
       "    tol = 0.001,\n",
       "    cache_size = 200,\n",
       "    max_iter = -1,\n",
       "    decision_function_shape = \"ovr\",\n",
       "    random_state = nothing)\u001b[34m @126\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load SVMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "* This should be done in such a way that you can swap out kernel functions. I'd like to see at least two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{SVMClassifier} @493\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  17%[====>                    ]  ETA: 0:00:14\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:07\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:04\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:02\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:01\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:06\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[22m _.measure           \u001b[0m\u001b[0m\u001b[22m _.measurement \u001b[0m\u001b[0m\u001b[22m _.per_fold                                  \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m accuracy            \u001b[0m\u001b[0m 0.956         \u001b[0m\u001b[0m [0.926, 0.937, 1.0, 0.947, 0.958, 0.968]    \u001b[0m\u001b[0m\n",
       "\u001b[0m false_negative_rate \u001b[0m\u001b[0m 0.078         \u001b[0m\u001b[0m [0.069, 0.132, 0.0, 0.0909, 0.0882, 0.0882] \u001b[0m\u001b[0m\n",
       "\u001b[0m false_positive_rate \u001b[0m\u001b[0m 0.0216        \u001b[0m\u001b[0m [0.0758, 0.0175, 0.0, 0.0196, 0.0164, 0.0]  \u001b[0m\u001b[0m\n",
       "\u001b[0m true_negative_rate  \u001b[0m\u001b[0m 0.978         \u001b[0m\u001b[0m [0.924, 0.982, 1.0, 0.98, 0.984, 1.0]       \u001b[0m\u001b[0m\n",
       "\u001b[0m true_positive_rate  \u001b[0m\u001b[0m 0.922         \u001b[0m\u001b[0m [0.931, 0.868, 1.0, 0.909, 0.912, 0.912]    \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "_.per_observation = [missing, missing, missing, missing, missing]\n",
       "_.fitted_params_per_fold = [  ]\n",
       "_.report_per_fold = [  ]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVMClassifier(kernel=\"linear\")\n",
    "svm_mach = machine(svm_model, X, y)\n",
    "fit!(svm_mach, rows=train, verbosity=2)\n",
    "svm_acc = evaluate!(svm_mach, resampling=CV(shuffle=true), measure=[accuracy, fnr, fpr, tnr, tpr], \n",
    "                        verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{SVMClassifier} @666\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  17%[====>                    ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[22m _.measure           \u001b[0m\u001b[0m\u001b[22m _.measurement \u001b[0m\u001b[0m\u001b[22m _.per_fold                                 \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m accuracy            \u001b[0m\u001b[0m 0.627         \u001b[0m\u001b[0m [0.621, 0.695, 0.653, 0.621, 0.526, 0.649] \u001b[0m\u001b[0m\n",
       "\u001b[0m false_negative_rate \u001b[0m\u001b[0m 1.0           \u001b[0m\u001b[0m [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]             \u001b[0m\u001b[0m\n",
       "\u001b[0m false_positive_rate \u001b[0m\u001b[0m 0.0           \u001b[0m\u001b[0m [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]             \u001b[0m\u001b[0m\n",
       "\u001b[0m true_negative_rate  \u001b[0m\u001b[0m 1.0           \u001b[0m\u001b[0m [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]             \u001b[0m\u001b[0m\n",
       "\u001b[0m true_positive_rate  \u001b[0m\u001b[0m 0.0           \u001b[0m\u001b[0m [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]             \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "_.per_observation = [missing, missing, missing, missing, missing]\n",
       "_.fitted_params_per_fold = [  ]\n",
       "_.report_per_fold = [  ]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVMClassifier(kernel=\"sigmoid\")\n",
    "svm_mach = machine(svm_model, X, y)\n",
    "fit!(svm_mach, rows=train, verbosity=2)\n",
    "svm_acc = evaluate!(svm_mach, resampling=CV(shuffle=true), measure=[accuracy, fnr, fpr, tnr, tpr], \n",
    "                        verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{SVMClassifier} @363\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  17%[====>                    ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[22m _.measure           \u001b[0m\u001b[0m\u001b[22m _.measurement \u001b[0m\u001b[0m\u001b[22m _.per_fold                                 \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m accuracy            \u001b[0m\u001b[0m 0.627         \u001b[0m\u001b[0m [0.632, 0.674, 0.632, 0.611, 0.579, 0.638] \u001b[0m\u001b[0m\n",
       "\u001b[0m false_negative_rate \u001b[0m\u001b[0m 1.0           \u001b[0m\u001b[0m [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]             \u001b[0m\u001b[0m\n",
       "\u001b[0m false_positive_rate \u001b[0m\u001b[0m 0.0           \u001b[0m\u001b[0m [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]             \u001b[0m\u001b[0m\n",
       "\u001b[0m true_negative_rate  \u001b[0m\u001b[0m 1.0           \u001b[0m\u001b[0m [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]             \u001b[0m\u001b[0m\n",
       "\u001b[0m true_positive_rate  \u001b[0m\u001b[0m 0.0           \u001b[0m\u001b[0m [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]             \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "_.per_observation = [missing, missing, missing, missing, missing]\n",
       "_.fitted_params_per_fold = [  ]\n",
       "_.report_per_fold = [  ]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVMClassifier(kernel=\"rbf\")\n",
    "svm_mach = machine(svm_model, X, y)\n",
    "fit!(svm_mach, rows=train, verbosity=2)\n",
    "svm_acc = evaluate!(svm_mach, resampling=CV(shuffle=true), measure=[accuracy, fnr, fpr, tnr, tpr], \n",
    "                        verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poly Degree 2 took a long wall clock time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{SVMClassifier} @064\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  17%[====>                    ]  ETA: 0:02:06\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  33%[========>                ]  ETA: 0:01:26\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  50%[============>            ]  ETA: 0:01:04\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:50\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:21\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:02:04\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[22m _.measure           \u001b[0m\u001b[0m\u001b[22m _.measurement \u001b[0m\u001b[0m\u001b[22m _.per_fold                                    \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m accuracy            \u001b[0m\u001b[0m 0.947         \u001b[0m\u001b[0m [0.968, 0.958, 0.905, 0.947, 0.968, 0.936]    \u001b[0m\u001b[0m\n",
       "\u001b[0m false_negative_rate \u001b[0m\u001b[0m 0.0897        \u001b[0m\u001b[0m [0.0345, 0.0476, 0.121, 0.108, 0.0556, 0.171] \u001b[0m\u001b[0m\n",
       "\u001b[0m false_positive_rate \u001b[0m\u001b[0m 0.0305        \u001b[0m\u001b[0m [0.0303, 0.0377, 0.0806, 0.0172, 0.0169, 0.0] \u001b[0m\u001b[0m\n",
       "\u001b[0m true_negative_rate  \u001b[0m\u001b[0m 0.97          \u001b[0m\u001b[0m [0.97, 0.962, 0.919, 0.983, 0.983, 1.0]       \u001b[0m\u001b[0m\n",
       "\u001b[0m true_positive_rate  \u001b[0m\u001b[0m 0.91          \u001b[0m\u001b[0m [0.966, 0.952, 0.879, 0.892, 0.944, 0.829]    \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "_.per_observation = [missing, missing, missing, missing, missing]\n",
       "_.fitted_params_per_fold = [  ]\n",
       "_.report_per_fold = [  ]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVMClassifier(kernel=\"poly\", degree=2)\n",
    "svm_mach = machine(svm_model, X, y)\n",
    "fit!(svm_mach, rows=train, verbosity=2)\n",
    "svm_acc = evaluate!(svm_mach, resampling=CV(shuffle=true), measure=[accuracy, fnr, fpr, tnr, tpr], \n",
    "                        verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't use crossentropy because it is a probabilistic concept, and SVM doesn't do probabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch / RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `linear` and `rbf`: \n",
    "* `linear` which has high bias\n",
    "* `rbf` which has high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First look at `linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{SVMClassifier} @194\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @317\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @791\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_linear_model = SVMClassifier(kernel=\"linear\", cache_size=1000)\n",
    "svm_linear_mach = machine(svm_linear_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{DeterministicTunedModel{Grid,}} @134\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      " Info: Attempting to evaluate 60 models.\n",
      " @ MLJTuning /home/andrew/.julia/packages/MLJTuning/Bbgvk/src/tuned_models.jl:494\n",
      "\u001b[33mEvaluating over 60 metamodels: 100%[=========================] Time: 0:15:36\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(parameter_name = \"C\",\n",
       " parameter_scale = :log10,\n",
       " parameter_values = [0.004999999999999999, 0.006319241014671491, 0.00798656140030127, 0.010093801273395193, 0.012757032600156442, 0.01612295272648197, 0.020376964829358878, 0.02575339038084062, 0.03254837615229085, 0.04113620670850234    607.7371250381435, 768.087473335914, 970.7459728719407, 1226.8755533199082, 1550.584463287389, 1959.7033874236104, 2476.767604479585, 3130.2582860074103, 3956.1713094906586, 4999.999999999999],\n",
       " measurements = [0.9402388951101157, 0.9402388951101157, 0.9384845091452035, 0.9420119447555058, 0.9402575587905936, 0.9402575587905936, 0.9402575587905936, 0.9420306084359836, 0.9402762224710712, 0.9385218365061588    0.9490668159761105, 0.9490668159761105, 0.9473124300111984, 0.9543299738708472, 0.952575587905935, 0.952575587905935, 0.950802538260545, 0.9473124300111984, 0.952575587905935, 0.9508212019410228],)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r1 = range(svm_linear_model, :C, lower=5*10^-6, upper=5*10^4, scale=:log10)\n",
    "r1 = range(svm_linear_model, :C, lower=5*10^-3, upper=5*10^3, scale=:log10)\n",
    "curve = learning_curve(svm_linear_mach, \n",
    "                        range=r1,\n",
    "                        resampling=CV(), \n",
    "                        measure=accuracy, \n",
    "                        acceleration=CPUProcesses(),\n",
    "                        resolution=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip400\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip400)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip401\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip400)\" d=\"\n",
       "M292.458 1410.9 L2352.76 1410.9 L2352.76 47.2441 L292.458 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip402\">\n",
       "    <rect x=\"292\" y=\"47\" width=\"2061\" height=\"1365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  448.286,1410.9 448.286,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  772.232,1410.9 772.232,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1096.18,1410.9 1096.18,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1420.12,1410.9 1420.12,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1744.07,1410.9 1744.07,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2068.02,1410.9 2068.02,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  292.458,1317.33 2352.76,1317.33 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  292.458,1134.4 2352.76,1134.4 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  292.458,951.47 2352.76,951.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  292.458,768.538 2352.76,768.538 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  292.458,585.606 2352.76,585.606 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  292.458,402.674 2352.76,402.674 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip402)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  292.458,219.742 2352.76,219.742 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,1410.9 2352.76,1410.9 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,1410.9 292.458,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  448.286,1410.9 448.286,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  772.232,1410.9 772.232,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1096.18,1410.9 1096.18,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1420.12,1410.9 1420.12,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1744.07,1410.9 1744.07,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2068.02,1410.9 2068.02,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,1317.33 317.182,1317.33 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,1134.4 317.182,1134.4 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,951.47 317.182,951.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,768.538 317.182,768.538 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,585.606 317.182,585.606 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,402.674 317.182,402.674 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,219.742 317.182,219.742 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip400)\" d=\"M 0 0 M402.78 1481.97 L410.419 1481.97 L410.419 1455.6 L402.109 1457.27 L402.109 1453.01 L410.373 1451.34 L415.049 1451.34 L415.049 1481.97 L422.687 1481.97 L422.687 1485.9 L402.78 1485.9 L402.78 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M437.757 1454.42 Q434.146 1454.42 432.317 1457.99 Q430.511 1461.53 430.511 1468.66 Q430.511 1475.77 432.317 1479.33 Q434.146 1482.87 437.757 1482.87 Q441.391 1482.87 443.197 1479.33 Q445.025 1475.77 445.025 1468.66 Q445.025 1461.53 443.197 1457.99 Q441.391 1454.42 437.757 1454.42 M437.757 1450.72 Q443.567 1450.72 446.623 1455.33 Q449.701 1459.91 449.701 1468.66 Q449.701 1477.39 446.623 1481.99 Q443.567 1486.58 437.757 1486.58 Q431.947 1486.58 428.868 1481.99 Q425.812 1477.39 425.812 1468.66 Q425.812 1459.91 428.868 1455.33 Q431.947 1450.72 437.757 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M449.701 1444.82 L473.813 1444.82 L473.813 1448.02 L449.701 1448.02 L449.701 1444.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M481.204 1455.3 L494.464 1455.3 L494.464 1458.49 L476.634 1458.49 L476.634 1455.3 Q478.797 1453.06 482.521 1449.3 Q486.264 1445.52 487.223 1444.43 Q489.047 1442.38 489.762 1440.96 Q490.495 1439.54 490.495 1438.16 Q490.495 1435.92 488.915 1434.51 Q487.354 1433.1 484.834 1433.1 Q483.047 1433.1 481.054 1433.72 Q479.079 1434.34 476.822 1435.6 L476.822 1431.77 Q479.117 1430.85 481.11 1430.38 Q483.104 1429.91 484.759 1429.91 Q489.122 1429.91 491.718 1432.09 Q494.313 1434.27 494.313 1437.92 Q494.313 1439.65 493.655 1441.21 Q493.015 1442.75 491.304 1444.86 Q490.834 1445.4 488.314 1448.02 Q485.793 1450.61 481.204 1455.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M726.576 1481.97 L734.215 1481.97 L734.215 1455.6 L725.905 1457.27 L725.905 1453.01 L734.168 1451.34 L738.844 1451.34 L738.844 1481.97 L746.483 1481.97 L746.483 1485.9 L726.576 1485.9 L726.576 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M761.552 1454.42 Q757.941 1454.42 756.113 1457.99 Q754.307 1461.53 754.307 1468.66 Q754.307 1475.77 756.113 1479.33 Q757.941 1482.87 761.552 1482.87 Q765.187 1482.87 766.992 1479.33 Q768.821 1475.77 768.821 1468.66 Q768.821 1461.53 766.992 1457.99 Q765.187 1454.42 761.552 1454.42 M761.552 1450.72 Q767.363 1450.72 770.418 1455.33 Q773.497 1459.91 773.497 1468.66 Q773.497 1477.39 770.418 1481.99 Q767.363 1486.58 761.552 1486.58 Q755.742 1486.58 752.664 1481.99 Q749.608 1477.39 749.608 1468.66 Q749.608 1459.91 752.664 1455.33 Q755.742 1450.72 761.552 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M773.497 1444.82 L797.608 1444.82 L797.608 1448.02 L773.497 1448.02 L773.497 1444.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M802.386 1455.3 L808.592 1455.3 L808.592 1433.87 L801.84 1435.23 L801.84 1431.77 L808.555 1430.41 L812.354 1430.41 L812.354 1455.3 L818.56 1455.3 L818.56 1458.49 L802.386 1458.49 L802.386 1455.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M1063.35 1481.97 L1070.99 1481.97 L1070.99 1455.6 L1062.68 1457.27 L1062.68 1453.01 L1070.94 1451.34 L1075.62 1451.34 L1075.62 1481.97 L1083.26 1481.97 L1083.26 1485.9 L1063.35 1485.9 L1063.35 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M1098.33 1454.42 Q1094.71 1454.42 1092.89 1457.99 Q1091.08 1461.53 1091.08 1468.66 Q1091.08 1475.77 1092.89 1479.33 Q1094.71 1482.87 1098.33 1482.87 Q1101.96 1482.87 1103.77 1479.33 Q1105.59 1475.77 1105.59 1468.66 Q1105.59 1461.53 1103.77 1457.99 Q1101.96 1454.42 1098.33 1454.42 M1098.33 1450.72 Q1104.14 1450.72 1107.19 1455.33 Q1110.27 1459.91 1110.27 1468.66 Q1110.27 1477.39 1107.19 1481.99 Q1104.14 1486.58 1098.33 1486.58 Q1092.52 1486.58 1089.44 1481.99 Q1086.38 1477.39 1086.38 1468.66 Q1086.38 1459.91 1089.44 1455.33 Q1092.52 1450.72 1098.33 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M1119.97 1432.91 Q1117.04 1432.91 1115.55 1435.81 Q1114.09 1438.69 1114.09 1444.48 Q1114.09 1450.26 1115.55 1453.15 Q1117.04 1456.03 1119.97 1456.03 Q1122.93 1456.03 1124.39 1453.15 Q1125.88 1450.26 1125.88 1444.48 Q1125.88 1438.69 1124.39 1435.81 Q1122.93 1432.91 1119.97 1432.91 M1119.97 1429.91 Q1124.7 1429.91 1127.18 1433.65 Q1129.68 1437.37 1129.68 1444.48 Q1129.68 1451.57 1127.18 1455.31 Q1124.7 1459.04 1119.97 1459.04 Q1115.25 1459.04 1112.75 1455.31 Q1110.27 1451.57 1110.27 1444.48 Q1110.27 1437.37 1112.75 1433.65 Q1115.25 1429.91 1119.97 1429.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M1388.64 1481.97 L1396.28 1481.97 L1396.28 1455.6 L1387.97 1457.27 L1387.97 1453.01 L1396.23 1451.34 L1400.91 1451.34 L1400.91 1481.97 L1408.55 1481.97 L1408.55 1485.9 L1388.64 1485.9 L1388.64 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M1423.62 1454.42 Q1420.01 1454.42 1418.18 1457.99 Q1416.37 1461.53 1416.37 1468.66 Q1416.37 1475.77 1418.18 1479.33 Q1420.01 1482.87 1423.62 1482.87 Q1427.25 1482.87 1429.06 1479.33 Q1430.88 1475.77 1430.88 1468.66 Q1430.88 1461.53 1429.06 1457.99 Q1427.25 1454.42 1423.62 1454.42 M1423.62 1450.72 Q1429.43 1450.72 1432.48 1455.33 Q1435.56 1459.91 1435.56 1468.66 Q1435.56 1477.39 1432.48 1481.99 Q1429.43 1486.58 1423.62 1486.58 Q1417.81 1486.58 1414.73 1481.99 Q1411.67 1477.39 1411.67 1468.66 Q1411.67 1459.91 1414.73 1455.33 Q1417.81 1450.72 1423.62 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M1436.11 1455.3 L1442.31 1455.3 L1442.31 1433.87 L1435.56 1435.23 L1435.56 1431.77 L1442.28 1430.41 L1446.07 1430.41 L1446.07 1455.3 L1452.28 1455.3 L1452.28 1458.49 L1436.11 1458.49 L1436.11 1455.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M1712.03 1481.97 L1719.67 1481.97 L1719.67 1455.6 L1711.36 1457.27 L1711.36 1453.01 L1719.62 1451.34 L1724.3 1451.34 L1724.3 1481.97 L1731.94 1481.97 L1731.94 1485.9 L1712.03 1485.9 L1712.03 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M1747.01 1454.42 Q1743.4 1454.42 1741.57 1457.99 Q1739.76 1461.53 1739.76 1468.66 Q1739.76 1475.77 1741.57 1479.33 Q1743.4 1482.87 1747.01 1482.87 Q1750.64 1482.87 1752.45 1479.33 Q1754.28 1475.77 1754.28 1468.66 Q1754.28 1461.53 1752.45 1457.99 Q1750.64 1454.42 1747.01 1454.42 M1747.01 1450.72 Q1752.82 1450.72 1755.87 1455.33 Q1758.95 1459.91 1758.95 1468.66 Q1758.95 1477.39 1755.87 1481.99 Q1752.82 1486.58 1747.01 1486.58 Q1741.2 1486.58 1738.12 1481.99 Q1735.06 1477.39 1735.06 1468.66 Q1735.06 1459.91 1738.12 1455.33 Q1741.2 1450.72 1747.01 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M1763.52 1455.3 L1776.78 1455.3 L1776.78 1458.49 L1758.95 1458.49 L1758.95 1455.3 Q1761.12 1453.06 1764.84 1449.3 Q1768.58 1445.52 1769.54 1444.43 Q1771.37 1442.38 1772.08 1440.96 Q1772.81 1439.54 1772.81 1438.16 Q1772.81 1435.92 1771.23 1434.51 Q1769.67 1433.1 1767.15 1433.1 Q1765.37 1433.1 1763.37 1433.72 Q1761.4 1434.34 1759.14 1435.6 L1759.14 1431.77 Q1761.43 1430.85 1763.43 1430.38 Q1765.42 1429.91 1767.08 1429.91 Q1771.44 1429.91 1774.04 1432.09 Q1776.63 1434.27 1776.63 1437.92 Q1776.63 1439.65 1775.97 1441.21 Q1775.33 1442.75 1773.62 1444.86 Q1773.15 1445.4 1770.63 1448.02 Q1768.11 1450.61 1763.52 1455.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M2035.65 1481.97 L2043.29 1481.97 L2043.29 1455.6 L2034.98 1457.27 L2034.98 1453.01 L2043.24 1451.34 L2047.92 1451.34 L2047.92 1481.97 L2055.56 1481.97 L2055.56 1485.9 L2035.65 1485.9 L2035.65 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M2070.62 1454.42 Q2067.01 1454.42 2065.18 1457.99 Q2063.38 1461.53 2063.38 1468.66 Q2063.38 1475.77 2065.18 1479.33 Q2067.01 1482.87 2070.62 1482.87 Q2074.26 1482.87 2076.06 1479.33 Q2077.89 1475.77 2077.89 1468.66 Q2077.89 1461.53 2076.06 1457.99 Q2074.26 1454.42 2070.62 1454.42 M2070.62 1450.72 Q2076.43 1450.72 2079.49 1455.33 Q2082.57 1459.91 2082.57 1468.66 Q2082.57 1477.39 2079.49 1481.99 Q2076.43 1486.58 2070.62 1486.58 Q2064.81 1486.58 2061.74 1481.99 Q2058.68 1477.39 2058.68 1468.66 Q2058.68 1459.91 2061.74 1455.33 Q2064.81 1450.72 2070.62 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M2095.26 1443.35 Q2097.99 1443.94 2099.51 1445.78 Q2101.06 1447.62 2101.06 1450.33 Q2101.06 1454.49 2098.2 1456.76 Q2095.34 1459.04 2090.07 1459.04 Q2088.31 1459.04 2086.42 1458.68 Q2084.56 1458.34 2082.57 1457.65 L2082.57 1453.98 Q2084.15 1454.9 2086.03 1455.37 Q2087.91 1455.84 2089.96 1455.84 Q2093.53 1455.84 2095.4 1454.43 Q2097.28 1453.02 2097.28 1450.33 Q2097.28 1447.85 2095.53 1446.46 Q2093.8 1445.05 2090.69 1445.05 L2087.42 1445.05 L2087.42 1441.92 L2090.84 1441.92 Q2093.65 1441.92 2095.13 1440.81 Q2096.62 1439.69 2096.62 1437.58 Q2096.62 1435.42 2095.08 1434.27 Q2093.55 1433.1 2090.69 1433.1 Q2089.13 1433.1 2087.35 1433.44 Q2085.56 1433.78 2083.42 1434.49 L2083.42 1431.11 Q2085.58 1430.51 2087.46 1430.21 Q2089.36 1429.91 2091.03 1429.91 Q2095.36 1429.91 2097.88 1431.88 Q2100.4 1433.84 2100.4 1437.18 Q2100.4 1439.52 2099.06 1441.13 Q2097.73 1442.73 2095.26 1443.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M141.237 1303.13 Q137.626 1303.13 135.797 1306.7 Q133.992 1310.24 133.992 1317.37 Q133.992 1324.48 135.797 1328.04 Q137.626 1331.58 141.237 1331.58 Q144.871 1331.58 146.677 1328.04 Q148.505 1324.48 148.505 1317.37 Q148.505 1310.24 146.677 1306.7 Q144.871 1303.13 141.237 1303.13 M141.237 1299.43 Q147.047 1299.43 150.103 1304.04 Q153.181 1308.62 153.181 1317.37 Q153.181 1326.1 150.103 1330.7 Q147.047 1335.29 141.237 1335.29 Q135.427 1335.29 132.348 1330.7 Q129.293 1326.1 129.293 1317.37 Q129.293 1308.62 132.348 1304.04 Q135.427 1299.43 141.237 1299.43 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M158.251 1328.73 L163.135 1328.73 L163.135 1334.61 L158.251 1334.61 L158.251 1328.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M168.343 1333.9 L168.343 1329.64 Q170.103 1330.47 171.908 1330.91 Q173.714 1331.35 175.45 1331.35 Q180.079 1331.35 182.51 1328.25 Q184.964 1325.12 185.311 1318.78 Q183.968 1320.77 181.908 1321.84 Q179.848 1322.9 177.348 1322.9 Q172.163 1322.9 169.13 1319.78 Q166.121 1316.63 166.121 1311.19 Q166.121 1305.86 169.269 1302.65 Q172.417 1299.43 177.649 1299.43 Q183.644 1299.43 186.792 1304.04 Q189.964 1308.62 189.964 1317.37 Q189.964 1325.54 186.075 1330.42 Q182.209 1335.29 175.658 1335.29 Q173.899 1335.29 172.093 1334.94 Q170.288 1334.59 168.343 1333.9 M177.649 1319.24 Q180.797 1319.24 182.626 1317.09 Q184.477 1314.94 184.477 1311.19 Q184.477 1307.46 182.626 1305.31 Q180.797 1303.13 177.649 1303.13 Q174.501 1303.13 172.649 1305.31 Q170.82 1307.46 170.82 1311.19 Q170.82 1314.94 172.649 1317.09 Q174.501 1319.24 177.649 1319.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M209.2 1315.98 Q212.556 1316.7 214.431 1318.97 Q216.329 1321.23 216.329 1324.57 Q216.329 1329.68 212.811 1332.48 Q209.292 1335.29 202.811 1335.29 Q200.635 1335.29 198.32 1334.85 Q196.028 1334.43 193.575 1333.57 L193.575 1329.06 Q195.519 1330.19 197.834 1330.77 Q200.149 1331.35 202.672 1331.35 Q207.07 1331.35 209.362 1329.61 Q211.676 1327.88 211.676 1324.57 Q211.676 1321.51 209.524 1319.8 Q207.394 1318.06 203.575 1318.06 L199.547 1318.06 L199.547 1314.22 L203.76 1314.22 Q207.209 1314.22 209.037 1312.86 Q210.866 1311.47 210.866 1308.87 Q210.866 1306.21 208.968 1304.8 Q207.093 1303.36 203.575 1303.36 Q201.653 1303.36 199.454 1303.78 Q197.255 1304.2 194.616 1305.08 L194.616 1300.91 Q197.278 1300.17 199.593 1299.8 Q201.931 1299.43 203.991 1299.43 Q209.315 1299.43 212.417 1301.86 Q215.519 1304.27 215.519 1308.39 Q215.519 1311.26 213.875 1313.25 Q212.232 1315.22 209.2 1315.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M220.218 1300.05 L242.44 1300.05 L242.44 1302.05 L229.894 1334.61 L225.01 1334.61 L236.815 1303.99 L220.218 1303.99 L220.218 1300.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M247.556 1300.05 L265.912 1300.05 L265.912 1303.99 L251.838 1303.99 L251.838 1312.46 Q252.857 1312.11 253.875 1311.95 Q254.894 1311.77 255.912 1311.77 Q261.699 1311.77 265.079 1314.94 Q268.458 1318.11 268.458 1323.53 Q268.458 1329.11 264.986 1332.21 Q261.514 1335.29 255.195 1335.29 Q253.019 1335.29 250.75 1334.92 Q248.505 1334.54 246.097 1333.8 L246.097 1329.11 Q248.181 1330.24 250.403 1330.79 Q252.625 1331.35 255.102 1331.35 Q259.107 1331.35 261.445 1329.24 Q263.783 1327.14 263.783 1323.53 Q263.783 1319.92 261.445 1317.81 Q259.107 1315.7 255.102 1315.7 Q253.227 1315.7 251.352 1316.12 Q249.5 1316.54 247.556 1317.42 L247.556 1300.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M138.205 1120.2 Q134.593 1120.2 132.765 1123.77 Q130.959 1127.31 130.959 1134.44 Q130.959 1141.54 132.765 1145.11 Q134.593 1148.65 138.205 1148.65 Q141.839 1148.65 143.644 1145.11 Q145.473 1141.54 145.473 1134.44 Q145.473 1127.31 143.644 1123.77 Q141.839 1120.2 138.205 1120.2 M138.205 1116.5 Q144.015 1116.5 147.07 1121.1 Q150.149 1125.69 150.149 1134.44 Q150.149 1143.16 147.07 1147.77 Q144.015 1152.35 138.205 1152.35 Q132.394 1152.35 129.316 1147.77 Q126.26 1143.16 126.26 1134.44 Q126.26 1125.69 129.316 1121.1 Q132.394 1116.5 138.205 1116.5 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M155.218 1145.8 L160.103 1145.8 L160.103 1151.68 L155.218 1151.68 L155.218 1145.8 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M165.311 1150.96 L165.311 1146.71 Q167.07 1147.54 168.876 1147.98 Q170.681 1148.42 172.417 1148.42 Q177.047 1148.42 179.477 1145.32 Q181.931 1142.19 182.278 1135.85 Q180.936 1137.84 178.876 1138.9 Q176.815 1139.97 174.315 1139.97 Q169.13 1139.97 166.098 1136.84 Q163.089 1133.7 163.089 1128.26 Q163.089 1122.93 166.237 1119.71 Q169.385 1116.5 174.616 1116.5 Q180.612 1116.5 183.76 1121.1 Q186.931 1125.69 186.931 1134.44 Q186.931 1142.61 183.042 1147.49 Q179.177 1152.35 172.626 1152.35 Q170.866 1152.35 169.061 1152.01 Q167.255 1151.66 165.311 1150.96 M174.616 1136.31 Q177.765 1136.31 179.593 1134.16 Q181.445 1132.01 181.445 1128.26 Q181.445 1124.53 179.593 1122.38 Q177.765 1120.2 174.616 1120.2 Q171.468 1120.2 169.616 1122.38 Q167.788 1124.53 167.788 1128.26 Q167.788 1132.01 169.616 1134.16 Q171.468 1136.31 174.616 1136.31 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M204.848 1121.2 L193.042 1139.65 L204.848 1139.65 L204.848 1121.2 M203.621 1117.12 L209.5 1117.12 L209.5 1139.65 L214.431 1139.65 L214.431 1143.53 L209.5 1143.53 L209.5 1151.68 L204.848 1151.68 L204.848 1143.53 L189.246 1143.53 L189.246 1139.02 L203.621 1117.12 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M229.5 1120.2 Q225.889 1120.2 224.061 1123.77 Q222.255 1127.31 222.255 1134.44 Q222.255 1141.54 224.061 1145.11 Q225.889 1148.65 229.5 1148.65 Q233.135 1148.65 234.94 1145.11 Q236.769 1141.54 236.769 1134.44 Q236.769 1127.31 234.94 1123.77 Q233.135 1120.2 229.5 1120.2 M229.5 1116.5 Q235.31 1116.5 238.366 1121.1 Q241.445 1125.69 241.445 1134.44 Q241.445 1143.16 238.366 1147.77 Q235.31 1152.35 229.5 1152.35 Q223.69 1152.35 220.612 1147.77 Q217.556 1143.16 217.556 1134.44 Q217.556 1125.69 220.612 1121.1 Q223.69 1116.5 229.5 1116.5 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M256.514 1120.2 Q252.903 1120.2 251.074 1123.77 Q249.269 1127.31 249.269 1134.44 Q249.269 1141.54 251.074 1145.11 Q252.903 1148.65 256.514 1148.65 Q260.148 1148.65 261.954 1145.11 Q263.783 1141.54 263.783 1134.44 Q263.783 1127.31 261.954 1123.77 Q260.148 1120.2 256.514 1120.2 M256.514 1116.5 Q262.324 1116.5 265.38 1121.1 Q268.458 1125.69 268.458 1134.44 Q268.458 1143.16 265.38 1147.77 Q262.324 1152.35 256.514 1152.35 Q250.704 1152.35 247.625 1147.77 Q244.57 1143.16 244.57 1134.44 Q244.57 1125.69 247.625 1121.1 Q250.704 1116.5 256.514 1116.5 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M140.797 937.269 Q137.186 937.269 135.357 940.834 Q133.552 944.375 133.552 951.505 Q133.552 958.612 135.357 962.176 Q137.186 965.718 140.797 965.718 Q144.431 965.718 146.237 962.176 Q148.066 958.612 148.066 951.505 Q148.066 944.375 146.237 940.834 Q144.431 937.269 140.797 937.269 M140.797 933.565 Q146.607 933.565 149.663 938.172 Q152.741 942.755 152.741 951.505 Q152.741 960.232 149.663 964.838 Q146.607 969.422 140.797 969.422 Q134.987 969.422 131.908 964.838 Q128.853 960.232 128.853 951.505 Q128.853 942.755 131.908 938.172 Q134.987 933.565 140.797 933.565 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M157.811 962.871 L162.695 962.871 L162.695 968.75 L157.811 968.75 L157.811 962.871 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M167.903 968.033 L167.903 963.774 Q169.663 964.607 171.468 965.047 Q173.274 965.486 175.01 965.486 Q179.64 965.486 182.07 962.385 Q184.524 959.26 184.871 952.917 Q183.528 954.908 181.468 955.973 Q179.408 957.037 176.908 957.037 Q171.723 957.037 168.69 953.912 Q165.681 950.764 165.681 945.325 Q165.681 940 168.829 936.783 Q171.978 933.565 177.209 933.565 Q183.204 933.565 186.352 938.172 Q189.524 942.755 189.524 951.505 Q189.524 959.676 185.635 964.561 Q181.769 969.422 175.218 969.422 Q173.459 969.422 171.653 969.074 Q169.848 968.727 167.903 968.033 M177.209 953.38 Q180.357 953.38 182.186 951.227 Q184.038 949.075 184.038 945.325 Q184.038 941.598 182.186 939.445 Q180.357 937.269 177.209 937.269 Q174.061 937.269 172.209 939.445 Q170.38 941.598 170.38 945.325 Q170.38 949.075 172.209 951.227 Q174.061 953.38 177.209 953.38 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M207.44 938.264 L195.635 956.713 L207.44 956.713 L207.44 938.264 M206.213 934.19 L212.093 934.19 L212.093 956.713 L217.024 956.713 L217.024 960.602 L212.093 960.602 L212.093 968.75 L207.44 968.75 L207.44 960.602 L191.839 960.602 L191.839 956.088 L206.213 934.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M226.121 964.815 L242.44 964.815 L242.44 968.75 L220.496 968.75 L220.496 964.815 Q223.158 962.061 227.741 957.431 Q232.348 952.778 233.528 951.436 Q235.773 948.912 236.653 947.176 Q237.556 945.417 237.556 943.727 Q237.556 940.973 235.611 939.237 Q233.69 937.501 230.588 937.501 Q228.389 937.501 225.936 938.264 Q223.505 939.028 220.727 940.579 L220.727 935.857 Q223.551 934.723 226.005 934.144 Q228.459 933.565 230.496 933.565 Q235.866 933.565 239.06 936.251 Q242.255 938.936 242.255 943.426 Q242.255 945.556 241.445 947.477 Q240.658 949.375 238.551 951.968 Q237.973 952.639 234.871 955.857 Q231.769 959.051 226.121 964.815 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M247.556 934.19 L265.912 934.19 L265.912 938.126 L251.838 938.126 L251.838 946.598 Q252.857 946.25 253.875 946.088 Q254.894 945.903 255.912 945.903 Q261.699 945.903 265.079 949.075 Q268.458 952.246 268.458 957.662 Q268.458 963.241 264.986 966.343 Q261.514 969.422 255.195 969.422 Q253.019 969.422 250.75 969.051 Q248.505 968.681 246.097 967.94 L246.097 963.241 Q248.181 964.375 250.403 964.931 Q252.625 965.486 255.102 965.486 Q259.107 965.486 261.445 963.38 Q263.783 961.274 263.783 957.662 Q263.783 954.051 261.445 951.945 Q259.107 949.838 255.102 949.838 Q253.227 949.838 251.352 950.255 Q249.5 950.672 247.556 951.551 L247.556 934.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M139.2 754.337 Q135.589 754.337 133.76 757.902 Q131.955 761.443 131.955 768.573 Q131.955 775.68 133.76 779.244 Q135.589 782.786 139.2 782.786 Q142.834 782.786 144.64 779.244 Q146.468 775.68 146.468 768.573 Q146.468 761.443 144.64 757.902 Q142.834 754.337 139.2 754.337 M139.2 750.633 Q145.01 750.633 148.066 755.24 Q151.144 759.823 151.144 768.573 Q151.144 777.3 148.066 781.906 Q145.01 786.49 139.2 786.49 Q133.39 786.49 130.311 781.906 Q127.256 777.3 127.256 768.573 Q127.256 759.823 130.311 755.24 Q133.39 750.633 139.2 750.633 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M156.214 779.939 L161.098 779.939 L161.098 785.818 L156.214 785.818 L156.214 779.939 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M166.306 785.101 L166.306 780.842 Q168.065 781.675 169.871 782.115 Q171.677 782.554 173.413 782.554 Q178.042 782.554 180.473 779.453 Q182.927 776.328 183.274 769.985 Q181.931 771.976 179.871 773.041 Q177.811 774.105 175.311 774.105 Q170.126 774.105 167.093 770.98 Q164.084 767.832 164.084 762.393 Q164.084 757.068 167.232 753.851 Q170.38 750.633 175.612 750.633 Q181.607 750.633 184.755 755.24 Q187.926 759.823 187.926 768.573 Q187.926 776.744 184.038 781.629 Q180.172 786.49 173.621 786.49 Q171.862 786.49 170.056 786.142 Q168.251 785.795 166.306 785.101 M175.612 770.448 Q178.76 770.448 180.589 768.295 Q182.44 766.143 182.44 762.393 Q182.44 758.666 180.589 756.513 Q178.76 754.337 175.612 754.337 Q172.464 754.337 170.612 756.513 Q168.783 758.666 168.783 762.393 Q168.783 766.143 170.612 768.295 Q172.464 770.448 175.612 770.448 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M205.843 755.332 L194.038 773.781 L205.843 773.781 L205.843 755.332 M204.616 751.258 L210.496 751.258 L210.496 773.781 L215.426 773.781 L215.426 777.67 L210.496 777.67 L210.496 785.818 L205.843 785.818 L205.843 777.67 L190.241 777.67 L190.241 773.156 L204.616 751.258 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M220.542 751.258 L238.898 751.258 L238.898 755.194 L224.824 755.194 L224.824 763.666 Q225.843 763.318 226.861 763.156 Q227.88 762.971 228.898 762.971 Q234.686 762.971 238.065 766.143 Q241.445 769.314 241.445 774.73 Q241.445 780.309 237.973 783.411 Q234.5 786.49 228.181 786.49 Q226.005 786.49 223.736 786.119 Q221.491 785.749 219.084 785.008 L219.084 780.309 Q221.167 781.443 223.389 781.999 Q225.611 782.554 228.088 782.554 Q232.093 782.554 234.431 780.448 Q236.769 778.342 236.769 774.73 Q236.769 771.119 234.431 769.013 Q232.093 766.906 228.088 766.906 Q226.213 766.906 224.338 767.323 Q222.486 767.74 220.542 768.619 L220.542 751.258 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M256.514 754.337 Q252.903 754.337 251.074 757.902 Q249.269 761.443 249.269 768.573 Q249.269 775.68 251.074 779.244 Q252.903 782.786 256.514 782.786 Q260.148 782.786 261.954 779.244 Q263.783 775.68 263.783 768.573 Q263.783 761.443 261.954 757.902 Q260.148 754.337 256.514 754.337 M256.514 750.633 Q262.324 750.633 265.38 755.24 Q268.458 759.823 268.458 768.573 Q268.458 777.3 265.38 781.906 Q262.324 786.49 256.514 786.49 Q250.704 786.49 247.625 781.906 Q244.57 777.3 244.57 768.573 Q244.57 759.823 247.625 755.24 Q250.704 750.633 256.514 750.633 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M140.103 571.405 Q136.492 571.405 134.663 574.97 Q132.857 578.511 132.857 585.641 Q132.857 592.748 134.663 596.312 Q136.492 599.854 140.103 599.854 Q143.737 599.854 145.542 596.312 Q147.371 592.748 147.371 585.641 Q147.371 578.511 145.542 574.97 Q143.737 571.405 140.103 571.405 M140.103 567.701 Q145.913 567.701 148.968 572.308 Q152.047 576.891 152.047 585.641 Q152.047 594.368 148.968 598.974 Q145.913 603.558 140.103 603.558 Q134.293 603.558 131.214 598.974 Q128.158 594.368 128.158 585.641 Q128.158 576.891 131.214 572.308 Q134.293 567.701 140.103 567.701 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M157.116 597.007 L162.001 597.007 L162.001 602.886 L157.116 602.886 L157.116 597.007 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M167.209 602.169 L167.209 597.91 Q168.968 598.743 170.774 599.183 Q172.579 599.622 174.315 599.622 Q178.945 599.622 181.376 596.521 Q183.829 593.396 184.177 587.053 Q182.834 589.044 180.774 590.109 Q178.714 591.173 176.214 591.173 Q171.028 591.173 167.996 588.048 Q164.987 584.9 164.987 579.461 Q164.987 574.136 168.135 570.919 Q171.283 567.701 176.515 567.701 Q182.51 567.701 185.658 572.308 Q188.829 576.891 188.829 585.641 Q188.829 593.812 184.94 598.697 Q181.075 603.558 174.524 603.558 Q172.765 603.558 170.959 603.21 Q169.153 602.863 167.209 602.169 M176.515 587.516 Q179.663 587.516 181.491 585.363 Q183.343 583.211 183.343 579.461 Q183.343 575.734 181.491 573.581 Q179.663 571.405 176.515 571.405 Q173.366 571.405 171.515 573.581 Q169.686 575.734 169.686 579.461 Q169.686 583.211 171.515 585.363 Q173.366 587.516 176.515 587.516 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M206.746 572.4 L194.94 590.849 L206.746 590.849 L206.746 572.4 M205.519 568.326 L211.399 568.326 L211.399 590.849 L216.329 590.849 L216.329 594.738 L211.399 594.738 L211.399 602.886 L206.746 602.886 L206.746 594.738 L191.144 594.738 L191.144 590.224 L205.519 568.326 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M220.218 568.326 L242.44 568.326 L242.44 570.317 L229.894 602.886 L225.01 602.886 L236.815 572.262 L220.218 572.262 L220.218 568.326 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M247.556 568.326 L265.912 568.326 L265.912 572.262 L251.838 572.262 L251.838 580.734 Q252.857 580.386 253.875 580.224 Q254.894 580.039 255.912 580.039 Q261.699 580.039 265.079 583.211 Q268.458 586.382 268.458 591.798 Q268.458 597.377 264.986 600.479 Q261.514 603.558 255.195 603.558 Q253.019 603.558 250.75 603.187 Q248.505 602.817 246.097 602.076 L246.097 597.377 Q248.181 598.511 250.403 599.067 Q252.625 599.622 255.102 599.622 Q259.107 599.622 261.445 597.516 Q263.783 595.41 263.783 591.798 Q263.783 588.187 261.445 586.081 Q259.107 583.974 255.102 583.974 Q253.227 583.974 251.352 584.391 Q249.5 584.808 247.556 585.687 L247.556 568.326 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M139.686 388.473 Q136.075 388.473 134.246 392.038 Q132.441 395.579 132.441 402.709 Q132.441 409.816 134.246 413.38 Q136.075 416.922 139.686 416.922 Q143.32 416.922 145.126 413.38 Q146.954 409.816 146.954 402.709 Q146.954 395.579 145.126 392.038 Q143.32 388.473 139.686 388.473 M139.686 384.769 Q145.496 384.769 148.552 389.376 Q151.63 393.959 151.63 402.709 Q151.63 411.436 148.552 416.042 Q145.496 420.626 139.686 420.626 Q133.876 420.626 130.797 416.042 Q127.742 411.436 127.742 402.709 Q127.742 393.959 130.797 389.376 Q133.876 384.769 139.686 384.769 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M156.7 414.075 L161.584 414.075 L161.584 419.954 L156.7 419.954 L156.7 414.075 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M166.792 419.237 L166.792 414.978 Q168.552 415.811 170.357 416.251 Q172.163 416.69 173.899 416.69 Q178.528 416.69 180.959 413.589 Q183.413 410.464 183.76 404.121 Q182.417 406.112 180.357 407.177 Q178.297 408.241 175.797 408.241 Q170.612 408.241 167.579 405.116 Q164.57 401.968 164.57 396.529 Q164.57 391.204 167.718 387.987 Q170.866 384.769 176.098 384.769 Q182.093 384.769 185.241 389.376 Q188.413 393.959 188.413 402.709 Q188.413 410.88 184.524 415.765 Q180.658 420.626 174.107 420.626 Q172.348 420.626 170.542 420.278 Q168.737 419.931 166.792 419.237 M176.098 404.584 Q179.246 404.584 181.075 402.431 Q182.927 400.279 182.927 396.529 Q182.927 392.802 181.075 390.649 Q179.246 388.473 176.098 388.473 Q172.95 388.473 171.098 390.649 Q169.269 392.802 169.269 396.529 Q169.269 400.279 171.098 402.431 Q172.95 404.584 176.098 404.584 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M193.528 385.394 L211.885 385.394 L211.885 389.33 L197.811 389.33 L197.811 397.802 Q198.829 397.454 199.848 397.292 Q200.866 397.107 201.885 397.107 Q207.672 397.107 211.051 400.279 Q214.431 403.45 214.431 408.866 Q214.431 414.445 210.959 417.547 Q207.487 420.626 201.167 420.626 Q198.991 420.626 196.723 420.255 Q194.477 419.885 192.07 419.144 L192.07 414.445 Q194.153 415.579 196.376 416.135 Q198.598 416.69 201.075 416.69 Q205.079 416.69 207.417 414.584 Q209.755 412.478 209.755 408.866 Q209.755 405.255 207.417 403.149 Q205.079 401.042 201.075 401.042 Q199.2 401.042 197.325 401.459 Q195.473 401.876 193.528 402.755 L193.528 385.394 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M229.5 388.473 Q225.889 388.473 224.061 392.038 Q222.255 395.579 222.255 402.709 Q222.255 409.816 224.061 413.38 Q225.889 416.922 229.5 416.922 Q233.135 416.922 234.94 413.38 Q236.769 409.816 236.769 402.709 Q236.769 395.579 234.94 392.038 Q233.135 388.473 229.5 388.473 M229.5 384.769 Q235.31 384.769 238.366 389.376 Q241.445 393.959 241.445 402.709 Q241.445 411.436 238.366 416.042 Q235.31 420.626 229.5 420.626 Q223.69 420.626 220.612 416.042 Q217.556 411.436 217.556 402.709 Q217.556 393.959 220.612 389.376 Q223.69 384.769 229.5 384.769 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M256.514 388.473 Q252.903 388.473 251.074 392.038 Q249.269 395.579 249.269 402.709 Q249.269 409.816 251.074 413.38 Q252.903 416.922 256.514 416.922 Q260.148 416.922 261.954 413.38 Q263.783 409.816 263.783 402.709 Q263.783 395.579 261.954 392.038 Q260.148 388.473 256.514 388.473 M256.514 384.769 Q262.324 384.769 265.38 389.376 Q268.458 393.959 268.458 402.709 Q268.458 411.436 265.38 416.042 Q262.324 420.626 256.514 420.626 Q250.704 420.626 247.625 416.042 Q244.57 411.436 244.57 402.709 Q244.57 393.959 247.625 389.376 Q250.704 384.769 256.514 384.769 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M142.279 205.541 Q138.667 205.541 136.839 209.106 Q135.033 212.647 135.033 219.777 Q135.033 226.884 136.839 230.448 Q138.667 233.99 142.279 233.99 Q145.913 233.99 147.718 230.448 Q149.547 226.884 149.547 219.777 Q149.547 212.647 147.718 209.106 Q145.913 205.541 142.279 205.541 M142.279 201.837 Q148.089 201.837 151.144 206.444 Q154.223 211.027 154.223 219.777 Q154.223 228.504 151.144 233.11 Q148.089 237.694 142.279 237.694 Q136.468 237.694 133.39 233.11 Q130.334 228.504 130.334 219.777 Q130.334 211.027 133.39 206.444 Q136.468 201.837 142.279 201.837 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M159.292 231.143 L164.177 231.143 L164.177 237.022 L159.292 237.022 L159.292 231.143 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M169.385 236.305 L169.385 232.046 Q171.144 232.879 172.95 233.319 Q174.755 233.758 176.491 233.758 Q181.121 233.758 183.552 230.657 Q186.005 227.532 186.352 221.189 Q185.01 223.18 182.95 224.245 Q180.889 225.309 178.39 225.309 Q173.204 225.309 170.172 222.184 Q167.163 219.036 167.163 213.597 Q167.163 208.272 170.311 205.055 Q173.459 201.837 178.69 201.837 Q184.686 201.837 187.834 206.444 Q191.005 211.027 191.005 219.777 Q191.005 227.948 187.116 232.833 Q183.251 237.694 176.7 237.694 Q174.94 237.694 173.135 237.346 Q171.329 236.999 169.385 236.305 M178.69 221.652 Q181.839 221.652 183.667 219.499 Q185.519 217.347 185.519 213.597 Q185.519 209.87 183.667 207.717 Q181.839 205.541 178.69 205.541 Q175.542 205.541 173.69 207.717 Q171.862 209.87 171.862 213.597 Q171.862 217.347 173.69 219.499 Q175.542 221.652 178.69 221.652 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M196.121 202.462 L214.477 202.462 L214.477 206.398 L200.403 206.398 L200.403 214.87 Q201.422 214.522 202.44 214.36 Q203.459 214.175 204.477 214.175 Q210.264 214.175 213.644 217.347 Q217.024 220.518 217.024 225.934 Q217.024 231.513 213.551 234.615 Q210.079 237.694 203.76 237.694 Q201.584 237.694 199.315 237.323 Q197.07 236.953 194.663 236.212 L194.663 231.513 Q196.746 232.647 198.968 233.203 Q201.19 233.758 203.667 233.758 Q207.672 233.758 210.01 231.652 Q212.348 229.546 212.348 225.934 Q212.348 222.323 210.01 220.217 Q207.672 218.11 203.667 218.11 Q201.792 218.11 199.917 218.527 Q198.065 218.944 196.121 219.823 L196.121 202.462 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M226.121 233.087 L242.44 233.087 L242.44 237.022 L220.496 237.022 L220.496 233.087 Q223.158 230.333 227.741 225.703 Q232.348 221.05 233.528 219.708 Q235.773 217.184 236.653 215.448 Q237.556 213.689 237.556 211.999 Q237.556 209.245 235.611 207.509 Q233.69 205.773 230.588 205.773 Q228.389 205.773 225.936 206.536 Q223.505 207.3 220.727 208.851 L220.727 204.129 Q223.551 202.995 226.005 202.416 Q228.459 201.837 230.496 201.837 Q235.866 201.837 239.06 204.523 Q242.255 207.208 242.255 211.698 Q242.255 213.828 241.445 215.749 Q240.658 217.647 238.551 220.24 Q237.973 220.911 234.871 224.129 Q231.769 227.323 226.121 233.087 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M247.556 202.462 L265.912 202.462 L265.912 206.398 L251.838 206.398 L251.838 214.87 Q252.857 214.522 253.875 214.36 Q254.894 214.175 255.912 214.175 Q261.699 214.175 265.079 217.347 Q268.458 220.518 268.458 225.934 Q268.458 231.513 264.986 234.615 Q261.514 237.694 255.195 237.694 Q253.019 237.694 250.75 237.323 Q248.505 236.953 246.097 236.212 L246.097 231.513 Q248.181 232.647 250.403 233.203 Q252.625 233.758 255.102 233.758 Q259.107 233.758 261.445 231.652 Q263.783 229.546 263.783 225.934 Q263.783 222.323 261.445 220.217 Q259.107 218.11 255.102 218.11 Q253.227 218.11 251.352 218.527 Q249.5 218.944 247.556 219.823 L247.556 202.462 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M1341.77 1518.33 L1341.77 1525.11 Q1338.52 1522.08 1334.83 1520.59 Q1331.17 1519.09 1327.03 1519.09 Q1318.88 1519.09 1314.55 1524.09 Q1310.23 1529.05 1310.23 1538.47 Q1310.23 1547.86 1314.55 1552.86 Q1318.88 1557.82 1327.03 1557.82 Q1331.17 1557.82 1334.83 1556.33 Q1338.52 1554.83 1341.77 1551.81 L1341.77 1558.53 Q1338.39 1560.82 1334.61 1561.96 Q1330.85 1563.11 1326.65 1563.11 Q1315.86 1563.11 1309.65 1556.52 Q1303.45 1549.9 1303.45 1538.47 Q1303.45 1527.01 1309.65 1520.43 Q1315.86 1513.81 1326.65 1513.81 Q1330.91 1513.81 1334.67 1514.95 Q1338.46 1516.07 1341.77 1518.33 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M44.1444 1020.68 L50.9239 1020.68 Q47.9002 1023.93 46.4043 1027.62 Q44.9083 1031.28 44.9083 1035.42 Q44.9083 1043.57 49.9054 1047.9 Q54.8707 1052.23 64.2919 1052.23 Q73.6813 1052.23 78.6784 1047.9 Q83.6436 1043.57 83.6436 1035.42 Q83.6436 1031.28 82.1477 1027.62 Q80.6518 1023.93 77.6281 1020.68 L84.3439 1020.68 Q86.6355 1024.06 87.7814 1027.84 Q88.9272 1031.6 88.9272 1035.8 Q88.9272 1046.59 82.3387 1052.8 Q75.7183 1059 64.2919 1059 Q52.8336 1059 46.2451 1052.8 Q39.6248 1046.59 39.6248 1035.8 Q39.6248 1031.54 40.7706 1027.78 Q41.8846 1023.99 44.1444 1020.68 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M38.479 1014.54 L38.479 1008.68 L88.0042 1008.68 L88.0042 1014.54 L38.479 1014.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M70.0847 986.34 Q70.0847 993.438 71.7079 996.175 Q73.3312 998.913 77.2461 998.913 Q80.3653 998.913 82.2114 996.876 Q84.0256 994.807 84.0256 991.274 Q84.0256 986.404 80.5881 983.476 Q77.1188 980.516 71.3897 980.516 L70.0847 980.516 L70.0847 986.34 M67.6657 974.659 L88.0042 974.659 L88.0042 980.516 L82.5933 980.516 Q85.8398 982.521 87.3994 985.513 Q88.9272 988.505 88.9272 992.833 Q88.9272 998.308 85.8716 1001.55 Q82.7843 1004.77 77.6281 1004.77 Q71.6125 1004.77 68.5569 1000.76 Q65.5014 996.717 65.5014 988.728 L65.5014 980.516 L64.9285 980.516 Q60.8862 980.516 58.6901 983.189 Q56.4621 985.831 56.4621 990.637 Q56.4621 993.693 57.1941 996.589 Q57.9262 999.486 59.3903 1002.16 L53.9795 1002.16 Q52.7381 998.945 52.1334 995.921 Q51.4968 992.897 51.4968 990.033 Q51.4968 982.298 55.5072 978.479 Q59.5176 974.659 67.6657 974.659 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M53.4065 945.791 L58.9447 945.791 Q57.6716 948.273 57.035 950.947 Q56.3984 953.621 56.3984 956.485 Q56.3984 960.846 57.7352 963.042 Q59.072 965.206 61.7456 965.206 Q63.7826 965.206 64.9603 963.647 Q66.1061 962.087 67.1565 957.376 L67.6021 955.371 Q68.9389 949.133 71.3897 946.523 Q73.8086 943.881 78.1691 943.881 Q83.1344 943.881 86.0308 947.828 Q88.9272 951.743 88.9272 958.618 Q88.9272 961.482 88.3543 964.602 Q87.8132 967.689 86.6992 971.126 L80.6518 971.126 Q82.3387 967.88 83.198 964.729 Q84.0256 961.578 84.0256 958.49 Q84.0256 954.353 82.6251 952.125 Q81.1929 949.897 78.6147 949.897 Q76.2276 949.897 74.9545 951.52 Q73.6813 953.111 72.5037 958.554 L72.0262 960.591 Q70.8804 966.034 68.5251 968.453 Q66.138 970.872 62.0002 970.872 Q56.9713 970.872 54.2341 967.307 Q51.4968 963.742 51.4968 957.185 Q51.4968 953.939 51.9743 951.074 Q52.4517 948.21 53.4065 945.791 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M53.4065 915.013 L58.9447 915.013 Q57.6716 917.495 57.035 920.169 Q56.3984 922.842 56.3984 925.707 Q56.3984 930.068 57.7352 932.264 Q59.072 934.428 61.7456 934.428 Q63.7826 934.428 64.9603 932.868 Q66.1061 931.309 67.1565 926.598 L67.6021 924.593 Q68.9389 918.355 71.3897 915.745 Q73.8086 913.103 78.1691 913.103 Q83.1344 913.103 86.0308 917.05 Q88.9272 920.965 88.9272 927.84 Q88.9272 930.704 88.3543 933.823 Q87.8132 936.911 86.6992 940.348 L80.6518 940.348 Q82.3387 937.102 83.198 933.951 Q84.0256 930.8 84.0256 927.712 Q84.0256 923.575 82.6251 921.347 Q81.1929 919.119 78.6147 919.119 Q76.2276 919.119 74.9545 920.742 Q73.6813 922.333 72.5037 927.776 L72.0262 929.813 Q70.8804 935.256 68.5251 937.675 Q66.138 940.094 62.0002 940.094 Q56.9713 940.094 54.2341 936.529 Q51.4968 932.964 51.4968 926.407 Q51.4968 923.161 51.9743 920.296 Q52.4517 917.432 53.4065 915.013 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M52.3562 906.96 L52.3562 901.104 L88.0042 901.104 L88.0042 906.96 L52.3562 906.96 M38.479 906.96 L38.479 901.104 L45.895 901.104 L45.895 906.96 L38.479 906.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M38.479 876.914 L43.3487 876.914 L43.3487 882.516 Q43.3487 885.667 44.6219 886.908 Q45.895 888.118 49.2052 888.118 L52.3562 888.118 L52.3562 878.474 L56.9077 878.474 L56.9077 888.118 L88.0042 888.118 L88.0042 894.006 L56.9077 894.006 L56.9077 899.608 L52.3562 899.608 L52.3562 894.006 L49.8736 894.006 Q43.9216 894.006 41.2162 891.237 Q38.479 888.468 38.479 882.452 L38.479 876.914 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M52.3562 870.771 L52.3562 864.915 L88.0042 864.915 L88.0042 870.771 L52.3562 870.771 M38.479 870.771 L38.479 864.915 L45.895 864.915 L45.895 870.771 L38.479 870.771 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M53.7248 833.118 L59.1993 833.118 Q57.8307 835.6 57.1623 838.115 Q56.4621 840.598 56.4621 843.144 Q56.4621 848.841 60.0905 851.992 Q63.6872 855.143 70.212 855.143 Q76.7369 855.143 80.3653 851.992 Q83.9619 848.841 83.9619 843.144 Q83.9619 840.598 83.2935 838.115 Q82.5933 835.6 81.2247 833.118 L86.6355 833.118 Q87.7814 835.569 88.3543 838.21 Q88.9272 840.82 88.9272 843.78 Q88.9272 851.833 83.8664 856.575 Q78.8057 861.318 70.212 861.318 Q61.491 861.318 56.4939 856.544 Q51.4968 851.738 51.4968 843.398 Q51.4968 840.693 52.0697 838.115 Q52.6108 835.537 53.7248 833.118 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M70.0847 810.774 Q70.0847 817.872 71.7079 820.609 Q73.3312 823.346 77.2461 823.346 Q80.3653 823.346 82.2114 821.309 Q84.0256 819.241 84.0256 815.708 Q84.0256 810.838 80.5881 807.91 Q77.1188 804.95 71.3897 804.95 L70.0847 804.95 L70.0847 810.774 M67.6657 799.093 L88.0042 799.093 L88.0042 804.95 L82.5933 804.95 Q85.8398 806.955 87.3994 809.947 Q88.9272 812.939 88.9272 817.267 Q88.9272 822.742 85.8716 825.988 Q82.7843 829.203 77.6281 829.203 Q71.6125 829.203 68.5569 825.193 Q65.5014 821.15 65.5014 813.161 L65.5014 804.95 L64.9285 804.95 Q60.8862 804.95 58.6901 807.623 Q56.4621 810.265 56.4621 815.071 Q56.4621 818.127 57.1941 821.023 Q57.9262 823.919 59.3903 826.593 L53.9795 826.593 Q52.7381 823.378 52.1334 820.355 Q51.4968 817.331 51.4968 814.466 Q51.4968 806.732 55.5072 802.913 Q59.5176 799.093 67.6657 799.093 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M42.2347 787.157 L52.3562 787.157 L52.3562 775.094 L56.9077 775.094 L56.9077 787.157 L76.2594 787.157 Q80.6199 787.157 81.8613 785.98 Q83.1026 784.77 83.1026 781.11 L83.1026 775.094 L88.0042 775.094 L88.0042 781.11 Q88.0042 787.889 85.4897 790.468 Q82.9434 793.046 76.2594 793.046 L56.9077 793.046 L56.9077 797.343 L52.3562 797.343 L52.3562 793.046 L42.2347 793.046 L42.2347 787.157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M52.3562 768.952 L52.3562 763.095 L88.0042 763.095 L88.0042 768.952 L52.3562 768.952 M38.479 768.952 L38.479 763.095 L45.895 763.095 L45.895 768.952 L38.479 768.952 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M56.4621 743.139 Q56.4621 747.849 60.1542 750.586 Q63.8145 753.324 70.212 753.324 Q76.6095 753.324 80.3017 750.618 Q83.9619 747.881 83.9619 743.139 Q83.9619 738.46 80.2698 735.723 Q76.5777 732.985 70.212 732.985 Q63.8781 732.985 60.186 735.723 Q56.4621 738.46 56.4621 743.139 M51.4968 743.139 Q51.4968 735.5 56.4621 731.139 Q61.4273 726.779 70.212 726.779 Q78.9649 726.779 83.9619 731.139 Q88.9272 735.5 88.9272 743.139 Q88.9272 750.809 83.9619 755.17 Q78.9649 759.498 70.212 759.498 Q61.4273 759.498 56.4621 755.17 Q51.4968 750.809 51.4968 743.139 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M66.4881 691.003 L88.0042 691.003 L88.0042 696.86 L66.679 696.86 Q61.6183 696.86 59.1038 698.833 Q56.5894 700.807 56.5894 704.753 Q56.5894 709.496 59.6131 712.233 Q62.6368 714.97 67.8567 714.97 L88.0042 714.97 L88.0042 720.859 L52.3562 720.859 L52.3562 714.97 L57.8944 714.97 Q54.6797 712.87 53.0883 710.037 Q51.4968 707.172 51.4968 703.448 Q51.4968 697.305 55.3163 694.154 Q59.1038 691.003 66.4881 691.003 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M46.818 648.003 L70.4666 656.724 L70.4666 639.25 L46.818 648.003 M40.4842 651.632 L40.4842 644.343 L88.0042 626.232 L88.0042 632.916 L75.8138 637.245 L75.8138 658.666 L88.0042 662.994 L88.0042 669.774 L40.4842 651.632 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M53.7248 595.582 L59.1993 595.582 Q57.8307 598.064 57.1623 600.579 Q56.4621 603.061 56.4621 605.607 Q56.4621 611.305 60.0905 614.456 Q63.6872 617.607 70.212 617.607 Q76.7369 617.607 80.3653 614.456 Q83.9619 611.305 83.9619 605.607 Q83.9619 603.061 83.2935 600.579 Q82.5933 598.064 81.2247 595.582 L86.6355 595.582 Q87.7814 598.032 88.3543 600.674 Q88.9272 603.284 88.9272 606.244 Q88.9272 614.297 83.8664 619.039 Q78.8057 623.782 70.212 623.782 Q61.491 623.782 56.4939 619.007 Q51.4968 614.201 51.4968 605.862 Q51.4968 603.157 52.0697 600.579 Q52.6108 598 53.7248 595.582 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M53.7248 563.785 L59.1993 563.785 Q57.8307 566.267 57.1623 568.782 Q56.4621 571.265 56.4621 573.811 Q56.4621 579.508 60.0905 582.659 Q63.6872 585.81 70.212 585.81 Q76.7369 585.81 80.3653 582.659 Q83.9619 579.508 83.9619 573.811 Q83.9619 571.265 83.2935 568.782 Q82.5933 566.267 81.2247 563.785 L86.6355 563.785 Q87.7814 566.236 88.3543 568.877 Q88.9272 571.487 88.9272 574.447 Q88.9272 582.5 83.8664 587.242 Q78.8057 591.985 70.212 591.985 Q61.491 591.985 56.4939 587.211 Q51.4968 582.404 51.4968 574.065 Q51.4968 571.36 52.0697 568.782 Q52.6108 566.204 53.7248 563.785 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M73.9359 558.247 L52.3562 558.247 L52.3562 552.39 L73.7131 552.39 Q78.7739 552.39 81.3202 550.417 Q83.8346 548.443 83.8346 544.497 Q83.8346 539.754 80.8109 537.017 Q77.7872 534.248 72.5673 534.248 L52.3562 534.248 L52.3562 528.391 L88.0042 528.391 L88.0042 534.248 L82.5296 534.248 Q85.7762 536.38 87.3676 539.213 Q88.9272 542.014 88.9272 545.738 Q88.9272 551.881 85.1078 555.064 Q81.2883 558.247 73.9359 558.247 M51.4968 543.51 L51.4968 543.51 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M57.8307 501.592 Q57.2578 502.579 57.0032 503.756 Q56.7167 504.902 56.7167 506.302 Q56.7167 511.268 59.9632 513.941 Q63.1779 516.583 69.2253 516.583 L88.0042 516.583 L88.0042 522.471 L52.3562 522.471 L52.3562 516.583 L57.8944 516.583 Q54.6479 514.737 53.0883 511.777 Q51.4968 508.817 51.4968 504.584 Q51.4968 503.979 51.5923 503.247 Q51.656 502.515 51.8151 501.624 L57.8307 501.592 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M70.0847 479.248 Q70.0847 486.346 71.7079 489.083 Q73.3312 491.82 77.2461 491.82 Q80.3653 491.82 82.2114 489.783 Q84.0256 487.715 84.0256 484.182 Q84.0256 479.312 80.5881 476.384 Q77.1188 473.424 71.3897 473.424 L70.0847 473.424 L70.0847 479.248 M67.6657 467.567 L88.0042 467.567 L88.0042 473.424 L82.5933 473.424 Q85.8398 475.429 87.3994 478.421 Q88.9272 481.413 88.9272 485.741 Q88.9272 491.216 85.8716 494.462 Q82.7843 497.677 77.6281 497.677 Q71.6125 497.677 68.5569 493.667 Q65.5014 489.624 65.5014 481.635 L65.5014 473.424 L64.9285 473.424 Q60.8862 473.424 58.6901 476.097 Q56.4621 478.739 56.4621 483.545 Q56.4621 486.601 57.1941 489.497 Q57.9262 492.393 59.3903 495.067 L53.9795 495.067 Q52.7381 491.852 52.1334 488.829 Q51.4968 485.805 51.4968 482.94 Q51.4968 475.206 55.5072 471.387 Q59.5176 467.567 67.6657 467.567 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M53.7248 435.77 L59.1993 435.77 Q57.8307 438.253 57.1623 440.768 Q56.4621 443.25 56.4621 445.796 Q56.4621 451.494 60.0905 454.645 Q63.6872 457.796 70.212 457.796 Q76.7369 457.796 80.3653 454.645 Q83.9619 451.494 83.9619 445.796 Q83.9619 443.25 83.2935 440.768 Q82.5933 438.253 81.2247 435.77 L86.6355 435.77 Q87.7814 438.221 88.3543 440.863 Q88.9272 443.473 88.9272 446.433 Q88.9272 454.486 83.8664 459.228 Q78.8057 463.971 70.212 463.971 Q61.491 463.971 56.4939 459.196 Q51.4968 454.39 51.4968 446.051 Q51.4968 443.346 52.0697 440.768 Q52.6108 438.189 53.7248 435.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M91.3143 414.795 Q97.68 417.278 99.6216 419.633 Q101.563 421.989 101.563 425.935 L101.563 430.614 L96.6615 430.614 L96.6615 427.177 Q96.6615 424.758 95.5157 423.421 Q94.3699 422.084 90.1048 420.461 L87.4312 419.411 L52.3562 433.829 L52.3562 427.622 L80.238 416.482 L52.3562 405.342 L52.3562 399.136 L91.3143 414.795 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip402)\" style=\"stroke:#009af9; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  350.769,1116.92 383.712,1116.92 416.656,1245.3 449.6,987.183 482.543,1115.56 515.487,1115.56 548.431,1115.56 581.374,985.817 614.318,1114.19 647.262,1242.56 \n",
       "  680.206,1372.3 713.149,987.183 746.093,730.436 779.037,345.316 811.98,345.316 844.924,345.316 877.868,345.316 910.811,215.577 943.755,343.95 976.699,216.943 \n",
       "  1009.64,345.316 1042.59,345.316 1075.53,87.2037 1108.47,216.943 1141.42,343.95 1174.36,214.211 1207.3,214.211 1240.25,342.585 1273.19,343.95 1306.14,85.838 \n",
       "  1339.08,214.211 1372.02,85.838 1404.97,729.07 1437.91,985.817 1470.85,856.078 1503.8,1114.19 1536.74,727.705 1569.68,342.585 1602.63,472.324 1635.57,214.211 \n",
       "  1668.52,600.697 1701.46,730.436 1734.4,602.063 1767.35,214.211 1800.29,214.211 1833.23,214.211 1866.18,214.211 1899.12,342.585 1932.07,214.211 1965.01,342.585 \n",
       "  1997.95,470.958 2030.9,470.958 2063.84,599.331 2096.78,85.838 2129.73,214.211 2162.67,214.211 2195.61,343.95 2228.56,599.331 2261.5,214.211 2294.45,342.585 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip400)\" d=\"\n",
       "M1855.83 213.659 L2284.08 213.659 L2284.08 92.6992 L1855.83 92.6992  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1855.83,213.659 2284.08,213.659 2284.08,92.6992 1855.83,92.6992 1855.83,213.659 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip400)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1878.72,153.179 2016.08,153.179 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip400)\" d=\"M 0 0 M2052.16 170.459 L2038.97 135.899 L2043.85 135.899 L2054.8 164.996 L2065.77 135.899 L2070.63 135.899 L2057.46 170.459 L2052.16 170.459 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M2083.2 157.427 Q2078.04 157.427 2076.05 158.607 Q2074.06 159.788 2074.06 162.635 Q2074.06 164.904 2075.54 166.246 Q2077.05 167.566 2079.62 167.566 Q2083.16 167.566 2085.29 165.066 Q2087.44 162.543 2087.44 158.376 L2087.44 157.427 L2083.2 157.427 M2091.7 155.668 L2091.7 170.459 L2087.44 170.459 L2087.44 166.524 Q2085.98 168.885 2083.81 170.019 Q2081.63 171.13 2078.48 171.13 Q2074.5 171.13 2072.14 168.908 Q2069.8 166.663 2069.8 162.913 Q2069.8 158.538 2072.72 156.316 Q2075.66 154.094 2081.47 154.094 L2087.44 154.094 L2087.44 153.677 Q2087.44 150.737 2085.49 149.14 Q2083.57 147.519 2080.08 147.519 Q2077.86 147.519 2075.75 148.052 Q2073.64 148.584 2071.7 149.649 L2071.7 145.714 Q2074.04 144.811 2076.24 144.371 Q2078.43 143.908 2080.52 143.908 Q2086.14 143.908 2088.92 146.825 Q2091.7 149.742 2091.7 155.668 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M2096.17 134.441 L2100.43 134.441 L2100.43 170.459 L2096.17 170.459 L2096.17 134.441 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M2104.89 144.533 L2109.15 144.533 L2109.15 170.459 L2104.89 170.459 L2104.89 144.533 M2104.89 134.441 L2109.15 134.441 L2109.15 139.834 L2104.89 139.834 L2104.89 134.441 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M2130.68 148.469 L2130.68 134.441 L2134.94 134.441 L2134.94 170.459 L2130.68 170.459 L2130.68 166.57 Q2129.34 168.885 2127.28 170.019 Q2125.24 171.13 2122.37 171.13 Q2117.67 171.13 2114.71 167.38 Q2111.77 163.631 2111.77 157.519 Q2111.77 151.408 2114.71 147.658 Q2117.67 143.908 2122.37 143.908 Q2125.24 143.908 2127.28 145.043 Q2129.34 146.154 2130.68 148.469 M2116.17 157.519 Q2116.17 162.218 2118.09 164.904 Q2120.03 167.566 2123.41 167.566 Q2126.79 167.566 2128.74 164.904 Q2130.68 162.218 2130.68 157.519 Q2130.68 152.82 2128.74 150.158 Q2126.79 147.473 2123.41 147.473 Q2120.03 147.473 2118.09 150.158 Q2116.17 152.82 2116.17 157.519 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M2151.19 157.427 Q2146.03 157.427 2144.04 158.607 Q2142.05 159.788 2142.05 162.635 Q2142.05 164.904 2143.53 166.246 Q2145.03 167.566 2147.6 167.566 Q2151.14 167.566 2153.27 165.066 Q2155.43 162.543 2155.43 158.376 L2155.43 157.427 L2151.19 157.427 M2159.68 155.668 L2159.68 170.459 L2155.43 170.459 L2155.43 166.524 Q2153.97 168.885 2151.79 170.019 Q2149.61 171.13 2146.47 171.13 Q2142.49 171.13 2140.12 168.908 Q2137.79 166.663 2137.79 162.913 Q2137.79 158.538 2140.7 156.316 Q2143.64 154.094 2149.45 154.094 L2155.43 154.094 L2155.43 153.677 Q2155.43 150.737 2153.48 149.14 Q2151.56 147.519 2148.06 147.519 Q2145.84 147.519 2143.74 148.052 Q2141.63 148.584 2139.68 149.649 L2139.68 145.714 Q2142.02 144.811 2144.22 144.371 Q2146.42 143.908 2148.5 143.908 Q2154.13 143.908 2156.91 146.825 Q2159.68 149.742 2159.68 155.668 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M2168.36 137.172 L2168.36 144.533 L2177.14 144.533 L2177.14 147.844 L2168.36 147.844 L2168.36 161.918 Q2168.36 165.089 2169.22 165.992 Q2170.1 166.894 2172.76 166.894 L2177.14 166.894 L2177.14 170.459 L2172.76 170.459 Q2167.83 170.459 2165.96 168.63 Q2164.08 166.779 2164.08 161.918 L2164.08 147.844 L2160.96 147.844 L2160.96 144.533 L2164.08 144.533 L2164.08 137.172 L2168.36 137.172 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M2181.61 144.533 L2185.86 144.533 L2185.86 170.459 L2181.61 170.459 L2181.61 144.533 M2181.61 134.441 L2185.86 134.441 L2185.86 139.834 L2181.61 139.834 L2181.61 134.441 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M2200.38 147.519 Q2196.95 147.519 2194.96 150.205 Q2192.97 152.867 2192.97 157.519 Q2192.97 162.172 2194.94 164.857 Q2196.93 167.519 2200.38 167.519 Q2203.78 167.519 2205.77 164.834 Q2207.76 162.149 2207.76 157.519 Q2207.76 152.913 2205.77 150.228 Q2203.78 147.519 2200.38 147.519 M2200.38 143.908 Q2205.93 143.908 2209.11 147.519 Q2212.28 151.131 2212.28 157.519 Q2212.28 163.885 2209.11 167.519 Q2205.93 171.13 2200.38 171.13 Q2194.8 171.13 2191.63 167.519 Q2188.48 163.885 2188.48 157.519 Q2188.48 151.131 2191.63 147.519 Q2194.8 143.908 2200.38 143.908 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip400)\" d=\"M 0 0 M2238.29 154.811 L2238.29 170.459 L2234.04 170.459 L2234.04 154.95 Q2234.04 151.269 2232.6 149.441 Q2231.17 147.612 2228.29 147.612 Q2224.85 147.612 2222.86 149.811 Q2220.86 152.01 2220.86 155.806 L2220.86 170.459 L2216.58 170.459 L2216.58 144.533 L2220.86 144.533 L2220.86 148.561 Q2222.39 146.223 2224.45 145.066 Q2226.54 143.908 2229.24 143.908 Q2233.71 143.908 2236 146.686 Q2238.29 149.441 2238.29 154.811 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(curve.parameter_values,\n",
    "     curve.measurements,\n",
    "     xscale=curve.parameter_scale,\n",
    "     xlab=curve.parameter_name,\n",
    "     ylab=\"Classification Accuracy\",\n",
    "     label=\"Validation\", lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95433"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = round(maximum(curve.measurements), digits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.447567486554116"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_C = curve.parameter_values[argmax(curve.measurements)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second look at `rbf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJBase.NumericRange(Float64, :gamma, ... )"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = range(Float64, :C, lower=5*10^-0, upper=5*10^7, scale=:log10)\n",
    "r2 = range(Float64, :gamma, lower=10^-10, upper=10^-1, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeterministicTunedModel(\n",
       "    model = SVMClassifier(\n",
       "            C = 1.0,\n",
       "            kernel = \"rbf\",\n",
       "            degree = 3,\n",
       "            gamma = \"auto\",\n",
       "            coef0 = 0.0,\n",
       "            shrinking = true,\n",
       "            tol = 0.001,\n",
       "            cache_size = 1000,\n",
       "            max_iter = -1,\n",
       "            decision_function_shape = \"ovr\",\n",
       "            random_state = nothing),\n",
       "    tuning = Grid(\n",
       "            goal = 100,\n",
       "            resolution = 10,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = CV(\n",
       "            nfolds = 6,\n",
       "            shuffle = false,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = accuracy(),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = MLJBase.NumericRange{Float64,MLJBase.Bounded,Symbol}[\u001b[34mNumericRange{Float64,} @565\u001b[39m, \u001b[34mNumericRange{Float64,} @534\u001b[39m],\n",
       "    train_best = true,\n",
       "    repeats = 1,\n",
       "    n = nothing,\n",
       "    acceleration = CPUThreads{Int64}(1),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true)\u001b[34m @819\u001b[39m"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_rbf_model = SVMClassifier(kernel=\"rbf\", cache_size=1000)\n",
    "self_tuning_svm_model = TunedModel(model=svm_rbf_model,\n",
    "                                    tuning=Grid(goal=100),\n",
    "                                    resampling=CV(), \n",
    "                                    measure=accuracy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1,r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DeterministicTunedModel{Grid,}} @514\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @593\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @121\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_svm_mach = machine(self_tuning_svm_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{DeterministicTunedModel{Grid,}} @514\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      " Info: Attempting to evaluate 100 models.\n",
      " @ MLJTuning /home/andrew/.julia/packages/MLJTuning/Bbgvk/src/tuned_models.jl:494\n",
      "\u001b[33mEvaluating over 100 metamodels: 100%[=========================] Time: 0:00:14\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DeterministicTunedModel{Grid,}} @514\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @593\u001b[39m  `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @121\u001b[39m  `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = fit!(self_tuning_svm_mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip440\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip440)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip441\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip440)\" d=\"\n",
       "M264.865 1410.9 L2112.76 1410.9 L2112.76 47.2441 L264.865 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip442\">\n",
       "    <rect x=\"264\" y=\"47\" width=\"1849\" height=\"1365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  392.132,1410.9 392.132,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  641.174,1410.9 641.174,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  890.216,1410.9 890.216,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1139.26,1410.9 1139.26,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1388.3,1410.9 1388.3,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1637.34,1410.9 1637.34,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1886.38,1410.9 1886.38,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  264.865,1372.3 2112.76,1372.3 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  264.865,1086.42 2112.76,1086.42 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  264.865,800.541 2112.76,800.541 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  264.865,514.66 2112.76,514.66 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip442)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  264.865,228.779 2112.76,228.779 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  264.865,1410.9 2112.76,1410.9 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  264.865,1410.9 264.865,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  392.132,1410.9 392.132,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  641.174,1410.9 641.174,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  890.216,1410.9 890.216,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1139.26,1410.9 1139.26,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1388.3,1410.9 1388.3,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1637.34,1410.9 1637.34,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1886.38,1410.9 1886.38,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  264.865,1372.3 287.039,1372.3 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  264.865,1086.42 287.039,1086.42 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  264.865,800.541 287.039,800.541 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  264.865,514.66 287.039,514.66 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  264.865,228.779 287.039,228.779 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip440)\" d=\"M 0 0 M360.648 1481.97 L368.286 1481.97 L368.286 1455.6 L359.976 1457.27 L359.976 1453.01 L368.24 1451.34 L372.916 1451.34 L372.916 1481.97 L380.555 1481.97 L380.555 1485.9 L360.648 1485.9 L360.648 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M395.624 1454.42 Q392.013 1454.42 390.184 1457.99 Q388.379 1461.53 388.379 1468.66 Q388.379 1475.77 390.184 1479.33 Q392.013 1482.87 395.624 1482.87 Q399.258 1482.87 401.064 1479.33 Q402.893 1475.77 402.893 1468.66 Q402.893 1461.53 401.064 1457.99 Q399.258 1454.42 395.624 1454.42 M395.624 1450.72 Q401.434 1450.72 404.49 1455.33 Q407.569 1459.91 407.569 1468.66 Q407.569 1477.39 404.49 1481.99 Q401.434 1486.58 395.624 1486.58 Q389.814 1486.58 386.735 1481.99 Q383.68 1477.39 383.68 1468.66 Q383.68 1459.91 386.735 1455.33 Q389.814 1450.72 395.624 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M408.114 1455.3 L414.321 1455.3 L414.321 1433.87 L407.569 1435.23 L407.569 1431.77 L414.283 1430.41 L418.082 1430.41 L418.082 1455.3 L424.289 1455.3 L424.289 1458.49 L408.114 1458.49 L408.114 1455.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M609.135 1481.97 L616.774 1481.97 L616.774 1455.6 L608.463 1457.27 L608.463 1453.01 L616.727 1451.34 L621.403 1451.34 L621.403 1481.97 L629.042 1481.97 L629.042 1485.9 L609.135 1485.9 L609.135 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M644.111 1454.42 Q640.5 1454.42 638.672 1457.99 Q636.866 1461.53 636.866 1468.66 Q636.866 1475.77 638.672 1479.33 Q640.5 1482.87 644.111 1482.87 Q647.746 1482.87 649.551 1479.33 Q651.38 1475.77 651.38 1468.66 Q651.38 1461.53 649.551 1457.99 Q647.746 1454.42 644.111 1454.42 M644.111 1450.72 Q649.922 1450.72 652.977 1455.33 Q656.056 1459.91 656.056 1468.66 Q656.056 1477.39 652.977 1481.99 Q649.922 1486.58 644.111 1486.58 Q638.301 1486.58 635.223 1481.99 Q632.167 1477.39 632.167 1468.66 Q632.167 1459.91 635.223 1455.33 Q638.301 1450.72 644.111 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M660.626 1455.3 L673.885 1455.3 L673.885 1458.49 L656.056 1458.49 L656.056 1455.3 Q658.219 1453.06 661.943 1449.3 Q665.685 1445.52 666.645 1444.43 Q668.469 1442.38 669.184 1440.96 Q669.917 1439.54 669.917 1438.16 Q669.917 1435.92 668.337 1434.51 Q666.776 1433.1 664.256 1433.1 Q662.469 1433.1 660.476 1433.72 Q658.501 1434.34 656.244 1435.6 L656.244 1431.77 Q658.538 1430.85 660.532 1430.38 Q662.526 1429.91 664.181 1429.91 Q668.544 1429.91 671.14 1432.09 Q673.735 1434.27 673.735 1437.92 Q673.735 1439.65 673.077 1441.21 Q672.437 1442.75 670.726 1444.86 Q670.256 1445.4 667.735 1448.02 Q665.215 1450.61 660.626 1455.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M857.848 1481.97 L865.486 1481.97 L865.486 1455.6 L857.176 1457.27 L857.176 1453.01 L865.44 1451.34 L870.116 1451.34 L870.116 1481.97 L877.755 1481.97 L877.755 1485.9 L857.848 1485.9 L857.848 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M892.824 1454.42 Q889.213 1454.42 887.384 1457.99 Q885.579 1461.53 885.579 1468.66 Q885.579 1475.77 887.384 1479.33 Q889.213 1482.87 892.824 1482.87 Q896.458 1482.87 898.264 1479.33 Q900.093 1475.77 900.093 1468.66 Q900.093 1461.53 898.264 1457.99 Q896.458 1454.42 892.824 1454.42 M892.824 1450.72 Q898.634 1450.72 901.69 1455.33 Q904.769 1459.91 904.769 1468.66 Q904.769 1477.39 901.69 1481.99 Q898.634 1486.58 892.824 1486.58 Q887.014 1486.58 883.935 1481.99 Q880.88 1477.39 880.88 1468.66 Q880.88 1459.91 883.935 1455.33 Q887.014 1450.72 892.824 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M917.464 1443.35 Q920.191 1443.94 921.714 1445.78 Q923.257 1447.62 923.257 1450.33 Q923.257 1454.49 920.398 1456.76 Q917.539 1459.04 912.273 1459.04 Q910.505 1459.04 908.624 1458.68 Q906.762 1458.34 904.769 1457.65 L904.769 1453.98 Q906.348 1454.9 908.229 1455.37 Q910.11 1455.84 912.16 1455.84 Q915.733 1455.84 917.595 1454.43 Q919.476 1453.02 919.476 1450.33 Q919.476 1447.85 917.727 1446.46 Q915.997 1445.05 912.894 1445.05 L909.621 1445.05 L909.621 1441.92 L913.044 1441.92 Q915.846 1441.92 917.332 1440.81 Q918.818 1439.69 918.818 1437.58 Q918.818 1435.42 917.276 1434.27 Q915.752 1433.1 912.894 1433.1 Q911.332 1433.1 909.546 1433.44 Q907.759 1433.78 905.615 1434.49 L905.615 1431.11 Q907.778 1430.51 909.659 1430.21 Q911.558 1429.91 913.232 1429.91 Q917.558 1429.91 920.078 1431.88 Q922.598 1433.84 922.598 1437.18 Q922.598 1439.52 921.263 1441.13 Q919.928 1442.73 917.464 1443.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M1105.9 1481.97 L1113.54 1481.97 L1113.54 1455.6 L1105.23 1457.27 L1105.23 1453.01 L1113.49 1451.34 L1118.17 1451.34 L1118.17 1481.97 L1125.81 1481.97 L1125.81 1485.9 L1105.9 1485.9 L1105.9 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M1140.88 1454.42 Q1137.27 1454.42 1135.44 1457.99 Q1133.63 1461.53 1133.63 1468.66 Q1133.63 1475.77 1135.44 1479.33 Q1137.27 1482.87 1140.88 1482.87 Q1144.51 1482.87 1146.32 1479.33 Q1148.15 1475.77 1148.15 1468.66 Q1148.15 1461.53 1146.32 1457.99 Q1144.51 1454.42 1140.88 1454.42 M1140.88 1450.72 Q1146.69 1450.72 1149.74 1455.33 Q1152.82 1459.91 1152.82 1468.66 Q1152.82 1477.39 1149.74 1481.99 Q1146.69 1486.58 1140.88 1486.58 Q1135.07 1486.58 1131.99 1481.99 Q1128.93 1477.39 1128.93 1468.66 Q1128.93 1459.91 1131.99 1455.33 Q1135.07 1450.72 1140.88 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M1165.5 1433.72 L1155.91 1448.71 L1165.5 1448.71 L1165.5 1433.72 M1164.5 1430.41 L1169.28 1430.41 L1169.28 1448.71 L1173.29 1448.71 L1173.29 1451.87 L1169.28 1451.87 L1169.28 1458.49 L1165.5 1458.49 L1165.5 1451.87 L1152.82 1451.87 L1152.82 1448.21 L1164.5 1430.41 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M1356.09 1481.97 L1363.73 1481.97 L1363.73 1455.6 L1355.42 1457.27 L1355.42 1453.01 L1363.68 1451.34 L1368.36 1451.34 L1368.36 1481.97 L1376 1481.97 L1376 1485.9 L1356.09 1485.9 L1356.09 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M1391.07 1454.42 Q1387.46 1454.42 1385.63 1457.99 Q1383.82 1461.53 1383.82 1468.66 Q1383.82 1475.77 1385.63 1479.33 Q1387.46 1482.87 1391.07 1482.87 Q1394.7 1482.87 1396.51 1479.33 Q1398.34 1475.77 1398.34 1468.66 Q1398.34 1461.53 1396.51 1457.99 Q1394.7 1454.42 1391.07 1454.42 M1391.07 1450.72 Q1396.88 1450.72 1399.93 1455.33 Q1403.01 1459.91 1403.01 1468.66 Q1403.01 1477.39 1399.93 1481.99 Q1396.88 1486.58 1391.07 1486.58 Q1385.26 1486.58 1382.18 1481.99 Q1379.12 1477.39 1379.12 1468.66 Q1379.12 1459.91 1382.18 1455.33 Q1385.26 1450.72 1391.07 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M1404.2 1430.41 L1419.11 1430.41 L1419.11 1433.61 L1407.68 1433.61 L1407.68 1440.49 Q1408.5 1440.21 1409.33 1440.08 Q1410.16 1439.93 1410.99 1439.93 Q1415.69 1439.93 1418.43 1442.51 Q1421.18 1445.08 1421.18 1449.48 Q1421.18 1454.02 1418.36 1456.54 Q1415.54 1459.04 1410.4 1459.04 Q1408.64 1459.04 1406.79 1458.74 Q1404.97 1458.44 1403.01 1457.84 L1403.01 1454.02 Q1404.71 1454.94 1406.51 1455.39 Q1408.32 1455.84 1410.33 1455.84 Q1413.58 1455.84 1415.48 1454.13 Q1417.38 1452.42 1417.38 1449.48 Q1417.38 1446.55 1415.48 1444.84 Q1413.58 1443.13 1410.33 1443.13 Q1408.81 1443.13 1407.28 1443.47 Q1405.78 1443.8 1404.2 1444.52 L1404.2 1430.41 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M1604.52 1481.97 L1612.16 1481.97 L1612.16 1455.6 L1603.85 1457.27 L1603.85 1453.01 L1612.11 1451.34 L1616.79 1451.34 L1616.79 1481.97 L1624.43 1481.97 L1624.43 1485.9 L1604.52 1485.9 L1604.52 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M1639.5 1454.42 Q1635.89 1454.42 1634.06 1457.99 Q1632.25 1461.53 1632.25 1468.66 Q1632.25 1475.77 1634.06 1479.33 Q1635.89 1482.87 1639.5 1482.87 Q1643.13 1482.87 1644.94 1479.33 Q1646.77 1475.77 1646.77 1468.66 Q1646.77 1461.53 1644.94 1457.99 Q1643.13 1454.42 1639.5 1454.42 M1639.5 1450.72 Q1645.31 1450.72 1648.36 1455.33 Q1651.44 1459.91 1651.44 1468.66 Q1651.44 1477.39 1648.36 1481.99 Q1645.31 1486.58 1639.5 1486.58 Q1633.69 1486.58 1630.61 1481.99 Q1627.55 1477.39 1627.55 1468.66 Q1627.55 1459.91 1630.61 1455.33 Q1633.69 1450.72 1639.5 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M1661.47 1442.94 Q1658.91 1442.94 1657.41 1444.69 Q1655.92 1446.44 1655.92 1449.48 Q1655.92 1452.51 1657.41 1454.28 Q1658.91 1456.03 1661.47 1456.03 Q1664.03 1456.03 1665.51 1454.28 Q1667.02 1452.51 1667.02 1449.48 Q1667.02 1446.44 1665.51 1444.69 Q1664.03 1442.94 1661.47 1442.94 M1669.01 1431.03 L1669.01 1434.49 Q1667.58 1433.82 1666.11 1433.46 Q1664.66 1433.1 1663.24 1433.1 Q1659.47 1433.1 1657.48 1435.64 Q1655.51 1438.18 1655.22 1443.32 Q1656.33 1441.68 1658.01 1440.81 Q1659.68 1439.93 1661.69 1439.93 Q1665.93 1439.93 1668.37 1442.51 Q1670.83 1445.06 1670.83 1449.48 Q1670.83 1453.81 1668.28 1456.42 Q1665.72 1459.04 1661.47 1459.04 Q1656.6 1459.04 1654.02 1455.31 Q1651.44 1451.57 1651.44 1444.48 Q1651.44 1437.82 1654.6 1433.87 Q1657.76 1429.91 1663.09 1429.91 Q1664.51 1429.91 1665.96 1430.19 Q1667.43 1430.47 1669.01 1431.03 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M1854.23 1481.97 L1861.87 1481.97 L1861.87 1455.6 L1853.56 1457.27 L1853.56 1453.01 L1861.82 1451.34 L1866.5 1451.34 L1866.5 1481.97 L1874.14 1481.97 L1874.14 1485.9 L1854.23 1485.9 L1854.23 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M1889.21 1454.42 Q1885.6 1454.42 1883.77 1457.99 Q1881.96 1461.53 1881.96 1468.66 Q1881.96 1475.77 1883.77 1479.33 Q1885.6 1482.87 1889.21 1482.87 Q1892.84 1482.87 1894.65 1479.33 Q1896.48 1475.77 1896.48 1468.66 Q1896.48 1461.53 1894.65 1457.99 Q1892.84 1454.42 1889.21 1454.42 M1889.21 1450.72 Q1895.02 1450.72 1898.07 1455.33 Q1901.15 1459.91 1901.15 1468.66 Q1901.15 1477.39 1898.07 1481.99 Q1895.02 1486.58 1889.21 1486.58 Q1883.4 1486.58 1880.32 1481.99 Q1877.26 1477.39 1877.26 1468.66 Q1877.26 1459.91 1880.32 1455.33 Q1883.4 1450.72 1889.21 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M1901.15 1430.41 L1919.21 1430.41 L1919.21 1432.03 L1909.01 1458.49 L1905.05 1458.49 L1914.64 1433.61 L1901.15 1433.61 L1901.15 1430.41 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M126.931 1392.1 L134.57 1392.1 L134.57 1365.73 L126.26 1367.4 L126.26 1363.14 L134.524 1361.47 L139.2 1361.47 L139.2 1392.1 L146.839 1392.1 L146.839 1396.03 L126.931 1396.03 L126.931 1392.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M161.908 1364.55 Q158.297 1364.55 156.468 1368.11 Q154.663 1371.66 154.663 1378.79 Q154.663 1385.89 156.468 1389.46 Q158.297 1393 161.908 1393 Q165.542 1393 167.348 1389.46 Q169.177 1385.89 169.177 1378.79 Q169.177 1371.66 167.348 1368.11 Q165.542 1364.55 161.908 1364.55 M161.908 1360.85 Q167.718 1360.85 170.774 1365.45 Q173.852 1370.04 173.852 1378.79 Q173.852 1387.51 170.774 1392.12 Q167.718 1396.7 161.908 1396.7 Q156.098 1396.7 153.019 1392.12 Q149.964 1387.51 149.964 1378.79 Q149.964 1370.04 153.019 1365.45 Q156.098 1360.85 161.908 1360.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M173.852 1354.95 L197.964 1354.95 L197.964 1358.14 L173.852 1358.14 L173.852 1354.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M202.741 1365.42 L208.948 1365.42 L208.948 1344 L202.196 1345.35 L202.196 1341.89 L208.91 1340.54 L212.709 1340.54 L212.709 1365.42 L218.916 1365.42 L218.916 1368.62 L202.741 1368.62 L202.741 1365.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M231.16 1343.04 Q228.226 1343.04 226.74 1345.94 Q225.273 1348.82 225.273 1354.61 Q225.273 1360.38 226.74 1363.28 Q228.226 1366.16 231.16 1366.16 Q234.113 1366.16 235.58 1363.28 Q237.065 1360.38 237.065 1354.61 Q237.065 1348.82 235.58 1345.94 Q234.113 1343.04 231.16 1343.04 M231.16 1340.03 Q235.881 1340.03 238.363 1343.77 Q240.865 1347.5 240.865 1354.61 Q240.865 1361.7 238.363 1365.44 Q235.881 1369.17 231.16 1369.17 Q226.439 1369.17 223.938 1365.44 Q221.455 1361.7 221.455 1354.61 Q221.455 1347.5 223.938 1343.77 Q226.439 1340.03 231.16 1340.03 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M147.959 1106.21 L155.597 1106.21 L155.597 1079.85 L147.287 1081.52 L147.287 1077.26 L155.551 1075.59 L160.227 1075.59 L160.227 1106.21 L167.866 1106.21 L167.866 1110.15 L147.959 1110.15 L147.959 1106.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M182.935 1078.67 Q179.324 1078.67 177.495 1082.23 Q175.69 1085.77 175.69 1092.9 Q175.69 1100.01 177.495 1103.58 Q179.324 1107.12 182.935 1107.12 Q186.569 1107.12 188.375 1103.58 Q190.204 1100.01 190.204 1092.9 Q190.204 1085.77 188.375 1082.23 Q186.569 1078.67 182.935 1078.67 M182.935 1074.96 Q188.745 1074.96 191.801 1079.57 Q194.88 1084.15 194.88 1092.9 Q194.88 1101.63 191.801 1106.24 Q188.745 1110.82 182.935 1110.82 Q177.125 1110.82 174.046 1106.24 Q170.991 1101.63 170.991 1092.9 Q170.991 1084.15 174.046 1079.57 Q177.125 1074.96 182.935 1074.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M194.88 1069.07 L218.991 1069.07 L218.991 1072.26 L194.88 1072.26 L194.88 1069.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M231.235 1069.4 Q228.527 1069.4 226.966 1070.85 Q225.423 1072.3 225.423 1074.84 Q225.423 1077.38 226.966 1078.83 Q228.527 1080.28 231.235 1080.28 Q233.943 1080.28 235.504 1078.83 Q237.065 1077.36 237.065 1074.84 Q237.065 1072.3 235.504 1070.85 Q233.962 1069.4 231.235 1069.4 M227.436 1067.79 Q224.991 1067.18 223.618 1065.51 Q222.264 1063.84 222.264 1061.43 Q222.264 1058.06 224.652 1056.11 Q227.06 1054.15 231.235 1054.15 Q235.429 1054.15 237.818 1056.11 Q240.206 1058.06 240.206 1061.43 Q240.206 1063.84 238.833 1065.51 Q237.479 1067.18 235.053 1067.79 Q237.799 1068.43 239.322 1070.29 Q240.865 1072.15 240.865 1074.84 Q240.865 1078.92 238.363 1081.1 Q235.881 1083.28 231.235 1083.28 Q226.589 1083.28 224.088 1081.1 Q221.605 1078.92 221.605 1074.84 Q221.605 1072.15 223.148 1070.29 Q224.69 1068.43 227.436 1067.79 M226.044 1061.79 Q226.044 1063.97 227.398 1065.19 Q228.771 1066.41 231.235 1066.41 Q233.68 1066.41 235.053 1065.19 Q236.445 1063.97 236.445 1061.79 Q236.445 1059.61 235.053 1058.38 Q233.68 1057.16 231.235 1057.16 Q228.771 1057.16 227.398 1058.38 Q226.044 1059.61 226.044 1061.79 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M147.752 820.333 L155.39 820.333 L155.39 793.967 L147.08 795.634 L147.08 791.375 L155.344 789.708 L160.02 789.708 L160.02 820.333 L167.659 820.333 L167.659 824.268 L147.752 824.268 L147.752 820.333 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M182.728 792.787 Q179.117 792.787 177.289 796.352 Q175.483 799.893 175.483 807.023 Q175.483 814.129 177.289 817.694 Q179.117 821.236 182.728 821.236 Q186.363 821.236 188.168 817.694 Q189.997 814.129 189.997 807.023 Q189.997 799.893 188.168 796.352 Q186.363 792.787 182.728 792.787 M182.728 789.083 Q188.538 789.083 191.594 793.69 Q194.673 798.273 194.673 807.023 Q194.673 815.75 191.594 820.356 Q188.538 824.94 182.728 824.94 Q176.918 824.94 173.839 820.356 Q170.784 815.75 170.784 807.023 Q170.784 798.273 173.839 793.69 Q176.918 789.083 182.728 789.083 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M194.673 783.185 L218.784 783.185 L218.784 786.382 L194.673 786.382 L194.673 783.185 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M231.498 781.304 Q228.94 781.304 227.436 783.053 Q225.95 784.802 225.95 787.849 Q225.95 790.877 227.436 792.645 Q228.94 794.394 231.498 794.394 Q234.056 794.394 235.542 792.645 Q237.047 790.877 237.047 787.849 Q237.047 784.802 235.542 783.053 Q234.056 781.304 231.498 781.304 M239.04 769.398 L239.04 772.859 Q237.611 772.182 236.144 771.825 Q234.696 771.467 233.266 771.467 Q229.505 771.467 227.511 774.006 Q225.536 776.545 225.254 781.68 Q226.364 780.044 228.038 779.179 Q229.712 778.295 231.724 778.295 Q235.956 778.295 238.401 780.871 Q240.865 783.429 240.865 787.849 Q240.865 792.175 238.307 794.789 Q235.749 797.403 231.498 797.403 Q226.627 797.403 224.05 793.679 Q221.474 789.937 221.474 782.846 Q221.474 776.188 224.633 772.238 Q227.793 768.27 233.116 768.27 Q234.545 768.27 235.993 768.552 Q237.46 768.834 239.04 769.398 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M147.488 534.452 L155.127 534.452 L155.127 508.086 L146.817 509.753 L146.817 505.494 L155.081 503.827 L159.757 503.827 L159.757 534.452 L167.396 534.452 L167.396 538.387 L147.488 538.387 L147.488 534.452 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M182.465 506.906 Q178.854 506.906 177.025 510.471 Q175.22 514.012 175.22 521.142 Q175.22 528.248 177.025 531.813 Q178.854 535.355 182.465 535.355 Q186.099 535.355 187.905 531.813 Q189.733 528.248 189.733 521.142 Q189.733 514.012 187.905 510.471 Q186.099 506.906 182.465 506.906 M182.465 503.202 Q188.275 503.202 191.331 507.809 Q194.409 512.392 194.409 521.142 Q194.409 529.869 191.331 534.475 Q188.275 539.058 182.465 539.058 Q176.655 539.058 173.576 534.475 Q170.521 529.869 170.521 521.142 Q170.521 512.392 173.576 507.809 Q176.655 503.202 182.465 503.202 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M194.409 497.304 L218.521 497.304 L218.521 500.501 L194.409 500.501 L194.409 497.304 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M233.078 486.207 L223.486 501.197 L233.078 501.197 L233.078 486.207 M232.081 482.897 L236.859 482.897 L236.859 501.197 L240.865 501.197 L240.865 504.356 L236.859 504.356 L236.859 510.977 L233.078 510.977 L233.078 504.356 L220.402 504.356 L220.402 500.689 L232.081 482.897 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M149.181 248.571 L156.82 248.571 L156.82 222.205 L148.51 223.872 L148.51 219.613 L156.774 217.946 L161.449 217.946 L161.449 248.571 L169.088 248.571 L169.088 252.506 L149.181 252.506 L149.181 248.571 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M184.158 221.025 Q180.547 221.025 178.718 224.59 Q176.912 228.131 176.912 235.261 Q176.912 242.367 178.718 245.932 Q180.547 249.474 184.158 249.474 Q187.792 249.474 189.597 245.932 Q191.426 242.367 191.426 235.261 Q191.426 228.131 189.597 224.59 Q187.792 221.025 184.158 221.025 M184.158 217.321 Q189.968 217.321 193.023 221.928 Q196.102 226.511 196.102 235.261 Q196.102 243.988 193.023 248.594 Q189.968 253.177 184.158 253.177 Q178.348 253.177 175.269 248.594 Q172.213 243.988 172.213 235.261 Q172.213 226.511 175.269 221.928 Q178.348 217.321 184.158 217.321 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M196.102 211.423 L220.214 211.423 L220.214 214.62 L196.102 214.62 L196.102 211.423 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M227.605 221.898 L240.865 221.898 L240.865 225.096 L223.035 225.096 L223.035 221.898 Q225.198 219.66 228.922 215.899 Q232.664 212.118 233.624 211.028 Q235.448 208.977 236.163 207.567 Q236.896 206.138 236.896 204.765 Q236.896 202.526 235.316 201.116 Q233.755 199.705 231.235 199.705 Q229.448 199.705 227.455 200.326 Q225.48 200.947 223.223 202.207 L223.223 198.37 Q225.517 197.448 227.511 196.978 Q229.505 196.508 231.16 196.508 Q235.523 196.508 238.119 198.69 Q240.714 200.871 240.714 204.52 Q240.714 206.25 240.056 207.811 Q239.416 209.354 237.705 211.46 Q237.235 212.006 234.714 214.62 Q232.194 217.215 227.605 221.898 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M1207.97 1518.33 L1207.97 1525.11 Q1204.72 1522.08 1201.03 1520.59 Q1197.37 1519.09 1193.23 1519.09 Q1185.09 1519.09 1180.76 1524.09 Q1176.43 1529.05 1176.43 1538.47 Q1176.43 1547.86 1180.76 1552.86 Q1185.09 1557.82 1193.23 1557.82 Q1197.37 1557.82 1201.03 1556.33 Q1204.72 1554.83 1207.97 1551.81 L1207.97 1558.53 Q1204.6 1560.82 1200.81 1561.96 Q1197.05 1563.11 1192.85 1563.11 Q1182.06 1563.11 1175.86 1556.52 Q1169.65 1549.9 1169.65 1538.47 Q1169.65 1527.01 1175.86 1520.43 Q1182.06 1513.81 1192.85 1513.81 Q1197.12 1513.81 1200.87 1514.95 Q1204.66 1516.07 1207.97 1518.33 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M69.7664 810.981 Q63.4007 810.981 59.8996 813.623 Q56.3984 816.233 56.3984 820.975 Q56.3984 825.686 59.8996 828.328 Q63.4007 830.938 69.7664 830.938 Q76.1003 830.938 79.6014 828.328 Q83.1026 825.686 83.1026 820.975 Q83.1026 816.233 79.6014 813.623 Q76.1003 810.981 69.7664 810.981 M83.58 805.125 Q92.683 805.125 97.1071 809.167 Q101.563 813.209 101.563 821.548 Q101.563 824.636 101.086 827.373 Q100.64 830.11 99.6852 832.688 L93.9879 832.688 Q95.3884 830.11 96.0568 827.596 Q96.7252 825.081 96.7252 822.471 Q96.7252 816.71 93.7015 813.846 Q90.7096 810.981 84.6303 810.981 L81.7339 810.981 Q84.885 812.795 86.4446 815.628 Q88.0042 818.461 88.0042 822.408 Q88.0042 828.964 83.0071 832.975 Q78.01 836.985 69.7664 836.985 Q61.491 836.985 56.4939 832.975 Q51.4968 828.964 51.4968 822.408 Q51.4968 818.461 53.0564 815.628 Q54.616 812.795 57.7671 810.981 L52.3562 810.981 L52.3562 805.125 L83.58 805.125 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M70.0847 782.781 Q70.0847 789.879 71.7079 792.616 Q73.3312 795.353 77.2461 795.353 Q80.3653 795.353 82.2114 793.316 Q84.0256 791.247 84.0256 787.714 Q84.0256 782.845 80.5881 779.916 Q77.1188 776.956 71.3897 776.956 L70.0847 776.956 L70.0847 782.781 M67.6657 771.1 L88.0042 771.1 L88.0042 776.956 L82.5933 776.956 Q85.8398 778.962 87.3994 781.953 Q88.9272 784.945 88.9272 789.274 Q88.9272 794.749 85.8716 797.995 Q82.7843 801.21 77.6281 801.21 Q71.6125 801.21 68.5569 797.199 Q65.5014 793.157 65.5014 785.168 L65.5014 776.956 L64.9285 776.956 Q60.8862 776.956 58.6901 779.63 Q56.4621 782.272 56.4621 787.078 Q56.4621 790.133 57.1941 793.03 Q57.9262 795.926 59.3903 798.6 L53.9795 798.6 Q52.7381 795.385 52.1334 792.361 Q51.4968 789.338 51.4968 786.473 Q51.4968 778.739 55.5072 774.919 Q59.5176 771.1 67.6657 771.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M59.1993 737.203 Q55.2526 735.006 53.3747 731.951 Q51.4968 728.895 51.4968 724.758 Q51.4968 719.188 55.4117 716.164 Q59.2948 713.14 66.4881 713.14 L88.0042 713.14 L88.0042 719.028 L66.679 719.028 Q61.5546 719.028 59.072 720.843 Q56.5894 722.657 56.5894 726.381 Q56.5894 730.932 59.6131 733.574 Q62.6368 736.216 67.8567 736.216 L88.0042 736.216 L88.0042 742.104 L66.679 742.104 Q61.5228 742.104 59.072 743.918 Q56.5894 745.733 56.5894 749.52 Q56.5894 754.008 59.6449 756.65 Q62.6686 759.292 67.8567 759.292 L88.0042 759.292 L88.0042 765.18 L52.3562 765.18 L52.3562 759.292 L57.8944 759.292 Q54.616 757.286 53.0564 754.485 Q51.4968 751.685 51.4968 747.833 Q51.4968 743.95 53.4702 741.245 Q55.4436 738.508 59.1993 737.203 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M59.1993 679.243 Q55.2526 677.047 53.3747 673.991 Q51.4968 670.936 51.4968 666.798 Q51.4968 661.228 55.4117 658.204 Q59.2948 655.18 66.4881 655.18 L88.0042 655.18 L88.0042 661.069 L66.679 661.069 Q61.5546 661.069 59.072 662.883 Q56.5894 664.697 56.5894 668.421 Q56.5894 672.973 59.6131 675.614 Q62.6368 678.256 67.8567 678.256 L88.0042 678.256 L88.0042 684.144 L66.679 684.144 Q61.5228 684.144 59.072 685.959 Q56.5894 687.773 56.5894 691.56 Q56.5894 696.048 59.6449 698.69 Q62.6686 701.332 67.8567 701.332 L88.0042 701.332 L88.0042 707.22 L52.3562 707.22 L52.3562 701.332 L57.8944 701.332 Q54.616 699.327 53.0564 696.526 Q51.4968 693.725 51.4968 689.874 Q51.4968 685.99 53.4702 683.285 Q55.4436 680.548 59.1993 679.243 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M70.0847 632.837 Q70.0847 639.935 71.7079 642.672 Q73.3312 645.409 77.2461 645.409 Q80.3653 645.409 82.2114 643.372 Q84.0256 641.303 84.0256 637.77 Q84.0256 632.9 80.5881 629.972 Q77.1188 627.012 71.3897 627.012 L70.0847 627.012 L70.0847 632.837 M67.6657 621.156 L88.0042 621.156 L88.0042 627.012 L82.5933 627.012 Q85.8398 629.017 87.3994 632.009 Q88.9272 635.001 88.9272 639.33 Q88.9272 644.804 85.8716 648.051 Q82.7843 651.266 77.6281 651.266 Q71.6125 651.266 68.5569 647.255 Q65.5014 643.213 65.5014 635.224 L65.5014 627.012 L64.9285 627.012 Q60.8862 627.012 58.6901 629.686 Q56.4621 632.328 56.4621 637.134 Q56.4621 640.189 57.1941 643.086 Q57.9262 645.982 59.3903 648.656 L53.9795 648.656 Q52.7381 645.441 52.1334 642.417 Q51.4968 639.393 51.4968 636.529 Q51.4968 628.795 55.5072 624.975 Q59.5176 621.156 67.6657 621.156 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><circle clip-path=\"url(#clip442)\" cx=\"1285.66\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1479.36\" cy=\"943.481\" r=\"60\" fill=\"#f9fc9d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"510.863\" cy=\"1229.36\" r=\"49\" fill=\"#fb9c06\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1479.36\" cy=\"800.541\" r=\"57\" fill=\"#f2e45d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"898.261\" cy=\"1372.3\" r=\"52\" fill=\"#fbba1f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1866.76\" cy=\"514.66\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1479.36\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"317.163\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"704.562\" cy=\"657.6\" r=\"59\" fill=\"#f4f78d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"317.163\" cy=\"514.66\" r=\"57\" fill=\"#f1e864\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1673.06\" cy=\"1086.42\" r=\"61\" fill=\"#fbfea4\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1479.36\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1285.66\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"704.562\" cy=\"800.541\" r=\"59\" fill=\"#f1f27d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"510.863\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1673.06\" cy=\"514.66\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1091.96\" cy=\"800.541\" r=\"61\" fill=\"#fbfea4\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"2060.46\" cy=\"1086.42\" r=\"60\" fill=\"#f8fc9d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1285.66\" cy=\"657.6\" r=\"59\" fill=\"#f1f27d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"2060.46\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1866.76\" cy=\"800.541\" r=\"57\" fill=\"#f2e45e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1866.76\" cy=\"657.6\" r=\"57\" fill=\"#f3e156\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1479.36\" cy=\"1086.42\" r=\"60\" fill=\"#f6fa95\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1091.96\" cy=\"1229.36\" r=\"55\" fill=\"#f7d13d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"317.163\" cy=\"1372.3\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1091.96\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1091.96\" cy=\"1086.42\" r=\"57\" fill=\"#f3e056\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"2060.46\" cy=\"657.6\" r=\"57\" fill=\"#f3e156\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1673.06\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"510.863\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"510.863\" cy=\"514.66\" r=\"57\" fill=\"#f1e865\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1673.06\" cy=\"657.6\" r=\"57\" fill=\"#f3e156\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"510.863\" cy=\"657.6\" r=\"59\" fill=\"#f4f78d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"898.261\" cy=\"1229.36\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"704.562\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1673.06\" cy=\"943.481\" r=\"59\" fill=\"#f4f78e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1673.06\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1673.06\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"898.261\" cy=\"657.6\" r=\"60\" fill=\"#f6fa95\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"510.863\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"317.163\" cy=\"657.6\" r=\"56\" fill=\"#f4dd4f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"898.261\" cy=\"943.481\" r=\"56\" fill=\"#f4dd4f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1285.66\" cy=\"800.541\" r=\"59\" fill=\"#f1f27d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1866.76\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1866.76\" cy=\"1372.3\" r=\"57\" fill=\"#f1e865\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"2060.46\" cy=\"800.541\" r=\"57\" fill=\"#f2e45e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"2060.46\" cy=\"943.481\" r=\"57\" fill=\"#f1e865\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"898.261\" cy=\"514.66\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"2060.46\" cy=\"1372.3\" r=\"59\" fill=\"#f4f78e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"2060.46\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1091.96\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1673.06\" cy=\"1229.36\" r=\"58\" fill=\"#f1ef75\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"510.863\" cy=\"943.481\" r=\"55\" fill=\"#f8cd37\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1866.76\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1479.36\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"2060.46\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"510.863\" cy=\"800.541\" r=\"57\" fill=\"#f2e45d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1479.36\" cy=\"514.66\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"510.863\" cy=\"1372.3\" r=\"14\" fill=\"#34095f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1091.96\" cy=\"943.481\" r=\"59\" fill=\"#f4f78d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"2060.46\" cy=\"514.66\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1091.96\" cy=\"657.6\" r=\"58\" fill=\"#f1ef75\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"898.261\" cy=\"1086.42\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"704.562\" cy=\"1372.3\" r=\"48\" fill=\"#fa9406\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"704.562\" cy=\"514.66\" r=\"56\" fill=\"#f4dd4f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1285.66\" cy=\"1372.3\" r=\"56\" fill=\"#f5d949\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"317.163\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"317.163\" cy=\"943.481\" r=\"53\" fill=\"#fac228\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1285.66\" cy=\"514.66\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"704.562\" cy=\"1086.42\" r=\"56\" fill=\"#f5d949\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1479.36\" cy=\"1229.36\" r=\"57\" fill=\"#f2e45d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1285.66\" cy=\"943.481\" r=\"60\" fill=\"#f9fc9d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"898.261\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"704.562\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"2060.46\" cy=\"1229.36\" r=\"60\" fill=\"#f9fc9d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1673.06\" cy=\"1372.3\" r=\"57\" fill=\"#f3e156\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"317.163\" cy=\"800.541\" r=\"56\" fill=\"#f4dd4f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1285.66\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"898.261\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"704.562\" cy=\"1229.36\" r=\"53\" fill=\"#fabe24\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"704.562\" cy=\"943.481\" r=\"55\" fill=\"#f7d13d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1285.66\" cy=\"1229.36\" r=\"56\" fill=\"#f4dd4f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1091.96\" cy=\"1372.3\" r=\"53\" fill=\"#fac228\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1866.76\" cy=\"1229.36\" r=\"61\" fill=\"#fcfea4\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"898.261\" cy=\"800.541\" r=\"60\" fill=\"#f8fc9d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1673.06\" cy=\"800.541\" r=\"57\" fill=\"#f1e865\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1866.76\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1479.36\" cy=\"657.6\" r=\"57\" fill=\"#f3e156\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"704.562\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1479.36\" cy=\"1372.3\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1091.96\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"317.163\" cy=\"371.719\" r=\"53\" fill=\"#fac228\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1285.66\" cy=\"1086.42\" r=\"59\" fill=\"#f1f27d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"898.261\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"317.163\" cy=\"1086.42\" r=\"51\" fill=\"#fbab0f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1866.76\" cy=\"1086.42\" r=\"60\" fill=\"#f8fc9d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1866.76\" cy=\"943.481\" r=\"57\" fill=\"#f1e865\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"1091.96\" cy=\"514.66\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"510.863\" cy=\"1086.42\" r=\"53\" fill=\"#fac228\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip442)\" cx=\"317.163\" cy=\"1229.36\" r=\"32\" fill=\"#b33259\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip443\">\n",
       "    <rect x=\"2160\" y=\"47\" width=\"73\" height=\"1365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<g clip-path=\"url(#clip443)\">\n",
       "<image width=\"72\" height=\"1364\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAEgAAAVUCAYAAABzwV4AAAAL0klEQVR4nO3dwY3sRhAFQY5Q/lsh\n",
       "L6XfLQtUeSQPERYsEg+zZJM7+/v3/H0f/tdfb/8AXydQECgIFAQKc+6/b/8Mn2ZBQaAgUBAozL1/\n",
       "3v4ZPs2CgkBBoCBQcCUdLCgIFAQKAgWBwly/xVYWFAQKAgWBwtzjQ3pjQUGgIFAQKLiSDhYUBAoC\n",
       "BYGCQMFvsWBBQaAgUBAoOA8KFhQECgIFgcI8rqRXFhQECgIFgYJAwXlQsKAgUBAoCBTmcR60sqAg\n",
       "UBAoCBTmOf+8/TN8mgUFgYJAQaAgUPBUI1hQECgIFAQKzoOCBQWBgkBBoOBDOlhQECgIFAQKAgXn\n",
       "QcGCgkBBoCBQmJ9bjZUFBYGCQEGgMM/xpf8bCwoCBYGCQEGg4FYjWFAQKAgUBApuNYIFBYGCQEGg\n",
       "4P2gYEFBoCBQECgIFObnVmNlQUGgIFAQKHiJM1hQECgIFAQKrqSDBQWBgkBBoCBQ8Og5WFAQKAgU\n",
       "BApuNYIFBYGCQEGg4Eo6WFAQKAgUBAoCBbcawYKCQEGgIFCY55y3f4ZPs6AgUBAoCBRcSQcLCgIF\n",
       "gYJAQaDgqUawoCBQECgIFHxIBwsKAgWBgkBhfg7tVxYUBAoCBYGCQMGtRrCgIFAQKAgUvB8ULCgI\n",
       "FAQKAgXnQcGCgkBBoCBQECg4DwoWFAQKAgWBgvOgYEFBoCBQECj4kA4WFAQKAgWBgkDBXz0HCwoC\n",
       "BYGCQMGtRrCgIFAQKAgUfEgHCwoCBYGCQEGg4LdYsKAgUBAoCBR8SAcLCgIFgYJAYZ7rQ3pjQUGg\n",
       "IFAQKAgU3GoECwoCBYGCQMGHdLCgIFAQKAgUfEgHCwoCBYGCQEGgMM+5b/8Mn2ZBQaAgUBAouNUI\n",
       "FhQECgIFgYIr6WBBQaAgUBAoCBTcagQLCgIFgYJAwa1GsKAgUBAoCBTmcSG9sqAgUBAoCBQECvNc\n",
       "txobCwoCBYGCQMGtRrCgIFAQKAgUfEgHCwoCBYGCQEGgML4Eb2dBQaAgUBAouNUIFhQECgIFgYIP\n",
       "6WBBQaAgUBAoCBTmOb+3f4ZPs6AgUBAoCBTmXh/SGwsKAgWBgkDBeVCwoCBQECgIFBx3BAsKAgWB\n",
       "gkBBoDD3aLRRJwgUBAoCBbcawYKCQEGgIFCYx5PVlQUFgYJAQaAgUJjrVmNlQUGgIFAQKMzj0H6l\n",
       "ThAoCBQECq6kgwUFgYJAQaAgUPBUI1hQECgIFAQKXuIM6gSBgkBBoOAlzmBBQaAgUBAoCBQ81QgW\n",
       "FAQKAgWBwtyr0UadIFAQKAgUnAcFCwoCBYGCQEGg4DwoWFAQKAgUBArzOA9aqRMECgIFgYIr6WBB\n",
       "QaAgUBAoCBT8FgsWFAQKAgWBgj/qDRYUBAoCBYGCP+oN6gSBgkBBoCBQ8P/mgwUFgYJAQaDg0D5Y\n",
       "UBAoCBQECr70P6gTBAoCBYGCQMF5ULCgIFAQKAgUnAcFCwoCBYGCQMGVdLCgIFAQKAgUBArecg3q\n",
       "BIGCQEGg4FYjWFAQKAgUBAoO7YMFBYGCQEGgIFBwqxEsKAgUBAoCBf+6JlhQECgIFAQK/glkUCcI\n",
       "FAQKAgWBgvOgYEFBoCBQECh49BwsKAgUBAoCBVfSwYKCQEGgIFAQKMzxW2xlQUGgIFAQKPij3qBO\n",
       "ECgIFAQKzoOCBQWBgkBBoCBQ8FssWFAQKAgUBAreDwoWFAQKAgWBgivpYEFBoCBQECgIFPwWCxYU\n",
       "BAoCBYGCr6YI6gSBgkBBoDDHof3KgoJAQaAgUBAoOA8KFhQECgIFgYIP6WBBQaAgUBAo+JAOFhQE\n",
       "CgIFgYJAwfcHBQsKAgWBgkDBrUawoCBQECgIFHxIBwsKAgWBgkBBoOBPEYI6QaAgUBAo+FOEYEFB\n",
       "oCBQECg4DwoWFAQKAgWBgkDBb7FgQUGgIFAQKHiJM1hQECgIFAQKrqSDBQWBgkBBoCBQmPv4Lbax\n",
       "oCBQECgIFJwHBQsKAgWBgkDBeVCwoCBQECgIFAQKbjWCBQWBgkBBoOBWI1hQECgIFAQKrqSDBQWB\n",
       "gkBBoCBQcKsRLCgIFAQKAgUf0sGCgkBBoCBQmONN+5UFBYGCQEGg4Eo6WFAQKAgUBAoCBY+egwUF\n",
       "gYJAQaAw9779I3ybBQWBgkBBoOBKOlhQECgIFAQKAgVf8hYsKAgUBAoCBbcawYKCQEGgIFDwflCw\n",
       "oCBQECgIFAQK/hQhWFAQKAgUBAreDwoWFAQKAgWBgkP7YEFBoCBQECgIFDzVCBYUBAoCBYGCQ/tg\n",
       "QUGgIFAQKDgPChYUBAoCBYGCQMGj52BBQaAgUBAoOA8KFhQECgIFgYInq8GCgkBBoCBQECjMefsn\n",
       "+DgLCgIFgYJAwa1GsKAgUBAoCBS8HxQsKAgUBAoCBYGC86BgQUGgIFAQKDgPChYUBAoCBYGCK+lg\n",
       "QUGgIFAQKAgU3GoECwoCBYGCQMGj52BBQaAgUBAojC9e2FlQECgIFAQKAgW3GsGCgkBBoCBQ8Og5\n",
       "WFAQKAgUBAqerAYLCgIFgYJAQaDgViNYUBAoCBQECm41ggUFgYJAQaAwxwtCKwsKAgWBgkBBoOA8\n",
       "KFhQECgIFAQKzoOCBQWBgkBBoOBKOlhQECgIFAQKAgX/bz5YUBAoCBQECv7ffLCgIFAQKAgUvB8U\n",
       "LCgIFAQKAgWBgm/BCxYUBAoCBYGCL3kLFhQECgIFgYInq8GCgkBBoCBQECh4yzVYUBAoCBQECv5e\n",
       "LFhQECgIFAQKrqSDBQWBgkBBoCBQ8JZrsKAgUBAoCBS8xBksKAgUBAoCBVfSwYKCQEGgIFAQKHjL\n",
       "NVhQECgIFAQKvj8oWFAQKAgUBArOg4IFBYGCQEGgIFDw6DlYUBAoCBQECm41ggUFgYJAQaDgSjpY\n",
       "UBAoCBQECgIFtxrBgoJAQaAgUPAhHSwoCBQECgIF50HBgoJAQaAgUBAouNUIFhQECgIFgYJbjWBB\n",
       "QaAgUBAouJIOFhQECgIFgYJAwfcHBQsKAgWBgkDBv64JFhQECgIFgYLzoGBBQaAgUBAoCBTmPn6N\n",
       "bSwoCBQECgIFtxrBgoJAQaAgUPBkNVhQECgIFAQKnqwGCwoCBYGCQEGg4DwoWFAQKAgUBAqerAYL\n",
       "CgIFgYJAwZV0sKAgUBAoCBQECr4/KFhQECgIFAQKc7wgtLKgIFAQKAgUvB8ULCgIFAQKAgWBwly3\n",
       "GisLCgIFgYJAwa1GsKAgUBAoCBQc2gcLCgIFgYJAQaDg/aBgQUGgIFAQKMzx92IrCwoCBYGCQMF5\n",
       "ULCgIFAQKAgUBAq+miJYUBAoCBQECt4PChYUBAoCBYGCQ/tgQUGgIFAQKAgUnAcFCwoCBYGCQGHO\n",
       "40RoY0FBoCBQECg4DwoWFAQKAgWBgkDBb7FgQUGgIFAQKDi0DxYUBAoCBYGCQ/tgQUGgIFAQKAgU\n",
       "5vzcamwsKAgUBAoCBbcawYKCQEGgIFCY60N6ZUFBoCBQECgIFNxqBAsKAgWBgkBhzs+H9MaCgkBB\n",
       "oCBQcCUdLCgIFAQKAgWBgqcawYKCQEGgIFBwqxEsKAgUBAoCBR/SwYKCQEGgIFAQKHj0HCwoCBQE\n",
       "CgKFuc+ft3+GT7OgIFAQKAgUnAcFCwoCBYGCQEGg4P2gYEFBoCBQECjMuc6DNhYUBAoCBYGCK+lg\n",
       "QUGgIFAQKAgU5nj0vLKgIFAQKAgU3GoECwoCBYGCQMGVdLCgIFAQKAgUBApzr1uNjQUFgYJAQaDg\n",
       "PChYUBAoCBQECl7iDBYUBAoCBYGCQMGtRrCgIFAQKAgU5rrVWFlQECgIFAQKrqSDBQWBgkBBoCBQ\n",
       "mOP9oJUFBYGCQEGg4PukgwUFgYJAQaDgTftgQUGgIFAQKAgUPNUIFhQECgIFgYJbjWBBQaAgUBAo\n",
       "+JAOFhQECgIFgYJAwaPnYEFBoCBQECi41QgWFAQKAgWBwjyerK4sKAgUBAoCBYGCW41gQUGgIFAQ\n",
       "KHiJM1hQECgIFAQK87iSXllQECgIFAQKAgXvBwULCgIFgYJAwaPnYEFBoCBQECg4DwoWFAQKAgWB\n",
       "gkBh7nPf/hk+zYKCQEGgIFBwHhQsKAgUBAoChXlcSa8sKAgUBAoChf8AJ8ztt7m0cDkAAAAASUVO\n",
       "RK5CYII=\n",
       "\" transform=\"translate(2161, 47)\"/>\n",
       "</g>\n",
       "<path clip-path=\"url(#clip440)\" d=\"M 0 0 M2280.7 1301.14 Q2277.09 1301.14 2275.26 1304.71 Q2273.45 1308.25 2273.45 1315.38 Q2273.45 1322.48 2275.26 1326.05 Q2277.09 1329.59 2280.7 1329.59 Q2284.33 1329.59 2286.14 1326.05 Q2287.97 1322.48 2287.97 1315.38 Q2287.97 1308.25 2286.14 1304.71 Q2284.33 1301.14 2280.7 1301.14 M2280.7 1297.44 Q2286.51 1297.44 2289.57 1302.04 Q2292.64 1306.63 2292.64 1315.38 Q2292.64 1324.1 2289.57 1328.71 Q2286.51 1333.29 2280.7 1333.29 Q2274.89 1333.29 2271.81 1328.71 Q2268.76 1324.1 2268.76 1315.38 Q2268.76 1306.63 2271.81 1302.04 Q2274.89 1297.44 2280.7 1297.44 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2297.71 1326.74 L2302.6 1326.74 L2302.6 1332.62 L2297.71 1332.62 L2297.71 1326.74 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2318.25 1313.48 Q2315.1 1313.48 2313.25 1315.63 Q2311.42 1317.79 2311.42 1321.54 Q2311.42 1325.26 2313.25 1327.44 Q2315.1 1329.59 2318.25 1329.59 Q2321.39 1329.59 2323.22 1327.44 Q2325.08 1325.26 2325.08 1321.54 Q2325.08 1317.79 2323.22 1315.63 Q2321.39 1313.48 2318.25 1313.48 M2327.53 1298.83 L2327.53 1303.09 Q2325.77 1302.25 2323.96 1301.81 Q2322.18 1301.37 2320.42 1301.37 Q2315.79 1301.37 2313.34 1304.5 Q2310.91 1307.62 2310.56 1313.94 Q2311.93 1311.93 2313.99 1310.86 Q2316.05 1309.78 2318.52 1309.78 Q2323.73 1309.78 2326.74 1312.95 Q2329.77 1316.1 2329.77 1321.54 Q2329.77 1326.86 2326.63 1330.08 Q2323.48 1333.29 2318.25 1333.29 Q2312.25 1333.29 2309.08 1328.71 Q2305.91 1324.1 2305.91 1315.38 Q2305.91 1307.18 2309.8 1302.32 Q2313.69 1297.44 2320.24 1297.44 Q2322 1297.44 2323.78 1297.79 Q2325.58 1298.13 2327.53 1298.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2334.89 1298.06 L2353.25 1298.06 L2353.25 1302 L2339.17 1302 L2339.17 1310.47 Q2340.19 1310.12 2341.21 1309.96 Q2342.23 1309.78 2343.25 1309.78 Q2349.03 1309.78 2352.41 1312.95 Q2355.79 1316.12 2355.79 1321.54 Q2355.79 1327.11 2352.32 1330.22 Q2348.85 1333.29 2342.53 1333.29 Q2340.35 1333.29 2338.08 1332.92 Q2335.84 1332.55 2333.43 1331.81 L2333.43 1327.11 Q2335.51 1328.25 2337.74 1328.8 Q2339.96 1329.36 2342.44 1329.36 Q2346.44 1329.36 2348.78 1327.25 Q2351.12 1325.15 2351.12 1321.54 Q2351.12 1317.92 2348.78 1315.82 Q2346.44 1313.71 2342.44 1313.71 Q2340.56 1313.71 2338.69 1314.13 Q2336.83 1314.54 2334.89 1315.42 L2334.89 1298.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2280.7 1090.81 Q2277.09 1090.81 2275.26 1094.38 Q2273.45 1097.92 2273.45 1105.05 Q2273.45 1112.16 2275.26 1115.72 Q2277.09 1119.26 2280.7 1119.26 Q2284.33 1119.26 2286.14 1115.72 Q2287.97 1112.16 2287.97 1105.05 Q2287.97 1097.92 2286.14 1094.38 Q2284.33 1090.81 2280.7 1090.81 M2280.7 1087.11 Q2286.51 1087.11 2289.57 1091.72 Q2292.64 1096.3 2292.64 1105.05 Q2292.64 1113.78 2289.57 1118.38 Q2286.51 1122.97 2280.7 1122.97 Q2274.89 1122.97 2271.81 1118.38 Q2268.76 1113.78 2268.76 1105.05 Q2268.76 1096.3 2271.81 1091.72 Q2274.89 1087.11 2280.7 1087.11 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2297.71 1116.42 L2302.6 1116.42 L2302.6 1122.3 L2297.71 1122.3 L2297.71 1116.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2306.49 1087.74 L2328.71 1087.74 L2328.71 1089.73 L2316.16 1122.3 L2311.28 1122.3 L2323.08 1091.67 L2306.49 1091.67 L2306.49 1087.74 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2343.78 1090.81 Q2340.17 1090.81 2338.34 1094.38 Q2336.53 1097.92 2336.53 1105.05 Q2336.53 1112.16 2338.34 1115.72 Q2340.17 1119.26 2343.78 1119.26 Q2347.41 1119.26 2349.22 1115.72 Q2351.05 1112.16 2351.05 1105.05 Q2351.05 1097.92 2349.22 1094.38 Q2347.41 1090.81 2343.78 1090.81 M2343.78 1087.11 Q2349.59 1087.11 2352.64 1091.72 Q2355.72 1096.3 2355.72 1105.05 Q2355.72 1113.78 2352.64 1118.38 Q2349.59 1122.97 2343.78 1122.97 Q2337.97 1122.97 2334.89 1118.38 Q2331.83 1113.78 2331.83 1105.05 Q2331.83 1096.3 2334.89 1091.72 Q2337.97 1087.11 2343.78 1087.11 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2280.7 880.488 Q2277.09 880.488 2275.26 884.053 Q2273.45 887.594 2273.45 894.724 Q2273.45 901.83 2275.26 905.395 Q2277.09 908.937 2280.7 908.937 Q2284.33 908.937 2286.14 905.395 Q2287.97 901.83 2287.97 894.724 Q2287.97 887.594 2286.14 884.053 Q2284.33 880.488 2280.7 880.488 M2280.7 876.784 Q2286.51 876.784 2289.57 881.391 Q2292.64 885.974 2292.64 894.724 Q2292.64 903.451 2289.57 908.057 Q2286.51 912.641 2280.7 912.641 Q2274.89 912.641 2271.81 908.057 Q2268.76 903.451 2268.76 894.724 Q2268.76 885.974 2271.81 881.391 Q2274.89 876.784 2280.7 876.784 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2297.71 906.09 L2302.6 906.09 L2302.6 911.969 L2297.71 911.969 L2297.71 906.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2306.49 877.409 L2328.71 877.409 L2328.71 879.4 L2316.16 911.969 L2311.28 911.969 L2323.08 881.344 L2306.49 881.344 L2306.49 877.409 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2333.83 877.409 L2352.18 877.409 L2352.18 881.344 L2338.11 881.344 L2338.11 889.817 Q2339.13 889.469 2340.14 889.307 Q2341.16 889.122 2342.18 889.122 Q2347.97 889.122 2351.35 892.293 Q2354.73 895.465 2354.73 900.881 Q2354.73 906.46 2351.26 909.562 Q2347.78 912.641 2341.46 912.641 Q2339.29 912.641 2337.02 912.27 Q2334.77 911.9 2332.37 911.159 L2332.37 906.46 Q2334.45 907.594 2336.67 908.15 Q2338.89 908.705 2341.37 908.705 Q2345.38 908.705 2347.71 906.599 Q2350.05 904.492 2350.05 900.881 Q2350.05 897.27 2347.71 895.164 Q2345.38 893.057 2341.37 893.057 Q2339.5 893.057 2337.62 893.474 Q2335.77 893.891 2333.83 894.77 L2333.83 877.409 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2280.7 670.161 Q2277.09 670.161 2275.26 673.726 Q2273.45 677.267 2273.45 684.397 Q2273.45 691.504 2275.26 695.068 Q2277.09 698.61 2280.7 698.61 Q2284.33 698.61 2286.14 695.068 Q2287.97 691.504 2287.97 684.397 Q2287.97 677.267 2286.14 673.726 Q2284.33 670.161 2280.7 670.161 M2280.7 666.457 Q2286.51 666.457 2289.57 671.064 Q2292.64 675.647 2292.64 684.397 Q2292.64 693.124 2289.57 697.73 Q2286.51 702.314 2280.7 702.314 Q2274.89 702.314 2271.81 697.73 Q2268.76 693.124 2268.76 684.397 Q2268.76 675.647 2271.81 671.064 Q2274.89 666.457 2280.7 666.457 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2297.71 695.763 L2302.6 695.763 L2302.6 701.642 L2297.71 701.642 L2297.71 695.763 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2317.67 685.23 Q2314.33 685.23 2312.41 687.013 Q2310.51 688.795 2310.51 691.92 Q2310.51 695.045 2312.41 696.828 Q2314.33 698.61 2317.67 698.61 Q2321 698.61 2322.92 696.828 Q2324.84 695.022 2324.84 691.92 Q2324.84 688.795 2322.92 687.013 Q2321.02 685.23 2317.67 685.23 M2312.99 683.24 Q2309.98 682.499 2308.29 680.439 Q2306.63 678.379 2306.63 675.416 Q2306.63 671.272 2309.57 668.865 Q2312.53 666.457 2317.67 666.457 Q2322.83 666.457 2325.77 668.865 Q2328.71 671.272 2328.71 675.416 Q2328.71 678.379 2327.02 680.439 Q2325.35 682.499 2322.37 683.24 Q2325.75 684.027 2327.62 686.318 Q2329.52 688.61 2329.52 691.92 Q2329.52 696.943 2326.44 699.628 Q2323.39 702.314 2317.67 702.314 Q2311.95 702.314 2308.87 699.628 Q2305.82 696.943 2305.82 691.92 Q2305.82 688.61 2307.71 686.318 Q2309.61 684.027 2312.99 683.24 M2311.28 675.855 Q2311.28 678.541 2312.95 680.045 Q2314.64 681.55 2317.67 681.55 Q2320.68 681.55 2322.37 680.045 Q2324.08 678.541 2324.08 675.855 Q2324.08 673.17 2322.37 671.666 Q2320.68 670.161 2317.67 670.161 Q2314.64 670.161 2312.95 671.666 Q2311.28 673.17 2311.28 675.855 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2344.59 670.161 Q2340.98 670.161 2339.15 673.726 Q2337.34 677.267 2337.34 684.397 Q2337.34 691.504 2339.15 695.068 Q2340.98 698.61 2344.59 698.61 Q2348.22 698.61 2350.03 695.068 Q2351.86 691.504 2351.86 684.397 Q2351.86 677.267 2350.03 673.726 Q2348.22 670.161 2344.59 670.161 M2344.59 666.457 Q2350.4 666.457 2353.45 671.064 Q2356.53 675.647 2356.53 684.397 Q2356.53 693.124 2353.45 697.73 Q2350.4 702.314 2344.59 702.314 Q2338.78 702.314 2335.7 697.73 Q2332.64 693.124 2332.64 684.397 Q2332.64 675.647 2335.7 671.064 Q2338.78 666.457 2344.59 666.457 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2280.7 459.834 Q2277.09 459.834 2275.26 463.399 Q2273.45 466.941 2273.45 474.07 Q2273.45 481.177 2275.26 484.741 Q2277.09 488.283 2280.7 488.283 Q2284.33 488.283 2286.14 484.741 Q2287.97 481.177 2287.97 474.07 Q2287.97 466.941 2286.14 463.399 Q2284.33 459.834 2280.7 459.834 M2280.7 456.13 Q2286.51 456.13 2289.57 460.737 Q2292.64 465.32 2292.64 474.07 Q2292.64 482.797 2289.57 487.403 Q2286.51 491.987 2280.7 491.987 Q2274.89 491.987 2271.81 487.403 Q2268.76 482.797 2268.76 474.07 Q2268.76 465.32 2271.81 460.737 Q2274.89 456.13 2280.7 456.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2297.71 485.436 L2302.6 485.436 L2302.6 491.315 L2297.71 491.315 L2297.71 485.436 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2317.67 474.904 Q2314.33 474.904 2312.41 476.686 Q2310.51 478.468 2310.51 481.593 Q2310.51 484.718 2312.41 486.501 Q2314.33 488.283 2317.67 488.283 Q2321 488.283 2322.92 486.501 Q2324.84 484.695 2324.84 481.593 Q2324.84 478.468 2322.92 476.686 Q2321.02 474.904 2317.67 474.904 M2312.99 472.913 Q2309.98 472.172 2308.29 470.112 Q2306.63 468.052 2306.63 465.089 Q2306.63 460.945 2309.57 458.538 Q2312.53 456.13 2317.67 456.13 Q2322.83 456.13 2325.77 458.538 Q2328.71 460.945 2328.71 465.089 Q2328.71 468.052 2327.02 470.112 Q2325.35 472.172 2322.37 472.913 Q2325.75 473.7 2327.62 475.991 Q2329.52 478.283 2329.52 481.593 Q2329.52 486.616 2326.44 489.302 Q2323.39 491.987 2317.67 491.987 Q2311.95 491.987 2308.87 489.302 Q2305.82 486.616 2305.82 481.593 Q2305.82 478.283 2307.71 475.991 Q2309.61 473.7 2312.99 472.913 M2311.28 465.529 Q2311.28 468.214 2312.95 469.718 Q2314.64 471.223 2317.67 471.223 Q2320.68 471.223 2322.37 469.718 Q2324.08 468.214 2324.08 465.529 Q2324.08 462.843 2322.37 461.339 Q2320.68 459.834 2317.67 459.834 Q2314.64 459.834 2312.95 461.339 Q2311.28 462.843 2311.28 465.529 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2334.64 456.755 L2352.99 456.755 L2352.99 460.691 L2338.92 460.691 L2338.92 469.163 Q2339.94 468.816 2340.95 468.654 Q2341.97 468.468 2342.99 468.468 Q2348.78 468.468 2352.16 471.64 Q2355.54 474.811 2355.54 480.228 Q2355.54 485.806 2352.07 488.908 Q2348.59 491.987 2342.27 491.987 Q2340.1 491.987 2337.83 491.616 Q2335.58 491.246 2333.18 490.505 L2333.18 485.806 Q2335.26 486.94 2337.48 487.496 Q2339.7 488.052 2342.18 488.052 Q2346.19 488.052 2348.52 485.945 Q2350.86 483.839 2350.86 480.228 Q2350.86 476.616 2348.52 474.51 Q2346.19 472.404 2342.18 472.404 Q2340.31 472.404 2338.43 472.82 Q2336.58 473.237 2334.64 474.116 L2334.64 456.755 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2280.7 249.507 Q2277.09 249.507 2275.26 253.072 Q2273.45 256.614 2273.45 263.743 Q2273.45 270.85 2275.26 274.415 Q2277.09 277.956 2280.7 277.956 Q2284.33 277.956 2286.14 274.415 Q2287.97 270.85 2287.97 263.743 Q2287.97 256.614 2286.14 253.072 Q2284.33 249.507 2280.7 249.507 M2280.7 245.804 Q2286.51 245.804 2289.57 250.41 Q2292.64 254.993 2292.64 263.743 Q2292.64 272.47 2289.57 277.077 Q2286.51 281.66 2280.7 281.66 Q2274.89 281.66 2271.81 277.077 Q2268.76 272.47 2268.76 263.743 Q2268.76 254.993 2271.81 250.41 Q2274.89 245.804 2280.7 245.804 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2297.71 275.109 L2302.6 275.109 L2302.6 280.989 L2297.71 280.989 L2297.71 275.109 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2307.81 280.271 L2307.81 276.012 Q2309.57 276.845 2311.37 277.285 Q2313.18 277.725 2314.91 277.725 Q2319.54 277.725 2321.97 274.623 Q2324.43 271.498 2324.77 265.155 Q2323.43 267.146 2321.37 268.211 Q2319.31 269.276 2316.81 269.276 Q2311.63 269.276 2308.59 266.151 Q2305.58 263.003 2305.58 257.563 Q2305.58 252.239 2308.73 249.021 Q2311.88 245.804 2317.11 245.804 Q2323.11 245.804 2326.26 250.41 Q2329.43 254.993 2329.43 263.743 Q2329.43 271.915 2325.54 276.799 Q2321.67 281.66 2315.12 281.66 Q2313.36 281.66 2311.56 281.313 Q2309.75 280.965 2307.81 280.271 M2317.11 265.618 Q2320.26 265.618 2322.09 263.466 Q2323.94 261.313 2323.94 257.563 Q2323.94 253.836 2322.09 251.683 Q2320.26 249.507 2317.11 249.507 Q2313.96 249.507 2312.11 251.683 Q2310.28 253.836 2310.28 257.563 Q2310.28 261.313 2312.11 263.466 Q2313.96 265.618 2317.11 265.618 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2344.5 249.507 Q2340.89 249.507 2339.06 253.072 Q2337.25 256.614 2337.25 263.743 Q2337.25 270.85 2339.06 274.415 Q2340.89 277.956 2344.5 277.956 Q2348.13 277.956 2349.94 274.415 Q2351.76 270.85 2351.76 263.743 Q2351.76 256.614 2349.94 253.072 Q2348.13 249.507 2344.5 249.507 M2344.5 245.804 Q2350.31 245.804 2353.36 250.41 Q2356.44 254.993 2356.44 263.743 Q2356.44 272.47 2353.36 277.077 Q2350.31 281.66 2344.5 281.66 Q2338.69 281.66 2335.61 277.077 Q2332.55 272.47 2332.55 263.743 Q2332.55 254.993 2335.61 250.41 Q2338.69 245.804 2344.5 245.804 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2280.7 39.1804 Q2277.09 39.1804 2275.26 42.7452 Q2273.45 46.2868 2273.45 53.4164 Q2273.45 60.5229 2275.26 64.0877 Q2277.09 67.6293 2280.7 67.6293 Q2284.33 67.6293 2286.14 64.0877 Q2287.97 60.5229 2287.97 53.4164 Q2287.97 46.2868 2286.14 42.7452 Q2284.33 39.1804 2280.7 39.1804 M2280.7 35.4767 Q2286.51 35.4767 2289.57 40.0832 Q2292.64 44.6665 2292.64 53.4164 Q2292.64 62.1432 2289.57 66.7497 Q2286.51 71.333 2280.7 71.333 Q2274.89 71.333 2271.81 66.7497 Q2268.76 62.1432 2268.76 53.4164 Q2268.76 44.6665 2271.81 40.0832 Q2274.89 35.4767 2280.7 35.4767 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2297.71 64.7821 L2302.6 64.7821 L2302.6 70.6617 L2297.71 70.6617 L2297.71 64.7821 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2307.81 69.9441 L2307.81 65.6849 Q2309.57 66.5182 2311.37 66.958 Q2313.18 67.3978 2314.91 67.3978 Q2319.54 67.3978 2321.97 64.296 Q2324.43 61.171 2324.77 54.8285 Q2323.43 56.8192 2321.37 57.884 Q2319.31 58.9488 2316.81 58.9488 Q2311.63 58.9488 2308.59 55.8238 Q2305.58 52.6757 2305.58 47.2359 Q2305.58 41.9119 2308.73 38.6943 Q2311.88 35.4767 2317.11 35.4767 Q2323.11 35.4767 2326.26 40.0832 Q2329.43 44.6665 2329.43 53.4164 Q2329.43 61.5877 2325.54 66.4719 Q2321.67 71.333 2315.12 71.333 Q2313.36 71.333 2311.56 70.9858 Q2309.75 70.6386 2307.81 69.9441 M2317.11 55.2914 Q2320.26 55.2914 2322.09 53.1387 Q2323.94 50.9859 2323.94 47.2359 Q2323.94 43.5091 2322.09 41.3563 Q2320.26 39.1804 2317.11 39.1804 Q2313.96 39.1804 2312.11 41.3563 Q2310.28 43.5091 2310.28 47.2359 Q2310.28 50.9859 2312.11 53.1387 Q2313.96 55.2914 2317.11 55.2914 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip440)\" d=\"M 0 0 M2334.54 36.1017 L2352.9 36.1017 L2352.9 40.0369 L2338.82 40.0369 L2338.82 48.5091 Q2339.84 48.1618 2340.86 47.9998 Q2341.88 47.8146 2342.9 47.8146 Q2348.69 47.8146 2352.07 50.9859 Q2355.45 54.1572 2355.45 59.5738 Q2355.45 65.1525 2351.97 68.2543 Q2348.5 71.333 2342.18 71.333 Q2340.01 71.333 2337.74 70.9626 Q2335.49 70.5923 2333.08 69.8515 L2333.08 65.1525 Q2335.17 66.2867 2337.39 66.8423 Q2339.61 67.3978 2342.09 67.3978 Q2346.09 67.3978 2348.43 65.2914 Q2350.77 63.1849 2350.77 59.5738 Q2350.77 55.9627 2348.43 53.8563 Q2346.09 51.7498 2342.09 51.7498 Q2340.21 51.7498 2338.34 52.1664 Q2336.49 52.5831 2334.54 53.4627 L2334.54 36.1017 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip440)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2232.76,1410.9 2232.76,1318.97 2256.76,1318.97 2232.76,1318.97 2232.76,1108.64 2256.76,1108.64 2232.76,1108.64 2232.76,898.318 2256.76,898.318 2232.76,898.318 \n",
       "  2232.76,687.991 2256.76,687.991 2232.76,687.991 2232.76,477.664 2256.76,477.664 2232.76,477.664 2232.76,267.337 2256.76,267.337 2232.76,267.337 2232.76,57.0105 \n",
       "  2256.76,57.0105 2232.76,57.0105 2232.76,47.2441 \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(self_tuning_svm_mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(best_model = \u001b[34mSVMClassifier @543\u001b[39m,\n",
       " best_fitted_params = (support = Int32[12, 14, 22, 48, 70, 105, 112, 125, 131, 150    228, 234, 240, 262, 317, 363, 380, 385, 394, 395],\n",
       "                       support_vectors = [14.2 20.53  0.2534 0.07858; 11.75 20.18  0.3168 0.07987;  ; 12.45 15.7  0.3985 0.1244; 16.46 20.11  0.3054 0.09519],\n",
       "                       n_support = Int32[23, 21],\n",
       "                       dual_coef = [-8.340502686000295e6 -2.060000331555374e6  8.340502686000295e6 8.340502686000295e6],\n",
       "                       coef = nothing,\n",
       "                       intercept = [2797.6162307287295],\n",
       "                       fit_status = 0,\n",
       "                       classes = UInt32[0x00000001, 0x00000002],),)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rbf = fitted_params(self_tuning_svm_mach)\n",
    "best_rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVMClassifier(\n",
       "    C = 8.340502686000295e6,\n",
       "    kernel = \"rbf\",\n",
       "    degree = 3,\n",
       "    gamma = 1.0e-9,\n",
       "    coef0 = 0.0,\n",
       "    shrinking = true,\n",
       "    tol = 0.001,\n",
       "    cache_size = 1000,\n",
       "    max_iter = -1,\n",
       "    decision_function_shape = \"ovr\",\n",
       "    random_state = nothing)\u001b[34m @543\u001b[39m"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rbf.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95232"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss = round(z.report.best_result.measurement[1],digits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = \"Figures/LearningCurve_DT_merge_purity_thresh:$(best_mpt)_loss:$(best_loss)\"\n",
    "# png(replace(fn,'.' => ','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{SVMClassifier} @248\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  17%[====>                    ]  ETA: 0:00:13\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:11\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:07\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:04\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:02\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:13\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[22m _.measure           \u001b[0m\u001b[0m\u001b[22m _.measurement \u001b[0m\u001b[0m\u001b[22m _.per_fold                                     \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m accuracy            \u001b[0m\u001b[0m 0.954         \u001b[0m\u001b[0m [0.958, 0.968, 0.947, 0.958, 0.926, 0.968]     \u001b[0m\u001b[0m\n",
       "\u001b[0m false_negative_rate \u001b[0m\u001b[0m 0.0831        \u001b[0m\u001b[0m [0.125, 0.0882, 0.0909, 0.114, 0.0233, 0.0571] \u001b[0m\u001b[0m\n",
       "\u001b[0m false_positive_rate \u001b[0m\u001b[0m 0.0274        \u001b[0m\u001b[0m [0.0, 0.0, 0.0323, 0.0, 0.115, 0.0169]         \u001b[0m\u001b[0m\n",
       "\u001b[0m true_negative_rate  \u001b[0m\u001b[0m 0.973         \u001b[0m\u001b[0m [1.0, 1.0, 0.968, 1.0, 0.885, 0.983]           \u001b[0m\u001b[0m\n",
       "\u001b[0m true_positive_rate  \u001b[0m\u001b[0m 0.917         \u001b[0m\u001b[0m [0.875, 0.912, 0.909, 0.886, 0.977, 0.943]     \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "_.per_observation = [missing, missing, missing, missing, missing]\n",
       "_.fitted_params_per_fold = [  ]\n",
       "_.report_per_fold = [  ]\n"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_linear_model = SVMClassifier(kernel=\"linear\", C = best_C)\n",
    "svm_mach = machine(best_linear_model, X, y)\n",
    "lf = fit!(svm_mach, rows=train, verbosity=2)\n",
    "svm_acc = evaluate!(svm_mach, resampling=CV(shuffle=true), measure=[accuracy, fnr, fpr, tnr, tpr], \n",
    "                        verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " Info: Training \u001b[34mMachine{SVMClassifier} @617\u001b[39m.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  17%[====>                    ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:00\u001b[39m Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      " Warning: The classes are un-ordered,\n",
      " using: negative='B' and positive='M'.\n",
      " To suppress this warning, consider coercing to OrderedFactor.\n",
      " @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[22m _.measure           \u001b[0m\u001b[0m\u001b[22m _.measurement \u001b[0m\u001b[0m\u001b[22m _.per_fold                                      \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "\u001b[0m accuracy            \u001b[0m\u001b[0m 0.949         \u001b[0m\u001b[0m [0.968, 0.947, 0.937, 0.958, 0.958, 0.926]      \u001b[0m\u001b[0m\n",
       "\u001b[0m false_negative_rate \u001b[0m\u001b[0m 0.0853        \u001b[0m\u001b[0m [0.0909, 0.0541, 0.143, 0.0714, 0.0714, 0.0811] \u001b[0m\u001b[0m\n",
       "\u001b[0m false_positive_rate \u001b[0m\u001b[0m 0.0312        \u001b[0m\u001b[0m [0.0, 0.0517, 0.0167, 0.0189, 0.0299, 0.0702]   \u001b[0m\u001b[0m\n",
       "\u001b[0m true_negative_rate  \u001b[0m\u001b[0m 0.969         \u001b[0m\u001b[0m [1.0, 0.948, 0.983, 0.981, 0.97, 0.93]          \u001b[0m\u001b[0m\n",
       "\u001b[0m true_positive_rate  \u001b[0m\u001b[0m 0.915         \u001b[0m\u001b[0m [0.909, 0.946, 0.857, 0.929, 0.929, 0.919]      \u001b[0m\u001b[0m\n",
       "\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\u001b[0m\n",
       "_.per_observation = [missing, missing, missing, missing, missing]\n",
       "_.fitted_params_per_fold = [  ]\n",
       "_.report_per_fold = [  ]\n"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rbf_model = SVMClassifier(kernel=\"rbf\", C=best_rbf.best_model.C, gamma=best_rbf.best_model.gamma)\n",
    "svm_mach = machine(best_rbf_model, X, y)\n",
    "fit!(svm_mach, rows=train, verbosity=2)\n",
    "svm_acc = evaluate!(svm_mach, resampling=CV(shuffle=true), measure=[accuracy, fnr, fpr, tnr, tpr], \n",
    "                        verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (10, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (11, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (12, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (13, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (14, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (15, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (16, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (17, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (18, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (19, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (20, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (21, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (22, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (23, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (24, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (25, 1.0, 0.8625)\n",
      "(d, train_metric, valid_metric) = (26, 1.0, 0.8625)\n",
      "(d, train_metric, valid_metric) = (27, 1.0, 0.8625)\n",
      "(d, train_metric, valid_metric) = (28, 1.0, 0.8625)\n",
      "(d, train_metric, valid_metric) = (29, 1.0, 0.8625)\n",
      "(d, train_metric, valid_metric) = (30, 1.0, 0.8625)\n",
      "(d, train_metric, valid_metric) = (31, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (32, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (33, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (34, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (35, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (36, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (37, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (38, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (39, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (40, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (41, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (42, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (43, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (44, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (45, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (46, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (47, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (48, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (49, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (50, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (51, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (52, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (53, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (54, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (55, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (56, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (57, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (58, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (59, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (60, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (61, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (62, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (63, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (64, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (65, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (66, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (67, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (68, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (69, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (70, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (71, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (72, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (73, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (74, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (75, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (76, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (77, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (78, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (79, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (80, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (81, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (82, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (83, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (84, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (85, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (86, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (87, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (88, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (89, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (90, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (91, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (92, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (93, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (94, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (95, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (96, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (97, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (98, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (99, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (100, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (101, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (102, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (103, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (104, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (105, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (106, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (107, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (108, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (109, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (110, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (111, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (112, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (113, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (114, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (115, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (116, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (117, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (118, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (119, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (120, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (121, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (122, 1.0, 0.9125)\n",
      "(d, train_metric, valid_metric) = (123, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (124, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (125, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (126, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (127, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (128, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (129, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (130, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (131, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (132, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (133, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (134, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (135, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (136, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (137, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (138, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (139, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (140, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (141, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (142, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (143, 0.993006993006993, 0.925)\n",
      "(d, train_metric, valid_metric) = (144, 0.9930555555555556, 0.925)\n",
      "(d, train_metric, valid_metric) = (145, 0.993103448275862, 0.925)\n",
      "(d, train_metric, valid_metric) = (146, 0.9931506849315068, 0.925)\n",
      "(d, train_metric, valid_metric) = (147, 0.9931972789115646, 0.925)\n",
      "(d, train_metric, valid_metric) = (148, 0.9932432432432432, 0.925)\n",
      "(d, train_metric, valid_metric) = (149, 0.9932885906040269, 0.925)\n",
      "(d, train_metric, valid_metric) = (150, 0.9933333333333333, 0.925)\n",
      "(d, train_metric, valid_metric) = (151, 0.9933774834437086, 0.925)\n",
      "(d, train_metric, valid_metric) = (152, 0.993421052631579, 0.925)\n",
      "(d, train_metric, valid_metric) = (153, 0.9934640522875817, 0.925)\n",
      "(d, train_metric, valid_metric) = (154, 0.9935064935064936, 0.925)\n",
      "(d, train_metric, valid_metric) = (155, 0.9935483870967742, 0.925)\n",
      "(d, train_metric, valid_metric) = (156, 0.9871794871794872, 0.925)\n",
      "(d, train_metric, valid_metric) = (157, 0.9872611464968153, 0.925)\n",
      "(d, train_metric, valid_metric) = (158, 0.9873417721518988, 0.925)\n",
      "(d, train_metric, valid_metric) = (159, 0.9874213836477987, 0.925)\n",
      "(d, train_metric, valid_metric) = (160, 0.9875, 0.925)\n",
      "(d, train_metric, valid_metric) = (161, 0.9875776397515528, 0.925)\n",
      "(d, train_metric, valid_metric) = (162, 0.9876543209876543, 0.925)\n",
      "(d, train_metric, valid_metric) = (163, 0.9877300613496932, 0.925)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (164, 0.9878048780487805, 0.925)\n",
      "(d, train_metric, valid_metric) = (165, 0.9878787878787879, 0.925)\n",
      "(d, train_metric, valid_metric) = (166, 0.9879518072289156, 0.925)\n",
      "(d, train_metric, valid_metric) = (167, 0.9880239520958084, 0.925)\n",
      "(d, train_metric, valid_metric) = (168, 0.9880952380952381, 0.925)\n",
      "(d, train_metric, valid_metric) = (169, 0.9881656804733728, 0.925)\n",
      "(d, train_metric, valid_metric) = (170, 0.9882352941176471, 0.925)\n",
      "(d, train_metric, valid_metric) = (171, 0.9883040935672515, 0.925)\n",
      "(d, train_metric, valid_metric) = (172, 0.9883720930232558, 0.925)\n",
      "(d, train_metric, valid_metric) = (173, 0.9884393063583815, 0.925)\n",
      "(d, train_metric, valid_metric) = (174, 0.9885057471264368, 0.925)\n",
      "(d, train_metric, valid_metric) = (175, 0.9885714285714285, 0.925)\n",
      "(d, train_metric, valid_metric) = (176, 0.9886363636363636, 0.925)\n",
      "(d, train_metric, valid_metric) = (177, 0.9887005649717514, 0.925)\n",
      "(d, train_metric, valid_metric) = (178, 0.9887640449438202, 0.925)\n",
      "(d, train_metric, valid_metric) = (179, 0.9888268156424581, 0.925)\n",
      "(d, train_metric, valid_metric) = (180, 0.9888888888888889, 0.925)\n",
      "(d, train_metric, valid_metric) = (181, 0.988950276243094, 0.925)\n",
      "(d, train_metric, valid_metric) = (182, 0.989010989010989, 0.925)\n",
      "(d, train_metric, valid_metric) = (183, 0.9890710382513661, 0.925)\n",
      "(d, train_metric, valid_metric) = (184, 0.9891304347826086, 0.925)\n",
      "(d, train_metric, valid_metric) = (185, 0.9891891891891892, 0.925)\n",
      "(d, train_metric, valid_metric) = (186, 0.989247311827957, 0.925)\n",
      "(d, train_metric, valid_metric) = (187, 0.9893048128342246, 0.925)\n",
      "(d, train_metric, valid_metric) = (188, 0.9840425531914894, 0.925)\n",
      "(d, train_metric, valid_metric) = (189, 0.9841269841269842, 0.925)\n",
      "(d, train_metric, valid_metric) = (190, 0.968421052631579, 0.925)\n",
      "(d, train_metric, valid_metric) = (191, 0.9685863874345549, 0.925)\n",
      "(d, train_metric, valid_metric) = (192, 0.96875, 0.925)\n",
      "(d, train_metric, valid_metric) = (193, 0.9689119170984456, 0.925)\n",
      "(d, train_metric, valid_metric) = (194, 0.9690721649484536, 0.925)\n",
      "(d, train_metric, valid_metric) = (195, 0.9743589743589743, 0.925)\n",
      "(d, train_metric, valid_metric) = (196, 0.9744897959183674, 0.925)\n",
      "(d, train_metric, valid_metric) = (197, 0.9746192893401016, 0.925)\n",
      "(d, train_metric, valid_metric) = (198, 0.9747474747474747, 0.9375)\n",
      "(d, train_metric, valid_metric) = (199, 0.9748743718592965, 0.9375)\n",
      "(d, train_metric, valid_metric) = (200, 0.975, 0.9375)\n",
      "(d, train_metric, valid_metric) = (201, 0.9751243781094527, 0.9375)\n",
      "(d, train_metric, valid_metric) = (202, 0.9752475247524752, 0.9375)\n",
      "(d, train_metric, valid_metric) = (203, 0.9753694581280788, 0.9375)\n",
      "(d, train_metric, valid_metric) = (204, 0.9754901960784313, 0.9375)\n",
      "(d, train_metric, valid_metric) = (205, 0.975609756097561, 0.9375)\n",
      "(d, train_metric, valid_metric) = (206, 0.9757281553398058, 0.925)\n",
      "(d, train_metric, valid_metric) = (207, 0.9758454106280193, 0.925)\n",
      "(d, train_metric, valid_metric) = (208, 0.9759615384615384, 0.925)\n",
      "(d, train_metric, valid_metric) = (209, 0.9760765550239234, 0.925)\n",
      "(d, train_metric, valid_metric) = (210, 0.9761904761904762, 0.925)\n",
      "(d, train_metric, valid_metric) = (211, 0.976303317535545, 0.925)\n",
      "(d, train_metric, valid_metric) = (212, 0.9764150943396226, 0.925)\n",
      "(d, train_metric, valid_metric) = (213, 0.9765258215962441, 0.925)\n",
      "(d, train_metric, valid_metric) = (214, 0.9766355140186916, 0.925)\n",
      "(d, train_metric, valid_metric) = (215, 0.9767441860465116, 0.925)\n",
      "(d, train_metric, valid_metric) = (216, 0.9768518518518519, 0.925)\n",
      "(d, train_metric, valid_metric) = (217, 0.9769585253456221, 0.925)\n",
      "(d, train_metric, valid_metric) = (218, 0.9724770642201834, 0.95)\n",
      "(d, train_metric, valid_metric) = (219, 0.9726027397260274, 0.95)\n",
      "(d, train_metric, valid_metric) = (220, 0.9727272727272728, 0.95)\n",
      "(d, train_metric, valid_metric) = (221, 0.9728506787330317, 0.95)\n",
      "(d, train_metric, valid_metric) = (222, 0.972972972972973, 0.95)\n",
      "(d, train_metric, valid_metric) = (223, 0.9730941704035875, 0.95)\n",
      "(d, train_metric, valid_metric) = (224, 0.9732142857142857, 0.95)\n",
      "(d, train_metric, valid_metric) = (225, 0.9733333333333334, 0.95)\n",
      "(d, train_metric, valid_metric) = (226, 0.9734513274336283, 0.95)\n",
      "(d, train_metric, valid_metric) = (227, 0.973568281938326, 0.95)\n",
      "(d, train_metric, valid_metric) = (228, 0.9736842105263158, 0.95)\n",
      "(d, train_metric, valid_metric) = (229, 0.9737991266375546, 0.95)\n",
      "(d, train_metric, valid_metric) = (230, 0.9739130434782609, 0.925)\n",
      "(d, train_metric, valid_metric) = (231, 0.974025974025974, 0.925)\n",
      "(d, train_metric, valid_metric) = (232, 0.9741379310344828, 0.925)\n",
      "(d, train_metric, valid_metric) = (233, 0.9742489270386266, 0.925)\n",
      "(d, train_metric, valid_metric) = (234, 0.9743589743589743, 0.925)\n",
      "(d, train_metric, valid_metric) = (235, 0.9744680851063829, 0.925)\n",
      "(d, train_metric, valid_metric) = (236, 0.9745762711864406, 0.925)\n",
      "(d, train_metric, valid_metric) = (237, 0.9662447257383966, 0.925)\n",
      "(d, train_metric, valid_metric) = (238, 0.9663865546218487, 0.925)\n",
      "(d, train_metric, valid_metric) = (239, 0.9665271966527197, 0.925)\n",
      "(d, train_metric, valid_metric) = (240, 0.9666666666666667, 0.925)\n",
      "(d, train_metric, valid_metric) = (241, 0.966804979253112, 0.925)\n",
      "(d, train_metric, valid_metric) = (242, 0.9669421487603306, 0.925)\n",
      "(d, train_metric, valid_metric) = (243, 0.9670781893004116, 0.925)\n",
      "(d, train_metric, valid_metric) = (244, 0.9672131147540983, 0.925)\n",
      "(d, train_metric, valid_metric) = (245, 0.9673469387755103, 0.9125)\n",
      "(d, train_metric, valid_metric) = (246, 0.967479674796748, 0.9125)\n",
      "(d, train_metric, valid_metric) = (247, 0.9676113360323887, 0.9125)\n",
      "(d, train_metric, valid_metric) = (248, 0.967741935483871, 0.9125)\n",
      "(d, train_metric, valid_metric) = (249, 0.9678714859437751, 0.9125)\n",
      "(d, train_metric, valid_metric) = (250, 0.968, 0.9125)\n",
      "(d, train_metric, valid_metric) = (251, 0.9681274900398407, 0.9125)\n",
      "(d, train_metric, valid_metric) = (252, 0.9722222222222222, 0.9125)\n",
      "(d, train_metric, valid_metric) = (253, 0.9723320158102767, 0.9125)\n",
      "(d, train_metric, valid_metric) = (254, 0.9724409448818898, 0.9125)\n",
      "(d, train_metric, valid_metric) = (255, 0.9725490196078431, 0.9125)\n",
      "(d, train_metric, valid_metric) = (256, 0.97265625, 0.9125)\n",
      "(d, train_metric, valid_metric) = (257, 0.9727626459143969, 0.9125)\n",
      "(d, train_metric, valid_metric) = (258, 0.9728682170542635, 0.925)\n",
      "(d, train_metric, valid_metric) = (259, 0.972972972972973, 0.925)\n",
      "(d, train_metric, valid_metric) = (260, 0.9730769230769231, 0.925)\n",
      "(d, train_metric, valid_metric) = (261, 0.9731800766283525, 0.925)\n",
      "(d, train_metric, valid_metric) = (262, 0.9732824427480916, 0.925)\n",
      "(d, train_metric, valid_metric) = (263, 0.973384030418251, 0.925)\n",
      "(d, train_metric, valid_metric) = (264, 0.9734848484848485, 0.925)\n",
      "(d, train_metric, valid_metric) = (265, 0.9735849056603774, 0.925)\n",
      "(d, train_metric, valid_metric) = (266, 0.9736842105263158, 0.925)\n",
      "(d, train_metric, valid_metric) = (267, 0.9737827715355806, 0.925)\n",
      "(d, train_metric, valid_metric) = (268, 0.9738805970149254, 0.925)\n",
      "(d, train_metric, valid_metric) = (269, 0.9739776951672863, 0.925)\n",
      "(d, train_metric, valid_metric) = (270, 0.9740740740740741, 0.925)\n",
      "(d, train_metric, valid_metric) = (271, 0.974169741697417, 0.925)\n",
      "(d, train_metric, valid_metric) = (272, 0.9705882352941176, 0.925)\n",
      "(d, train_metric, valid_metric) = (273, 0.9743589743589743, 0.925)\n",
      "(d, train_metric, valid_metric) = (274, 0.9744525547445255, 0.925)\n",
      "(d, train_metric, valid_metric) = (275, 0.9745454545454545, 0.925)\n",
      "(d, train_metric, valid_metric) = (276, 0.967391304347826, 0.925)\n",
      "(d, train_metric, valid_metric) = (277, 0.9675090252707581, 0.925)\n",
      "(d, train_metric, valid_metric) = (278, 0.9676258992805755, 0.925)\n",
      "(d, train_metric, valid_metric) = (279, 0.967741935483871, 0.925)\n",
      "(d, train_metric, valid_metric) = (280, 0.9785714285714285, 0.9125)\n",
      "(d, train_metric, valid_metric) = (281, 0.9679715302491103, 0.925)\n",
      "(d, train_metric, valid_metric) = (282, 0.9680851063829787, 0.925)\n",
      "(d, train_metric, valid_metric) = (283, 0.9681978798586572, 0.925)\n",
      "(d, train_metric, valid_metric) = (284, 0.9683098591549296, 0.925)\n",
      "(d, train_metric, valid_metric) = (285, 0.968421052631579, 0.925)\n",
      "(d, train_metric, valid_metric) = (286, 0.9685314685314685, 0.925)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (287, 0.9790940766550522, 0.9125)\n",
      "(d, train_metric, valid_metric) = (288, 0.96875, 0.925)\n",
      "(d, train_metric, valid_metric) = (289, 0.9688581314878892, 0.925)\n",
      "(d, train_metric, valid_metric) = (290, 0.9689655172413794, 0.925)\n",
      "(d, train_metric, valid_metric) = (291, 0.9690721649484536, 0.925)\n",
      "(d, train_metric, valid_metric) = (292, 0.9691780821917808, 0.925)\n",
      "(d, train_metric, valid_metric) = (293, 0.9692832764505119, 0.925)\n",
      "(d, train_metric, valid_metric) = (294, 0.9693877551020408, 0.925)\n",
      "(d, train_metric, valid_metric) = (295, 0.9694915254237289, 0.925)\n",
      "(d, train_metric, valid_metric) = (296, 0.9695945945945946, 0.925)\n",
      "(d, train_metric, valid_metric) = (297, 0.9696969696969697, 0.925)\n",
      "(d, train_metric, valid_metric) = (298, 0.9697986577181208, 0.925)\n",
      "(d, train_metric, valid_metric) = (299, 0.9698996655518395, 0.925)\n",
      "(d, train_metric, valid_metric) = (300, 0.97, 0.925)\n",
      "(d, train_metric, valid_metric) = (301, 0.9700996677740864, 0.925)\n",
      "(d, train_metric, valid_metric) = (302, 0.9701986754966887, 0.925)\n",
      "(d, train_metric, valid_metric) = (303, 0.9702970297029703, 0.925)\n",
      "(d, train_metric, valid_metric) = (304, 0.9802631578947368, 0.9125)\n",
      "(d, train_metric, valid_metric) = (305, 0.980327868852459, 0.9375)\n",
      "(d, train_metric, valid_metric) = (306, 0.9803921568627451, 0.9375)\n",
      "(d, train_metric, valid_metric) = (307, 0.9804560260586319, 0.9375)\n",
      "(d, train_metric, valid_metric) = (308, 0.9707792207792207, 0.95)\n",
      "(d, train_metric, valid_metric) = (309, 0.970873786407767, 0.95)\n",
      "(d, train_metric, valid_metric) = (310, 0.9709677419354839, 0.9375)\n",
      "(d, train_metric, valid_metric) = (311, 0.9710610932475884, 0.95)\n",
      "(d, train_metric, valid_metric) = (312, 0.9711538461538461, 0.95)\n",
      "(d, train_metric, valid_metric) = (313, 0.9712460063897763, 0.95)\n",
      "(d, train_metric, valid_metric) = (314, 0.9713375796178344, 0.95)\n",
      "(d, train_metric, valid_metric) = (315, 0.9714285714285714, 0.95)\n",
      "(d, train_metric, valid_metric) = (316, 0.9715189873417721, 0.95)\n",
      "(d, train_metric, valid_metric) = (317, 0.9716088328075709, 0.95)\n",
      "(d, train_metric, valid_metric) = (318, 0.9716981132075472, 0.95)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10:1:318, Any[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0    0.970873786407767, 0.9709677419354839, 0.9710610932475884, 0.9711538461538461, 0.9712460063897763, 0.9713375796178344, 0.9714285714285714, 0.9715189873417721, 0.9716088328075709, 0.9716981132075472], Any[0.925, 0.925, 0.925, 0.925, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9    0.95, 0.9375, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95, 0.95])"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_schedule, training_losses, valid_losses = learn_curve(best_linear_model, X[train,:], y[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip680\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip680)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip681\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip680)\" d=\"\n",
       "M201.691 1486.45 L2352.76 1486.45 L2352.76 47.2441 L201.691 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip682\">\n",
       "    <rect x=\"201\" y=\"47\" width=\"2152\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  526.116,1486.45 526.116,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  855.549,1486.45 855.549,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1184.98,1486.45 1184.98,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1514.41,1486.45 1514.41,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1843.85,1486.45 1843.85,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2173.28,1486.45 2173.28,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  201.691,1322.28 2352.76,1322.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  201.691,1075.42 2352.76,1075.42 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  201.691,828.561 2352.76,828.561 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  201.691,581.7 2352.76,581.7 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  201.691,334.838 2352.76,334.838 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  201.691,87.9763 2352.76,87.9763 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  201.691,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  201.691,1486.45 201.691,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  526.116,1486.45 526.116,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  855.549,1486.45 855.549,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1184.98,1486.45 1184.98,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1514.41,1486.45 1514.41,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1843.85,1486.45 1843.85,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2173.28,1486.45 2173.28,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  201.691,1322.28 227.503,1322.28 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  201.691,1075.42 227.503,1075.42 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  201.691,828.561 227.503,828.561 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  201.691,581.7 227.503,581.7 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  201.691,334.838 227.503,334.838 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  201.691,87.9763 227.503,87.9763 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip680)\" d=\"M 0 0 M502.887 1505.36 L521.243 1505.36 L521.243 1509.3 L507.169 1509.3 L507.169 1517.77 Q508.188 1517.42 509.207 1517.26 Q510.225 1517.07 511.244 1517.07 Q517.031 1517.07 520.41 1520.24 Q523.79 1523.42 523.79 1528.83 Q523.79 1534.41 520.318 1537.51 Q516.845 1540.59 510.526 1540.59 Q508.35 1540.59 506.082 1540.22 Q503.836 1539.85 501.429 1539.11 L501.429 1534.41 Q503.512 1535.54 505.734 1536.1 Q507.957 1536.66 510.433 1536.66 Q514.438 1536.66 516.776 1534.55 Q519.114 1532.44 519.114 1528.83 Q519.114 1525.22 516.776 1523.11 Q514.438 1521.01 510.433 1521.01 Q508.558 1521.01 506.683 1521.42 Q504.832 1521.84 502.887 1522.72 L502.887 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M538.859 1508.44 Q535.248 1508.44 533.419 1512 Q531.614 1515.55 531.614 1522.67 Q531.614 1529.78 533.419 1533.35 Q535.248 1536.89 538.859 1536.89 Q542.493 1536.89 544.299 1533.35 Q546.128 1529.78 546.128 1522.67 Q546.128 1515.55 544.299 1512 Q542.493 1508.44 538.859 1508.44 M538.859 1504.73 Q544.669 1504.73 547.725 1509.34 Q550.804 1513.92 550.804 1522.67 Q550.804 1531.4 547.725 1536.01 Q544.669 1540.59 538.859 1540.59 Q533.049 1540.59 529.97 1536.01 Q526.915 1531.4 526.915 1522.67 Q526.915 1513.92 529.97 1509.34 Q533.049 1504.73 538.859 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M818.917 1535.98 L826.556 1535.98 L826.556 1509.62 L818.246 1511.29 L818.246 1507.03 L826.51 1505.36 L831.186 1505.36 L831.186 1535.98 L838.825 1535.98 L838.825 1539.92 L818.917 1539.92 L818.917 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M853.894 1508.44 Q850.283 1508.44 848.454 1512 Q846.649 1515.55 846.649 1522.67 Q846.649 1529.78 848.454 1533.35 Q850.283 1536.89 853.894 1536.89 Q857.528 1536.89 859.334 1533.35 Q861.162 1529.78 861.162 1522.67 Q861.162 1515.55 859.334 1512 Q857.528 1508.44 853.894 1508.44 M853.894 1504.73 Q859.704 1504.73 862.76 1509.34 Q865.838 1513.92 865.838 1522.67 Q865.838 1531.4 862.76 1536.01 Q859.704 1540.59 853.894 1540.59 Q848.084 1540.59 845.005 1536.01 Q841.95 1531.4 841.95 1522.67 Q841.95 1513.92 845.005 1509.34 Q848.084 1504.73 853.894 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M880.908 1508.44 Q877.297 1508.44 875.468 1512 Q873.662 1515.55 873.662 1522.67 Q873.662 1529.78 875.468 1533.35 Q877.297 1536.89 880.908 1536.89 Q884.542 1536.89 886.348 1533.35 Q888.176 1529.78 888.176 1522.67 Q888.176 1515.55 886.348 1512 Q884.542 1508.44 880.908 1508.44 M880.908 1504.73 Q886.718 1504.73 889.773 1509.34 Q892.852 1513.92 892.852 1522.67 Q892.852 1531.4 889.773 1536.01 Q886.718 1540.59 880.908 1540.59 Q875.098 1540.59 872.019 1536.01 Q868.963 1531.4 868.963 1522.67 Q868.963 1513.92 872.019 1509.34 Q875.098 1504.73 880.908 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M1148.85 1535.98 L1156.49 1535.98 L1156.49 1509.62 L1148.18 1511.29 L1148.18 1507.03 L1156.44 1505.36 L1161.12 1505.36 L1161.12 1535.98 L1168.76 1535.98 L1168.76 1539.92 L1148.85 1539.92 L1148.85 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M1173.87 1505.36 L1192.23 1505.36 L1192.23 1509.3 L1178.15 1509.3 L1178.15 1517.77 Q1179.17 1517.42 1180.19 1517.26 Q1181.21 1517.07 1182.23 1517.07 Q1188.01 1517.07 1191.39 1520.24 Q1194.77 1523.42 1194.77 1528.83 Q1194.77 1534.41 1191.3 1537.51 Q1187.83 1540.59 1181.51 1540.59 Q1179.33 1540.59 1177.07 1540.22 Q1174.82 1539.85 1172.41 1539.11 L1172.41 1534.41 Q1174.5 1535.54 1176.72 1536.1 Q1178.94 1536.66 1181.42 1536.66 Q1185.42 1536.66 1187.76 1534.55 Q1190.1 1532.44 1190.1 1528.83 Q1190.1 1525.22 1187.76 1523.11 Q1185.42 1521.01 1181.42 1521.01 Q1179.54 1521.01 1177.67 1521.42 Q1175.82 1521.84 1173.87 1522.72 L1173.87 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M1209.84 1508.44 Q1206.23 1508.44 1204.4 1512 Q1202.6 1515.55 1202.6 1522.67 Q1202.6 1529.78 1204.4 1533.35 Q1206.23 1536.89 1209.84 1536.89 Q1213.48 1536.89 1215.28 1533.35 Q1217.11 1529.78 1217.11 1522.67 Q1217.11 1515.55 1215.28 1512 Q1213.48 1508.44 1209.84 1508.44 M1209.84 1504.73 Q1215.65 1504.73 1218.71 1509.34 Q1221.79 1513.92 1221.79 1522.67 Q1221.79 1531.4 1218.71 1536.01 Q1215.65 1540.59 1209.84 1540.59 Q1204.03 1540.59 1200.95 1536.01 Q1197.9 1531.4 1197.9 1522.67 Q1197.9 1513.92 1200.95 1509.34 Q1204.03 1504.73 1209.84 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M1482.05 1535.98 L1498.37 1535.98 L1498.37 1539.92 L1476.43 1539.92 L1476.43 1535.98 Q1479.09 1533.23 1483.67 1528.6 Q1488.28 1523.95 1489.46 1522.61 Q1491.71 1520.08 1492.59 1518.35 Q1493.49 1516.59 1493.49 1514.9 Q1493.49 1512.14 1491.54 1510.41 Q1489.62 1508.67 1486.52 1508.67 Q1484.32 1508.67 1481.87 1509.43 Q1479.44 1510.2 1476.66 1511.75 L1476.66 1507.03 Q1479.48 1505.89 1481.94 1505.31 Q1484.39 1504.73 1486.43 1504.73 Q1491.8 1504.73 1494.99 1507.42 Q1498.19 1510.11 1498.19 1514.6 Q1498.19 1516.73 1497.38 1518.65 Q1496.59 1520.54 1494.48 1523.14 Q1493.91 1523.81 1490.8 1527.03 Q1487.7 1530.22 1482.05 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M1513.44 1508.44 Q1509.83 1508.44 1508 1512 Q1506.2 1515.55 1506.2 1522.67 Q1506.2 1529.78 1508 1533.35 Q1509.83 1536.89 1513.44 1536.89 Q1517.08 1536.89 1518.88 1533.35 Q1520.71 1529.78 1520.71 1522.67 Q1520.71 1515.55 1518.88 1512 Q1517.08 1508.44 1513.44 1508.44 M1513.44 1504.73 Q1519.25 1504.73 1522.31 1509.34 Q1525.39 1513.92 1525.39 1522.67 Q1525.39 1531.4 1522.31 1536.01 Q1519.25 1540.59 1513.44 1540.59 Q1507.63 1540.59 1504.55 1536.01 Q1501.5 1531.4 1501.5 1522.67 Q1501.5 1513.92 1504.55 1509.34 Q1507.63 1504.73 1513.44 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M1540.46 1508.44 Q1536.85 1508.44 1535.02 1512 Q1533.21 1515.55 1533.21 1522.67 Q1533.21 1529.78 1535.02 1533.35 Q1536.85 1536.89 1540.46 1536.89 Q1544.09 1536.89 1545.9 1533.35 Q1547.72 1529.78 1547.72 1522.67 Q1547.72 1515.55 1545.9 1512 Q1544.09 1508.44 1540.46 1508.44 M1540.46 1504.73 Q1546.27 1504.73 1549.32 1509.34 Q1552.4 1513.92 1552.4 1522.67 Q1552.4 1531.4 1549.32 1536.01 Q1546.27 1540.59 1540.46 1540.59 Q1534.65 1540.59 1531.57 1536.01 Q1528.51 1531.4 1528.51 1522.67 Q1528.51 1513.92 1531.57 1509.34 Q1534.65 1504.73 1540.46 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M1811.98 1535.98 L1828.3 1535.98 L1828.3 1539.92 L1806.36 1539.92 L1806.36 1535.98 Q1809.02 1533.23 1813.61 1528.6 Q1818.21 1523.95 1819.39 1522.61 Q1821.64 1520.08 1822.52 1518.35 Q1823.42 1516.59 1823.42 1514.9 Q1823.42 1512.14 1821.48 1510.41 Q1819.55 1508.67 1816.45 1508.67 Q1814.25 1508.67 1811.8 1509.43 Q1809.37 1510.2 1806.59 1511.75 L1806.59 1507.03 Q1809.42 1505.89 1811.87 1505.31 Q1814.32 1504.73 1816.36 1504.73 Q1821.73 1504.73 1824.92 1507.42 Q1828.12 1510.11 1828.12 1514.6 Q1828.12 1516.73 1827.31 1518.65 Q1826.52 1520.54 1824.42 1523.14 Q1823.84 1523.81 1820.73 1527.03 Q1817.63 1530.22 1811.98 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M1833.42 1505.36 L1851.78 1505.36 L1851.78 1509.3 L1837.7 1509.3 L1837.7 1517.77 Q1838.72 1517.42 1839.74 1517.26 Q1840.76 1517.07 1841.78 1517.07 Q1847.56 1517.07 1850.94 1520.24 Q1854.32 1523.42 1854.32 1528.83 Q1854.32 1534.41 1850.85 1537.51 Q1847.38 1540.59 1841.06 1540.59 Q1838.88 1540.59 1836.61 1540.22 Q1834.37 1539.85 1831.96 1539.11 L1831.96 1534.41 Q1834.04 1535.54 1836.27 1536.1 Q1838.49 1536.66 1840.97 1536.66 Q1844.97 1536.66 1847.31 1534.55 Q1849.65 1532.44 1849.65 1528.83 Q1849.65 1525.22 1847.31 1523.11 Q1844.97 1521.01 1840.97 1521.01 Q1839.09 1521.01 1837.22 1521.42 Q1835.36 1521.84 1833.42 1522.72 L1833.42 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M1869.39 1508.44 Q1865.78 1508.44 1863.95 1512 Q1862.15 1515.55 1862.15 1522.67 Q1862.15 1529.78 1863.95 1533.35 Q1865.78 1536.89 1869.39 1536.89 Q1873.03 1536.89 1874.83 1533.35 Q1876.66 1529.78 1876.66 1522.67 Q1876.66 1515.55 1874.83 1512 Q1873.03 1508.44 1869.39 1508.44 M1869.39 1504.73 Q1875.2 1504.73 1878.26 1509.34 Q1881.34 1513.92 1881.34 1522.67 Q1881.34 1531.4 1878.26 1536.01 Q1875.2 1540.59 1869.39 1540.59 Q1863.58 1540.59 1860.5 1536.01 Q1857.45 1531.4 1857.45 1522.67 Q1857.45 1513.92 1860.5 1509.34 Q1863.58 1504.73 1869.39 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M2150.51 1521.29 Q2153.87 1522 2155.75 1524.27 Q2157.64 1526.54 2157.64 1529.87 Q2157.64 1534.99 2154.13 1537.79 Q2150.61 1540.59 2144.13 1540.59 Q2141.95 1540.59 2139.64 1540.15 Q2137.34 1539.73 2134.89 1538.88 L2134.89 1534.36 Q2136.83 1535.5 2139.15 1536.08 Q2141.46 1536.66 2143.99 1536.66 Q2148.39 1536.66 2150.68 1534.92 Q2152.99 1533.18 2152.99 1529.87 Q2152.99 1526.82 2150.84 1525.11 Q2148.71 1523.37 2144.89 1523.37 L2140.86 1523.37 L2140.86 1519.53 L2145.07 1519.53 Q2148.52 1519.53 2150.35 1518.16 Q2152.18 1516.77 2152.18 1514.18 Q2152.18 1511.52 2150.28 1510.11 Q2148.41 1508.67 2144.89 1508.67 Q2142.97 1508.67 2140.77 1509.09 Q2138.57 1509.5 2135.93 1510.38 L2135.93 1506.22 Q2138.59 1505.48 2140.91 1505.11 Q2143.25 1504.73 2145.31 1504.73 Q2150.63 1504.73 2153.73 1507.17 Q2156.83 1509.57 2156.83 1513.69 Q2156.83 1516.56 2155.19 1518.55 Q2153.55 1520.52 2150.51 1521.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M2172.71 1508.44 Q2169.1 1508.44 2167.27 1512 Q2165.47 1515.55 2165.47 1522.67 Q2165.47 1529.78 2167.27 1533.35 Q2169.1 1536.89 2172.71 1536.89 Q2176.35 1536.89 2178.15 1533.35 Q2179.98 1529.78 2179.98 1522.67 Q2179.98 1515.55 2178.15 1512 Q2176.35 1508.44 2172.71 1508.44 M2172.71 1504.73 Q2178.52 1504.73 2181.58 1509.34 Q2184.66 1513.92 2184.66 1522.67 Q2184.66 1531.4 2181.58 1536.01 Q2178.52 1540.59 2172.71 1540.59 Q2166.9 1540.59 2163.82 1536.01 Q2160.77 1531.4 2160.77 1522.67 Q2160.77 1513.92 2163.82 1509.34 Q2166.9 1504.73 2172.71 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M2199.73 1508.44 Q2196.12 1508.44 2194.29 1512 Q2192.48 1515.55 2192.48 1522.67 Q2192.48 1529.78 2194.29 1533.35 Q2196.12 1536.89 2199.73 1536.89 Q2203.36 1536.89 2205.17 1533.35 Q2207 1529.78 2207 1522.67 Q2207 1515.55 2205.17 1512 Q2203.36 1508.44 2199.73 1508.44 M2199.73 1504.73 Q2205.54 1504.73 2208.59 1509.34 Q2211.67 1513.92 2211.67 1522.67 Q2211.67 1531.4 2208.59 1536.01 Q2205.54 1540.59 2199.73 1540.59 Q2193.92 1540.59 2190.84 1536.01 Q2187.78 1531.4 2187.78 1522.67 Q2187.78 1513.92 2190.84 1509.34 Q2193.92 1504.73 2199.73 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M76.7421 1308.08 Q73.131 1308.08 71.3023 1311.65 Q69.4967 1315.19 69.4967 1322.32 Q69.4967 1329.43 71.3023 1332.99 Q73.131 1336.53 76.7421 1336.53 Q80.3763 1336.53 82.1818 1332.99 Q84.0105 1329.43 84.0105 1322.32 Q84.0105 1315.19 82.1818 1311.65 Q80.3763 1308.08 76.7421 1308.08 M76.7421 1304.38 Q82.5522 1304.38 85.6077 1308.99 Q88.6864 1313.57 88.6864 1322.32 Q88.6864 1331.05 85.6077 1335.65 Q82.5522 1340.24 76.7421 1340.24 Q70.9319 1340.24 67.8532 1335.65 Q64.7977 1331.05 64.7977 1322.32 Q64.7977 1313.57 67.8532 1308.99 Q70.9319 1304.38 76.7421 1304.38 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M93.7559 1333.69 L98.6401 1333.69 L98.6401 1339.56 L93.7559 1339.56 L93.7559 1333.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M113.709 1323.15 Q110.376 1323.15 108.455 1324.94 Q106.557 1326.72 106.557 1329.84 Q106.557 1332.97 108.455 1334.75 Q110.376 1336.53 113.709 1336.53 Q117.043 1336.53 118.964 1334.75 Q120.885 1332.94 120.885 1329.84 Q120.885 1326.72 118.964 1324.94 Q117.066 1323.15 113.709 1323.15 M109.034 1321.16 Q106.024 1320.42 104.334 1318.36 Q102.668 1316.3 102.668 1313.34 Q102.668 1309.19 105.608 1306.79 Q108.571 1304.38 113.709 1304.38 Q118.871 1304.38 121.811 1306.79 Q124.751 1309.19 124.751 1313.34 Q124.751 1316.3 123.061 1318.36 Q121.395 1320.42 118.408 1321.16 Q121.788 1321.95 123.663 1324.24 Q125.561 1326.53 125.561 1329.84 Q125.561 1334.87 122.483 1337.55 Q119.427 1340.24 113.709 1340.24 Q107.992 1340.24 104.913 1337.55 Q101.858 1334.87 101.858 1329.84 Q101.858 1326.53 103.756 1324.24 Q105.654 1321.95 109.034 1321.16 M107.321 1313.78 Q107.321 1316.46 108.987 1317.97 Q110.677 1319.47 113.709 1319.47 Q116.719 1319.47 118.408 1317.97 Q120.121 1316.46 120.121 1313.78 Q120.121 1311.09 118.408 1309.59 Q116.719 1308.08 113.709 1308.08 Q110.677 1308.08 108.987 1309.59 Q107.321 1311.09 107.321 1313.78 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M129.45 1305 L151.672 1305 L151.672 1307 L139.126 1339.56 L134.242 1339.56 L146.047 1308.94 L129.45 1308.94 L129.45 1305 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M156.788 1305 L175.144 1305 L175.144 1308.94 L161.07 1308.94 L161.07 1317.41 Q162.089 1317.06 163.107 1316.9 Q164.126 1316.72 165.144 1316.72 Q170.931 1316.72 174.311 1319.89 Q177.691 1323.06 177.691 1328.48 Q177.691 1334.06 174.218 1337.16 Q170.746 1340.24 164.427 1340.24 Q162.251 1340.24 159.982 1339.87 Q157.737 1339.5 155.33 1338.75 L155.33 1334.06 Q157.413 1335.19 159.635 1335.75 Q161.857 1336.3 164.334 1336.3 Q168.339 1336.3 170.677 1334.19 Q173.015 1332.09 173.015 1328.48 Q173.015 1324.87 170.677 1322.76 Q168.339 1320.65 164.334 1320.65 Q162.459 1320.65 160.584 1321.07 Q158.732 1321.49 156.788 1322.37 L156.788 1305 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M74.9365 1061.22 Q71.3254 1061.22 69.4967 1064.79 Q67.6912 1068.33 67.6912 1075.46 Q67.6912 1082.56 69.4967 1086.13 Q71.3254 1089.67 74.9365 1089.67 Q78.5707 1089.67 80.3763 1086.13 Q82.205 1082.56 82.205 1075.46 Q82.205 1068.33 80.3763 1064.79 Q78.5707 1061.22 74.9365 1061.22 M74.9365 1057.52 Q80.7467 1057.52 83.8022 1062.12 Q86.8809 1066.71 86.8809 1075.46 Q86.8809 1084.18 83.8022 1088.79 Q80.7467 1093.37 74.9365 1093.37 Q69.1264 1093.37 66.0477 1088.79 Q62.9921 1084.18 62.9921 1075.46 Q62.9921 1066.71 66.0477 1062.12 Q69.1264 1057.52 74.9365 1057.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M91.9503 1086.82 L96.8345 1086.82 L96.8345 1092.7 L91.9503 1092.7 L91.9503 1086.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M102.043 1091.99 L102.043 1087.73 Q103.802 1088.56 105.608 1089 Q107.413 1089.44 109.149 1089.44 Q113.779 1089.44 116.209 1086.34 Q118.663 1083.21 119.01 1076.87 Q117.668 1078.86 115.608 1079.93 Q113.547 1080.99 111.047 1080.99 Q105.862 1080.99 102.83 1077.87 Q99.8206 1074.72 99.8206 1069.28 Q99.8206 1063.95 102.969 1060.74 Q106.117 1057.52 111.348 1057.52 Q117.344 1057.52 120.492 1062.12 Q123.663 1066.71 123.663 1075.46 Q123.663 1083.63 119.774 1088.51 Q115.909 1093.37 109.358 1093.37 Q107.598 1093.37 105.793 1093.03 Q103.987 1092.68 102.043 1091.99 M111.348 1077.33 Q114.496 1077.33 116.325 1075.18 Q118.177 1073.03 118.177 1069.28 Q118.177 1065.55 116.325 1063.4 Q114.496 1061.22 111.348 1061.22 Q108.2 1061.22 106.348 1063.4 Q104.52 1065.55 104.52 1069.28 Q104.52 1073.03 106.348 1075.18 Q108.2 1077.33 111.348 1077.33 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M138.732 1061.22 Q135.121 1061.22 133.293 1064.79 Q131.487 1068.33 131.487 1075.46 Q131.487 1082.56 133.293 1086.13 Q135.121 1089.67 138.732 1089.67 Q142.367 1089.67 144.172 1086.13 Q146.001 1082.56 146.001 1075.46 Q146.001 1068.33 144.172 1064.79 Q142.367 1061.22 138.732 1061.22 M138.732 1057.52 Q144.543 1057.52 147.598 1062.12 Q150.677 1066.71 150.677 1075.46 Q150.677 1084.18 147.598 1088.79 Q144.543 1093.37 138.732 1093.37 Q132.922 1093.37 129.844 1088.79 Q126.788 1084.18 126.788 1075.46 Q126.788 1066.71 129.844 1062.12 Q132.922 1057.52 138.732 1057.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M165.746 1061.22 Q162.135 1061.22 160.306 1064.79 Q158.501 1068.33 158.501 1075.46 Q158.501 1082.56 160.306 1086.13 Q162.135 1089.67 165.746 1089.67 Q169.38 1089.67 171.186 1086.13 Q173.015 1082.56 173.015 1075.46 Q173.015 1068.33 171.186 1064.79 Q169.38 1061.22 165.746 1061.22 M165.746 1057.52 Q171.556 1057.52 174.612 1062.12 Q177.691 1066.71 177.691 1075.46 Q177.691 1084.18 174.612 1088.79 Q171.556 1093.37 165.746 1093.37 Q159.936 1093.37 156.857 1088.79 Q153.802 1084.18 153.802 1075.46 Q153.802 1066.71 156.857 1062.12 Q159.936 1057.52 165.746 1057.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M77.5291 814.36 Q73.918 814.36 72.0893 817.925 Q70.2838 821.467 70.2838 828.596 Q70.2838 835.703 72.0893 839.267 Q73.918 842.809 77.5291 842.809 Q81.1633 842.809 82.9689 839.267 Q84.7976 835.703 84.7976 828.596 Q84.7976 821.467 82.9689 817.925 Q81.1633 814.36 77.5291 814.36 M77.5291 810.656 Q83.3392 810.656 86.3948 815.263 Q89.4735 819.846 89.4735 828.596 Q89.4735 837.323 86.3948 841.929 Q83.3392 846.513 77.5291 846.513 Q71.7189 846.513 68.6402 841.929 Q65.5847 837.323 65.5847 828.596 Q65.5847 819.846 68.6402 815.263 Q71.7189 810.656 77.5291 810.656 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M94.5429 839.962 L99.4271 839.962 L99.4271 845.841 L94.5429 845.841 L94.5429 839.962 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M104.635 845.124 L104.635 840.865 Q106.395 841.698 108.2 842.138 Q110.006 842.578 111.742 842.578 Q116.371 842.578 118.802 839.476 Q121.256 836.351 121.603 830.008 Q120.26 831.999 118.2 833.064 Q116.14 834.129 113.64 834.129 Q108.455 834.129 105.422 831.004 Q102.413 827.855 102.413 822.416 Q102.413 817.092 105.561 813.874 Q108.709 810.656 113.941 810.656 Q119.936 810.656 123.084 815.263 Q126.256 819.846 126.256 828.596 Q126.256 836.767 122.367 841.652 Q118.501 846.513 111.95 846.513 Q110.191 846.513 108.385 846.165 Q106.58 845.818 104.635 845.124 M113.941 830.471 Q117.089 830.471 118.918 828.318 Q120.77 826.166 120.77 822.416 Q120.77 818.689 118.918 816.536 Q117.089 814.36 113.941 814.36 Q110.793 814.36 108.941 816.536 Q107.112 818.689 107.112 822.416 Q107.112 826.166 108.941 828.318 Q110.793 830.471 113.941 830.471 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M135.353 841.906 L151.672 841.906 L151.672 845.841 L129.728 845.841 L129.728 841.906 Q132.39 839.152 136.973 834.522 Q141.58 829.869 142.76 828.527 Q145.006 826.004 145.885 824.267 Q146.788 822.508 146.788 820.818 Q146.788 818.064 144.844 816.328 Q142.922 814.592 139.82 814.592 Q137.621 814.592 135.168 815.355 Q132.737 816.119 129.959 817.67 L129.959 812.948 Q132.783 811.814 135.237 811.235 Q137.691 810.656 139.728 810.656 Q145.098 810.656 148.293 813.342 Q151.487 816.027 151.487 820.517 Q151.487 822.647 150.677 824.568 Q149.89 826.467 147.783 829.059 Q147.205 829.73 144.103 832.948 Q141.001 836.142 135.353 841.906 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M156.788 811.281 L175.144 811.281 L175.144 815.217 L161.07 815.217 L161.07 823.689 Q162.089 823.342 163.107 823.179 Q164.126 822.994 165.144 822.994 Q170.931 822.994 174.311 826.166 Q177.691 829.337 177.691 834.754 Q177.691 840.332 174.218 843.434 Q170.746 846.513 164.427 846.513 Q162.251 846.513 159.982 846.142 Q157.737 845.772 155.33 845.031 L155.33 840.332 Q157.413 841.466 159.635 842.022 Q161.857 842.578 164.334 842.578 Q168.339 842.578 170.677 840.471 Q173.015 838.365 173.015 834.754 Q173.015 831.142 170.677 829.036 Q168.339 826.929 164.334 826.929 Q162.459 826.929 160.584 827.346 Q158.732 827.763 156.788 828.642 L156.788 811.281 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M75.9319 567.498 Q72.3208 567.498 70.4921 571.063 Q68.6865 574.605 68.6865 581.734 Q68.6865 588.841 70.4921 592.406 Q72.3208 595.947 75.9319 595.947 Q79.5661 595.947 81.3717 592.406 Q83.2004 588.841 83.2004 581.734 Q83.2004 574.605 81.3717 571.063 Q79.5661 567.498 75.9319 567.498 M75.9319 563.795 Q81.742 563.795 84.7976 568.401 Q87.8763 572.984 87.8763 581.734 Q87.8763 590.461 84.7976 595.068 Q81.742 599.651 75.9319 599.651 Q70.1217 599.651 67.043 595.068 Q63.9875 590.461 63.9875 581.734 Q63.9875 572.984 67.043 568.401 Q70.1217 563.795 75.9319 563.795 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M92.9457 593.1 L97.8299 593.1 L97.8299 598.98 L92.9457 598.98 L92.9457 593.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M103.038 598.262 L103.038 594.003 Q104.797 594.836 106.603 595.276 Q108.409 595.716 110.145 595.716 Q114.774 595.716 117.205 592.614 Q119.658 589.489 120.006 583.146 Q118.663 585.137 116.603 586.202 Q114.543 587.267 112.043 587.267 Q106.858 587.267 103.825 584.142 Q100.816 580.994 100.816 575.554 Q100.816 570.23 103.964 567.012 Q107.112 563.795 112.344 563.795 Q118.339 563.795 121.487 568.401 Q124.658 572.984 124.658 581.734 Q124.658 589.906 120.77 594.79 Q116.904 599.651 110.353 599.651 Q108.594 599.651 106.788 599.304 Q104.983 598.957 103.038 598.262 M112.344 583.609 Q115.492 583.609 117.321 581.457 Q119.172 579.304 119.172 575.554 Q119.172 571.827 117.321 569.674 Q115.492 567.498 112.344 567.498 Q109.196 567.498 107.344 569.674 Q105.515 571.827 105.515 575.554 Q105.515 579.304 107.344 581.457 Q109.196 583.609 112.344 583.609 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M129.774 564.42 L148.131 564.42 L148.131 568.355 L134.057 568.355 L134.057 576.827 Q135.075 576.48 136.094 576.318 Q137.112 576.133 138.131 576.133 Q143.918 576.133 147.297 579.304 Q150.677 582.475 150.677 587.892 Q150.677 593.47 147.205 596.572 Q143.732 599.651 137.413 599.651 Q135.237 599.651 132.969 599.281 Q130.723 598.91 128.316 598.17 L128.316 593.47 Q130.399 594.605 132.621 595.16 Q134.844 595.716 137.32 595.716 Q141.325 595.716 143.663 593.609 Q146.001 591.503 146.001 587.892 Q146.001 584.281 143.663 582.174 Q141.325 580.068 137.32 580.068 Q135.445 580.068 133.57 580.484 Q131.719 580.901 129.774 581.781 L129.774 564.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M165.746 567.498 Q162.135 567.498 160.306 571.063 Q158.501 574.605 158.501 581.734 Q158.501 588.841 160.306 592.406 Q162.135 595.947 165.746 595.947 Q169.38 595.947 171.186 592.406 Q173.015 588.841 173.015 581.734 Q173.015 574.605 171.186 571.063 Q169.38 567.498 165.746 567.498 M165.746 563.795 Q171.556 563.795 174.612 568.401 Q177.691 572.984 177.691 581.734 Q177.691 590.461 174.612 595.068 Q171.556 599.651 165.746 599.651 Q159.936 599.651 156.857 595.068 Q153.802 590.461 153.802 581.734 Q153.802 572.984 156.857 568.401 Q159.936 563.795 165.746 563.795 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M76.8346 320.637 Q73.2236 320.637 71.3949 324.201 Q69.5893 327.743 69.5893 334.873 Q69.5893 341.979 71.3949 345.544 Q73.2236 349.086 76.8346 349.086 Q80.4689 349.086 82.2744 345.544 Q84.1031 341.979 84.1031 334.873 Q84.1031 327.743 82.2744 324.201 Q80.4689 320.637 76.8346 320.637 M76.8346 316.933 Q82.6448 316.933 85.7003 321.539 Q88.779 326.123 88.779 334.873 Q88.779 343.6 85.7003 348.206 Q82.6448 352.789 76.8346 352.789 Q71.0245 352.789 67.9458 348.206 Q64.8903 343.6 64.8903 334.873 Q64.8903 326.123 67.9458 321.539 Q71.0245 316.933 76.8346 316.933 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M93.8484 346.238 L98.7327 346.238 L98.7327 352.118 L93.8484 352.118 L93.8484 346.238 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M103.941 351.4 L103.941 347.141 Q105.7 347.974 107.506 348.414 Q109.311 348.854 111.047 348.854 Q115.677 348.854 118.108 345.752 Q120.561 342.627 120.908 336.285 Q119.566 338.275 117.506 339.34 Q115.446 340.405 112.946 340.405 Q107.76 340.405 104.728 337.28 Q101.719 334.132 101.719 328.692 Q101.719 323.368 104.867 320.151 Q108.015 316.933 113.246 316.933 Q119.242 316.933 122.39 321.539 Q125.561 326.123 125.561 334.873 Q125.561 343.044 121.672 347.928 Q117.807 352.789 111.256 352.789 Q109.497 352.789 107.691 352.442 Q105.885 352.095 103.941 351.4 M113.246 336.748 Q116.395 336.748 118.223 334.595 Q120.075 332.442 120.075 328.692 Q120.075 324.965 118.223 322.813 Q116.395 320.637 113.246 320.637 Q110.098 320.637 108.247 322.813 Q106.418 324.965 106.418 328.692 Q106.418 332.442 108.247 334.595 Q110.098 336.748 113.246 336.748 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M129.45 317.558 L151.672 317.558 L151.672 319.549 L139.126 352.118 L134.242 352.118 L146.047 321.493 L129.45 321.493 L129.45 317.558 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M156.788 317.558 L175.144 317.558 L175.144 321.493 L161.07 321.493 L161.07 329.965 Q162.089 329.618 163.107 329.456 Q164.126 329.271 165.144 329.271 Q170.931 329.271 174.311 332.442 Q177.691 335.613 177.691 341.03 Q177.691 346.609 174.218 349.711 Q170.746 352.789 164.427 352.789 Q162.251 352.789 159.982 352.419 Q157.737 352.049 155.33 351.308 L155.33 346.609 Q157.413 347.743 159.635 348.299 Q161.857 348.854 164.334 348.854 Q168.339 348.854 170.677 346.748 Q173.015 344.641 173.015 341.03 Q173.015 337.419 170.677 335.313 Q168.339 333.206 164.334 333.206 Q162.459 333.206 160.584 333.623 Q158.732 334.039 156.788 334.919 L156.788 317.558 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M66.7884 101.321 L74.4272 101.321 L74.4272 74.9555 L66.1171 76.6222 L66.1171 72.3629 L74.381 70.6963 L79.0569 70.6963 L79.0569 101.321 L86.6957 101.321 L86.6957 105.256 L66.7884 105.256 L66.7884 101.321 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M91.7651 99.3767 L96.6494 99.3767 L96.6494 105.256 L91.7651 105.256 L91.7651 99.3767 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M111.719 73.775 Q108.108 73.775 106.279 77.3398 Q104.473 80.8814 104.473 88.011 Q104.473 95.1174 106.279 98.6822 Q108.108 102.224 111.719 102.224 Q115.353 102.224 117.159 98.6822 Q118.987 95.1174 118.987 88.011 Q118.987 80.8814 117.159 77.3398 Q115.353 73.775 111.719 73.775 M111.719 70.0713 Q117.529 70.0713 120.584 74.6777 Q123.663 79.261 123.663 88.011 Q123.663 96.7378 120.584 101.344 Q117.529 105.928 111.719 105.928 Q105.909 105.928 102.83 101.344 Q99.7743 96.7378 99.7743 88.011 Q99.7743 79.261 102.83 74.6777 Q105.909 70.0713 111.719 70.0713 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M138.732 73.775 Q135.121 73.775 133.293 77.3398 Q131.487 80.8814 131.487 88.011 Q131.487 95.1174 133.293 98.6822 Q135.121 102.224 138.732 102.224 Q142.367 102.224 144.172 98.6822 Q146.001 95.1174 146.001 88.011 Q146.001 80.8814 144.172 77.3398 Q142.367 73.775 138.732 73.775 M138.732 70.0713 Q144.543 70.0713 147.598 74.6777 Q150.677 79.261 150.677 88.011 Q150.677 96.7378 147.598 101.344 Q144.543 105.928 138.732 105.928 Q132.922 105.928 129.844 101.344 Q126.788 96.7378 126.788 88.011 Q126.788 79.261 129.844 74.6777 Q132.922 70.0713 138.732 70.0713 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M165.746 73.775 Q162.135 73.775 160.306 77.3398 Q158.501 80.8814 158.501 88.011 Q158.501 95.1174 160.306 98.6822 Q162.135 102.224 165.746 102.224 Q169.38 102.224 171.186 98.6822 Q173.015 95.1174 173.015 88.011 Q173.015 80.8814 171.186 77.3398 Q169.38 73.775 165.746 73.775 M165.746 70.0713 Q171.556 70.0713 174.612 74.6777 Q177.691 79.261 177.691 88.011 Q177.691 96.7378 174.612 101.344 Q171.556 105.928 165.746 105.928 Q159.936 105.928 156.857 101.344 Q153.802 96.7378 153.802 88.011 Q153.802 79.261 156.857 74.6777 Q159.936 70.0713 165.746 70.0713 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip682)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  262.57,87.9763 269.158,87.9763 275.747,87.9763 282.336,87.9763 288.924,87.9763 295.513,87.9763 302.102,87.9763 308.69,87.9763 315.279,87.9763 321.868,87.9763 \n",
       "  328.456,87.9763 335.045,87.9763 341.634,87.9763 348.222,87.9763 354.811,87.9763 361.4,87.9763 367.988,87.9763 374.577,87.9763 381.166,87.9763 387.754,87.9763 \n",
       "  394.343,87.9763 400.932,87.9763 407.52,87.9763 414.109,87.9763 420.698,87.9763 427.286,87.9763 433.875,87.9763 440.464,87.9763 447.052,87.9763 453.641,87.9763 \n",
       "  460.23,87.9763 466.818,87.9763 473.407,87.9763 479.996,87.9763 486.584,87.9763 493.173,87.9763 499.762,87.9763 506.35,87.9763 512.939,87.9763 519.527,87.9763 \n",
       "  526.116,87.9763 532.705,87.9763 539.293,87.9763 545.882,87.9763 552.471,87.9763 559.059,87.9763 565.648,87.9763 572.237,87.9763 578.825,87.9763 585.414,87.9763 \n",
       "  592.003,87.9763 598.591,87.9763 605.18,87.9763 611.769,87.9763 618.357,87.9763 624.946,87.9763 631.535,87.9763 638.123,87.9763 644.712,87.9763 651.301,87.9763 \n",
       "  657.889,87.9763 664.478,87.9763 671.067,87.9763 677.655,87.9763 684.244,87.9763 690.833,87.9763 697.421,87.9763 704.01,87.9763 710.599,87.9763 717.187,87.9763 \n",
       "  723.776,87.9763 730.365,87.9763 736.953,87.9763 743.542,87.9763 750.131,87.9763 756.719,87.9763 763.308,87.9763 769.897,87.9763 776.485,87.9763 783.074,87.9763 \n",
       "  789.662,87.9763 796.251,87.9763 802.84,87.9763 809.428,87.9763 816.017,87.9763 822.606,87.9763 829.194,87.9763 835.783,87.9763 842.372,87.9763 848.96,87.9763 \n",
       "  855.549,87.9763 862.138,87.9763 868.726,87.9763 875.315,87.9763 881.904,87.9763 888.492,87.9763 895.081,87.9763 901.67,87.9763 908.258,87.9763 914.847,87.9763 \n",
       "  921.436,87.9763 928.024,87.9763 934.613,87.9763 941.202,87.9763 947.79,87.9763 954.379,87.9763 960.968,87.9763 967.556,87.9763 974.145,87.9763 980.734,87.9763 \n",
       "  987.322,87.9763 993.911,87.9763 1000.5,87.9763 1007.09,87.9763 1013.68,87.9763 1020.27,87.9763 1026.85,87.9763 1033.44,87.9763 1040.03,87.9763 1046.62,87.9763 \n",
       "  1053.21,87.9763 1059.8,87.9763 1066.39,87.9763 1072.97,87.9763 1079.56,87.9763 1086.15,87.9763 1092.74,87.9763 1099.33,87.9763 1105.92,87.9763 1112.51,87.9763 \n",
       "  1119.1,87.9763 1125.68,87.9763 1132.27,87.9763 1138.86,157.029 1145.45,156.549 1152.04,156.076 1158.63,155.61 1165.22,155.15 1171.8,154.696 1178.39,154.248 \n",
       "  1184.98,153.806 1191.57,153.37 1198.16,152.94 1204.75,152.515 1211.34,152.096 1217.93,151.683 1224.51,214.572 1231.1,213.766 1237.69,212.97 1244.28,212.183 \n",
       "  1250.87,211.407 1257.46,210.64 1264.05,209.883 1270.63,209.135 1277.22,208.397 1283.81,207.667 1290.4,206.946 1296.99,206.233 1303.58,205.529 1310.17,204.834 \n",
       "  1316.76,204.146 1323.34,203.467 1329.93,202.796 1336.52,202.132 1343.11,201.476 1349.7,200.827 1356.29,200.186 1362.88,199.552 1369.46,198.925 1376.05,198.306 \n",
       "  1382.64,197.693 1389.23,197.086 1395.82,196.487 1402.41,195.894 1409,195.307 1415.59,194.727 1422.17,194.153 1428.76,193.586 1435.35,245.548 1441.94,244.714 \n",
       "  1448.53,399.802 1455.12,398.169 1461.71,396.553 1468.29,394.955 1474.88,393.372 1481.47,341.168 1488.06,339.876 1494.65,338.597 1501.24,337.332 1507.83,336.078 \n",
       "  1514.41,334.838 1521,333.61 1527.59,332.394 1534.18,331.19 1540.77,329.998 1547.36,328.817 1553.95,327.648 1560.54,326.49 1567.12,325.343 1573.71,324.208 \n",
       "  1580.3,323.083 1586.89,321.968 1593.48,320.865 1600.07,319.771 1606.66,318.688 1613.24,317.615 1619.83,316.552 1626.42,315.499 1633.01,359.751 1639.6,358.51 \n",
       "  1646.19,357.28 1652.78,356.061 1659.37,354.854 1665.95,353.657 1672.54,352.471 1679.13,351.295 1685.72,350.13 1692.31,348.975 1698.9,347.831 1705.49,346.696 \n",
       "  1712.07,345.571 1718.66,344.456 1725.25,343.35 1731.84,342.254 1738.43,341.168 1745.02,340.09 1751.61,339.022 1758.2,421.292 1764.78,419.891 1771.37,418.502 \n",
       "  1777.96,417.125 1784.55,415.759 1791.14,414.405 1797.73,413.062 1804.32,411.729 1810.9,410.408 1817.49,409.097 1824.08,407.797 1830.67,406.508 1837.26,405.228 \n",
       "  1843.85,403.959 1850.44,402.7 1857.03,362.267 1863.61,361.183 1870.2,360.107 1876.79,359.04 1883.38,357.981 1889.97,356.931 1896.56,355.888 1903.15,354.854 \n",
       "  1909.73,353.827 1916.32,352.809 1922.91,351.798 1929.5,350.795 1936.09,349.799 1942.68,348.811 1949.27,347.831 1955.86,346.857 1962.44,345.891 1969.03,344.933 \n",
       "  1975.62,343.981 1982.21,343.036 1988.8,378.402 1995.39,341.168 2001.98,340.244 2008.56,339.326 2015.15,409.97 2021.74,408.807 2028.33,407.653 2034.92,406.508 \n",
       "  2041.51,299.572 2048.1,404.24 2054.68,403.119 2061.27,402.005 2067.86,400.9 2074.45,399.802 2081.04,398.711 2087.63,294.411 2094.22,396.553 2100.81,395.486 \n",
       "  2107.39,394.425 2113.98,393.372 2120.57,392.326 2127.16,391.288 2133.75,390.256 2140.34,389.231 2146.93,388.213 2153.51,387.203 2160.1,386.198 2166.69,385.201 \n",
       "  2173.28,384.21 2179.87,383.226 2186.46,382.249 2193.05,381.277 2199.64,282.867 2206.22,282.228 2212.81,281.593 2219.4,280.963 2225.99,376.516 2232.58,375.582 \n",
       "  2239.17,374.654 2245.76,373.733 2252.34,372.817 2258.93,371.907 2265.52,371.002 2272.11,370.104 2278.7,369.211 2285.29,368.324 2291.88,367.442 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip682)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  262.57,828.561 269.158,828.561 275.747,828.561 282.336,828.561 288.924,1075.42 295.513,1075.42 302.102,1075.42 308.69,1075.42 315.279,1075.42 321.868,1075.42 \n",
       "  328.456,1322.28 335.045,1322.28 341.634,1322.28 348.222,1322.28 354.811,1322.28 361.4,1445.72 367.988,1445.72 374.577,1445.72 381.166,1445.72 387.754,1445.72 \n",
       "  394.343,1445.72 400.932,1322.28 407.52,1322.28 414.109,1322.28 420.698,1322.28 427.286,1322.28 433.875,1322.28 440.464,1322.28 447.052,1322.28 453.641,951.992 \n",
       "  460.23,951.992 466.818,951.992 473.407,951.992 479.996,951.992 486.584,951.992 493.173,951.992 499.762,951.992 506.35,951.992 512.939,951.992 519.527,951.992 \n",
       "  526.116,951.992 532.705,951.992 539.293,951.992 545.882,951.992 552.471,951.992 559.059,951.992 565.648,951.992 572.237,828.561 578.825,828.561 585.414,828.561 \n",
       "  592.003,828.561 598.591,828.561 605.18,828.561 611.769,828.561 618.357,828.561 624.946,828.561 631.535,828.561 638.123,828.561 644.712,828.561 651.301,828.561 \n",
       "  657.889,828.561 664.478,828.561 671.067,828.561 677.655,828.561 684.244,828.561 690.833,828.561 697.421,828.561 704.01,828.561 710.599,828.561 717.187,828.561 \n",
       "  723.776,828.561 730.365,828.561 736.953,828.561 743.542,1075.42 750.131,1075.42 756.719,1075.42 763.308,1075.42 769.897,1075.42 776.485,1075.42 783.074,1075.42 \n",
       "  789.662,1075.42 796.251,1075.42 802.84,1075.42 809.428,1075.42 816.017,1075.42 822.606,951.992 829.194,951.992 835.783,951.992 842.372,951.992 848.96,951.992 \n",
       "  855.549,951.992 862.138,951.992 868.726,951.992 875.315,951.992 881.904,951.992 888.492,951.992 895.081,951.992 901.67,951.992 908.258,951.992 914.847,951.992 \n",
       "  921.436,951.992 928.024,951.992 934.613,951.992 941.202,951.992 947.79,951.992 954.379,951.992 960.968,951.992 967.556,951.992 974.145,951.992 980.734,951.992 \n",
       "  987.322,951.992 993.911,951.992 1000.5,951.992 1007.09,1075.42 1013.68,1075.42 1020.27,1075.42 1026.85,1075.42 1033.44,1075.42 1040.03,1075.42 1046.62,1075.42 \n",
       "  1053.21,1075.42 1059.8,1075.42 1066.39,1075.42 1072.97,1075.42 1079.56,1075.42 1086.15,1075.42 1092.74,1075.42 1099.33,1075.42 1105.92,1075.42 1112.51,1075.42 \n",
       "  1119.1,1075.42 1125.68,1075.42 1132.27,1075.42 1138.86,828.561 1145.45,828.561 1152.04,828.561 1158.63,828.561 1165.22,828.561 1171.8,828.561 1178.39,828.561 \n",
       "  1184.98,828.561 1191.57,828.561 1198.16,828.561 1204.75,828.561 1211.34,828.561 1217.93,828.561 1224.51,828.561 1231.1,828.561 1237.69,828.561 1244.28,828.561 \n",
       "  1250.87,828.561 1257.46,828.561 1264.05,828.561 1270.63,828.561 1277.22,828.561 1283.81,828.561 1290.4,828.561 1296.99,828.561 1303.58,828.561 1310.17,828.561 \n",
       "  1316.76,828.561 1323.34,828.561 1329.93,828.561 1336.52,828.561 1343.11,828.561 1349.7,828.561 1356.29,828.561 1362.88,828.561 1369.46,828.561 1376.05,828.561 \n",
       "  1382.64,828.561 1389.23,828.561 1395.82,828.561 1402.41,828.561 1409,828.561 1415.59,828.561 1422.17,828.561 1428.76,828.561 1435.35,828.561 1441.94,828.561 \n",
       "  1448.53,828.561 1455.12,828.561 1461.71,828.561 1468.29,828.561 1474.88,828.561 1481.47,828.561 1488.06,828.561 1494.65,828.561 1501.24,705.131 1507.83,705.131 \n",
       "  1514.41,705.131 1521,705.131 1527.59,705.131 1534.18,705.131 1540.77,705.131 1547.36,705.131 1553.95,828.561 1560.54,828.561 1567.12,828.561 1573.71,828.561 \n",
       "  1580.3,828.561 1586.89,828.561 1593.48,828.561 1600.07,828.561 1606.66,828.561 1613.24,828.561 1619.83,828.561 1626.42,828.561 1633.01,581.7 1639.6,581.7 \n",
       "  1646.19,581.7 1652.78,581.7 1659.37,581.7 1665.95,581.7 1672.54,581.7 1679.13,581.7 1685.72,581.7 1692.31,581.7 1698.9,581.7 1705.49,581.7 \n",
       "  1712.07,828.561 1718.66,828.561 1725.25,828.561 1731.84,828.561 1738.43,828.561 1745.02,828.561 1751.61,828.561 1758.2,828.561 1764.78,828.561 1771.37,828.561 \n",
       "  1777.96,828.561 1784.55,828.561 1791.14,828.561 1797.73,828.561 1804.32,828.561 1810.9,951.992 1817.49,951.992 1824.08,951.992 1830.67,951.992 1837.26,951.992 \n",
       "  1843.85,951.992 1850.44,951.992 1857.03,951.992 1863.61,951.992 1870.2,951.992 1876.79,951.992 1883.38,951.992 1889.97,951.992 1896.56,828.561 1903.15,828.561 \n",
       "  1909.73,828.561 1916.32,828.561 1922.91,828.561 1929.5,828.561 1936.09,828.561 1942.68,828.561 1949.27,828.561 1955.86,828.561 1962.44,828.561 1969.03,828.561 \n",
       "  1975.62,828.561 1982.21,828.561 1988.8,828.561 1995.39,828.561 2001.98,828.561 2008.56,828.561 2015.15,828.561 2021.74,828.561 2028.33,828.561 2034.92,828.561 \n",
       "  2041.51,951.992 2048.1,828.561 2054.68,828.561 2061.27,828.561 2067.86,828.561 2074.45,828.561 2081.04,828.561 2087.63,951.992 2094.22,828.561 2100.81,828.561 \n",
       "  2107.39,828.561 2113.98,828.561 2120.57,828.561 2127.16,828.561 2133.75,828.561 2140.34,828.561 2146.93,828.561 2153.51,828.561 2160.1,828.561 2166.69,828.561 \n",
       "  2173.28,828.561 2179.87,828.561 2186.46,828.561 2193.05,828.561 2199.64,951.992 2206.22,705.131 2212.81,705.131 2219.4,705.131 2225.99,581.7 2232.58,581.7 \n",
       "  2239.17,705.131 2245.76,581.7 2252.34,581.7 2258.93,581.7 2265.52,581.7 2272.11,581.7 2278.7,581.7 2285.29,581.7 2291.88,581.7 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip680)\" d=\"\n",
       "M1991.03 276.658 L2281.05 276.658 L2281.05 95.2176 L1991.03 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1991.03,276.658 2281.05,276.658 2281.05,95.2176 1991.03,95.2176 1991.03,276.658 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip680)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2014.93,155.698 2158.33,155.698 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip680)\" d=\"M 0 0 M2196.08 175.385 Q2194.27 180.015 2192.56 181.427 Q2190.85 182.839 2187.97 182.839 L2184.57 182.839 L2184.57 179.274 L2187.07 179.274 Q2188.83 179.274 2189.8 178.44 Q2190.78 177.607 2191.96 174.505 L2192.72 172.561 L2182.23 147.052 L2186.75 147.052 L2194.85 167.329 L2202.95 147.052 L2207.47 147.052 L2196.08 175.385 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M2213.34 169.042 L2220.98 169.042 L2220.98 142.677 L2212.67 144.343 L2212.67 140.084 L2220.94 138.418 L2225.61 138.418 L2225.61 169.042 L2233.25 169.042 L2233.25 172.978 L2213.34 172.978 L2213.34 169.042 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip680)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2014.93,216.178 2158.33,216.178 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip680)\" d=\"M 0 0 M2196.08 235.865 Q2194.27 240.495 2192.56 241.907 Q2190.85 243.319 2187.97 243.319 L2184.57 243.319 L2184.57 239.754 L2187.07 239.754 Q2188.83 239.754 2189.8 238.92 Q2190.78 238.087 2191.96 234.985 L2192.72 233.041 L2182.23 207.532 L2186.75 207.532 L2194.85 227.809 L2202.95 207.532 L2207.47 207.532 L2196.08 235.865 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip680)\" d=\"M 0 0 M2216.56 229.522 L2232.88 229.522 L2232.88 233.458 L2210.94 233.458 L2210.94 229.522 Q2213.6 226.768 2218.18 222.138 Q2222.79 217.485 2223.97 216.143 Q2226.22 213.62 2227.09 211.884 Q2228 210.124 2228 208.435 Q2228 205.68 2226.05 203.944 Q2224.13 202.208 2221.03 202.208 Q2218.83 202.208 2216.38 202.972 Q2213.95 203.735 2211.17 205.286 L2211.17 200.564 Q2213.99 199.43 2216.45 198.851 Q2218.9 198.273 2220.94 198.273 Q2226.31 198.273 2229.5 200.958 Q2232.7 203.643 2232.7 208.134 Q2232.7 210.263 2231.89 212.185 Q2231.1 214.083 2228.99 216.675 Q2228.41 217.347 2225.31 220.564 Q2222.21 223.759 2216.56 229.522 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(data_schedule, training_losses)\n",
    "plot!(data_schedule, valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (10, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (11, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (12, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (13, 1.0, 0.925)\n",
      "(d, train_metric, valid_metric) = (14, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (15, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (16, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (17, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (18, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (19, 1.0, 0.9)\n",
      "(d, train_metric, valid_metric) = (20, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (21, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (22, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (23, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (24, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (25, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (26, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (27, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (28, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (29, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (30, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (31, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (32, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (33, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (34, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (35, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (36, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (37, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (38, 1.0, 0.875)\n",
      "(d, train_metric, valid_metric) = (39, 0.9743589743589743, 0.9)\n",
      "(d, train_metric, valid_metric) = (40, 0.975, 0.9)\n",
      "(d, train_metric, valid_metric) = (41, 0.975609756097561, 0.9)\n",
      "(d, train_metric, valid_metric) = (42, 0.9761904761904762, 0.925)\n",
      "(d, train_metric, valid_metric) = (43, 0.9767441860465116, 0.925)\n",
      "(d, train_metric, valid_metric) = (44, 0.9772727272727273, 0.925)\n",
      "(d, train_metric, valid_metric) = (45, 0.9777777777777777, 0.925)\n",
      "(d, train_metric, valid_metric) = (46, 0.9782608695652174, 0.925)\n",
      "(d, train_metric, valid_metric) = (47, 0.9787234042553191, 0.925)\n",
      "(d, train_metric, valid_metric) = (48, 0.9791666666666666, 0.925)\n",
      "(d, train_metric, valid_metric) = (49, 0.9795918367346939, 0.925)\n",
      "(d, train_metric, valid_metric) = (50, 0.98, 0.925)\n",
      "(d, train_metric, valid_metric) = (51, 0.9803921568627451, 0.925)\n",
      "(d, train_metric, valid_metric) = (52, 0.9807692307692307, 0.925)\n",
      "(d, train_metric, valid_metric) = (53, 0.9811320754716981, 0.925)\n",
      "(d, train_metric, valid_metric) = (54, 0.9814814814814815, 0.925)\n",
      "(d, train_metric, valid_metric) = (55, 0.9818181818181818, 0.925)\n",
      "(d, train_metric, valid_metric) = (56, 0.9821428571428571, 0.925)\n",
      "(d, train_metric, valid_metric) = (57, 0.9824561403508771, 0.9375)\n",
      "(d, train_metric, valid_metric) = (58, 0.9827586206896551, 0.9375)\n",
      "(d, train_metric, valid_metric) = (59, 0.9830508474576272, 0.9375)\n",
      "(d, train_metric, valid_metric) = (60, 0.9833333333333333, 0.9375)\n",
      "(d, train_metric, valid_metric) = (61, 0.9836065573770492, 0.9375)\n",
      "(d, train_metric, valid_metric) = (62, 0.9838709677419355, 0.9375)\n",
      "(d, train_metric, valid_metric) = (63, 0.9841269841269842, 0.9375)\n",
      "(d, train_metric, valid_metric) = (64, 0.984375, 0.9375)\n",
      "(d, train_metric, valid_metric) = (65, 0.9846153846153847, 0.9375)\n",
      "(d, train_metric, valid_metric) = (66, 0.9848484848484849, 0.9375)\n",
      "(d, train_metric, valid_metric) = (67, 0.9850746268656716, 0.9375)\n",
      "(d, train_metric, valid_metric) = (68, 0.9852941176470589, 0.9375)\n",
      "(d, train_metric, valid_metric) = (69, 0.9855072463768116, 0.9375)\n",
      "(d, train_metric, valid_metric) = (70, 0.9857142857142858, 0.9375)\n",
      "(d, train_metric, valid_metric) = (71, 0.9859154929577465, 0.9375)\n",
      "(d, train_metric, valid_metric) = (72, 0.9861111111111112, 0.9375)\n",
      "(d, train_metric, valid_metric) = (73, 0.9863013698630136, 0.9375)\n",
      "(d, train_metric, valid_metric) = (74, 0.9864864864864865, 0.9375)\n",
      "(d, train_metric, valid_metric) = (75, 0.9866666666666667, 0.9375)\n",
      "(d, train_metric, valid_metric) = (76, 0.9868421052631579, 0.9375)\n",
      "(d, train_metric, valid_metric) = (77, 0.987012987012987, 0.9375)\n",
      "(d, train_metric, valid_metric) = (78, 0.9871794871794872, 0.9375)\n",
      "(d, train_metric, valid_metric) = (79, 0.9873417721518988, 0.9375)\n",
      "(d, train_metric, valid_metric) = (80, 0.9875, 0.9375)\n",
      "(d, train_metric, valid_metric) = (81, 0.9876543209876543, 0.9375)\n",
      "(d, train_metric, valid_metric) = (82, 0.9878048780487805, 0.9375)\n",
      "(d, train_metric, valid_metric) = (83, 0.9879518072289156, 0.925)\n",
      "(d, train_metric, valid_metric) = (84, 0.9880952380952381, 0.925)\n",
      "(d, train_metric, valid_metric) = (85, 0.9882352941176471, 0.925)\n",
      "(d, train_metric, valid_metric) = (86, 0.9883720930232558, 0.925)\n",
      "(d, train_metric, valid_metric) = (87, 0.9770114942528736, 0.95)\n",
      "(d, train_metric, valid_metric) = (88, 0.9772727272727273, 0.95)\n",
      "(d, train_metric, valid_metric) = (89, 0.9775280898876404, 0.95)\n",
      "(d, train_metric, valid_metric) = (90, 0.9777777777777777, 0.95)\n",
      "(d, train_metric, valid_metric) = (91, 0.978021978021978, 0.95)\n",
      "(d, train_metric, valid_metric) = (92, 0.9782608695652174, 0.95)\n",
      "(d, train_metric, valid_metric) = (93, 0.978494623655914, 0.95)\n",
      "(d, train_metric, valid_metric) = (94, 0.9787234042553191, 0.95)\n",
      "(d, train_metric, valid_metric) = (95, 0.9894736842105263, 0.9625)\n",
      "(d, train_metric, valid_metric) = (96, 0.9895833333333334, 0.9625)\n",
      "(d, train_metric, valid_metric) = (97, 0.9896907216494846, 0.9625)\n",
      "(d, train_metric, valid_metric) = (98, 0.9897959183673469, 0.95)\n",
      "(d, train_metric, valid_metric) = (99, 0.98989898989899, 0.95)\n",
      "(d, train_metric, valid_metric) = (100, 0.99, 0.95)\n",
      "(d, train_metric, valid_metric) = (101, 0.9900990099009901, 0.95)\n",
      "(d, train_metric, valid_metric) = (102, 0.9901960784313726, 0.95)\n",
      "(d, train_metric, valid_metric) = (103, 0.9902912621359223, 0.95)\n",
      "(d, train_metric, valid_metric) = (104, 0.9903846153846154, 0.95)\n",
      "(d, train_metric, valid_metric) = (105, 0.9904761904761905, 0.95)\n",
      "(d, train_metric, valid_metric) = (106, 0.9905660377358491, 0.95)\n",
      "(d, train_metric, valid_metric) = (107, 0.9906542056074766, 0.95)\n",
      "(d, train_metric, valid_metric) = (108, 0.9907407407407407, 0.95)\n",
      "(d, train_metric, valid_metric) = (109, 0.9908256880733946, 0.95)\n",
      "(d, train_metric, valid_metric) = (110, 0.990909090909091, 0.95)\n",
      "(d, train_metric, valid_metric) = (111, 0.990990990990991, 0.95)\n",
      "(d, train_metric, valid_metric) = (112, 0.9910714285714286, 0.95)\n",
      "(d, train_metric, valid_metric) = (113, 0.9911504424778761, 0.95)\n",
      "(d, train_metric, valid_metric) = (114, 0.9912280701754386, 0.95)\n",
      "(d, train_metric, valid_metric) = (115, 0.9826086956521739, 0.95)\n",
      "(d, train_metric, valid_metric) = (116, 0.9827586206896551, 0.95)\n",
      "(d, train_metric, valid_metric) = (117, 0.9829059829059829, 0.95)\n",
      "(d, train_metric, valid_metric) = (118, 0.9830508474576272, 0.95)\n",
      "(d, train_metric, valid_metric) = (119, 0.9831932773109243, 0.95)\n",
      "(d, train_metric, valid_metric) = (120, 0.9833333333333333, 0.95)\n",
      "(d, train_metric, valid_metric) = (121, 0.9834710743801653, 0.95)\n",
      "(d, train_metric, valid_metric) = (122, 0.9836065573770492, 0.95)\n",
      "(d, train_metric, valid_metric) = (123, 0.991869918699187, 0.925)\n",
      "(d, train_metric, valid_metric) = (124, 0.9919354838709677, 0.925)\n",
      "(d, train_metric, valid_metric) = (125, 0.984, 0.9375)\n",
      "(d, train_metric, valid_metric) = (126, 0.9841269841269842, 0.9375)\n",
      "(d, train_metric, valid_metric) = (127, 0.984251968503937, 0.9375)\n",
      "(d, train_metric, valid_metric) = (128, 0.984375, 0.9375)\n",
      "(d, train_metric, valid_metric) = (129, 0.9844961240310077, 0.9375)\n",
      "(d, train_metric, valid_metric) = (130, 0.9846153846153847, 0.9375)\n",
      "(d, train_metric, valid_metric) = (131, 0.9847328244274809, 0.9375)\n",
      "(d, train_metric, valid_metric) = (132, 0.9848484848484849, 0.9375)\n",
      "(d, train_metric, valid_metric) = (133, 0.9849624060150376, 0.9375)\n",
      "(d, train_metric, valid_metric) = (134, 0.9850746268656716, 0.925)\n",
      "(d, train_metric, valid_metric) = (135, 0.9851851851851852, 0.925)\n",
      "(d, train_metric, valid_metric) = (136, 0.9852941176470589, 0.925)\n",
      "(d, train_metric, valid_metric) = (137, 0.9854014598540146, 0.925)\n",
      "(d, train_metric, valid_metric) = (138, 0.9855072463768116, 0.925)\n",
      "(d, train_metric, valid_metric) = (139, 0.9856115107913669, 0.925)\n",
      "(d, train_metric, valid_metric) = (140, 0.9928571428571429, 0.925)\n",
      "(d, train_metric, valid_metric) = (141, 0.9929078014184397, 0.925)\n",
      "(d, train_metric, valid_metric) = (142, 0.9929577464788732, 0.925)\n",
      "(d, train_metric, valid_metric) = (143, 0.9790209790209791, 0.9375)\n",
      "(d, train_metric, valid_metric) = (144, 0.9722222222222222, 0.925)\n",
      "(d, train_metric, valid_metric) = (145, 0.9793103448275862, 0.925)\n",
      "(d, train_metric, valid_metric) = (146, 0.9794520547945206, 0.925)\n",
      "(d, train_metric, valid_metric) = (147, 0.9795918367346939, 0.925)\n",
      "(d, train_metric, valid_metric) = (148, 0.9797297297297297, 0.925)\n",
      "(d, train_metric, valid_metric) = (149, 0.9798657718120806, 0.925)\n",
      "(d, train_metric, valid_metric) = (150, 0.98, 0.95)\n",
      "(d, train_metric, valid_metric) = (151, 0.9801324503311258, 0.95)\n",
      "(d, train_metric, valid_metric) = (152, 0.9802631578947368, 0.95)\n",
      "(d, train_metric, valid_metric) = (153, 0.9673202614379085, 0.9125)\n",
      "(d, train_metric, valid_metric) = (154, 0.9675324675324676, 0.9125)\n",
      "(d, train_metric, valid_metric) = (155, 0.967741935483871, 0.9125)\n",
      "(d, train_metric, valid_metric) = (156, 0.9615384615384616, 0.925)\n",
      "(d, train_metric, valid_metric) = (157, 0.9617834394904459, 0.925)\n",
      "(d, train_metric, valid_metric) = (158, 0.9620253164556962, 0.925)\n",
      "(d, train_metric, valid_metric) = (159, 0.9622641509433962, 0.925)\n",
      "(d, train_metric, valid_metric) = (160, 0.9625, 0.925)\n",
      "(d, train_metric, valid_metric) = (161, 0.9627329192546584, 0.925)\n",
      "(d, train_metric, valid_metric) = (162, 0.962962962962963, 0.925)\n",
      "(d, train_metric, valid_metric) = (163, 0.9631901840490797, 0.925)\n",
      "(d, train_metric, valid_metric) = (164, 0.9634146341463414, 0.925)\n",
      "(d, train_metric, valid_metric) = (165, 0.9636363636363636, 0.925)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (166, 0.963855421686747, 0.925)\n",
      "(d, train_metric, valid_metric) = (167, 0.9640718562874252, 0.925)\n",
      "(d, train_metric, valid_metric) = (168, 0.9642857142857143, 0.925)\n",
      "(d, train_metric, valid_metric) = (169, 0.9644970414201184, 0.925)\n",
      "(d, train_metric, valid_metric) = (170, 0.9647058823529412, 0.925)\n",
      "(d, train_metric, valid_metric) = (171, 0.9649122807017544, 0.925)\n",
      "(d, train_metric, valid_metric) = (172, 0.9651162790697675, 0.925)\n",
      "(d, train_metric, valid_metric) = (173, 0.9653179190751445, 0.925)\n",
      "(d, train_metric, valid_metric) = (174, 0.9712643678160919, 0.9375)\n",
      "(d, train_metric, valid_metric) = (175, 0.9714285714285714, 0.9375)\n",
      "(d, train_metric, valid_metric) = (176, 0.9715909090909091, 0.9375)\n",
      "(d, train_metric, valid_metric) = (177, 0.9717514124293786, 0.9375)\n",
      "(d, train_metric, valid_metric) = (178, 0.9719101123595506, 0.9375)\n",
      "(d, train_metric, valid_metric) = (179, 0.9720670391061452, 0.9375)\n",
      "(d, train_metric, valid_metric) = (180, 0.9722222222222222, 0.9375)\n",
      "(d, train_metric, valid_metric) = (181, 0.9723756906077348, 0.9375)\n",
      "(d, train_metric, valid_metric) = (182, 0.9725274725274725, 0.9375)\n",
      "(d, train_metric, valid_metric) = (183, 0.9726775956284153, 0.9375)\n",
      "(d, train_metric, valid_metric) = (184, 0.9728260869565217, 0.9375)\n",
      "(d, train_metric, valid_metric) = (185, 0.972972972972973, 0.9375)\n",
      "(d, train_metric, valid_metric) = (186, 0.9731182795698925, 0.9375)\n",
      "(d, train_metric, valid_metric) = (187, 0.9732620320855615, 0.9375)\n",
      "(d, train_metric, valid_metric) = (188, 0.9680851063829787, 0.925)\n",
      "(d, train_metric, valid_metric) = (189, 0.9682539682539683, 0.925)\n",
      "(d, train_metric, valid_metric) = (190, 0.9631578947368421, 0.9375)\n",
      "(d, train_metric, valid_metric) = (191, 0.9633507853403142, 0.9375)\n",
      "(d, train_metric, valid_metric) = (192, 0.9635416666666666, 0.9375)\n",
      "(d, train_metric, valid_metric) = (193, 0.9637305699481865, 0.9375)\n",
      "(d, train_metric, valid_metric) = (194, 0.9639175257731959, 0.9375)\n",
      "(d, train_metric, valid_metric) = (195, 0.9641025641025641, 0.9375)\n",
      "(d, train_metric, valid_metric) = (196, 0.9642857142857143, 0.9375)\n",
      "(d, train_metric, valid_metric) = (197, 0.9644670050761421, 0.9375)\n",
      "(d, train_metric, valid_metric) = (198, 0.9646464646464646, 0.9375)\n",
      "(d, train_metric, valid_metric) = (199, 0.964824120603015, 0.9375)\n",
      "(d, train_metric, valid_metric) = (200, 0.965, 0.9375)\n",
      "(d, train_metric, valid_metric) = (201, 0.9651741293532339, 0.9375)\n",
      "(d, train_metric, valid_metric) = (202, 0.9653465346534653, 0.9375)\n",
      "(d, train_metric, valid_metric) = (203, 0.9655172413793104, 0.9375)\n",
      "(d, train_metric, valid_metric) = (204, 0.9656862745098039, 0.9375)\n",
      "(d, train_metric, valid_metric) = (205, 0.9658536585365853, 0.9375)\n",
      "(d, train_metric, valid_metric) = (206, 0.9660194174757282, 0.925)\n",
      "(d, train_metric, valid_metric) = (207, 0.966183574879227, 0.925)\n",
      "(d, train_metric, valid_metric) = (208, 0.9663461538461539, 0.925)\n",
      "(d, train_metric, valid_metric) = (209, 0.9665071770334929, 0.925)\n",
      "(d, train_metric, valid_metric) = (210, 0.9666666666666667, 0.925)\n",
      "(d, train_metric, valid_metric) = (211, 0.966824644549763, 0.925)\n",
      "(d, train_metric, valid_metric) = (212, 0.9669811320754716, 0.925)\n",
      "(d, train_metric, valid_metric) = (213, 0.9671361502347418, 0.925)\n",
      "(d, train_metric, valid_metric) = (214, 0.9672897196261683, 0.925)\n",
      "(d, train_metric, valid_metric) = (215, 0.9674418604651163, 0.925)\n",
      "(d, train_metric, valid_metric) = (216, 0.9675925925925926, 0.925)\n",
      "(d, train_metric, valid_metric) = (217, 0.967741935483871, 0.925)\n",
      "(d, train_metric, valid_metric) = (218, 0.9587155963302753, 0.925)\n",
      "(d, train_metric, valid_metric) = (219, 0.9589041095890412, 0.925)\n",
      "(d, train_metric, valid_metric) = (220, 0.9590909090909091, 0.925)\n",
      "(d, train_metric, valid_metric) = (221, 0.9592760180995475, 0.925)\n",
      "(d, train_metric, valid_metric) = (222, 0.9594594594594594, 0.925)\n",
      "(d, train_metric, valid_metric) = (223, 0.9596412556053812, 0.925)\n",
      "(d, train_metric, valid_metric) = (224, 0.9553571428571429, 0.925)\n",
      "(d, train_metric, valid_metric) = (225, 0.96, 0.9375)\n",
      "(d, train_metric, valid_metric) = (226, 0.9601769911504425, 0.9375)\n",
      "(d, train_metric, valid_metric) = (227, 0.960352422907489, 0.9375)\n",
      "(d, train_metric, valid_metric) = (228, 0.9605263157894737, 0.9375)\n",
      "(d, train_metric, valid_metric) = (229, 0.9606986899563319, 0.9375)\n",
      "(d, train_metric, valid_metric) = (230, 0.9608695652173913, 0.9375)\n",
      "(d, train_metric, valid_metric) = (231, 0.961038961038961, 0.9375)\n",
      "(d, train_metric, valid_metric) = (232, 0.9612068965517241, 0.9375)\n",
      "(d, train_metric, valid_metric) = (233, 0.9613733905579399, 0.9375)\n",
      "(d, train_metric, valid_metric) = (234, 0.9615384615384616, 0.9375)\n",
      "(d, train_metric, valid_metric) = (235, 0.9617021276595745, 0.9375)\n",
      "(d, train_metric, valid_metric) = (236, 0.961864406779661, 0.9375)\n",
      "(d, train_metric, valid_metric) = (237, 0.9746835443037974, 0.925)\n",
      "(d, train_metric, valid_metric) = (238, 0.9747899159663865, 0.925)\n",
      "(d, train_metric, valid_metric) = (239, 0.9748953974895398, 0.925)\n",
      "(d, train_metric, valid_metric) = (240, 0.975, 0.925)\n",
      "(d, train_metric, valid_metric) = (241, 0.975103734439834, 0.925)\n",
      "(d, train_metric, valid_metric) = (242, 0.9752066115702479, 0.925)\n",
      "(d, train_metric, valid_metric) = (243, 0.9753086419753086, 0.925)\n",
      "(d, train_metric, valid_metric) = (244, 0.9754098360655737, 0.925)\n",
      "(d, train_metric, valid_metric) = (245, 0.9755102040816327, 0.925)\n",
      "(d, train_metric, valid_metric) = (246, 0.975609756097561, 0.925)\n",
      "(d, train_metric, valid_metric) = (247, 0.9757085020242915, 0.925)\n",
      "(d, train_metric, valid_metric) = (248, 0.9758064516129032, 0.925)\n",
      "(d, train_metric, valid_metric) = (249, 0.9759036144578314, 0.925)\n",
      "(d, train_metric, valid_metric) = (250, 0.968, 0.9375)\n",
      "(d, train_metric, valid_metric) = (251, 0.9760956175298805, 0.925)\n",
      "(d, train_metric, valid_metric) = (252, 0.9682539682539683, 0.925)\n",
      "(d, train_metric, valid_metric) = (253, 0.9683794466403162, 0.925)\n",
      "(d, train_metric, valid_metric) = (254, 0.9685039370078741, 0.925)\n",
      "(d, train_metric, valid_metric) = (255, 0.9725490196078431, 0.925)\n",
      "(d, train_metric, valid_metric) = (256, 0.97265625, 0.925)\n",
      "(d, train_metric, valid_metric) = (257, 0.9688715953307393, 0.925)\n",
      "(d, train_metric, valid_metric) = (258, 0.9612403100775194, 0.925)\n",
      "(d, train_metric, valid_metric) = (259, 0.9652509652509652, 0.925)\n",
      "(d, train_metric, valid_metric) = (260, 0.9653846153846154, 0.925)\n",
      "(d, train_metric, valid_metric) = (261, 0.9655172413793104, 0.925)\n",
      "(d, train_metric, valid_metric) = (262, 0.9656488549618321, 0.925)\n",
      "(d, train_metric, valid_metric) = (263, 0.9695817490494296, 0.925)\n",
      "(d, train_metric, valid_metric) = (264, 0.9659090909090909, 0.925)\n",
      "(d, train_metric, valid_metric) = (265, 0.9660377358490566, 0.925)\n",
      "(d, train_metric, valid_metric) = (266, 0.9661654135338346, 0.925)\n",
      "(d, train_metric, valid_metric) = (267, 0.9662921348314607, 0.925)\n",
      "(d, train_metric, valid_metric) = (268, 0.9626865671641791, 0.925)\n",
      "(d, train_metric, valid_metric) = (269, 0.9628252788104089, 0.925)\n",
      "(d, train_metric, valid_metric) = (270, 0.9666666666666667, 0.925)\n",
      "(d, train_metric, valid_metric) = (271, 0.9704797047970479, 0.925)\n",
      "(d, train_metric, valid_metric) = (272, 0.9705882352941176, 0.925)\n",
      "(d, train_metric, valid_metric) = (273, 0.9706959706959707, 0.925)\n",
      "(d, train_metric, valid_metric) = (274, 0.9708029197080292, 0.925)\n",
      "(d, train_metric, valid_metric) = (275, 0.9672727272727273, 0.925)\n",
      "(d, train_metric, valid_metric) = (276, 0.9601449275362319, 0.925)\n",
      "(d, train_metric, valid_metric) = (277, 0.9602888086642599, 0.925)\n",
      "(d, train_metric, valid_metric) = (278, 0.960431654676259, 0.925)\n",
      "(d, train_metric, valid_metric) = (279, 0.9605734767025089, 0.925)\n",
      "(d, train_metric, valid_metric) = (280, 0.9607142857142857, 0.925)\n",
      "(d, train_metric, valid_metric) = (281, 0.9644128113879004, 0.925)\n",
      "(d, train_metric, valid_metric) = (282, 0.9609929078014184, 0.925)\n",
      "(d, train_metric, valid_metric) = (283, 0.9611307420494699, 0.925)\n",
      "(d, train_metric, valid_metric) = (284, 0.9612676056338029, 0.925)\n",
      "(d, train_metric, valid_metric) = (285, 0.9614035087719298, 0.925)\n",
      "(d, train_metric, valid_metric) = (286, 0.9615384615384616, 0.925)\n",
      "(d, train_metric, valid_metric) = (287, 0.9616724738675958, 0.925)\n",
      "(d, train_metric, valid_metric) = (288, 0.9618055555555556, 0.925)\n",
      "(d, train_metric, valid_metric) = (289, 0.9619377162629758, 0.925)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (290, 0.9620689655172414, 0.925)\n",
      "(d, train_metric, valid_metric) = (291, 0.9621993127147767, 0.925)\n",
      "(d, train_metric, valid_metric) = (292, 0.9657534246575342, 0.925)\n",
      "(d, train_metric, valid_metric) = (293, 0.962457337883959, 0.925)\n",
      "(d, train_metric, valid_metric) = (294, 0.9625850340136054, 0.925)\n",
      "(d, train_metric, valid_metric) = (295, 0.9627118644067797, 0.925)\n",
      "(d, train_metric, valid_metric) = (296, 0.9662162162162162, 0.925)\n",
      "(d, train_metric, valid_metric) = (297, 0.9663299663299664, 0.925)\n",
      "(d, train_metric, valid_metric) = (298, 0.9664429530201343, 0.925)\n",
      "(d, train_metric, valid_metric) = (299, 0.9665551839464883, 0.925)\n",
      "(d, train_metric, valid_metric) = (300, 0.9666666666666667, 0.925)\n",
      "(d, train_metric, valid_metric) = (301, 0.9667774086378738, 0.925)\n",
      "(d, train_metric, valid_metric) = (302, 0.9668874172185431, 0.925)\n",
      "(d, train_metric, valid_metric) = (303, 0.966996699669967, 0.925)\n",
      "(d, train_metric, valid_metric) = (304, 0.9671052631578947, 0.925)\n",
      "(d, train_metric, valid_metric) = (305, 0.9672131147540983, 0.925)\n",
      "(d, train_metric, valid_metric) = (306, 0.9673202614379085, 0.925)\n",
      "(d, train_metric, valid_metric) = (307, 0.9674267100977199, 0.925)\n",
      "(d, train_metric, valid_metric) = (308, 0.961038961038961, 0.925)\n",
      "(d, train_metric, valid_metric) = (309, 0.9611650485436893, 0.925)\n",
      "(d, train_metric, valid_metric) = (310, 0.9580645161290322, 0.925)\n",
      "(d, train_metric, valid_metric) = (311, 0.9614147909967845, 0.925)\n",
      "(d, train_metric, valid_metric) = (312, 0.9615384615384616, 0.925)\n",
      "(d, train_metric, valid_metric) = (313, 0.9616613418530351, 0.925)\n",
      "(d, train_metric, valid_metric) = (314, 0.9585987261146497, 0.925)\n",
      "(d, train_metric, valid_metric) = (315, 0.9619047619047619, 0.925)\n",
      "(d, train_metric, valid_metric) = (316, 0.9588607594936709, 0.925)\n",
      "(d, train_metric, valid_metric) = (317, 0.9589905362776026, 0.925)\n",
      "(d, train_metric, valid_metric) = (318, 0.9622641509433962, 0.925)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10:1:318, Any[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0    0.9611650485436893, 0.9580645161290322, 0.9614147909967845, 0.9615384615384616, 0.9616613418530351, 0.9585987261146497, 0.9619047619047619, 0.9588607594936709, 0.9589905362776026, 0.9622641509433962], Any[0.925, 0.925, 0.925, 0.925, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9    0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925, 0.925])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_schedule, training_losses, valid_losses = learn_curve(best_rbf_model, X[train,:], y[train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip720\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip720)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip721\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip720)\" d=\"\n",
       "M174.839 1486.45 L2352.76 1486.45 L2352.76 47.2441 L174.839 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip722\">\n",
       "    <rect x=\"174\" y=\"47\" width=\"2179\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  503.314,1486.45 503.314,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  836.859,1486.45 836.859,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1170.4,1486.45 1170.4,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1503.95,1486.45 1503.95,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1837.5,1486.45 1837.5,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2171.04,1486.45 2171.04,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.839,1174.17 2352.76,1174.17 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.839,848.31 2352.76,848.31 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.839,522.453 2352.76,522.453 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.839,196.595 2352.76,196.595 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.839,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.839,1486.45 174.839,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  503.314,1486.45 503.314,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  836.859,1486.45 836.859,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1170.4,1486.45 1170.4,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1503.95,1486.45 1503.95,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1837.5,1486.45 1837.5,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2171.04,1486.45 2171.04,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.839,1174.17 200.974,1174.17 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.839,848.31 200.974,848.31 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.839,522.453 200.974,522.453 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.839,196.595 200.974,196.595 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip720)\" d=\"M 0 0 M480.085 1505.36 L498.442 1505.36 L498.442 1509.3 L484.368 1509.3 L484.368 1517.77 Q485.386 1517.42 486.405 1517.26 Q487.423 1517.07 488.442 1517.07 Q494.229 1517.07 497.608 1520.24 Q500.988 1523.42 500.988 1528.83 Q500.988 1534.41 497.516 1537.51 Q494.043 1540.59 487.724 1540.59 Q485.548 1540.59 483.28 1540.22 Q481.034 1539.85 478.627 1539.11 L478.627 1534.41 Q480.71 1535.54 482.932 1536.1 Q485.155 1536.66 487.631 1536.66 Q491.636 1536.66 493.974 1534.55 Q496.312 1532.44 496.312 1528.83 Q496.312 1525.22 493.974 1523.11 Q491.636 1521.01 487.631 1521.01 Q485.756 1521.01 483.881 1521.42 Q482.03 1521.84 480.085 1522.72 L480.085 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M516.057 1508.44 Q512.446 1508.44 510.617 1512 Q508.812 1515.55 508.812 1522.67 Q508.812 1529.78 510.617 1533.35 Q512.446 1536.89 516.057 1536.89 Q519.691 1536.89 521.497 1533.35 Q523.326 1529.78 523.326 1522.67 Q523.326 1515.55 521.497 1512 Q519.691 1508.44 516.057 1508.44 M516.057 1504.73 Q521.867 1504.73 524.923 1509.34 Q528.002 1513.92 528.002 1522.67 Q528.002 1531.4 524.923 1536.01 Q521.867 1540.59 516.057 1540.59 Q510.247 1540.59 507.168 1536.01 Q504.113 1531.4 504.113 1522.67 Q504.113 1513.92 507.168 1509.34 Q510.247 1504.73 516.057 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M800.228 1535.98 L807.867 1535.98 L807.867 1509.62 L799.556 1511.29 L799.556 1507.03 L807.82 1505.36 L812.496 1505.36 L812.496 1535.98 L820.135 1535.98 L820.135 1539.92 L800.228 1539.92 L800.228 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M835.204 1508.44 Q831.593 1508.44 829.765 1512 Q827.959 1515.55 827.959 1522.67 Q827.959 1529.78 829.765 1533.35 Q831.593 1536.89 835.204 1536.89 Q838.839 1536.89 840.644 1533.35 Q842.473 1529.78 842.473 1522.67 Q842.473 1515.55 840.644 1512 Q838.839 1508.44 835.204 1508.44 M835.204 1504.73 Q841.015 1504.73 844.07 1509.34 Q847.149 1513.92 847.149 1522.67 Q847.149 1531.4 844.07 1536.01 Q841.015 1540.59 835.204 1540.59 Q829.394 1540.59 826.316 1536.01 Q823.26 1531.4 823.26 1522.67 Q823.26 1513.92 826.316 1509.34 Q829.394 1504.73 835.204 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M862.218 1508.44 Q858.607 1508.44 856.778 1512 Q854.973 1515.55 854.973 1522.67 Q854.973 1529.78 856.778 1533.35 Q858.607 1536.89 862.218 1536.89 Q865.852 1536.89 867.658 1533.35 Q869.487 1529.78 869.487 1522.67 Q869.487 1515.55 867.658 1512 Q865.852 1508.44 862.218 1508.44 M862.218 1504.73 Q868.028 1504.73 871.084 1509.34 Q874.163 1513.92 874.163 1522.67 Q874.163 1531.4 871.084 1536.01 Q868.028 1540.59 862.218 1540.59 Q856.408 1540.59 853.329 1536.01 Q850.274 1531.4 850.274 1522.67 Q850.274 1513.92 853.329 1509.34 Q856.408 1504.73 862.218 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1134.27 1535.98 L1141.91 1535.98 L1141.91 1509.62 L1133.6 1511.29 L1133.6 1507.03 L1141.86 1505.36 L1146.54 1505.36 L1146.54 1535.98 L1154.18 1535.98 L1154.18 1539.92 L1134.27 1539.92 L1134.27 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1159.29 1505.36 L1177.65 1505.36 L1177.65 1509.3 L1163.58 1509.3 L1163.58 1517.77 Q1164.59 1517.42 1165.61 1517.26 Q1166.63 1517.07 1167.65 1517.07 Q1173.44 1517.07 1176.82 1520.24 Q1180.2 1523.42 1180.2 1528.83 Q1180.2 1534.41 1176.72 1537.51 Q1173.25 1540.59 1166.93 1540.59 Q1164.76 1540.59 1162.49 1540.22 Q1160.24 1539.85 1157.84 1539.11 L1157.84 1534.41 Q1159.92 1535.54 1162.14 1536.1 Q1164.36 1536.66 1166.84 1536.66 Q1170.84 1536.66 1173.18 1534.55 Q1175.52 1532.44 1175.52 1528.83 Q1175.52 1525.22 1173.18 1523.11 Q1170.84 1521.01 1166.84 1521.01 Q1164.96 1521.01 1163.09 1521.42 Q1161.24 1521.84 1159.29 1522.72 L1159.29 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1195.27 1508.44 Q1191.65 1508.44 1189.83 1512 Q1188.02 1515.55 1188.02 1522.67 Q1188.02 1529.78 1189.83 1533.35 Q1191.65 1536.89 1195.27 1536.89 Q1198.9 1536.89 1200.71 1533.35 Q1202.53 1529.78 1202.53 1522.67 Q1202.53 1515.55 1200.71 1512 Q1198.9 1508.44 1195.27 1508.44 M1195.27 1504.73 Q1201.08 1504.73 1204.13 1509.34 Q1207.21 1513.92 1207.21 1522.67 Q1207.21 1531.4 1204.13 1536.01 Q1201.08 1540.59 1195.27 1540.59 Q1189.46 1540.59 1186.38 1536.01 Q1183.32 1531.4 1183.32 1522.67 Q1183.32 1513.92 1186.38 1509.34 Q1189.46 1504.73 1195.27 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1471.59 1535.98 L1487.91 1535.98 L1487.91 1539.92 L1465.96 1539.92 L1465.96 1535.98 Q1468.63 1533.23 1473.21 1528.6 Q1477.82 1523.95 1479 1522.61 Q1481.24 1520.08 1482.12 1518.35 Q1483.02 1516.59 1483.02 1514.9 Q1483.02 1512.14 1481.08 1510.41 Q1479.16 1508.67 1476.06 1508.67 Q1473.86 1508.67 1471.4 1509.43 Q1468.97 1510.2 1466.2 1511.75 L1466.2 1507.03 Q1469.02 1505.89 1471.47 1505.31 Q1473.93 1504.73 1475.96 1504.73 Q1481.33 1504.73 1484.53 1507.42 Q1487.72 1510.11 1487.72 1514.6 Q1487.72 1516.73 1486.91 1518.65 Q1486.13 1520.54 1484.02 1523.14 Q1483.44 1523.81 1480.34 1527.03 Q1477.24 1530.22 1471.59 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1502.98 1508.44 Q1499.37 1508.44 1497.54 1512 Q1495.73 1515.55 1495.73 1522.67 Q1495.73 1529.78 1497.54 1533.35 Q1499.37 1536.89 1502.98 1536.89 Q1506.61 1536.89 1508.42 1533.35 Q1510.25 1529.78 1510.25 1522.67 Q1510.25 1515.55 1508.42 1512 Q1506.61 1508.44 1502.98 1508.44 M1502.98 1504.73 Q1508.79 1504.73 1511.84 1509.34 Q1514.92 1513.92 1514.92 1522.67 Q1514.92 1531.4 1511.84 1536.01 Q1508.79 1540.59 1502.98 1540.59 Q1497.17 1540.59 1494.09 1536.01 Q1491.03 1531.4 1491.03 1522.67 Q1491.03 1513.92 1494.09 1509.34 Q1497.17 1504.73 1502.98 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1529.99 1508.44 Q1526.38 1508.44 1524.55 1512 Q1522.75 1515.55 1522.75 1522.67 Q1522.75 1529.78 1524.55 1533.35 Q1526.38 1536.89 1529.99 1536.89 Q1533.63 1536.89 1535.43 1533.35 Q1537.26 1529.78 1537.26 1522.67 Q1537.26 1515.55 1535.43 1512 Q1533.63 1508.44 1529.99 1508.44 M1529.99 1504.73 Q1535.8 1504.73 1538.86 1509.34 Q1541.94 1513.92 1541.94 1522.67 Q1541.94 1531.4 1538.86 1536.01 Q1535.8 1540.59 1529.99 1540.59 Q1524.18 1540.59 1521.1 1536.01 Q1518.05 1531.4 1518.05 1522.67 Q1518.05 1513.92 1521.1 1509.34 Q1524.18 1504.73 1529.99 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1805.63 1535.98 L1821.95 1535.98 L1821.95 1539.92 L1800.01 1539.92 L1800.01 1535.98 Q1802.67 1533.23 1807.25 1528.6 Q1811.86 1523.95 1813.04 1522.61 Q1815.28 1520.08 1816.16 1518.35 Q1817.07 1516.59 1817.07 1514.9 Q1817.07 1512.14 1815.12 1510.41 Q1813.2 1508.67 1810.1 1508.67 Q1807.9 1508.67 1805.45 1509.43 Q1803.02 1510.2 1800.24 1511.75 L1800.24 1507.03 Q1803.06 1505.89 1805.52 1505.31 Q1807.97 1504.73 1810.01 1504.73 Q1815.38 1504.73 1818.57 1507.42 Q1821.77 1510.11 1821.77 1514.6 Q1821.77 1516.73 1820.96 1518.65 Q1820.17 1520.54 1818.06 1523.14 Q1817.48 1523.81 1814.38 1527.03 Q1811.28 1530.22 1805.63 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1827.07 1505.36 L1845.42 1505.36 L1845.42 1509.3 L1831.35 1509.3 L1831.35 1517.77 Q1832.37 1517.42 1833.39 1517.26 Q1834.4 1517.07 1835.42 1517.07 Q1841.21 1517.07 1844.59 1520.24 Q1847.97 1523.42 1847.97 1528.83 Q1847.97 1534.41 1844.5 1537.51 Q1841.03 1540.59 1834.71 1540.59 Q1832.53 1540.59 1830.26 1540.22 Q1828.02 1539.85 1825.61 1539.11 L1825.61 1534.41 Q1827.69 1535.54 1829.91 1536.1 Q1832.14 1536.66 1834.61 1536.66 Q1838.62 1536.66 1840.96 1534.55 Q1843.29 1532.44 1843.29 1528.83 Q1843.29 1525.22 1840.96 1523.11 Q1838.62 1521.01 1834.61 1521.01 Q1832.74 1521.01 1830.86 1521.42 Q1829.01 1521.84 1827.07 1522.72 L1827.07 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1863.04 1508.44 Q1859.43 1508.44 1857.6 1512 Q1855.79 1515.55 1855.79 1522.67 Q1855.79 1529.78 1857.6 1533.35 Q1859.43 1536.89 1863.04 1536.89 Q1866.67 1536.89 1868.48 1533.35 Q1870.31 1529.78 1870.31 1522.67 Q1870.31 1515.55 1868.48 1512 Q1866.67 1508.44 1863.04 1508.44 M1863.04 1504.73 Q1868.85 1504.73 1871.9 1509.34 Q1874.98 1513.92 1874.98 1522.67 Q1874.98 1531.4 1871.9 1536.01 Q1868.85 1540.59 1863.04 1540.59 Q1857.23 1540.59 1854.15 1536.01 Q1851.09 1531.4 1851.09 1522.67 Q1851.09 1513.92 1854.15 1509.34 Q1857.23 1504.73 1863.04 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M2148.27 1521.29 Q2151.63 1522 2153.51 1524.27 Q2155.4 1526.54 2155.4 1529.87 Q2155.4 1534.99 2151.89 1537.79 Q2148.37 1540.59 2141.89 1540.59 Q2139.71 1540.59 2137.39 1540.15 Q2135.1 1539.73 2132.65 1538.88 L2132.65 1534.36 Q2134.59 1535.5 2136.91 1536.08 Q2139.22 1536.66 2141.75 1536.66 Q2146.14 1536.66 2148.44 1534.92 Q2150.75 1533.18 2150.75 1529.87 Q2150.75 1526.82 2148.6 1525.11 Q2146.47 1523.37 2142.65 1523.37 L2138.62 1523.37 L2138.62 1519.53 L2142.83 1519.53 Q2146.28 1519.53 2148.11 1518.16 Q2149.94 1516.77 2149.94 1514.18 Q2149.94 1511.52 2148.04 1510.11 Q2146.17 1508.67 2142.65 1508.67 Q2140.73 1508.67 2138.53 1509.09 Q2136.33 1509.5 2133.69 1510.38 L2133.69 1506.22 Q2136.35 1505.48 2138.67 1505.11 Q2141.01 1504.73 2143.07 1504.73 Q2148.39 1504.73 2151.49 1507.17 Q2154.59 1509.57 2154.59 1513.69 Q2154.59 1516.56 2152.95 1518.55 Q2151.31 1520.52 2148.27 1521.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M2170.47 1508.44 Q2166.86 1508.44 2165.03 1512 Q2163.23 1515.55 2163.23 1522.67 Q2163.23 1529.78 2165.03 1533.35 Q2166.86 1536.89 2170.47 1536.89 Q2174.11 1536.89 2175.91 1533.35 Q2177.74 1529.78 2177.74 1522.67 Q2177.74 1515.55 2175.91 1512 Q2174.11 1508.44 2170.47 1508.44 M2170.47 1504.73 Q2176.28 1504.73 2179.34 1509.34 Q2182.42 1513.92 2182.42 1522.67 Q2182.42 1531.4 2179.34 1536.01 Q2176.28 1540.59 2170.47 1540.59 Q2164.66 1540.59 2161.58 1536.01 Q2158.53 1531.4 2158.53 1522.67 Q2158.53 1513.92 2161.58 1509.34 Q2164.66 1504.73 2170.47 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M2197.49 1508.44 Q2193.88 1508.44 2192.05 1512 Q2190.24 1515.55 2190.24 1522.67 Q2190.24 1529.78 2192.05 1533.35 Q2193.88 1536.89 2197.49 1536.89 Q2201.12 1536.89 2202.93 1533.35 Q2204.76 1529.78 2204.76 1522.67 Q2204.76 1515.55 2202.93 1512 Q2201.12 1508.44 2197.49 1508.44 M2197.49 1504.73 Q2203.3 1504.73 2206.35 1509.34 Q2209.43 1513.92 2209.43 1522.67 Q2209.43 1531.4 2206.35 1536.01 Q2203.3 1540.59 2197.49 1540.59 Q2191.68 1540.59 2188.6 1536.01 Q2185.54 1531.4 2185.54 1522.67 Q2185.54 1513.92 2188.6 1509.34 Q2191.68 1504.73 2197.49 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M75.0985 1159.97 Q71.4875 1159.97 69.6588 1163.53 Q67.8532 1167.07 67.8532 1174.2 Q67.8532 1181.31 69.6588 1184.87 Q71.4875 1188.42 75.0985 1188.42 Q78.7328 1188.42 80.5383 1184.87 Q82.367 1181.31 82.367 1174.2 Q82.367 1167.07 80.5383 1163.53 Q78.7328 1159.97 75.0985 1159.97 M75.0985 1156.26 Q80.9087 1156.26 83.9642 1160.87 Q87.0429 1165.45 87.0429 1174.2 Q87.0429 1182.93 83.9642 1187.54 Q80.9087 1192.12 75.0985 1192.12 Q69.2884 1192.12 66.2097 1187.54 Q63.1542 1182.93 63.1542 1174.2 Q63.1542 1165.45 66.2097 1160.87 Q69.2884 1156.26 75.0985 1156.26 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M92.1123 1185.57 L96.9966 1185.57 L96.9966 1191.45 L92.1123 1191.45 L92.1123 1185.57 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M102.205 1190.73 L102.205 1186.47 Q103.964 1187.3 105.77 1187.74 Q107.575 1188.18 109.311 1188.18 Q113.941 1188.18 116.371 1185.08 Q118.825 1181.96 119.172 1175.61 Q117.83 1177.61 115.77 1178.67 Q113.709 1179.73 111.209 1179.73 Q106.024 1179.73 102.992 1176.61 Q99.9827 1173.46 99.9827 1168.02 Q99.9827 1162.7 103.131 1159.48 Q106.279 1156.26 111.51 1156.26 Q117.506 1156.26 120.654 1160.87 Q123.825 1165.45 123.825 1174.2 Q123.825 1182.37 119.936 1187.26 Q116.071 1192.12 109.52 1192.12 Q107.76 1192.12 105.955 1191.77 Q104.149 1191.42 102.205 1190.73 M111.51 1176.08 Q114.659 1176.08 116.487 1173.92 Q118.339 1171.77 118.339 1168.02 Q118.339 1164.3 116.487 1162.14 Q114.659 1159.97 111.51 1159.97 Q108.362 1159.97 106.51 1162.14 Q104.682 1164.3 104.682 1168.02 Q104.682 1171.77 106.51 1173.92 Q108.362 1176.08 111.51 1176.08 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M138.894 1159.97 Q135.283 1159.97 133.455 1163.53 Q131.649 1167.07 131.649 1174.2 Q131.649 1181.31 133.455 1184.87 Q135.283 1188.42 138.894 1188.42 Q142.529 1188.42 144.334 1184.87 Q146.163 1181.31 146.163 1174.2 Q146.163 1167.07 144.334 1163.53 Q142.529 1159.97 138.894 1159.97 M138.894 1156.26 Q144.705 1156.26 147.76 1160.87 Q150.839 1165.45 150.839 1174.2 Q150.839 1182.93 147.76 1187.54 Q144.705 1192.12 138.894 1192.12 Q133.084 1192.12 130.006 1187.54 Q126.95 1182.93 126.95 1174.2 Q126.95 1165.45 130.006 1160.87 Q133.084 1156.26 138.894 1156.26 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M75.7467 834.109 Q72.1356 834.109 70.3069 837.674 Q68.5014 841.215 68.5014 848.345 Q68.5014 855.452 70.3069 859.016 Q72.1356 862.558 75.7467 862.558 Q79.3809 862.558 81.1865 859.016 Q83.0152 855.452 83.0152 848.345 Q83.0152 841.215 81.1865 837.674 Q79.3809 834.109 75.7467 834.109 M75.7467 830.405 Q81.5568 830.405 84.6124 835.012 Q87.6911 839.595 87.6911 848.345 Q87.6911 857.072 84.6124 861.678 Q81.5568 866.262 75.7467 866.262 Q69.9365 866.262 66.8578 861.678 Q63.8023 857.072 63.8023 848.345 Q63.8023 839.595 66.8578 835.012 Q69.9365 830.405 75.7467 830.405 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M92.7605 859.711 L97.6447 859.711 L97.6447 865.59 L92.7605 865.59 L92.7605 859.711 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M102.853 864.873 L102.853 860.614 Q104.612 861.447 106.418 861.887 Q108.223 862.326 109.959 862.326 Q114.589 862.326 117.02 859.225 Q119.473 856.1 119.821 849.757 Q118.478 851.748 116.418 852.813 Q114.358 853.877 111.858 853.877 Q106.672 853.877 103.64 850.752 Q100.631 847.604 100.631 842.165 Q100.631 836.841 103.779 833.623 Q106.927 830.405 112.159 830.405 Q118.154 830.405 121.302 835.012 Q124.473 839.595 124.473 848.345 Q124.473 856.516 120.584 861.401 Q116.719 866.262 110.168 866.262 Q108.409 866.262 106.603 865.914 Q104.797 865.567 102.853 864.873 M112.159 850.22 Q115.307 850.22 117.135 848.067 Q118.987 845.915 118.987 842.165 Q118.987 838.438 117.135 836.285 Q115.307 834.109 112.159 834.109 Q109.01 834.109 107.159 836.285 Q105.33 838.438 105.33 842.165 Q105.33 845.915 107.159 848.067 Q109.01 850.22 112.159 850.22 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M143.709 846.956 Q147.066 847.674 148.941 849.942 Q150.839 852.211 150.839 855.544 Q150.839 860.66 147.32 863.461 Q143.802 866.262 137.32 866.262 Q135.145 866.262 132.83 865.822 Q130.538 865.405 128.084 864.549 L128.084 860.035 Q130.029 861.169 132.344 861.748 Q134.658 862.326 137.182 862.326 Q141.58 862.326 143.871 860.59 Q146.186 858.854 146.186 855.544 Q146.186 852.489 144.033 850.776 Q141.904 849.04 138.084 849.04 L134.057 849.04 L134.057 845.197 L138.27 845.197 Q141.719 845.197 143.547 843.831 Q145.376 842.442 145.376 839.85 Q145.376 837.188 143.478 835.776 Q141.603 834.341 138.084 834.341 Q136.163 834.341 133.964 834.757 Q131.765 835.174 129.126 836.053 L129.126 831.887 Q131.788 831.146 134.103 830.776 Q136.441 830.405 138.501 830.405 Q143.825 830.405 146.927 832.836 Q150.029 835.243 150.029 839.364 Q150.029 842.234 148.385 844.225 Q146.742 846.192 143.709 846.956 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M74.9365 508.252 Q71.3254 508.252 69.4967 511.816 Q67.6912 515.358 67.6912 522.488 Q67.6912 529.594 69.4967 533.159 Q71.3254 536.7 74.9365 536.7 Q78.5707 536.7 80.3763 533.159 Q82.205 529.594 82.205 522.488 Q82.205 515.358 80.3763 511.816 Q78.5707 508.252 74.9365 508.252 M74.9365 504.548 Q80.7467 504.548 83.8022 509.154 Q86.8809 513.738 86.8809 522.488 Q86.8809 531.214 83.8022 535.821 Q80.7467 540.404 74.9365 540.404 Q69.1264 540.404 66.0477 535.821 Q62.9921 531.214 62.9921 522.488 Q62.9921 513.738 66.0477 509.154 Q69.1264 504.548 74.9365 504.548 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M91.9503 533.853 L96.8345 533.853 L96.8345 539.733 L91.9503 539.733 L91.9503 533.853 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M102.043 539.015 L102.043 534.756 Q103.802 535.589 105.608 536.029 Q107.413 536.469 109.149 536.469 Q113.779 536.469 116.209 533.367 Q118.663 530.242 119.01 523.9 Q117.668 525.89 115.608 526.955 Q113.547 528.02 111.047 528.02 Q105.862 528.02 102.83 524.895 Q99.8206 521.747 99.8206 516.307 Q99.8206 510.983 102.969 507.765 Q106.117 504.548 111.348 504.548 Q117.344 504.548 120.492 509.154 Q123.663 513.738 123.663 522.488 Q123.663 530.659 119.774 535.543 Q115.909 540.404 109.358 540.404 Q107.598 540.404 105.793 540.057 Q103.987 539.71 102.043 539.015 M111.348 524.363 Q114.496 524.363 116.325 522.21 Q118.177 520.057 118.177 516.307 Q118.177 512.58 116.325 510.427 Q114.496 508.252 111.348 508.252 Q108.2 508.252 106.348 510.427 Q104.52 512.58 104.52 516.307 Q104.52 520.057 106.348 522.21 Q108.2 524.363 111.348 524.363 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M139.311 520.589 Q136.163 520.589 134.311 522.742 Q132.482 524.895 132.482 528.645 Q132.482 532.372 134.311 534.548 Q136.163 536.7 139.311 536.7 Q142.459 536.7 144.288 534.548 Q146.14 532.372 146.14 528.645 Q146.14 524.895 144.288 522.742 Q142.459 520.589 139.311 520.589 M148.594 505.937 L148.594 510.196 Q146.834 509.363 145.029 508.923 Q143.246 508.483 141.487 508.483 Q136.857 508.483 134.404 511.608 Q131.973 514.733 131.626 521.052 Q132.992 519.039 135.052 517.974 Q137.112 516.886 139.589 516.886 Q144.797 516.886 147.806 520.057 Q150.839 523.205 150.839 528.645 Q150.839 533.969 147.691 537.187 Q144.543 540.404 139.311 540.404 Q133.316 540.404 130.145 535.821 Q126.973 531.214 126.973 522.488 Q126.973 514.293 130.862 509.432 Q134.751 504.548 141.302 504.548 Q143.061 504.548 144.844 504.895 Q146.649 505.242 148.594 505.937 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M75.2837 182.394 Q71.6726 182.394 69.8439 185.959 Q68.0384 189.501 68.0384 196.63 Q68.0384 203.737 69.8439 207.301 Q71.6726 210.843 75.2837 210.843 Q78.918 210.843 80.7235 207.301 Q82.5522 203.737 82.5522 196.63 Q82.5522 189.501 80.7235 185.959 Q78.918 182.394 75.2837 182.394 M75.2837 178.69 Q81.0939 178.69 84.1494 183.297 Q87.2281 187.88 87.2281 196.63 Q87.2281 205.357 84.1494 209.963 Q81.0939 214.547 75.2837 214.547 Q69.4736 214.547 66.3949 209.963 Q63.3393 205.357 63.3393 196.63 Q63.3393 187.88 66.3949 183.297 Q69.4736 178.69 75.2837 178.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M92.2975 207.996 L97.1818 207.996 L97.1818 213.875 L92.2975 213.875 L92.2975 207.996 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M102.39 213.158 L102.39 208.899 Q104.149 209.732 105.955 210.172 Q107.76 210.612 109.497 210.612 Q114.126 210.612 116.557 207.51 Q119.01 204.385 119.358 198.042 Q118.015 200.033 115.955 201.098 Q113.895 202.163 111.395 202.163 Q106.209 202.163 103.177 199.038 Q100.168 195.889 100.168 190.45 Q100.168 185.126 103.316 181.908 Q106.464 178.69 111.696 178.69 Q117.691 178.69 120.839 183.297 Q124.01 187.88 124.01 196.63 Q124.01 204.801 120.121 209.686 Q116.256 214.547 109.705 214.547 Q107.946 214.547 106.14 214.2 Q104.334 213.852 102.39 213.158 M111.696 198.505 Q114.844 198.505 116.672 196.352 Q118.524 194.2 118.524 190.45 Q118.524 186.723 116.672 184.57 Q114.844 182.394 111.696 182.394 Q108.547 182.394 106.696 184.57 Q104.867 186.723 104.867 190.45 Q104.867 194.2 106.696 196.352 Q108.547 198.505 111.696 198.505 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M129.219 213.158 L129.219 208.899 Q130.978 209.732 132.783 210.172 Q134.589 210.612 136.325 210.612 Q140.955 210.612 143.385 207.51 Q145.839 204.385 146.186 198.042 Q144.844 200.033 142.783 201.098 Q140.723 202.163 138.223 202.163 Q133.038 202.163 130.006 199.038 Q126.996 195.889 126.996 190.45 Q126.996 185.126 130.145 181.908 Q133.293 178.69 138.524 178.69 Q144.519 178.69 147.668 183.297 Q150.839 187.88 150.839 196.63 Q150.839 204.801 146.95 209.686 Q143.084 214.547 136.533 214.547 Q134.774 214.547 132.969 214.2 Q131.163 213.852 129.219 213.158 M138.524 198.505 Q141.672 198.505 143.501 196.352 Q145.353 194.2 145.353 190.45 Q145.353 186.723 143.501 184.57 Q141.672 182.394 138.524 182.394 Q135.376 182.394 133.524 184.57 Q131.695 186.723 131.695 190.45 Q131.695 194.2 133.524 196.352 Q135.376 198.505 138.524 198.505 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip722)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  236.478,87.9763 243.149,87.9763 249.82,87.9763 256.491,87.9763 263.162,87.9763 269.833,87.9763 276.503,87.9763 283.174,87.9763 289.845,87.9763 296.516,87.9763 \n",
       "  303.187,87.9763 309.858,87.9763 316.529,87.9763 323.2,87.9763 329.871,87.9763 336.542,87.9763 343.213,87.9763 349.883,87.9763 356.554,87.9763 363.225,87.9763 \n",
       "  369.896,87.9763 376.567,87.9763 383.238,87.9763 389.909,87.9763 396.58,87.9763 403.251,87.9763 409.922,87.9763 416.592,87.9763 423.263,87.9763 429.934,366.487 \n",
       "  436.605,359.524 443.276,352.901 449.947,346.593 456.618,340.579 463.289,334.838 469.96,329.352 476.631,324.105 483.302,319.081 489.972,314.266 496.643,309.648 \n",
       "  503.314,305.215 509.985,300.955 516.656,296.859 523.327,292.918 529.998,289.123 536.669,285.466 543.34,281.939 550.011,278.536 556.681,275.251 563.352,272.077 \n",
       "  570.023,269.008 576.694,266.04 583.365,263.168 590.036,260.388 596.707,257.694 603.378,255.083 610.049,252.551 616.72,250.094 623.391,247.71 630.061,245.395 \n",
       "  636.732,243.146 643.403,240.961 650.074,238.836 656.745,236.77 663.416,234.759 670.087,232.802 676.758,230.896 683.429,229.04 690.1,227.232 696.77,225.469 \n",
       "  703.441,223.75 710.112,222.074 716.783,220.439 723.454,218.843 730.125,217.285 736.796,215.764 743.467,214.278 750.138,337.675 756.809,334.838 763.48,332.064 \n",
       "  770.15,329.352 776.821,326.7 783.492,324.105 790.163,321.566 796.834,319.081 803.505,202.312 810.176,201.121 816.847,199.955 823.518,198.812 830.189,197.693 \n",
       "  836.859,196.595 843.53,195.52 850.201,194.466 856.872,193.432 863.543,192.418 870.214,191.423 876.885,190.447 883.556,189.49 890.227,188.55 896.898,187.627 \n",
       "  903.569,186.721 910.239,185.831 916.91,184.958 923.581,184.099 930.252,183.256 936.923,276.879 943.594,275.251 950.265,273.65 956.936,272.077 963.607,270.529 \n",
       "  970.278,269.008 976.948,267.512 983.619,266.04 990.29,176.285 996.961,175.572 1003.63,261.767 1010.3,260.388 1016.97,259.03 1023.64,257.694 1030.32,256.378 \n",
       "  1036.99,255.083 1043.66,253.807 1050.33,252.551 1057,251.313 1063.67,250.094 1070.34,248.894 1077.01,247.71 1083.68,246.544 1090.35,245.395 1097.02,244.263 \n",
       "  1103.7,165.561 1110.37,165.011 1117.04,164.469 1123.71,315.849 1130.38,389.696 1137.05,312.706 1143.72,311.166 1150.39,309.648 1157.06,308.15 1163.73,306.673 \n",
       "  1170.4,305.215 1177.08,303.776 1183.75,302.356 1190.42,442.941 1197.09,440.636 1203.76,438.361 1210.43,505.742 1217.1,503.081 1223.77,500.454 1230.44,497.86 \n",
       "  1237.11,495.298 1243.78,492.768 1250.46,490.269 1257.13,487.801 1263.8,485.363 1270.47,482.955 1277.14,480.576 1283.81,478.225 1290.48,475.902 1297.15,473.606 \n",
       "  1303.82,471.338 1310.49,469.096 1317.16,466.88 1323.84,464.69 1330.51,400.1 1337.18,398.317 1343.85,396.553 1350.52,394.81 1357.19,393.086 1363.86,391.382 \n",
       "  1370.53,389.696 1377.2,388.029 1383.87,386.381 1390.54,384.75 1397.22,383.137 1403.89,381.542 1410.56,379.963 1417.23,378.402 1423.9,434.633 1430.57,432.799 \n",
       "  1437.24,488.152 1443.91,486.057 1450.58,483.984 1457.25,481.932 1463.92,479.901 1470.6,477.891 1477.27,475.902 1483.94,473.933 1490.61,471.983 1497.28,470.054 \n",
       "  1503.95,468.143 1510.62,466.252 1517.29,464.379 1523.96,462.525 1530.63,460.689 1537.3,458.871 1543.98,457.07 1550.65,455.287 1557.32,453.522 1563.99,451.772 \n",
       "  1570.66,450.04 1577.33,448.324 1584,446.624 1590.67,444.941 1597.34,443.273 1604.01,441.62 1610.68,439.983 1617.36,438.361 1624.03,536.404 1630.7,534.356 \n",
       "  1637.37,532.327 1644.04,530.317 1650.71,528.324 1657.38,526.35 1664.05,572.883 1670.72,522.453 1677.39,520.53 1684.06,518.625 1690.74,516.736 1697.41,514.864 \n",
       "  1704.08,513.008 1710.75,511.168 1717.42,509.344 1724.09,507.535 1730.76,505.742 1737.43,503.965 1744.1,502.202 1750.77,362.961 1757.44,361.806 1764.12,360.66 \n",
       "  1770.79,359.524 1777.46,358.397 1784.13,357.28 1790.8,356.172 1797.47,355.073 1804.14,353.982 1810.81,352.901 1817.48,351.828 1824.15,350.765 1830.82,349.709 \n",
       "  1837.5,435.558 1844.17,347.624 1850.84,432.799 1857.51,431.436 1864.18,430.084 1870.85,386.147 1877.52,384.982 1884.19,426.09 1890.86,508.981 1897.53,465.417 \n",
       "  1904.2,463.966 1910.88,462.525 1917.55,461.096 1924.22,418.377 1930.89,458.269 1937.56,456.872 1944.23,455.485 1950.9,454.108 1957.57,493.272 1964.24,491.765 \n",
       "  1970.91,450.04 1977.58,408.623 1984.26,407.444 1990.93,406.274 1997.6,405.112 2004.27,443.457 2010.94,520.879 2017.61,519.316 2024.28,517.764 2030.95,516.224 \n",
       "  2037.62,514.694 2044.29,474.521 2050.96,511.668 2057.64,510.171 2064.31,508.684 2070.98,507.208 2077.65,505.742 2084.32,504.287 2090.99,502.841 2097.66,501.406 \n",
       "  2104.33,499.98 2111,498.564 2117.67,459.96 2124.34,495.761 2131.02,494.374 2137.69,492.997 2144.36,454.933 2151.03,453.697 2157.7,452.47 2164.37,451.251 \n",
       "  2171.04,450.04 2177.71,448.837 2184.38,447.642 2191.05,446.455 2197.72,445.276 2204.39,444.105 2211.07,442.941 2217.74,441.785 2224.41,511.168 2231.08,509.798 \n",
       "  2237.75,543.476 2244.42,507.086 2251.09,505.742 2257.76,504.408 2264.43,537.673 2271.1,501.764 2277.77,534.827 2284.45,533.418 2291.12,497.86 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  236.478,902.62 243.149,902.62 249.82,902.62 256.491,902.62 263.162,1174.17 269.833,1174.17 276.503,1174.17 283.174,1174.17 289.845,1174.17 296.516,1174.17 \n",
       "  303.187,1445.72 309.858,1445.72 316.529,1445.72 323.2,1445.72 329.871,1445.72 336.542,1445.72 343.213,1445.72 349.883,1445.72 356.554,1445.72 363.225,1445.72 \n",
       "  369.896,1445.72 376.567,1445.72 383.238,1445.72 389.909,1445.72 396.58,1445.72 403.251,1445.72 409.922,1445.72 416.592,1445.72 423.263,1445.72 429.934,1174.17 \n",
       "  436.605,1174.17 443.276,1174.17 449.947,902.62 456.618,902.62 463.289,902.62 469.96,902.62 476.631,902.62 483.302,902.62 489.972,902.62 496.643,902.62 \n",
       "  503.314,902.62 509.985,902.62 516.656,902.62 523.327,902.62 529.998,902.62 536.669,902.62 543.34,902.62 550.011,766.846 556.681,766.846 563.352,766.846 \n",
       "  570.023,766.846 576.694,766.846 583.365,766.846 590.036,766.846 596.707,766.846 603.378,766.846 610.049,766.846 616.72,766.846 623.391,766.846 630.061,766.846 \n",
       "  636.732,766.846 643.403,766.846 650.074,766.846 656.745,766.846 663.416,766.846 670.087,766.846 676.758,766.846 683.429,766.846 690.1,766.846 696.77,766.846 \n",
       "  703.441,766.846 710.112,766.846 716.783,766.846 723.454,902.62 730.125,902.62 736.796,902.62 743.467,902.62 750.138,631.072 756.809,631.072 763.48,631.072 \n",
       "  770.15,631.072 776.821,631.072 783.492,631.072 790.163,631.072 796.834,631.072 803.505,495.298 810.176,495.298 816.847,495.298 823.518,631.072 830.189,631.072 \n",
       "  836.859,631.072 843.53,631.072 850.201,631.072 856.872,631.072 863.543,631.072 870.214,631.072 876.885,631.072 883.556,631.072 890.227,631.072 896.898,631.072 \n",
       "  903.569,631.072 910.239,631.072 916.91,631.072 923.581,631.072 930.252,631.072 936.923,631.072 943.594,631.072 950.265,631.072 956.936,631.072 963.607,631.072 \n",
       "  970.278,631.072 976.948,631.072 983.619,631.072 990.29,902.62 996.961,902.62 1003.63,766.846 1010.3,766.846 1016.97,766.846 1023.64,766.846 1030.32,766.846 \n",
       "  1036.99,766.846 1043.66,766.846 1050.33,766.846 1057,766.846 1063.67,902.62 1070.34,902.62 1077.01,902.62 1083.68,902.62 1090.35,902.62 1097.02,902.62 \n",
       "  1103.7,902.62 1110.37,902.62 1117.04,902.62 1123.71,766.846 1130.38,902.62 1137.05,902.62 1143.72,902.62 1150.39,902.62 1157.06,902.62 1163.73,902.62 \n",
       "  1170.4,631.072 1177.08,631.072 1183.75,631.072 1190.42,1038.39 1197.09,1038.39 1203.76,1038.39 1210.43,902.62 1217.1,902.62 1223.77,902.62 1230.44,902.62 \n",
       "  1237.11,902.62 1243.78,902.62 1250.46,902.62 1257.13,902.62 1263.8,902.62 1270.47,902.62 1277.14,902.62 1283.81,902.62 1290.48,902.62 1297.15,902.62 \n",
       "  1303.82,902.62 1310.49,902.62 1317.16,902.62 1323.84,902.62 1330.51,766.846 1337.18,766.846 1343.85,766.846 1350.52,766.846 1357.19,766.846 1363.86,766.846 \n",
       "  1370.53,766.846 1377.2,766.846 1383.87,766.846 1390.54,766.846 1397.22,766.846 1403.89,766.846 1410.56,766.846 1417.23,766.846 1423.9,902.62 1430.57,902.62 \n",
       "  1437.24,766.846 1443.91,766.846 1450.58,766.846 1457.25,766.846 1463.92,766.846 1470.6,766.846 1477.27,766.846 1483.94,766.846 1490.61,766.846 1497.28,766.846 \n",
       "  1503.95,766.846 1510.62,766.846 1517.29,766.846 1523.96,766.846 1530.63,766.846 1537.3,766.846 1543.98,902.62 1550.65,902.62 1557.32,902.62 1563.99,902.62 \n",
       "  1570.66,902.62 1577.33,902.62 1584,902.62 1590.67,902.62 1597.34,902.62 1604.01,902.62 1610.68,902.62 1617.36,902.62 1624.03,902.62 1630.7,902.62 \n",
       "  1637.37,902.62 1644.04,902.62 1650.71,902.62 1657.38,902.62 1664.05,902.62 1670.72,766.846 1677.39,766.846 1684.06,766.846 1690.74,766.846 1697.41,766.846 \n",
       "  1704.08,766.846 1710.75,766.846 1717.42,766.846 1724.09,766.846 1730.76,766.846 1737.43,766.846 1744.1,766.846 1750.77,902.62 1757.44,902.62 1764.12,902.62 \n",
       "  1770.79,902.62 1777.46,902.62 1784.13,902.62 1790.8,902.62 1797.47,902.62 1804.14,902.62 1810.81,902.62 1817.48,902.62 1824.15,902.62 1830.82,902.62 \n",
       "  1837.5,766.846 1844.17,902.62 1850.84,902.62 1857.51,902.62 1864.18,902.62 1870.85,902.62 1877.52,902.62 1884.19,902.62 1890.86,902.62 1897.53,902.62 \n",
       "  1904.2,902.62 1910.88,902.62 1917.55,902.62 1924.22,902.62 1930.89,902.62 1937.56,902.62 1944.23,902.62 1950.9,902.62 1957.57,902.62 1964.24,902.62 \n",
       "  1970.91,902.62 1977.58,902.62 1984.26,902.62 1990.93,902.62 1997.6,902.62 2004.27,902.62 2010.94,902.62 2017.61,902.62 2024.28,902.62 2030.95,902.62 \n",
       "  2037.62,902.62 2044.29,902.62 2050.96,902.62 2057.64,902.62 2064.31,902.62 2070.98,902.62 2077.65,902.62 2084.32,902.62 2090.99,902.62 2097.66,902.62 \n",
       "  2104.33,902.62 2111,902.62 2117.67,902.62 2124.34,902.62 2131.02,902.62 2137.69,902.62 2144.36,902.62 2151.03,902.62 2157.7,902.62 2164.37,902.62 \n",
       "  2171.04,902.62 2177.71,902.62 2184.38,902.62 2191.05,902.62 2197.72,902.62 2204.39,902.62 2211.07,902.62 2217.74,902.62 2224.41,902.62 2231.08,902.62 \n",
       "  2237.75,902.62 2244.42,902.62 2251.09,902.62 2257.76,902.62 2264.43,902.62 2271.1,902.62 2277.77,902.62 2284.45,902.62 2291.12,902.62 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip720)\" d=\"\n",
       "M1987.15 276.658 L2280.16 276.658 L2280.16 95.2176 L1987.15 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1987.15,276.658 2280.16,276.658 2280.16,95.2176 1987.15,95.2176 1987.15,276.658 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2011.35,155.698 2156.54,155.698 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip720)\" d=\"M 0 0 M2194.58 175.385 Q2192.78 180.015 2191.07 181.427 Q2189.35 182.839 2186.48 182.839 L2183.08 182.839 L2183.08 179.274 L2185.58 179.274 Q2187.34 179.274 2188.31 178.44 Q2189.28 177.607 2190.46 174.505 L2191.23 172.561 L2180.74 147.052 L2185.26 147.052 L2193.36 167.329 L2201.46 147.052 L2205.97 147.052 L2194.58 175.385 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M2211.85 169.042 L2219.49 169.042 L2219.49 142.677 L2211.18 144.343 L2211.18 140.084 L2219.45 138.418 L2224.12 138.418 L2224.12 169.042 L2231.76 169.042 L2231.76 172.978 L2211.85 172.978 L2211.85 169.042 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip720)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2011.35,216.178 2156.54,216.178 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip720)\" d=\"M 0 0 M2194.58 235.865 Q2192.78 240.495 2191.07 241.907 Q2189.35 243.319 2186.48 243.319 L2183.08 243.319 L2183.08 239.754 L2185.58 239.754 Q2187.34 239.754 2188.31 238.92 Q2189.28 238.087 2190.46 234.985 L2191.23 233.041 L2180.74 207.532 L2185.26 207.532 L2193.36 227.809 L2201.46 207.532 L2205.97 207.532 L2194.58 235.865 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M2215.07 229.522 L2231.39 229.522 L2231.39 233.458 L2209.45 233.458 L2209.45 229.522 Q2212.11 226.768 2216.69 222.138 Q2221.3 217.485 2222.48 216.143 Q2224.72 213.62 2225.6 211.884 Q2226.51 210.124 2226.51 208.435 Q2226.51 205.68 2224.56 203.944 Q2222.64 202.208 2219.54 202.208 Q2217.34 202.208 2214.89 202.972 Q2212.46 203.735 2209.68 205.286 L2209.68 200.564 Q2212.5 199.43 2214.96 198.851 Q2217.41 198.273 2219.45 198.273 Q2224.82 198.273 2228.01 200.958 Q2231.2 203.643 2231.2 208.134 Q2231.2 210.263 2230.39 212.185 Q2229.61 214.083 2227.5 216.675 Q2226.92 217.347 2223.82 220.564 Q2220.72 223.759 2215.07 229.522 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(data_schedule, training_losses)\n",
    "plot!(data_schedule, valid_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_svm = SVMClassifier(kernel=\"linear\", C = best_C, cache_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_SVM = machine(final_svm, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(Final_SVM, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = MLJ.predict(Final_SVM, X[test,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(y2, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y2, y[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_svm_rbf = best_rbf.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_SVM = machine(final_svm, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit!(Final_SVM, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = MLJ.predict(Final_SVM, X[test,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy(y2, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y2, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
