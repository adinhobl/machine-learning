{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling DataFrames [a93c6f00-e57d-5684-b7b6-d8193f3e46c0]\n",
      "└ @ Base loading.jl:1278\n",
      "┌ Info: Precompiling CSV [336ed68f-0bac-5ca0-87d4-7b16caf5d00b]\n",
      "└ @ Base loading.jl:1278\n",
      "┌ Info: Precompiling MLJ [add582a8-e3ab-11e8-2d5e-e98b27df1bc7]\n",
      "└ @ Base loading.jl:1278\n",
      "[ Info: Model metadata loaded from registry. \n",
      "┌ Info: Precompiling Plots [91a5bcdd-55d7-5caf-9e0b-520d859cae80]\n",
      "└ @ Base loading.jl:1278\n"
     ]
    }
   ],
   "source": [
    "using DataFrames\n",
    "using CSV\n",
    "using MLJ\n",
    "using Plots\n",
    "using StatsBase\n",
    "\n",
    "include(\"../../lib.jl\")\n",
    "\n",
    "ENV[\"LINES\"]=50;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 2. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 3. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 4. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 5. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 6. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 7. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 8. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 9. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 10. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 11. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 12. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 13. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 14. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 15. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 16. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 17. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 18. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 19. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 20. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 21. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 22. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 23. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 24. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 25. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 26. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 27. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 28. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 29. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 30. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 31. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 32. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 33. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 34. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 35. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 36. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 37. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 38. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 39. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 40. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 41. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 42. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 43. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 44. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 45. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 46. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 47. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 48. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 49. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 50. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 51. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 52. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 53. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 54. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 55. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 56. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 57. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 58. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 59. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 60. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 61. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 62. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 63. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 64. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 65. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 66. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 67. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 68. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 69. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 70. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 71. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 72. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 73. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 74. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 75. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 76. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 77. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 78. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 79. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 80. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 81. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 82. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 83. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 84. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 85. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 86. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 87. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 88. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 89. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 90. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 91. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 92. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 93. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 94. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 95. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 96. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 97. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 98. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 99. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 100. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 101. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 102. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 103. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 104. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 105. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 106. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 107. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 108. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 109. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 110. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 111. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 112. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 113. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 114. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 115. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 116. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 117. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 118. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 119. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 120. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 121. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 122. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 123. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 124. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 125. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 126. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 127. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 128. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 129. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 130. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 131. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 132. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 133. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 134. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 135. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 136. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 137. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 138. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 139. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 140. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 141. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 142. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 143. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 144. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 145. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 146. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 147. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 148. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 149. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 150. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 151. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 152. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 153. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 154. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 155. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 156. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 157. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 158. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 159. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 160. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 161. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 162. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 163. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 164. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 165. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 166. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 167. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 168. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 169. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 170. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 171. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 172. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 173. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 174. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 175. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 176. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 177. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 178. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 179. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 180. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 181. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 182. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 183. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 184. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 185. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 186. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 187. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 188. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 189. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 190. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 191. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 192. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 193. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 194. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 195. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 196. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 197. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 198. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 199. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 200. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 201. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 202. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 203. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 204. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 205. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 206. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 207. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 208. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 209. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 210. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 211. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 212. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 213. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 214. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 215. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 216. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 217. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 218. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 219. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 220. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 221. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 222. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 223. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 224. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 225. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 226. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 227. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 228. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 229. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 230. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 231. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 232. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 233. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 234. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 235. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 236. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 237. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 238. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 239. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 240. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 241. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 242. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 243. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 244. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 245. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 246. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 247. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 248. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 249. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 250. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 251. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 252. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 253. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 254. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 255. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 256. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 257. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 258. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 259. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 260. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 261. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 262. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 263. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 264. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 265. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 266. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 267. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 268. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 269. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 270. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 271. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 272. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 273. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 274. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 275. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 276. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 277. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 278. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 279. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 280. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 281. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 282. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 283. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 284. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 285. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 286. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 287. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 288. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 289. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 290. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 291. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 292. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 293. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 294. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 295. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 296. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 297. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 298. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 299. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 300. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 301. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 302. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 303. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 304. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 305. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 306. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 307. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 308. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 309. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 310. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 311. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 312. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 313. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 314. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 315. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 316. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 317. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 318. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 319. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 320. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 321. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 322. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 323. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 324. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 325. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 326. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 327. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 328. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 329. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 330. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 331. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 332. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 333. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 334. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 335. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 336. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 337. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 338. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 339. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 340. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 341. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 342. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 343. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 344. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 345. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 346. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 347. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 348. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 349. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 350. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 351. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 352. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 353. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 354. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 355. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 356. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 357. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 358. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 359. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 360. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 361. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 362. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 363. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 364. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 365. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 366. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 367. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 368. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 369. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 370. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 371. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 372. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 373. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 374. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 375. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 376. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 377. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 378. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 379. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 380. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 381. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 382. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 383. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 384. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 385. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 386. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 387. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 388. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 389. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 390. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 391. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 392. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 393. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 394. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 395. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 396. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 397. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 398. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 399. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 400. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 401. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 402. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 403. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 404. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 405. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 406. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 407. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 408. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 409. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 410. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 411. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 412. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 413. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 414. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 415. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 416. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 417. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 418. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 419. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 420. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 421. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 422. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 423. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 424. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 425. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 426. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 427. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 428. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 429. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 430. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 431. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 432. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 433. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 434. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 435. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 436. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 437. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 438. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 439. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 440. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 441. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 442. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 443. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 444. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 445. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 446. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 447. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 448. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 449. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 450. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 451. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 452. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 453. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 454. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 455. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 456. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 457. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 458. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 459. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 460. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 461. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 462. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 463. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 464. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 465. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 466. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 467. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 468. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 469. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 470. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 471. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 472. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 473. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 474. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 475. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 476. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 477. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 478. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 479. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 480. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 481. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 482. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 483. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 484. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 485. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 486. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 487. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 488. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 489. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 490. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 491. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 492. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 493. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 494. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 495. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 496. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 497. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 498. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 499. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 500. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 501. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 502. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 503. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 504. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 505. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 506. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 507. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 508. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 509. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 510. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 511. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 512. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 513. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 514. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 515. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 516. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 517. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 518. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 519. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 520. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 521. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 522. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 523. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 524. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 525. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 526. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 527. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 528. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 529. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 530. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 531. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 532. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 533. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 534. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 535. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 536. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 537. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 538. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 539. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 540. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 541. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 542. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 543. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 544. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 545. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 546. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 547. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 548. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 549. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 550. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 551. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 552. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 553. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 554. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 555. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 556. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 557. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 558. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 559. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 560. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 561. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 562. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 563. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 564. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 565. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 566. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thread = 1 warning: only found 32 / 33 columns around data row: 567. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 568. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 569. Filling remaining columns with `missing`\n",
      "thread = 1 warning: only found 32 / 33 columns around data row: 570. Filling remaining columns with `missing`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>id</th><th>diagnosis</th><th>radius_mean</th><th>texture_mean</th><th>perimeter_mean</th><th>area_mean</th><th>smoothness_mean</th></tr><tr><th></th><th>Int64</th><th>String</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>569 rows × 33 columns (omitted printing of 26 columns)</p><tr><th>1</th><td>842302</td><td>M</td><td>17.99</td><td>10.38</td><td>122.8</td><td>1001.0</td><td>0.1184</td></tr><tr><th>2</th><td>842517</td><td>M</td><td>20.57</td><td>17.77</td><td>132.9</td><td>1326.0</td><td>0.08474</td></tr><tr><th>3</th><td>84300903</td><td>M</td><td>19.69</td><td>21.25</td><td>130.0</td><td>1203.0</td><td>0.1096</td></tr><tr><th>4</th><td>84348301</td><td>M</td><td>11.42</td><td>20.38</td><td>77.58</td><td>386.1</td><td>0.1425</td></tr><tr><th>5</th><td>84358402</td><td>M</td><td>20.29</td><td>14.34</td><td>135.1</td><td>1297.0</td><td>0.1003</td></tr><tr><th>6</th><td>843786</td><td>M</td><td>12.45</td><td>15.7</td><td>82.57</td><td>477.1</td><td>0.1278</td></tr><tr><th>7</th><td>844359</td><td>M</td><td>18.25</td><td>19.98</td><td>119.6</td><td>1040.0</td><td>0.09463</td></tr><tr><th>8</th><td>84458202</td><td>M</td><td>13.71</td><td>20.83</td><td>90.2</td><td>577.9</td><td>0.1189</td></tr><tr><th>9</th><td>844981</td><td>M</td><td>13.0</td><td>21.82</td><td>87.5</td><td>519.8</td><td>0.1273</td></tr><tr><th>10</th><td>84501001</td><td>M</td><td>12.46</td><td>24.04</td><td>83.97</td><td>475.9</td><td>0.1186</td></tr><tr><th>11</th><td>845636</td><td>M</td><td>16.02</td><td>23.24</td><td>102.7</td><td>797.8</td><td>0.08206</td></tr><tr><th>12</th><td>84610002</td><td>M</td><td>15.78</td><td>17.89</td><td>103.6</td><td>781.0</td><td>0.0971</td></tr><tr><th>13</th><td>846226</td><td>M</td><td>19.17</td><td>24.8</td><td>132.4</td><td>1123.0</td><td>0.0974</td></tr><tr><th>14</th><td>846381</td><td>M</td><td>15.85</td><td>23.95</td><td>103.7</td><td>782.7</td><td>0.08401</td></tr><tr><th>15</th><td>84667401</td><td>M</td><td>13.73</td><td>22.61</td><td>93.6</td><td>578.3</td><td>0.1131</td></tr><tr><th>16</th><td>84799002</td><td>M</td><td>14.54</td><td>27.54</td><td>96.73</td><td>658.8</td><td>0.1139</td></tr><tr><th>17</th><td>848406</td><td>M</td><td>14.68</td><td>20.13</td><td>94.74</td><td>684.5</td><td>0.09867</td></tr><tr><th>18</th><td>84862001</td><td>M</td><td>16.13</td><td>20.68</td><td>108.1</td><td>798.8</td><td>0.117</td></tr><tr><th>19</th><td>849014</td><td>M</td><td>19.81</td><td>22.15</td><td>130.0</td><td>1260.0</td><td>0.09831</td></tr><tr><th>20</th><td>8510426</td><td>B</td><td>13.54</td><td>14.36</td><td>87.46</td><td>566.3</td><td>0.09779</td></tr><tr><th>21</th><td>8510653</td><td>B</td><td>13.08</td><td>15.71</td><td>85.63</td><td>520.0</td><td>0.1075</td></tr><tr><th>22</th><td>8510824</td><td>B</td><td>9.504</td><td>12.44</td><td>60.34</td><td>273.9</td><td>0.1024</td></tr><tr><th>23</th><td>8511133</td><td>M</td><td>15.34</td><td>14.26</td><td>102.5</td><td>704.4</td><td>0.1073</td></tr><tr><th>24</th><td>851509</td><td>M</td><td>21.16</td><td>23.04</td><td>137.2</td><td>1404.0</td><td>0.09428</td></tr><tr><th>25</th><td>852552</td><td>M</td><td>16.65</td><td>21.38</td><td>110.0</td><td>904.6</td><td>0.1121</td></tr><tr><th>26</th><td>852631</td><td>M</td><td>17.14</td><td>16.4</td><td>116.0</td><td>912.7</td><td>0.1186</td></tr><tr><th>27</th><td>852763</td><td>M</td><td>14.58</td><td>21.53</td><td>97.41</td><td>644.8</td><td>0.1054</td></tr><tr><th>28</th><td>852781</td><td>M</td><td>18.61</td><td>20.25</td><td>122.1</td><td>1094.0</td><td>0.0944</td></tr><tr><th>29</th><td>852973</td><td>M</td><td>15.3</td><td>25.27</td><td>102.4</td><td>732.4</td><td>0.1082</td></tr><tr><th>30</th><td>853201</td><td>M</td><td>17.57</td><td>15.05</td><td>115.0</td><td>955.1</td><td>0.09847</td></tr><tr><th>31</th><td>853401</td><td>M</td><td>18.63</td><td>25.11</td><td>124.8</td><td>1088.0</td><td>0.1064</td></tr><tr><th>32</th><td>853612</td><td>M</td><td>11.84</td><td>18.7</td><td>77.93</td><td>440.6</td><td>0.1109</td></tr><tr><th>33</th><td>85382601</td><td>M</td><td>17.02</td><td>23.98</td><td>112.8</td><td>899.3</td><td>0.1197</td></tr><tr><th>34</th><td>854002</td><td>M</td><td>19.27</td><td>26.47</td><td>127.9</td><td>1162.0</td><td>0.09401</td></tr><tr><th>35</th><td>854039</td><td>M</td><td>16.13</td><td>17.88</td><td>107.0</td><td>807.2</td><td>0.104</td></tr><tr><th>36</th><td>854253</td><td>M</td><td>16.74</td><td>21.59</td><td>110.1</td><td>869.5</td><td>0.0961</td></tr><tr><th>37</th><td>854268</td><td>M</td><td>14.25</td><td>21.72</td><td>93.63</td><td>633.0</td><td>0.09823</td></tr><tr><th>38</th><td>854941</td><td>B</td><td>13.03</td><td>18.42</td><td>82.61</td><td>523.8</td><td>0.08983</td></tr><tr><th>39</th><td>855133</td><td>M</td><td>14.99</td><td>25.2</td><td>95.54</td><td>698.8</td><td>0.09387</td></tr><tr><th>40</th><td>855138</td><td>M</td><td>13.48</td><td>20.82</td><td>88.4</td><td>559.2</td><td>0.1016</td></tr><tr><th>41</th><td>855167</td><td>M</td><td>13.44</td><td>21.58</td><td>86.18</td><td>563.0</td><td>0.08162</td></tr><tr><th>42</th><td>855563</td><td>M</td><td>10.95</td><td>21.35</td><td>71.9</td><td>371.1</td><td>0.1227</td></tr><tr><th>43</th><td>855625</td><td>M</td><td>19.07</td><td>24.81</td><td>128.3</td><td>1104.0</td><td>0.09081</td></tr><tr><th>44</th><td>856106</td><td>M</td><td>13.28</td><td>20.28</td><td>87.32</td><td>545.2</td><td>0.1041</td></tr><tr><th>45</th><td>85638502</td><td>M</td><td>13.17</td><td>21.81</td><td>85.42</td><td>531.5</td><td>0.09714</td></tr><tr><th>46</th><td>857010</td><td>M</td><td>18.65</td><td>17.6</td><td>123.7</td><td>1076.0</td><td>0.1099</td></tr><tr><th>47</th><td>85713702</td><td>B</td><td>8.196</td><td>16.84</td><td>51.71</td><td>201.9</td><td>0.086</td></tr><tr><th>48</th><td>85715</td><td>M</td><td>13.17</td><td>18.66</td><td>85.98</td><td>534.6</td><td>0.1158</td></tr><tr><th>49</th><td>857155</td><td>B</td><td>12.05</td><td>14.63</td><td>78.04</td><td>449.3</td><td>0.1031</td></tr><tr><th>50</th><td>857156</td><td>B</td><td>13.49</td><td>22.3</td><td>86.91</td><td>561.0</td><td>0.08752</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& id & diagnosis & radius\\_mean & texture\\_mean & perimeter\\_mean & area\\_mean & smoothness\\_mean & \\\\\n",
       "\t\\hline\n",
       "\t& Int64 & String & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 842302 & M & 17.99 & 10.38 & 122.8 & 1001.0 & 0.1184 & $\\dots$ \\\\\n",
       "\t2 & 842517 & M & 20.57 & 17.77 & 132.9 & 1326.0 & 0.08474 & $\\dots$ \\\\\n",
       "\t3 & 84300903 & M & 19.69 & 21.25 & 130.0 & 1203.0 & 0.1096 & $\\dots$ \\\\\n",
       "\t4 & 84348301 & M & 11.42 & 20.38 & 77.58 & 386.1 & 0.1425 & $\\dots$ \\\\\n",
       "\t5 & 84358402 & M & 20.29 & 14.34 & 135.1 & 1297.0 & 0.1003 & $\\dots$ \\\\\n",
       "\t6 & 843786 & M & 12.45 & 15.7 & 82.57 & 477.1 & 0.1278 & $\\dots$ \\\\\n",
       "\t7 & 844359 & M & 18.25 & 19.98 & 119.6 & 1040.0 & 0.09463 & $\\dots$ \\\\\n",
       "\t8 & 84458202 & M & 13.71 & 20.83 & 90.2 & 577.9 & 0.1189 & $\\dots$ \\\\\n",
       "\t9 & 844981 & M & 13.0 & 21.82 & 87.5 & 519.8 & 0.1273 & $\\dots$ \\\\\n",
       "\t10 & 84501001 & M & 12.46 & 24.04 & 83.97 & 475.9 & 0.1186 & $\\dots$ \\\\\n",
       "\t11 & 845636 & M & 16.02 & 23.24 & 102.7 & 797.8 & 0.08206 & $\\dots$ \\\\\n",
       "\t12 & 84610002 & M & 15.78 & 17.89 & 103.6 & 781.0 & 0.0971 & $\\dots$ \\\\\n",
       "\t13 & 846226 & M & 19.17 & 24.8 & 132.4 & 1123.0 & 0.0974 & $\\dots$ \\\\\n",
       "\t14 & 846381 & M & 15.85 & 23.95 & 103.7 & 782.7 & 0.08401 & $\\dots$ \\\\\n",
       "\t15 & 84667401 & M & 13.73 & 22.61 & 93.6 & 578.3 & 0.1131 & $\\dots$ \\\\\n",
       "\t16 & 84799002 & M & 14.54 & 27.54 & 96.73 & 658.8 & 0.1139 & $\\dots$ \\\\\n",
       "\t17 & 848406 & M & 14.68 & 20.13 & 94.74 & 684.5 & 0.09867 & $\\dots$ \\\\\n",
       "\t18 & 84862001 & M & 16.13 & 20.68 & 108.1 & 798.8 & 0.117 & $\\dots$ \\\\\n",
       "\t19 & 849014 & M & 19.81 & 22.15 & 130.0 & 1260.0 & 0.09831 & $\\dots$ \\\\\n",
       "\t20 & 8510426 & B & 13.54 & 14.36 & 87.46 & 566.3 & 0.09779 & $\\dots$ \\\\\n",
       "\t21 & 8510653 & B & 13.08 & 15.71 & 85.63 & 520.0 & 0.1075 & $\\dots$ \\\\\n",
       "\t22 & 8510824 & B & 9.504 & 12.44 & 60.34 & 273.9 & 0.1024 & $\\dots$ \\\\\n",
       "\t23 & 8511133 & M & 15.34 & 14.26 & 102.5 & 704.4 & 0.1073 & $\\dots$ \\\\\n",
       "\t24 & 851509 & M & 21.16 & 23.04 & 137.2 & 1404.0 & 0.09428 & $\\dots$ \\\\\n",
       "\t25 & 852552 & M & 16.65 & 21.38 & 110.0 & 904.6 & 0.1121 & $\\dots$ \\\\\n",
       "\t26 & 852631 & M & 17.14 & 16.4 & 116.0 & 912.7 & 0.1186 & $\\dots$ \\\\\n",
       "\t27 & 852763 & M & 14.58 & 21.53 & 97.41 & 644.8 & 0.1054 & $\\dots$ \\\\\n",
       "\t28 & 852781 & M & 18.61 & 20.25 & 122.1 & 1094.0 & 0.0944 & $\\dots$ \\\\\n",
       "\t29 & 852973 & M & 15.3 & 25.27 & 102.4 & 732.4 & 0.1082 & $\\dots$ \\\\\n",
       "\t30 & 853201 & M & 17.57 & 15.05 & 115.0 & 955.1 & 0.09847 & $\\dots$ \\\\\n",
       "\t31 & 853401 & M & 18.63 & 25.11 & 124.8 & 1088.0 & 0.1064 & $\\dots$ \\\\\n",
       "\t32 & 853612 & M & 11.84 & 18.7 & 77.93 & 440.6 & 0.1109 & $\\dots$ \\\\\n",
       "\t33 & 85382601 & M & 17.02 & 23.98 & 112.8 & 899.3 & 0.1197 & $\\dots$ \\\\\n",
       "\t34 & 854002 & M & 19.27 & 26.47 & 127.9 & 1162.0 & 0.09401 & $\\dots$ \\\\\n",
       "\t35 & 854039 & M & 16.13 & 17.88 & 107.0 & 807.2 & 0.104 & $\\dots$ \\\\\n",
       "\t36 & 854253 & M & 16.74 & 21.59 & 110.1 & 869.5 & 0.0961 & $\\dots$ \\\\\n",
       "\t37 & 854268 & M & 14.25 & 21.72 & 93.63 & 633.0 & 0.09823 & $\\dots$ \\\\\n",
       "\t38 & 854941 & B & 13.03 & 18.42 & 82.61 & 523.8 & 0.08983 & $\\dots$ \\\\\n",
       "\t39 & 855133 & M & 14.99 & 25.2 & 95.54 & 698.8 & 0.09387 & $\\dots$ \\\\\n",
       "\t40 & 855138 & M & 13.48 & 20.82 & 88.4 & 559.2 & 0.1016 & $\\dots$ \\\\\n",
       "\t41 & 855167 & M & 13.44 & 21.58 & 86.18 & 563.0 & 0.08162 & $\\dots$ \\\\\n",
       "\t42 & 855563 & M & 10.95 & 21.35 & 71.9 & 371.1 & 0.1227 & $\\dots$ \\\\\n",
       "\t43 & 855625 & M & 19.07 & 24.81 & 128.3 & 1104.0 & 0.09081 & $\\dots$ \\\\\n",
       "\t44 & 856106 & M & 13.28 & 20.28 & 87.32 & 545.2 & 0.1041 & $\\dots$ \\\\\n",
       "\t45 & 85638502 & M & 13.17 & 21.81 & 85.42 & 531.5 & 0.09714 & $\\dots$ \\\\\n",
       "\t46 & 857010 & M & 18.65 & 17.6 & 123.7 & 1076.0 & 0.1099 & $\\dots$ \\\\\n",
       "\t47 & 85713702 & B & 8.196 & 16.84 & 51.71 & 201.9 & 0.086 & $\\dots$ \\\\\n",
       "\t48 & 85715 & M & 13.17 & 18.66 & 85.98 & 534.6 & 0.1158 & $\\dots$ \\\\\n",
       "\t49 & 857155 & B & 12.05 & 14.63 & 78.04 & 449.3 & 0.1031 & $\\dots$ \\\\\n",
       "\t50 & 857156 & B & 13.49 & 22.3 & 86.91 & 561.0 & 0.08752 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "569×33 DataFrame. Omitted printing of 28 columns\n",
       "│ Row │ id       │ diagnosis │ radius_mean │ texture_mean │ perimeter_mean │\n",
       "│     │ \u001b[90mInt64\u001b[39m    │ \u001b[90mString\u001b[39m    │ \u001b[90mFloat64\u001b[39m     │ \u001b[90mFloat64\u001b[39m      │ \u001b[90mFloat64\u001b[39m        │\n",
       "├─────┼──────────┼───────────┼─────────────┼──────────────┼────────────────┤\n",
       "│ 1   │ 842302   │ M         │ 17.99       │ 10.38        │ 122.8          │\n",
       "│ 2   │ 842517   │ M         │ 20.57       │ 17.77        │ 132.9          │\n",
       "│ 3   │ 84300903 │ M         │ 19.69       │ 21.25        │ 130.0          │\n",
       "│ 4   │ 84348301 │ M         │ 11.42       │ 20.38        │ 77.58          │\n",
       "│ 5   │ 84358402 │ M         │ 20.29       │ 14.34        │ 135.1          │\n",
       "│ 6   │ 843786   │ M         │ 12.45       │ 15.7         │ 82.57          │\n",
       "│ 7   │ 844359   │ M         │ 18.25       │ 19.98        │ 119.6          │\n",
       "│ 8   │ 84458202 │ M         │ 13.71       │ 20.83        │ 90.2           │\n",
       "│ 9   │ 844981   │ M         │ 13.0        │ 21.82        │ 87.5           │\n",
       "│ 10  │ 84501001 │ M         │ 12.46       │ 24.04        │ 83.97          │\n",
       "│ 11  │ 845636   │ M         │ 16.02       │ 23.24        │ 102.7          │\n",
       "│ 12  │ 84610002 │ M         │ 15.78       │ 17.89        │ 103.6          │\n",
       "│ 13  │ 846226   │ M         │ 19.17       │ 24.8         │ 132.4          │\n",
       "│ 14  │ 846381   │ M         │ 15.85       │ 23.95        │ 103.7          │\n",
       "│ 15  │ 84667401 │ M         │ 13.73       │ 22.61        │ 93.6           │\n",
       "│ 16  │ 84799002 │ M         │ 14.54       │ 27.54        │ 96.73          │\n",
       "│ 17  │ 848406   │ M         │ 14.68       │ 20.13        │ 94.74          │\n",
       "│ 18  │ 84862001 │ M         │ 16.13       │ 20.68        │ 108.1          │\n",
       "│ 19  │ 849014   │ M         │ 19.81       │ 22.15        │ 130.0          │\n",
       "│ 20  │ 8510426  │ B         │ 13.54       │ 14.36        │ 87.46          │\n",
       "⋮\n",
       "│ 549 │ 923169   │ B         │ 9.683       │ 19.34        │ 61.05          │\n",
       "│ 550 │ 923465   │ B         │ 10.82       │ 24.21        │ 68.89          │\n",
       "│ 551 │ 923748   │ B         │ 10.86       │ 21.48        │ 68.51          │\n",
       "│ 552 │ 923780   │ B         │ 11.13       │ 22.44        │ 71.49          │\n",
       "│ 553 │ 924084   │ B         │ 12.77       │ 29.43        │ 81.35          │\n",
       "│ 554 │ 924342   │ B         │ 9.333       │ 21.94        │ 59.01          │\n",
       "│ 555 │ 924632   │ B         │ 12.88       │ 28.92        │ 82.5           │\n",
       "│ 556 │ 924934   │ B         │ 10.29       │ 27.61        │ 65.67          │\n",
       "│ 557 │ 924964   │ B         │ 10.16       │ 19.59        │ 64.73          │\n",
       "│ 558 │ 925236   │ B         │ 9.423       │ 27.88        │ 59.26          │\n",
       "│ 559 │ 925277   │ B         │ 14.59       │ 22.68        │ 96.39          │\n",
       "│ 560 │ 925291   │ B         │ 11.51       │ 23.93        │ 74.52          │\n",
       "│ 561 │ 925292   │ B         │ 14.05       │ 27.15        │ 91.38          │\n",
       "│ 562 │ 925311   │ B         │ 11.2        │ 29.37        │ 70.67          │\n",
       "│ 563 │ 925622   │ M         │ 15.22       │ 30.62        │ 103.4          │\n",
       "│ 564 │ 926125   │ M         │ 20.92       │ 25.09        │ 143.0          │\n",
       "│ 565 │ 926424   │ M         │ 21.56       │ 22.39        │ 142.0          │\n",
       "│ 566 │ 926682   │ M         │ 20.13       │ 28.25        │ 131.2          │\n",
       "│ 567 │ 926954   │ M         │ 16.6        │ 28.08        │ 108.3          │\n",
       "│ 568 │ 927241   │ M         │ 20.6        │ 29.33        │ 140.1          │\n",
       "│ 569 │ 92751    │ B         │ 7.76        │ 24.54        │ 47.92          │"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CSV.read(\"./data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th><th>nunique</th><th>nmissing</th></tr><tr><th></th><th>Symbol</th><th>Union…</th><th>Any</th><th>Union…</th><th>Any</th><th>Union…</th><th>Nothing</th></tr></thead><tbody><p>31 rows × 8 columns (omitted printing of 1 columns)</p><tr><th>1</th><td>diagnosis</td><td></td><td>B</td><td></td><td>M</td><td>2</td><td></td></tr><tr><th>2</th><td>radius_mean</td><td>14.1273</td><td>6.981</td><td>13.37</td><td>28.11</td><td></td><td></td></tr><tr><th>3</th><td>texture_mean</td><td>19.2896</td><td>9.71</td><td>18.84</td><td>39.28</td><td></td><td></td></tr><tr><th>4</th><td>perimeter_mean</td><td>91.969</td><td>43.79</td><td>86.24</td><td>188.5</td><td></td><td></td></tr><tr><th>5</th><td>area_mean</td><td>654.889</td><td>143.5</td><td>551.1</td><td>2501.0</td><td></td><td></td></tr><tr><th>6</th><td>smoothness_mean</td><td>0.0963603</td><td>0.05263</td><td>0.09587</td><td>0.1634</td><td></td><td></td></tr><tr><th>7</th><td>compactness_mean</td><td>0.104341</td><td>0.01938</td><td>0.09263</td><td>0.3454</td><td></td><td></td></tr><tr><th>8</th><td>concavity_mean</td><td>0.0887993</td><td>0.0</td><td>0.06154</td><td>0.4268</td><td></td><td></td></tr><tr><th>9</th><td>concave points_mean</td><td>0.0489191</td><td>0.0</td><td>0.0335</td><td>0.2012</td><td></td><td></td></tr><tr><th>10</th><td>symmetry_mean</td><td>0.181162</td><td>0.106</td><td>0.1792</td><td>0.304</td><td></td><td></td></tr><tr><th>11</th><td>fractal_dimension_mean</td><td>0.0627976</td><td>0.04996</td><td>0.06154</td><td>0.09744</td><td></td><td></td></tr><tr><th>12</th><td>radius_se</td><td>0.405172</td><td>0.1115</td><td>0.3242</td><td>2.873</td><td></td><td></td></tr><tr><th>13</th><td>texture_se</td><td>1.21685</td><td>0.3602</td><td>1.108</td><td>4.885</td><td></td><td></td></tr><tr><th>14</th><td>perimeter_se</td><td>2.86606</td><td>0.757</td><td>2.287</td><td>21.98</td><td></td><td></td></tr><tr><th>15</th><td>area_se</td><td>40.3371</td><td>6.802</td><td>24.53</td><td>542.2</td><td></td><td></td></tr><tr><th>16</th><td>smoothness_se</td><td>0.00704098</td><td>0.001713</td><td>0.00638</td><td>0.03113</td><td></td><td></td></tr><tr><th>17</th><td>compactness_se</td><td>0.0254781</td><td>0.002252</td><td>0.02045</td><td>0.1354</td><td></td><td></td></tr><tr><th>18</th><td>concavity_se</td><td>0.0318937</td><td>0.0</td><td>0.02589</td><td>0.396</td><td></td><td></td></tr><tr><th>19</th><td>concave points_se</td><td>0.0117961</td><td>0.0</td><td>0.01093</td><td>0.05279</td><td></td><td></td></tr><tr><th>20</th><td>symmetry_se</td><td>0.0205423</td><td>0.007882</td><td>0.01873</td><td>0.07895</td><td></td><td></td></tr><tr><th>21</th><td>fractal_dimension_se</td><td>0.0037949</td><td>0.0008948</td><td>0.003187</td><td>0.02984</td><td></td><td></td></tr><tr><th>22</th><td>radius_worst</td><td>16.2692</td><td>7.93</td><td>14.97</td><td>36.04</td><td></td><td></td></tr><tr><th>23</th><td>texture_worst</td><td>25.6772</td><td>12.02</td><td>25.41</td><td>49.54</td><td></td><td></td></tr><tr><th>24</th><td>perimeter_worst</td><td>107.261</td><td>50.41</td><td>97.66</td><td>251.2</td><td></td><td></td></tr><tr><th>25</th><td>area_worst</td><td>880.583</td><td>185.2</td><td>686.5</td><td>4254.0</td><td></td><td></td></tr><tr><th>26</th><td>smoothness_worst</td><td>0.132369</td><td>0.07117</td><td>0.1313</td><td>0.2226</td><td></td><td></td></tr><tr><th>27</th><td>compactness_worst</td><td>0.254265</td><td>0.02729</td><td>0.2119</td><td>1.058</td><td></td><td></td></tr><tr><th>28</th><td>concavity_worst</td><td>0.272188</td><td>0.0</td><td>0.2267</td><td>1.252</td><td></td><td></td></tr><tr><th>29</th><td>concave points_worst</td><td>0.114606</td><td>0.0</td><td>0.09993</td><td>0.291</td><td></td><td></td></tr><tr><th>30</th><td>symmetry_worst</td><td>0.290076</td><td>0.1565</td><td>0.2822</td><td>0.6638</td><td></td><td></td></tr><tr><th>31</th><td>fractal_dimension_worst</td><td>0.0839458</td><td>0.05504</td><td>0.08004</td><td>0.2075</td><td></td><td></td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccc}\n",
       "\t& variable & mean & min & median & max & nunique & nmissing & \\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Union… & Any & Union… & Any & Union… & Nothing & \\\\\n",
       "\t\\hline\n",
       "\t1 & diagnosis &  & B &  & M & 2 &  & $\\dots$ \\\\\n",
       "\t2 & radius\\_mean & 14.1273 & 6.981 & 13.37 & 28.11 &  &  & $\\dots$ \\\\\n",
       "\t3 & texture\\_mean & 19.2896 & 9.71 & 18.84 & 39.28 &  &  & $\\dots$ \\\\\n",
       "\t4 & perimeter\\_mean & 91.969 & 43.79 & 86.24 & 188.5 &  &  & $\\dots$ \\\\\n",
       "\t5 & area\\_mean & 654.889 & 143.5 & 551.1 & 2501.0 &  &  & $\\dots$ \\\\\n",
       "\t6 & smoothness\\_mean & 0.0963603 & 0.05263 & 0.09587 & 0.1634 &  &  & $\\dots$ \\\\\n",
       "\t7 & compactness\\_mean & 0.104341 & 0.01938 & 0.09263 & 0.3454 &  &  & $\\dots$ \\\\\n",
       "\t8 & concavity\\_mean & 0.0887993 & 0.0 & 0.06154 & 0.4268 &  &  & $\\dots$ \\\\\n",
       "\t9 & concave points\\_mean & 0.0489191 & 0.0 & 0.0335 & 0.2012 &  &  & $\\dots$ \\\\\n",
       "\t10 & symmetry\\_mean & 0.181162 & 0.106 & 0.1792 & 0.304 &  &  & $\\dots$ \\\\\n",
       "\t11 & fractal\\_dimension\\_mean & 0.0627976 & 0.04996 & 0.06154 & 0.09744 &  &  & $\\dots$ \\\\\n",
       "\t12 & radius\\_se & 0.405172 & 0.1115 & 0.3242 & 2.873 &  &  & $\\dots$ \\\\\n",
       "\t13 & texture\\_se & 1.21685 & 0.3602 & 1.108 & 4.885 &  &  & $\\dots$ \\\\\n",
       "\t14 & perimeter\\_se & 2.86606 & 0.757 & 2.287 & 21.98 &  &  & $\\dots$ \\\\\n",
       "\t15 & area\\_se & 40.3371 & 6.802 & 24.53 & 542.2 &  &  & $\\dots$ \\\\\n",
       "\t16 & smoothness\\_se & 0.00704098 & 0.001713 & 0.00638 & 0.03113 &  &  & $\\dots$ \\\\\n",
       "\t17 & compactness\\_se & 0.0254781 & 0.002252 & 0.02045 & 0.1354 &  &  & $\\dots$ \\\\\n",
       "\t18 & concavity\\_se & 0.0318937 & 0.0 & 0.02589 & 0.396 &  &  & $\\dots$ \\\\\n",
       "\t19 & concave points\\_se & 0.0117961 & 0.0 & 0.01093 & 0.05279 &  &  & $\\dots$ \\\\\n",
       "\t20 & symmetry\\_se & 0.0205423 & 0.007882 & 0.01873 & 0.07895 &  &  & $\\dots$ \\\\\n",
       "\t21 & fractal\\_dimension\\_se & 0.0037949 & 0.0008948 & 0.003187 & 0.02984 &  &  & $\\dots$ \\\\\n",
       "\t22 & radius\\_worst & 16.2692 & 7.93 & 14.97 & 36.04 &  &  & $\\dots$ \\\\\n",
       "\t23 & texture\\_worst & 25.6772 & 12.02 & 25.41 & 49.54 &  &  & $\\dots$ \\\\\n",
       "\t24 & perimeter\\_worst & 107.261 & 50.41 & 97.66 & 251.2 &  &  & $\\dots$ \\\\\n",
       "\t25 & area\\_worst & 880.583 & 185.2 & 686.5 & 4254.0 &  &  & $\\dots$ \\\\\n",
       "\t26 & smoothness\\_worst & 0.132369 & 0.07117 & 0.1313 & 0.2226 &  &  & $\\dots$ \\\\\n",
       "\t27 & compactness\\_worst & 0.254265 & 0.02729 & 0.2119 & 1.058 &  &  & $\\dots$ \\\\\n",
       "\t28 & concavity\\_worst & 0.272188 & 0.0 & 0.2267 & 1.252 &  &  & $\\dots$ \\\\\n",
       "\t29 & concave points\\_worst & 0.114606 & 0.0 & 0.09993 & 0.291 &  &  & $\\dots$ \\\\\n",
       "\t30 & symmetry\\_worst & 0.290076 & 0.1565 & 0.2822 & 0.6638 &  &  & $\\dots$ \\\\\n",
       "\t31 & fractal\\_dimension\\_worst & 0.0839458 & 0.05504 & 0.08004 & 0.2075 &  &  & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "31×8 DataFrame. Omitted printing of 3 columns\n",
       "│ Row │ variable                │ mean       │ min       │ median   │ max     │\n",
       "│     │ \u001b[90mSymbol\u001b[39m                  │ \u001b[90mUnion…\u001b[39m     │ \u001b[90mAny\u001b[39m       │ \u001b[90mUnion…\u001b[39m   │ \u001b[90mAny\u001b[39m     │\n",
       "├─────┼─────────────────────────┼────────────┼───────────┼──────────┼─────────┤\n",
       "│ 1   │ diagnosis               │            │ B         │          │ M       │\n",
       "│ 2   │ radius_mean             │ 14.1273    │ 6.981     │ 13.37    │ 28.11   │\n",
       "│ 3   │ texture_mean            │ 19.2896    │ 9.71      │ 18.84    │ 39.28   │\n",
       "│ 4   │ perimeter_mean          │ 91.969     │ 43.79     │ 86.24    │ 188.5   │\n",
       "│ 5   │ area_mean               │ 654.889    │ 143.5     │ 551.1    │ 2501.0  │\n",
       "│ 6   │ smoothness_mean         │ 0.0963603  │ 0.05263   │ 0.09587  │ 0.1634  │\n",
       "│ 7   │ compactness_mean        │ 0.104341   │ 0.01938   │ 0.09263  │ 0.3454  │\n",
       "│ 8   │ concavity_mean          │ 0.0887993  │ 0.0       │ 0.06154  │ 0.4268  │\n",
       "│ 9   │ concave points_mean     │ 0.0489191  │ 0.0       │ 0.0335   │ 0.2012  │\n",
       "│ 10  │ symmetry_mean           │ 0.181162   │ 0.106     │ 0.1792   │ 0.304   │\n",
       "│ 11  │ fractal_dimension_mean  │ 0.0627976  │ 0.04996   │ 0.06154  │ 0.09744 │\n",
       "│ 12  │ radius_se               │ 0.405172   │ 0.1115    │ 0.3242   │ 2.873   │\n",
       "│ 13  │ texture_se              │ 1.21685    │ 0.3602    │ 1.108    │ 4.885   │\n",
       "│ 14  │ perimeter_se            │ 2.86606    │ 0.757     │ 2.287    │ 21.98   │\n",
       "│ 15  │ area_se                 │ 40.3371    │ 6.802     │ 24.53    │ 542.2   │\n",
       "│ 16  │ smoothness_se           │ 0.00704098 │ 0.001713  │ 0.00638  │ 0.03113 │\n",
       "│ 17  │ compactness_se          │ 0.0254781  │ 0.002252  │ 0.02045  │ 0.1354  │\n",
       "│ 18  │ concavity_se            │ 0.0318937  │ 0.0       │ 0.02589  │ 0.396   │\n",
       "│ 19  │ concave points_se       │ 0.0117961  │ 0.0       │ 0.01093  │ 0.05279 │\n",
       "│ 20  │ symmetry_se             │ 0.0205423  │ 0.007882  │ 0.01873  │ 0.07895 │\n",
       "│ 21  │ fractal_dimension_se    │ 0.0037949  │ 0.0008948 │ 0.003187 │ 0.02984 │\n",
       "│ 22  │ radius_worst            │ 16.2692    │ 7.93      │ 14.97    │ 36.04   │\n",
       "│ 23  │ texture_worst           │ 25.6772    │ 12.02     │ 25.41    │ 49.54   │\n",
       "│ 24  │ perimeter_worst         │ 107.261    │ 50.41     │ 97.66    │ 251.2   │\n",
       "│ 25  │ area_worst              │ 880.583    │ 185.2     │ 686.5    │ 4254.0  │\n",
       "│ 26  │ smoothness_worst        │ 0.132369   │ 0.07117   │ 0.1313   │ 0.2226  │\n",
       "│ 27  │ compactness_worst       │ 0.254265   │ 0.02729   │ 0.2119   │ 1.058   │\n",
       "│ 28  │ concavity_worst         │ 0.272188   │ 0.0       │ 0.2267   │ 1.252   │\n",
       "│ 29  │ concave points_worst    │ 0.114606   │ 0.0       │ 0.09993  │ 0.291   │\n",
       "│ 30  │ symmetry_worst          │ 0.290076   │ 0.1565    │ 0.2822   │ 0.6638  │\n",
       "│ 31  │ fractal_dimension_worst │ 0.0839458  │ 0.05504   │ 0.08004  │ 0.2075  │"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[:, Not([33, 1])]\n",
    "describe(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at class labels to see if dataset is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Int64} with 2 entries:\n",
       "  \"B\" => 357\n",
       "  \"M\" => 212"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = countmap(data[:diagnosis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.6274165202108963\n",
       " 0.37258347978910367"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(label_counts[i] / size(data)[1] for i in keys(label_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m─────────────────────────\u001b[0m┬\u001b[0m─────────────────────────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.names                 \u001b[0m│\u001b[0m\u001b[22m _.types                         \u001b[0m│\u001b[0m\u001b[22m _.scitypes    \u001b[0m│\u001b[0m\n",
       "├\u001b[0m─────────────────────────\u001b[0m┼\u001b[0m─────────────────────────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m diagnosis               \u001b[0m│\u001b[0m CategoricalValue{String,UInt32} \u001b[0m│\u001b[0m Multiclass{2} \u001b[0m│\u001b[0m\n",
       "│\u001b[0m radius_mean             \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m texture_mean            \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m perimeter_mean          \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m area_mean               \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m smoothness_mean         \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m compactness_mean        \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m concavity_mean          \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m concave points_mean     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m symmetry_mean           \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m fractal_dimension_mean  \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m radius_se               \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m texture_se              \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m perimeter_se            \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m area_se                 \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m smoothness_se           \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m compactness_se          \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m concavity_se            \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m concave points_se       \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m symmetry_se             \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m fractal_dimension_se    \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m radius_worst            \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m texture_worst           \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m perimeter_worst         \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m area_worst              \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m smoothness_worst        \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m compactness_worst       \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m concavity_worst         \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m concave points_worst    \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m symmetry_worst          \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m fractal_dimension_worst \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "└\u001b[0m─────────────────────────\u001b[0m┴\u001b[0m─────────────────────────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┘\u001b[0m\n",
       "_.nrows = 569\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coerce!(data, :diagnosis=>Multiclass)\n",
    "schema(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CategoricalValue{String,UInt32}[\"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\"  …  \"B\", \"B\", \"B\", \"M\", \"M\", \"M\", \"M\", \"M\", \"M\", \"B\"], 569×30 DataFrame. Omitted printing of 26 columns\n",
       "│ Row │ radius_mean │ texture_mean │ perimeter_mean │ area_mean │\n",
       "│     │ \u001b[90mFloat64\u001b[39m     │ \u001b[90mFloat64\u001b[39m      │ \u001b[90mFloat64\u001b[39m        │ \u001b[90mFloat64\u001b[39m   │\n",
       "├─────┼─────────────┼──────────────┼────────────────┼───────────┤\n",
       "│ 1   │ 17.99       │ 10.38        │ 122.8          │ 1001.0    │\n",
       "│ 2   │ 20.57       │ 17.77        │ 132.9          │ 1326.0    │\n",
       "│ 3   │ 19.69       │ 21.25        │ 130.0          │ 1203.0    │\n",
       "│ 4   │ 11.42       │ 20.38        │ 77.58          │ 386.1     │\n",
       "│ 5   │ 20.29       │ 14.34        │ 135.1          │ 1297.0    │\n",
       "│ 6   │ 12.45       │ 15.7         │ 82.57          │ 477.1     │\n",
       "│ 7   │ 18.25       │ 19.98        │ 119.6          │ 1040.0    │\n",
       "│ 8   │ 13.71       │ 20.83        │ 90.2           │ 577.9     │\n",
       "│ 9   │ 13.0        │ 21.82        │ 87.5           │ 519.8     │\n",
       "│ 10  │ 12.46       │ 24.04        │ 83.97          │ 475.9     │\n",
       "│ 11  │ 16.02       │ 23.24        │ 102.7          │ 797.8     │\n",
       "│ 12  │ 15.78       │ 17.89        │ 103.6          │ 781.0     │\n",
       "│ 13  │ 19.17       │ 24.8         │ 132.4          │ 1123.0    │\n",
       "│ 14  │ 15.85       │ 23.95        │ 103.7          │ 782.7     │\n",
       "│ 15  │ 13.73       │ 22.61        │ 93.6           │ 578.3     │\n",
       "│ 16  │ 14.54       │ 27.54        │ 96.73          │ 658.8     │\n",
       "│ 17  │ 14.68       │ 20.13        │ 94.74          │ 684.5     │\n",
       "│ 18  │ 16.13       │ 20.68        │ 108.1          │ 798.8     │\n",
       "│ 19  │ 19.81       │ 22.15        │ 130.0          │ 1260.0    │\n",
       "│ 20  │ 13.54       │ 14.36        │ 87.46          │ 566.3     │\n",
       "⋮\n",
       "│ 549 │ 9.683       │ 19.34        │ 61.05          │ 285.7     │\n",
       "│ 550 │ 10.82       │ 24.21        │ 68.89          │ 361.6     │\n",
       "│ 551 │ 10.86       │ 21.48        │ 68.51          │ 360.5     │\n",
       "│ 552 │ 11.13       │ 22.44        │ 71.49          │ 378.4     │\n",
       "│ 553 │ 12.77       │ 29.43        │ 81.35          │ 507.9     │\n",
       "│ 554 │ 9.333       │ 21.94        │ 59.01          │ 264.0     │\n",
       "│ 555 │ 12.88       │ 28.92        │ 82.5           │ 514.3     │\n",
       "│ 556 │ 10.29       │ 27.61        │ 65.67          │ 321.4     │\n",
       "│ 557 │ 10.16       │ 19.59        │ 64.73          │ 311.7     │\n",
       "│ 558 │ 9.423       │ 27.88        │ 59.26          │ 271.3     │\n",
       "│ 559 │ 14.59       │ 22.68        │ 96.39          │ 657.1     │\n",
       "│ 560 │ 11.51       │ 23.93        │ 74.52          │ 403.5     │\n",
       "│ 561 │ 14.05       │ 27.15        │ 91.38          │ 600.4     │\n",
       "│ 562 │ 11.2        │ 29.37        │ 70.67          │ 386.0     │\n",
       "│ 563 │ 15.22       │ 30.62        │ 103.4          │ 716.9     │\n",
       "│ 564 │ 20.92       │ 25.09        │ 143.0          │ 1347.0    │\n",
       "│ 565 │ 21.56       │ 22.39        │ 142.0          │ 1479.0    │\n",
       "│ 566 │ 20.13       │ 28.25        │ 131.2          │ 1261.0    │\n",
       "│ 567 │ 16.6        │ 28.08        │ 108.3          │ 858.1     │\n",
       "│ 568 │ 20.6        │ 29.33        │ 140.1          │ 1265.0    │\n",
       "│ 569 │ 7.76        │ 24.54        │ 47.92          │ 181.0     │)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, X = unpack(data, ==(:diagnosis), colname->true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition train and test data accoring to class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([483, 534, 392, 159, 390, 31, 320, 27, 170, 416  …  472, 339, 492, 524, 279, 7, 6, 178, 190, 76], [462, 24, 105, 98, 536, 223, 400, 382, 187, 361  …  500, 395, 533, 112, 396, 297, 106, 303, 415, 261])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data to use when trying to fit a single validation set\n",
    "train, test = partition(eachindex(y), 0.7, shuffle=true, rng=123) # gives 70:30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.628140703517588\n",
       " 0.37185929648241206"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts = countmap(data[train,:diagnosis])\n",
    "collect(train_counts[i] / size(train)[1] for i in keys(train_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.6257309941520468\n",
       " 0.3742690058479532"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_counts = countmap(data[test,:diagnosis])\n",
    "collect(test_counts[i] / size(test)[1] for i in keys(test_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five Learning Algorithms\n",
    "\n",
    "* Decision trees with some form of pruning\n",
    "* Neural networks\n",
    "* Boosting\n",
    "* Support Vector Machines\n",
    "* k-nearest neighbors\n",
    "\n",
    "\n",
    "##### Testing\n",
    "* Implement the algorithms\n",
    "* Design two *interesting* classification problems. For the purposes of this assignment, a classification problem is just a set of training examples and a set of test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43-element Array{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype),T} where T<:Tuple,1}:\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianLDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianQDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = ScikitLearn, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " (name = ExtraTreesClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GaussianNBClassifier, package_name = NaiveBayes, ... )\n",
       " (name = GaussianNBClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GaussianProcessClassifier, package_name = ScikitLearn, ... )\n",
       " (name = GradientBoostingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = KNNClassifier, package_name = NearestNeighbors, ... )\n",
       " (name = KNeighborsClassifier, package_name = ScikitLearn, ... )\n",
       " (name = LDA, package_name = MultivariateStats, ... )\n",
       " (name = LGBMClassifier, package_name = LightGBM, ... )\n",
       " (name = LinearBinaryClassifier, package_name = GLM, ... )\n",
       " (name = LinearSVC, package_name = LIBSVM, ... )\n",
       " (name = LogisticCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = LogisticClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = LogisticClassifier, package_name = ScikitLearn, ... )\n",
       " (name = MultinomialClassifier, package_name = MLJLinearModels, ... )\n",
       " (name = NeuralNetworkClassifier, package_name = MLJFlux, ... )\n",
       " (name = NuSVC, package_name = LIBSVM, ... )\n",
       " (name = PassiveAggressiveClassifier, package_name = ScikitLearn, ... )\n",
       " (name = PerceptronClassifier, package_name = ScikitLearn, ... )\n",
       " (name = ProbabilisticSGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVC, package_name = LIBSVM, ... )\n",
       " (name = SVMClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(matching(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling MLJScikitLearnInterface [5ae90465-5518-4432-b9d2-8a1def2f0cab]\n",
      "└ @ Base loading.jl:1278\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVMClassifier(\n",
       "    C = 1.0,\n",
       "    kernel = \"rbf\",\n",
       "    degree = 3,\n",
       "    gamma = \"auto\",\n",
       "    coef0 = 0.0,\n",
       "    shrinking = true,\n",
       "    tol = 0.001,\n",
       "    cache_size = 200,\n",
       "    max_iter = -1,\n",
       "    decision_function_shape = \"ovr\",\n",
       "    random_state = nothing)\u001b[34m @189\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load SVMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines\n",
    "* This should be done in such a way that you can swap out kernel functions. I'd like to see at least two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{SVMClassifier} @439\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  17%[====>                    ]  ETA: 0:00:12\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:06\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:04\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:03\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:01\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:07\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m─────────────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┬\u001b[0m───────────────────────────────────────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.measure           \u001b[0m│\u001b[0m\u001b[22m _.measurement \u001b[0m│\u001b[0m\u001b[22m _.per_fold                                    \u001b[0m│\u001b[0m\n",
       "├\u001b[0m─────────────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┼\u001b[0m───────────────────────────────────────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m accuracy            \u001b[0m│\u001b[0m 0.956         \u001b[0m│\u001b[0m [0.937, 0.968, 0.926, 0.958, 0.979, 0.968]    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m false_negative_rate \u001b[0m│\u001b[0m 0.0736        \u001b[0m│\u001b[0m [0.116, 0.027, 0.0857, 0.118, 0.0625, 0.0323] \u001b[0m│\u001b[0m\n",
       "│\u001b[0m false_positive_rate \u001b[0m│\u001b[0m 0.0254        \u001b[0m│\u001b[0m [0.0192, 0.0345, 0.0667, 0.0, 0.0, 0.0317]    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m true_negative_rate  \u001b[0m│\u001b[0m 0.975         \u001b[0m│\u001b[0m [0.981, 0.966, 0.933, 1.0, 1.0, 0.968]        \u001b[0m│\u001b[0m\n",
       "│\u001b[0m true_positive_rate  \u001b[0m│\u001b[0m 0.926         \u001b[0m│\u001b[0m [0.884, 0.973, 0.914, 0.882, 0.938, 0.968]    \u001b[0m│\u001b[0m\n",
       "└\u001b[0m─────────────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┴\u001b[0m───────────────────────────────────────────────\u001b[0m┘\u001b[0m\n",
       "_.per_observation = [missing, missing, missing, missing, missing]\n",
       "_.fitted_params_per_fold = [ … ]\n",
       "_.report_per_fold = [ … ]\n"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVMClassifier(kernel=\"linear\")\n",
    "svm_mach = machine(svm_model, X, y)\n",
    "fit!(svm_mach, rows=train, verbosity=2)\n",
    "svm_acc = evaluate!(svm_mach, resampling=CV(shuffle=true), measure=[accuracy, fnr, fpr, tnr, tpr], \n",
    "                        verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{SVMClassifier} @029\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  17%[====>                    ]  ETA: 0:00:00\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:00\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:00\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:00\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:00\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m─────────────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┬\u001b[0m────────────────────────────────────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.measure           \u001b[0m│\u001b[0m\u001b[22m _.measurement \u001b[0m│\u001b[0m\u001b[22m _.per_fold                                 \u001b[0m│\u001b[0m\n",
       "├\u001b[0m─────────────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┼\u001b[0m────────────────────────────────────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m accuracy            \u001b[0m│\u001b[0m 0.627         \u001b[0m│\u001b[0m [0.621, 0.632, 0.611, 0.663, 0.642, 0.596] \u001b[0m│\u001b[0m\n",
       "│\u001b[0m false_negative_rate \u001b[0m│\u001b[0m 1.0           \u001b[0m│\u001b[0m [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]             \u001b[0m│\u001b[0m\n",
       "│\u001b[0m false_positive_rate \u001b[0m│\u001b[0m 0.0           \u001b[0m│\u001b[0m [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]             \u001b[0m│\u001b[0m\n",
       "│\u001b[0m true_negative_rate  \u001b[0m│\u001b[0m 1.0           \u001b[0m│\u001b[0m [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]             \u001b[0m│\u001b[0m\n",
       "│\u001b[0m true_positive_rate  \u001b[0m│\u001b[0m 0.0           \u001b[0m│\u001b[0m [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]             \u001b[0m│\u001b[0m\n",
       "└\u001b[0m─────────────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┴\u001b[0m────────────────────────────────────────────\u001b[0m┘\u001b[0m\n",
       "_.per_observation = [missing, missing, missing, missing, missing]\n",
       "_.fitted_params_per_fold = [ … ]\n",
       "_.report_per_fold = [ … ]\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVMClassifier(kernel=\"sigmoid\")\n",
    "svm_mach = machine(svm_model, X, y)\n",
    "fit!(svm_mach, rows=train, verbosity=2)\n",
    "svm_acc = evaluate!(svm_mach, resampling=CV(shuffle=true), measure=[accuracy, fnr, fpr, tnr, tpr], \n",
    "                        verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{SVMClassifier} @524\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  17%[====>                    ]  ETA: 0:00:00\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:00\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:00\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:00\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:00\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m─────────────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┬\u001b[0m────────────────────────────────────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.measure           \u001b[0m│\u001b[0m\u001b[22m _.measurement \u001b[0m│\u001b[0m\u001b[22m _.per_fold                                 \u001b[0m│\u001b[0m\n",
       "├\u001b[0m─────────────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┼\u001b[0m────────────────────────────────────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m accuracy            \u001b[0m│\u001b[0m 0.627         \u001b[0m│\u001b[0m [0.579, 0.632, 0.695, 0.674, 0.568, 0.617] \u001b[0m│\u001b[0m\n",
       "│\u001b[0m false_negative_rate \u001b[0m│\u001b[0m 1.0           \u001b[0m│\u001b[0m [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]             \u001b[0m│\u001b[0m\n",
       "│\u001b[0m false_positive_rate \u001b[0m│\u001b[0m 0.0           \u001b[0m│\u001b[0m [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]             \u001b[0m│\u001b[0m\n",
       "│\u001b[0m true_negative_rate  \u001b[0m│\u001b[0m 1.0           \u001b[0m│\u001b[0m [1.0, 1.0, 1.0, 1.0, 1.0, 1.0]             \u001b[0m│\u001b[0m\n",
       "│\u001b[0m true_positive_rate  \u001b[0m│\u001b[0m 0.0           \u001b[0m│\u001b[0m [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]             \u001b[0m│\u001b[0m\n",
       "└\u001b[0m─────────────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┴\u001b[0m────────────────────────────────────────────\u001b[0m┘\u001b[0m\n",
       "_.per_observation = [missing, missing, missing, missing, missing]\n",
       "_.fitted_params_per_fold = [ … ]\n",
       "_.report_per_fold = [ … ]\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVMClassifier(kernel=\"rbf\")\n",
    "svm_mach = machine(svm_model, X, y)\n",
    "fit!(svm_mach, rows=train, verbosity=2)\n",
    "svm_acc = evaluate!(svm_mach, resampling=CV(shuffle=true), measure=[accuracy, fnr, fpr, tnr, tpr], \n",
    "                        verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Poly Degree 2 took a long wall clock time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{SVMClassifier} @297\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  17%[====>                    ]  ETA: 0:08:36\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  33%[========>                ]  ETA: 0:04:32\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  50%[============>            ]  ETA: 0:02:53\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  67%[================>        ]  ETA: 0:01:40\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:47\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:04:58\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m─────────────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┬\u001b[0m──────────────────────────────────────────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.measure           \u001b[0m│\u001b[0m\u001b[22m _.measurement \u001b[0m│\u001b[0m\u001b[22m _.per_fold                                       \u001b[0m│\u001b[0m\n",
       "├\u001b[0m─────────────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┼\u001b[0m──────────────────────────────────────────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m accuracy            \u001b[0m│\u001b[0m 0.951         \u001b[0m│\u001b[0m [0.916, 0.968, 0.968, 0.926, 0.958, 0.968]       \u001b[0m│\u001b[0m\n",
       "│\u001b[0m false_negative_rate \u001b[0m│\u001b[0m 0.0749        \u001b[0m│\u001b[0m [0.0833, 0.0625, 0.0571, 0.135, 0.0526, 0.0588]  \u001b[0m│\u001b[0m\n",
       "│\u001b[0m false_positive_rate \u001b[0m│\u001b[0m 0.0339        \u001b[0m│\u001b[0m [0.0847, 0.0159, 0.0167, 0.0345, 0.0351, 0.0167] \u001b[0m│\u001b[0m\n",
       "│\u001b[0m true_negative_rate  \u001b[0m│\u001b[0m 0.966         \u001b[0m│\u001b[0m [0.915, 0.984, 0.983, 0.966, 0.965, 0.983]       \u001b[0m│\u001b[0m\n",
       "│\u001b[0m true_positive_rate  \u001b[0m│\u001b[0m 0.925         \u001b[0m│\u001b[0m [0.917, 0.938, 0.943, 0.865, 0.947, 0.941]       \u001b[0m│\u001b[0m\n",
       "└\u001b[0m─────────────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┴\u001b[0m──────────────────────────────────────────────────\u001b[0m┘\u001b[0m\n",
       "_.per_observation = [missing, missing, missing, missing, missing]\n",
       "_.fitted_params_per_fold = [ … ]\n",
       "_.report_per_fold = [ … ]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVMClassifier(kernel=\"poly\", degree=2)\n",
    "svm_mach = machine(svm_model, X, y)\n",
    "fit!(svm_mach, rows=train, verbosity=2)\n",
    "svm_acc = evaluate!(svm_mach, resampling=CV(shuffle=true), measure=[accuracy, fnr, fpr, tnr, tpr], \n",
    "                        verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can't use crossentropy because it is a probabilistic concept, and SVM doesn't do probabilities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch / RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `linear` and `rbf`: \n",
    "* `linear` which has high bias\n",
    "* `rbf` which has high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First look at `linear`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{SVMClassifier} @570\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @014\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @972\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_linear_model = SVMClassifier(kernel=\"linear\", cache_size=1000)\n",
    "svm_linear_mach = machine(svm_linear_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{DeterministicTunedModel{Grid,…}} @638\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      "┌ Info: Attempting to evaluate 60 models.\n",
      "└ @ MLJTuning /home/andrew/.julia/packages/MLJTuning/Bbgvk/src/tuned_models.jl:494\n",
      "\u001b[33mEvaluating over 60 metamodels: 100%[=========================] Time: 0:15:12\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(parameter_name = \"C\",\n",
       " parameter_scale = :log10,\n",
       " parameter_values = [0.004999999999999999, 0.006319241014671491, 0.00798656140030127, 0.010093801273395193, 0.012757032600156442, 0.01612295272648197, 0.020376964829358878, 0.02575339038084062, 0.03254837615229085, 0.04113620670850234  …  607.7371250381435, 768.087473335914, 970.7459728719407, 1226.8755533199082, 1550.584463287389, 1959.7033874236104, 2476.767604479585, 3130.2582860074103, 3956.1713094906586, 4999.999999999999],\n",
       " measurements = [0.9402388951101157, 0.9402388951101157, 0.9384845091452035, 0.9420119447555058, 0.9402575587905936, 0.9402575587905936, 0.9402575587905936, 0.9420306084359836, 0.9402762224710712, 0.9385218365061588  …  0.9490668159761105, 0.9490668159761105, 0.9473124300111984, 0.9543299738708472, 0.952575587905935, 0.952575587905935, 0.950802538260545, 0.9473124300111984, 0.952575587905935, 0.9508212019410228],)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# r1 = range(svm_linear_model, :C, lower=5*10^-6, upper=5*10^4, scale=:log10)\n",
    "r1 = range(svm_linear_model, :C, lower=5*10^-3, upper=5*10^3, scale=:log10)\n",
    "curve = learning_curve(svm_linear_mach, \n",
    "                        range=r1,\n",
    "                        resampling=CV(), \n",
    "                        measure=accuracy, \n",
    "                        acceleration=CPUProcesses(),\n",
    "                        resolution=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip830\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip830)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip831\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip830)\" d=\"\n",
       "M292.458 1410.9 L2352.76 1410.9 L2352.76 47.2441 L292.458 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip832\">\n",
       "    <rect x=\"292\" y=\"47\" width=\"2061\" height=\"1365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  448.286,1410.9 448.286,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  772.232,1410.9 772.232,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1096.18,1410.9 1096.18,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1420.12,1410.9 1420.12,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1744.07,1410.9 1744.07,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2068.02,1410.9 2068.02,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  292.458,1317.33 2352.76,1317.33 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  292.458,1134.4 2352.76,1134.4 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  292.458,951.47 2352.76,951.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  292.458,768.538 2352.76,768.538 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  292.458,585.606 2352.76,585.606 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  292.458,402.674 2352.76,402.674 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip832)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  292.458,219.742 2352.76,219.742 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,1410.9 2352.76,1410.9 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,1410.9 292.458,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  448.286,1410.9 448.286,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  772.232,1410.9 772.232,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1096.18,1410.9 1096.18,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1420.12,1410.9 1420.12,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1744.07,1410.9 1744.07,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2068.02,1410.9 2068.02,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,1317.33 317.182,1317.33 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,1134.4 317.182,1134.4 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,951.47 317.182,951.47 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,768.538 317.182,768.538 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,585.606 317.182,585.606 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,402.674 317.182,402.674 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  292.458,219.742 317.182,219.742 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip830)\" d=\"M 0 0 M402.78 1481.97 L410.419 1481.97 L410.419 1455.6 L402.109 1457.27 L402.109 1453.01 L410.373 1451.34 L415.049 1451.34 L415.049 1481.97 L422.687 1481.97 L422.687 1485.9 L402.78 1485.9 L402.78 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M437.757 1454.42 Q434.146 1454.42 432.317 1457.99 Q430.511 1461.53 430.511 1468.66 Q430.511 1475.77 432.317 1479.33 Q434.146 1482.87 437.757 1482.87 Q441.391 1482.87 443.197 1479.33 Q445.025 1475.77 445.025 1468.66 Q445.025 1461.53 443.197 1457.99 Q441.391 1454.42 437.757 1454.42 M437.757 1450.72 Q443.567 1450.72 446.623 1455.33 Q449.701 1459.91 449.701 1468.66 Q449.701 1477.39 446.623 1481.99 Q443.567 1486.58 437.757 1486.58 Q431.947 1486.58 428.868 1481.99 Q425.812 1477.39 425.812 1468.66 Q425.812 1459.91 428.868 1455.33 Q431.947 1450.72 437.757 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M449.701 1444.82 L473.813 1444.82 L473.813 1448.02 L449.701 1448.02 L449.701 1444.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M481.204 1455.3 L494.464 1455.3 L494.464 1458.49 L476.634 1458.49 L476.634 1455.3 Q478.797 1453.06 482.521 1449.3 Q486.264 1445.52 487.223 1444.43 Q489.047 1442.38 489.762 1440.96 Q490.495 1439.54 490.495 1438.16 Q490.495 1435.92 488.915 1434.51 Q487.354 1433.1 484.834 1433.1 Q483.047 1433.1 481.054 1433.72 Q479.079 1434.34 476.822 1435.6 L476.822 1431.77 Q479.117 1430.85 481.11 1430.38 Q483.104 1429.91 484.759 1429.91 Q489.122 1429.91 491.718 1432.09 Q494.313 1434.27 494.313 1437.92 Q494.313 1439.65 493.655 1441.21 Q493.015 1442.75 491.304 1444.86 Q490.834 1445.4 488.314 1448.02 Q485.793 1450.61 481.204 1455.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M726.576 1481.97 L734.215 1481.97 L734.215 1455.6 L725.905 1457.27 L725.905 1453.01 L734.168 1451.34 L738.844 1451.34 L738.844 1481.97 L746.483 1481.97 L746.483 1485.9 L726.576 1485.9 L726.576 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M761.552 1454.42 Q757.941 1454.42 756.113 1457.99 Q754.307 1461.53 754.307 1468.66 Q754.307 1475.77 756.113 1479.33 Q757.941 1482.87 761.552 1482.87 Q765.187 1482.87 766.992 1479.33 Q768.821 1475.77 768.821 1468.66 Q768.821 1461.53 766.992 1457.99 Q765.187 1454.42 761.552 1454.42 M761.552 1450.72 Q767.363 1450.72 770.418 1455.33 Q773.497 1459.91 773.497 1468.66 Q773.497 1477.39 770.418 1481.99 Q767.363 1486.58 761.552 1486.58 Q755.742 1486.58 752.664 1481.99 Q749.608 1477.39 749.608 1468.66 Q749.608 1459.91 752.664 1455.33 Q755.742 1450.72 761.552 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M773.497 1444.82 L797.608 1444.82 L797.608 1448.02 L773.497 1448.02 L773.497 1444.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M802.386 1455.3 L808.592 1455.3 L808.592 1433.87 L801.84 1435.23 L801.84 1431.77 L808.555 1430.41 L812.354 1430.41 L812.354 1455.3 L818.56 1455.3 L818.56 1458.49 L802.386 1458.49 L802.386 1455.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M1063.35 1481.97 L1070.99 1481.97 L1070.99 1455.6 L1062.68 1457.27 L1062.68 1453.01 L1070.94 1451.34 L1075.62 1451.34 L1075.62 1481.97 L1083.26 1481.97 L1083.26 1485.9 L1063.35 1485.9 L1063.35 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M1098.33 1454.42 Q1094.71 1454.42 1092.89 1457.99 Q1091.08 1461.53 1091.08 1468.66 Q1091.08 1475.77 1092.89 1479.33 Q1094.71 1482.87 1098.33 1482.87 Q1101.96 1482.87 1103.77 1479.33 Q1105.59 1475.77 1105.59 1468.66 Q1105.59 1461.53 1103.77 1457.99 Q1101.96 1454.42 1098.33 1454.42 M1098.33 1450.72 Q1104.14 1450.72 1107.19 1455.33 Q1110.27 1459.91 1110.27 1468.66 Q1110.27 1477.39 1107.19 1481.99 Q1104.14 1486.58 1098.33 1486.58 Q1092.52 1486.58 1089.44 1481.99 Q1086.38 1477.39 1086.38 1468.66 Q1086.38 1459.91 1089.44 1455.33 Q1092.52 1450.72 1098.33 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M1119.97 1432.91 Q1117.04 1432.91 1115.55 1435.81 Q1114.09 1438.69 1114.09 1444.48 Q1114.09 1450.26 1115.55 1453.15 Q1117.04 1456.03 1119.97 1456.03 Q1122.93 1456.03 1124.39 1453.15 Q1125.88 1450.26 1125.88 1444.48 Q1125.88 1438.69 1124.39 1435.81 Q1122.93 1432.91 1119.97 1432.91 M1119.97 1429.91 Q1124.7 1429.91 1127.18 1433.65 Q1129.68 1437.37 1129.68 1444.48 Q1129.68 1451.57 1127.18 1455.31 Q1124.7 1459.04 1119.97 1459.04 Q1115.25 1459.04 1112.75 1455.31 Q1110.27 1451.57 1110.27 1444.48 Q1110.27 1437.37 1112.75 1433.65 Q1115.25 1429.91 1119.97 1429.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M1388.64 1481.97 L1396.28 1481.97 L1396.28 1455.6 L1387.97 1457.27 L1387.97 1453.01 L1396.23 1451.34 L1400.91 1451.34 L1400.91 1481.97 L1408.55 1481.97 L1408.55 1485.9 L1388.64 1485.9 L1388.64 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M1423.62 1454.42 Q1420.01 1454.42 1418.18 1457.99 Q1416.37 1461.53 1416.37 1468.66 Q1416.37 1475.77 1418.18 1479.33 Q1420.01 1482.87 1423.62 1482.87 Q1427.25 1482.87 1429.06 1479.33 Q1430.88 1475.77 1430.88 1468.66 Q1430.88 1461.53 1429.06 1457.99 Q1427.25 1454.42 1423.62 1454.42 M1423.62 1450.72 Q1429.43 1450.72 1432.48 1455.33 Q1435.56 1459.91 1435.56 1468.66 Q1435.56 1477.39 1432.48 1481.99 Q1429.43 1486.58 1423.62 1486.58 Q1417.81 1486.58 1414.73 1481.99 Q1411.67 1477.39 1411.67 1468.66 Q1411.67 1459.91 1414.73 1455.33 Q1417.81 1450.72 1423.62 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M1436.11 1455.3 L1442.31 1455.3 L1442.31 1433.87 L1435.56 1435.23 L1435.56 1431.77 L1442.28 1430.41 L1446.07 1430.41 L1446.07 1455.3 L1452.28 1455.3 L1452.28 1458.49 L1436.11 1458.49 L1436.11 1455.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M1712.03 1481.97 L1719.67 1481.97 L1719.67 1455.6 L1711.36 1457.27 L1711.36 1453.01 L1719.62 1451.34 L1724.3 1451.34 L1724.3 1481.97 L1731.94 1481.97 L1731.94 1485.9 L1712.03 1485.9 L1712.03 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M1747.01 1454.42 Q1743.4 1454.42 1741.57 1457.99 Q1739.76 1461.53 1739.76 1468.66 Q1739.76 1475.77 1741.57 1479.33 Q1743.4 1482.87 1747.01 1482.87 Q1750.64 1482.87 1752.45 1479.33 Q1754.28 1475.77 1754.28 1468.66 Q1754.28 1461.53 1752.45 1457.99 Q1750.64 1454.42 1747.01 1454.42 M1747.01 1450.72 Q1752.82 1450.72 1755.87 1455.33 Q1758.95 1459.91 1758.95 1468.66 Q1758.95 1477.39 1755.87 1481.99 Q1752.82 1486.58 1747.01 1486.58 Q1741.2 1486.58 1738.12 1481.99 Q1735.06 1477.39 1735.06 1468.66 Q1735.06 1459.91 1738.12 1455.33 Q1741.2 1450.72 1747.01 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M1763.52 1455.3 L1776.78 1455.3 L1776.78 1458.49 L1758.95 1458.49 L1758.95 1455.3 Q1761.12 1453.06 1764.84 1449.3 Q1768.58 1445.52 1769.54 1444.43 Q1771.37 1442.38 1772.08 1440.96 Q1772.81 1439.54 1772.81 1438.16 Q1772.81 1435.92 1771.23 1434.51 Q1769.67 1433.1 1767.15 1433.1 Q1765.37 1433.1 1763.37 1433.72 Q1761.4 1434.34 1759.14 1435.6 L1759.14 1431.77 Q1761.43 1430.85 1763.43 1430.38 Q1765.42 1429.91 1767.08 1429.91 Q1771.44 1429.91 1774.04 1432.09 Q1776.63 1434.27 1776.63 1437.92 Q1776.63 1439.65 1775.97 1441.21 Q1775.33 1442.75 1773.62 1444.86 Q1773.15 1445.4 1770.63 1448.02 Q1768.11 1450.61 1763.52 1455.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M2035.65 1481.97 L2043.29 1481.97 L2043.29 1455.6 L2034.98 1457.27 L2034.98 1453.01 L2043.24 1451.34 L2047.92 1451.34 L2047.92 1481.97 L2055.56 1481.97 L2055.56 1485.9 L2035.65 1485.9 L2035.65 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M2070.62 1454.42 Q2067.01 1454.42 2065.18 1457.99 Q2063.38 1461.53 2063.38 1468.66 Q2063.38 1475.77 2065.18 1479.33 Q2067.01 1482.87 2070.62 1482.87 Q2074.26 1482.87 2076.06 1479.33 Q2077.89 1475.77 2077.89 1468.66 Q2077.89 1461.53 2076.06 1457.99 Q2074.26 1454.42 2070.62 1454.42 M2070.62 1450.72 Q2076.43 1450.72 2079.49 1455.33 Q2082.57 1459.91 2082.57 1468.66 Q2082.57 1477.39 2079.49 1481.99 Q2076.43 1486.58 2070.62 1486.58 Q2064.81 1486.58 2061.74 1481.99 Q2058.68 1477.39 2058.68 1468.66 Q2058.68 1459.91 2061.74 1455.33 Q2064.81 1450.72 2070.62 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M2095.26 1443.35 Q2097.99 1443.94 2099.51 1445.78 Q2101.06 1447.62 2101.06 1450.33 Q2101.06 1454.49 2098.2 1456.76 Q2095.34 1459.04 2090.07 1459.04 Q2088.31 1459.04 2086.42 1458.68 Q2084.56 1458.34 2082.57 1457.65 L2082.57 1453.98 Q2084.15 1454.9 2086.03 1455.37 Q2087.91 1455.84 2089.96 1455.84 Q2093.53 1455.84 2095.4 1454.43 Q2097.28 1453.02 2097.28 1450.33 Q2097.28 1447.85 2095.53 1446.46 Q2093.8 1445.05 2090.69 1445.05 L2087.42 1445.05 L2087.42 1441.92 L2090.84 1441.92 Q2093.65 1441.92 2095.13 1440.81 Q2096.62 1439.69 2096.62 1437.58 Q2096.62 1435.42 2095.08 1434.27 Q2093.55 1433.1 2090.69 1433.1 Q2089.13 1433.1 2087.35 1433.44 Q2085.56 1433.78 2083.42 1434.49 L2083.42 1431.11 Q2085.58 1430.51 2087.46 1430.21 Q2089.36 1429.91 2091.03 1429.91 Q2095.36 1429.91 2097.88 1431.88 Q2100.4 1433.84 2100.4 1437.18 Q2100.4 1439.52 2099.06 1441.13 Q2097.73 1442.73 2095.26 1443.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M141.237 1303.13 Q137.626 1303.13 135.797 1306.7 Q133.992 1310.24 133.992 1317.37 Q133.992 1324.48 135.797 1328.04 Q137.626 1331.58 141.237 1331.58 Q144.871 1331.58 146.677 1328.04 Q148.505 1324.48 148.505 1317.37 Q148.505 1310.24 146.677 1306.7 Q144.871 1303.13 141.237 1303.13 M141.237 1299.43 Q147.047 1299.43 150.103 1304.04 Q153.181 1308.62 153.181 1317.37 Q153.181 1326.1 150.103 1330.7 Q147.047 1335.29 141.237 1335.29 Q135.427 1335.29 132.348 1330.7 Q129.293 1326.1 129.293 1317.37 Q129.293 1308.62 132.348 1304.04 Q135.427 1299.43 141.237 1299.43 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M158.251 1328.73 L163.135 1328.73 L163.135 1334.61 L158.251 1334.61 L158.251 1328.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M168.343 1333.9 L168.343 1329.64 Q170.103 1330.47 171.908 1330.91 Q173.714 1331.35 175.45 1331.35 Q180.079 1331.35 182.51 1328.25 Q184.964 1325.12 185.311 1318.78 Q183.968 1320.77 181.908 1321.84 Q179.848 1322.9 177.348 1322.9 Q172.163 1322.9 169.13 1319.78 Q166.121 1316.63 166.121 1311.19 Q166.121 1305.86 169.269 1302.65 Q172.417 1299.43 177.649 1299.43 Q183.644 1299.43 186.792 1304.04 Q189.964 1308.62 189.964 1317.37 Q189.964 1325.54 186.075 1330.42 Q182.209 1335.29 175.658 1335.29 Q173.899 1335.29 172.093 1334.94 Q170.288 1334.59 168.343 1333.9 M177.649 1319.24 Q180.797 1319.24 182.626 1317.09 Q184.477 1314.94 184.477 1311.19 Q184.477 1307.46 182.626 1305.31 Q180.797 1303.13 177.649 1303.13 Q174.501 1303.13 172.649 1305.31 Q170.82 1307.46 170.82 1311.19 Q170.82 1314.94 172.649 1317.09 Q174.501 1319.24 177.649 1319.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M209.2 1315.98 Q212.556 1316.7 214.431 1318.97 Q216.329 1321.23 216.329 1324.57 Q216.329 1329.68 212.811 1332.48 Q209.292 1335.29 202.811 1335.29 Q200.635 1335.29 198.32 1334.85 Q196.028 1334.43 193.575 1333.57 L193.575 1329.06 Q195.519 1330.19 197.834 1330.77 Q200.149 1331.35 202.672 1331.35 Q207.07 1331.35 209.362 1329.61 Q211.676 1327.88 211.676 1324.57 Q211.676 1321.51 209.524 1319.8 Q207.394 1318.06 203.575 1318.06 L199.547 1318.06 L199.547 1314.22 L203.76 1314.22 Q207.209 1314.22 209.037 1312.86 Q210.866 1311.47 210.866 1308.87 Q210.866 1306.21 208.968 1304.8 Q207.093 1303.36 203.575 1303.36 Q201.653 1303.36 199.454 1303.78 Q197.255 1304.2 194.616 1305.08 L194.616 1300.91 Q197.278 1300.17 199.593 1299.8 Q201.931 1299.43 203.991 1299.43 Q209.315 1299.43 212.417 1301.86 Q215.519 1304.27 215.519 1308.39 Q215.519 1311.26 213.875 1313.25 Q212.232 1315.22 209.2 1315.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M220.218 1300.05 L242.44 1300.05 L242.44 1302.05 L229.894 1334.61 L225.01 1334.61 L236.815 1303.99 L220.218 1303.99 L220.218 1300.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M247.556 1300.05 L265.912 1300.05 L265.912 1303.99 L251.838 1303.99 L251.838 1312.46 Q252.857 1312.11 253.875 1311.95 Q254.894 1311.77 255.912 1311.77 Q261.699 1311.77 265.079 1314.94 Q268.458 1318.11 268.458 1323.53 Q268.458 1329.11 264.986 1332.21 Q261.514 1335.29 255.195 1335.29 Q253.019 1335.29 250.75 1334.92 Q248.505 1334.54 246.097 1333.8 L246.097 1329.11 Q248.181 1330.24 250.403 1330.79 Q252.625 1331.35 255.102 1331.35 Q259.107 1331.35 261.445 1329.24 Q263.783 1327.14 263.783 1323.53 Q263.783 1319.92 261.445 1317.81 Q259.107 1315.7 255.102 1315.7 Q253.227 1315.7 251.352 1316.12 Q249.5 1316.54 247.556 1317.42 L247.556 1300.05 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M138.205 1120.2 Q134.593 1120.2 132.765 1123.77 Q130.959 1127.31 130.959 1134.44 Q130.959 1141.54 132.765 1145.11 Q134.593 1148.65 138.205 1148.65 Q141.839 1148.65 143.644 1145.11 Q145.473 1141.54 145.473 1134.44 Q145.473 1127.31 143.644 1123.77 Q141.839 1120.2 138.205 1120.2 M138.205 1116.5 Q144.015 1116.5 147.07 1121.1 Q150.149 1125.69 150.149 1134.44 Q150.149 1143.16 147.07 1147.77 Q144.015 1152.35 138.205 1152.35 Q132.394 1152.35 129.316 1147.77 Q126.26 1143.16 126.26 1134.44 Q126.26 1125.69 129.316 1121.1 Q132.394 1116.5 138.205 1116.5 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M155.218 1145.8 L160.103 1145.8 L160.103 1151.68 L155.218 1151.68 L155.218 1145.8 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M165.311 1150.96 L165.311 1146.71 Q167.07 1147.54 168.876 1147.98 Q170.681 1148.42 172.417 1148.42 Q177.047 1148.42 179.477 1145.32 Q181.931 1142.19 182.278 1135.85 Q180.936 1137.84 178.876 1138.9 Q176.815 1139.97 174.315 1139.97 Q169.13 1139.97 166.098 1136.84 Q163.089 1133.7 163.089 1128.26 Q163.089 1122.93 166.237 1119.71 Q169.385 1116.5 174.616 1116.5 Q180.612 1116.5 183.76 1121.1 Q186.931 1125.69 186.931 1134.44 Q186.931 1142.61 183.042 1147.49 Q179.177 1152.35 172.626 1152.35 Q170.866 1152.35 169.061 1152.01 Q167.255 1151.66 165.311 1150.96 M174.616 1136.31 Q177.765 1136.31 179.593 1134.16 Q181.445 1132.01 181.445 1128.26 Q181.445 1124.53 179.593 1122.38 Q177.765 1120.2 174.616 1120.2 Q171.468 1120.2 169.616 1122.38 Q167.788 1124.53 167.788 1128.26 Q167.788 1132.01 169.616 1134.16 Q171.468 1136.31 174.616 1136.31 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M204.848 1121.2 L193.042 1139.65 L204.848 1139.65 L204.848 1121.2 M203.621 1117.12 L209.5 1117.12 L209.5 1139.65 L214.431 1139.65 L214.431 1143.53 L209.5 1143.53 L209.5 1151.68 L204.848 1151.68 L204.848 1143.53 L189.246 1143.53 L189.246 1139.02 L203.621 1117.12 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M229.5 1120.2 Q225.889 1120.2 224.061 1123.77 Q222.255 1127.31 222.255 1134.44 Q222.255 1141.54 224.061 1145.11 Q225.889 1148.65 229.5 1148.65 Q233.135 1148.65 234.94 1145.11 Q236.769 1141.54 236.769 1134.44 Q236.769 1127.31 234.94 1123.77 Q233.135 1120.2 229.5 1120.2 M229.5 1116.5 Q235.31 1116.5 238.366 1121.1 Q241.445 1125.69 241.445 1134.44 Q241.445 1143.16 238.366 1147.77 Q235.31 1152.35 229.5 1152.35 Q223.69 1152.35 220.612 1147.77 Q217.556 1143.16 217.556 1134.44 Q217.556 1125.69 220.612 1121.1 Q223.69 1116.5 229.5 1116.5 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M256.514 1120.2 Q252.903 1120.2 251.074 1123.77 Q249.269 1127.31 249.269 1134.44 Q249.269 1141.54 251.074 1145.11 Q252.903 1148.65 256.514 1148.65 Q260.148 1148.65 261.954 1145.11 Q263.783 1141.54 263.783 1134.44 Q263.783 1127.31 261.954 1123.77 Q260.148 1120.2 256.514 1120.2 M256.514 1116.5 Q262.324 1116.5 265.38 1121.1 Q268.458 1125.69 268.458 1134.44 Q268.458 1143.16 265.38 1147.77 Q262.324 1152.35 256.514 1152.35 Q250.704 1152.35 247.625 1147.77 Q244.57 1143.16 244.57 1134.44 Q244.57 1125.69 247.625 1121.1 Q250.704 1116.5 256.514 1116.5 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M140.797 937.269 Q137.186 937.269 135.357 940.834 Q133.552 944.375 133.552 951.505 Q133.552 958.612 135.357 962.176 Q137.186 965.718 140.797 965.718 Q144.431 965.718 146.237 962.176 Q148.066 958.612 148.066 951.505 Q148.066 944.375 146.237 940.834 Q144.431 937.269 140.797 937.269 M140.797 933.565 Q146.607 933.565 149.663 938.172 Q152.741 942.755 152.741 951.505 Q152.741 960.232 149.663 964.838 Q146.607 969.422 140.797 969.422 Q134.987 969.422 131.908 964.838 Q128.853 960.232 128.853 951.505 Q128.853 942.755 131.908 938.172 Q134.987 933.565 140.797 933.565 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M157.811 962.871 L162.695 962.871 L162.695 968.75 L157.811 968.75 L157.811 962.871 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M167.903 968.033 L167.903 963.774 Q169.663 964.607 171.468 965.047 Q173.274 965.486 175.01 965.486 Q179.64 965.486 182.07 962.385 Q184.524 959.26 184.871 952.917 Q183.528 954.908 181.468 955.973 Q179.408 957.037 176.908 957.037 Q171.723 957.037 168.69 953.912 Q165.681 950.764 165.681 945.325 Q165.681 940 168.829 936.783 Q171.978 933.565 177.209 933.565 Q183.204 933.565 186.352 938.172 Q189.524 942.755 189.524 951.505 Q189.524 959.676 185.635 964.561 Q181.769 969.422 175.218 969.422 Q173.459 969.422 171.653 969.074 Q169.848 968.727 167.903 968.033 M177.209 953.38 Q180.357 953.38 182.186 951.227 Q184.038 949.075 184.038 945.325 Q184.038 941.598 182.186 939.445 Q180.357 937.269 177.209 937.269 Q174.061 937.269 172.209 939.445 Q170.38 941.598 170.38 945.325 Q170.38 949.075 172.209 951.227 Q174.061 953.38 177.209 953.38 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M207.44 938.264 L195.635 956.713 L207.44 956.713 L207.44 938.264 M206.213 934.19 L212.093 934.19 L212.093 956.713 L217.024 956.713 L217.024 960.602 L212.093 960.602 L212.093 968.75 L207.44 968.75 L207.44 960.602 L191.839 960.602 L191.839 956.088 L206.213 934.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M226.121 964.815 L242.44 964.815 L242.44 968.75 L220.496 968.75 L220.496 964.815 Q223.158 962.061 227.741 957.431 Q232.348 952.778 233.528 951.436 Q235.773 948.912 236.653 947.176 Q237.556 945.417 237.556 943.727 Q237.556 940.973 235.611 939.237 Q233.69 937.501 230.588 937.501 Q228.389 937.501 225.936 938.264 Q223.505 939.028 220.727 940.579 L220.727 935.857 Q223.551 934.723 226.005 934.144 Q228.459 933.565 230.496 933.565 Q235.866 933.565 239.06 936.251 Q242.255 938.936 242.255 943.426 Q242.255 945.556 241.445 947.477 Q240.658 949.375 238.551 951.968 Q237.973 952.639 234.871 955.857 Q231.769 959.051 226.121 964.815 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M247.556 934.19 L265.912 934.19 L265.912 938.126 L251.838 938.126 L251.838 946.598 Q252.857 946.25 253.875 946.088 Q254.894 945.903 255.912 945.903 Q261.699 945.903 265.079 949.075 Q268.458 952.246 268.458 957.662 Q268.458 963.241 264.986 966.343 Q261.514 969.422 255.195 969.422 Q253.019 969.422 250.75 969.051 Q248.505 968.681 246.097 967.94 L246.097 963.241 Q248.181 964.375 250.403 964.931 Q252.625 965.486 255.102 965.486 Q259.107 965.486 261.445 963.38 Q263.783 961.274 263.783 957.662 Q263.783 954.051 261.445 951.945 Q259.107 949.838 255.102 949.838 Q253.227 949.838 251.352 950.255 Q249.5 950.672 247.556 951.551 L247.556 934.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M139.2 754.337 Q135.589 754.337 133.76 757.902 Q131.955 761.443 131.955 768.573 Q131.955 775.68 133.76 779.244 Q135.589 782.786 139.2 782.786 Q142.834 782.786 144.64 779.244 Q146.468 775.68 146.468 768.573 Q146.468 761.443 144.64 757.902 Q142.834 754.337 139.2 754.337 M139.2 750.633 Q145.01 750.633 148.066 755.24 Q151.144 759.823 151.144 768.573 Q151.144 777.3 148.066 781.906 Q145.01 786.49 139.2 786.49 Q133.39 786.49 130.311 781.906 Q127.256 777.3 127.256 768.573 Q127.256 759.823 130.311 755.24 Q133.39 750.633 139.2 750.633 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M156.214 779.939 L161.098 779.939 L161.098 785.818 L156.214 785.818 L156.214 779.939 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M166.306 785.101 L166.306 780.842 Q168.065 781.675 169.871 782.115 Q171.677 782.554 173.413 782.554 Q178.042 782.554 180.473 779.453 Q182.927 776.328 183.274 769.985 Q181.931 771.976 179.871 773.041 Q177.811 774.105 175.311 774.105 Q170.126 774.105 167.093 770.98 Q164.084 767.832 164.084 762.393 Q164.084 757.068 167.232 753.851 Q170.38 750.633 175.612 750.633 Q181.607 750.633 184.755 755.24 Q187.926 759.823 187.926 768.573 Q187.926 776.744 184.038 781.629 Q180.172 786.49 173.621 786.49 Q171.862 786.49 170.056 786.142 Q168.251 785.795 166.306 785.101 M175.612 770.448 Q178.76 770.448 180.589 768.295 Q182.44 766.143 182.44 762.393 Q182.44 758.666 180.589 756.513 Q178.76 754.337 175.612 754.337 Q172.464 754.337 170.612 756.513 Q168.783 758.666 168.783 762.393 Q168.783 766.143 170.612 768.295 Q172.464 770.448 175.612 770.448 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M205.843 755.332 L194.038 773.781 L205.843 773.781 L205.843 755.332 M204.616 751.258 L210.496 751.258 L210.496 773.781 L215.426 773.781 L215.426 777.67 L210.496 777.67 L210.496 785.818 L205.843 785.818 L205.843 777.67 L190.241 777.67 L190.241 773.156 L204.616 751.258 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M220.542 751.258 L238.898 751.258 L238.898 755.194 L224.824 755.194 L224.824 763.666 Q225.843 763.318 226.861 763.156 Q227.88 762.971 228.898 762.971 Q234.686 762.971 238.065 766.143 Q241.445 769.314 241.445 774.73 Q241.445 780.309 237.973 783.411 Q234.5 786.49 228.181 786.49 Q226.005 786.49 223.736 786.119 Q221.491 785.749 219.084 785.008 L219.084 780.309 Q221.167 781.443 223.389 781.999 Q225.611 782.554 228.088 782.554 Q232.093 782.554 234.431 780.448 Q236.769 778.342 236.769 774.73 Q236.769 771.119 234.431 769.013 Q232.093 766.906 228.088 766.906 Q226.213 766.906 224.338 767.323 Q222.486 767.74 220.542 768.619 L220.542 751.258 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M256.514 754.337 Q252.903 754.337 251.074 757.902 Q249.269 761.443 249.269 768.573 Q249.269 775.68 251.074 779.244 Q252.903 782.786 256.514 782.786 Q260.148 782.786 261.954 779.244 Q263.783 775.68 263.783 768.573 Q263.783 761.443 261.954 757.902 Q260.148 754.337 256.514 754.337 M256.514 750.633 Q262.324 750.633 265.38 755.24 Q268.458 759.823 268.458 768.573 Q268.458 777.3 265.38 781.906 Q262.324 786.49 256.514 786.49 Q250.704 786.49 247.625 781.906 Q244.57 777.3 244.57 768.573 Q244.57 759.823 247.625 755.24 Q250.704 750.633 256.514 750.633 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M140.103 571.405 Q136.492 571.405 134.663 574.97 Q132.857 578.511 132.857 585.641 Q132.857 592.748 134.663 596.312 Q136.492 599.854 140.103 599.854 Q143.737 599.854 145.542 596.312 Q147.371 592.748 147.371 585.641 Q147.371 578.511 145.542 574.97 Q143.737 571.405 140.103 571.405 M140.103 567.701 Q145.913 567.701 148.968 572.308 Q152.047 576.891 152.047 585.641 Q152.047 594.368 148.968 598.974 Q145.913 603.558 140.103 603.558 Q134.293 603.558 131.214 598.974 Q128.158 594.368 128.158 585.641 Q128.158 576.891 131.214 572.308 Q134.293 567.701 140.103 567.701 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M157.116 597.007 L162.001 597.007 L162.001 602.886 L157.116 602.886 L157.116 597.007 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M167.209 602.169 L167.209 597.91 Q168.968 598.743 170.774 599.183 Q172.579 599.622 174.315 599.622 Q178.945 599.622 181.376 596.521 Q183.829 593.396 184.177 587.053 Q182.834 589.044 180.774 590.109 Q178.714 591.173 176.214 591.173 Q171.028 591.173 167.996 588.048 Q164.987 584.9 164.987 579.461 Q164.987 574.136 168.135 570.919 Q171.283 567.701 176.515 567.701 Q182.51 567.701 185.658 572.308 Q188.829 576.891 188.829 585.641 Q188.829 593.812 184.94 598.697 Q181.075 603.558 174.524 603.558 Q172.765 603.558 170.959 603.21 Q169.153 602.863 167.209 602.169 M176.515 587.516 Q179.663 587.516 181.491 585.363 Q183.343 583.211 183.343 579.461 Q183.343 575.734 181.491 573.581 Q179.663 571.405 176.515 571.405 Q173.366 571.405 171.515 573.581 Q169.686 575.734 169.686 579.461 Q169.686 583.211 171.515 585.363 Q173.366 587.516 176.515 587.516 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M206.746 572.4 L194.94 590.849 L206.746 590.849 L206.746 572.4 M205.519 568.326 L211.399 568.326 L211.399 590.849 L216.329 590.849 L216.329 594.738 L211.399 594.738 L211.399 602.886 L206.746 602.886 L206.746 594.738 L191.144 594.738 L191.144 590.224 L205.519 568.326 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M220.218 568.326 L242.44 568.326 L242.44 570.317 L229.894 602.886 L225.01 602.886 L236.815 572.262 L220.218 572.262 L220.218 568.326 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M247.556 568.326 L265.912 568.326 L265.912 572.262 L251.838 572.262 L251.838 580.734 Q252.857 580.386 253.875 580.224 Q254.894 580.039 255.912 580.039 Q261.699 580.039 265.079 583.211 Q268.458 586.382 268.458 591.798 Q268.458 597.377 264.986 600.479 Q261.514 603.558 255.195 603.558 Q253.019 603.558 250.75 603.187 Q248.505 602.817 246.097 602.076 L246.097 597.377 Q248.181 598.511 250.403 599.067 Q252.625 599.622 255.102 599.622 Q259.107 599.622 261.445 597.516 Q263.783 595.41 263.783 591.798 Q263.783 588.187 261.445 586.081 Q259.107 583.974 255.102 583.974 Q253.227 583.974 251.352 584.391 Q249.5 584.808 247.556 585.687 L247.556 568.326 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M139.686 388.473 Q136.075 388.473 134.246 392.038 Q132.441 395.579 132.441 402.709 Q132.441 409.816 134.246 413.38 Q136.075 416.922 139.686 416.922 Q143.32 416.922 145.126 413.38 Q146.954 409.816 146.954 402.709 Q146.954 395.579 145.126 392.038 Q143.32 388.473 139.686 388.473 M139.686 384.769 Q145.496 384.769 148.552 389.376 Q151.63 393.959 151.63 402.709 Q151.63 411.436 148.552 416.042 Q145.496 420.626 139.686 420.626 Q133.876 420.626 130.797 416.042 Q127.742 411.436 127.742 402.709 Q127.742 393.959 130.797 389.376 Q133.876 384.769 139.686 384.769 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M156.7 414.075 L161.584 414.075 L161.584 419.954 L156.7 419.954 L156.7 414.075 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M166.792 419.237 L166.792 414.978 Q168.552 415.811 170.357 416.251 Q172.163 416.69 173.899 416.69 Q178.528 416.69 180.959 413.589 Q183.413 410.464 183.76 404.121 Q182.417 406.112 180.357 407.177 Q178.297 408.241 175.797 408.241 Q170.612 408.241 167.579 405.116 Q164.57 401.968 164.57 396.529 Q164.57 391.204 167.718 387.987 Q170.866 384.769 176.098 384.769 Q182.093 384.769 185.241 389.376 Q188.413 393.959 188.413 402.709 Q188.413 410.88 184.524 415.765 Q180.658 420.626 174.107 420.626 Q172.348 420.626 170.542 420.278 Q168.737 419.931 166.792 419.237 M176.098 404.584 Q179.246 404.584 181.075 402.431 Q182.927 400.279 182.927 396.529 Q182.927 392.802 181.075 390.649 Q179.246 388.473 176.098 388.473 Q172.95 388.473 171.098 390.649 Q169.269 392.802 169.269 396.529 Q169.269 400.279 171.098 402.431 Q172.95 404.584 176.098 404.584 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M193.528 385.394 L211.885 385.394 L211.885 389.33 L197.811 389.33 L197.811 397.802 Q198.829 397.454 199.848 397.292 Q200.866 397.107 201.885 397.107 Q207.672 397.107 211.051 400.279 Q214.431 403.45 214.431 408.866 Q214.431 414.445 210.959 417.547 Q207.487 420.626 201.167 420.626 Q198.991 420.626 196.723 420.255 Q194.477 419.885 192.07 419.144 L192.07 414.445 Q194.153 415.579 196.376 416.135 Q198.598 416.69 201.075 416.69 Q205.079 416.69 207.417 414.584 Q209.755 412.478 209.755 408.866 Q209.755 405.255 207.417 403.149 Q205.079 401.042 201.075 401.042 Q199.2 401.042 197.325 401.459 Q195.473 401.876 193.528 402.755 L193.528 385.394 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M229.5 388.473 Q225.889 388.473 224.061 392.038 Q222.255 395.579 222.255 402.709 Q222.255 409.816 224.061 413.38 Q225.889 416.922 229.5 416.922 Q233.135 416.922 234.94 413.38 Q236.769 409.816 236.769 402.709 Q236.769 395.579 234.94 392.038 Q233.135 388.473 229.5 388.473 M229.5 384.769 Q235.31 384.769 238.366 389.376 Q241.445 393.959 241.445 402.709 Q241.445 411.436 238.366 416.042 Q235.31 420.626 229.5 420.626 Q223.69 420.626 220.612 416.042 Q217.556 411.436 217.556 402.709 Q217.556 393.959 220.612 389.376 Q223.69 384.769 229.5 384.769 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M256.514 388.473 Q252.903 388.473 251.074 392.038 Q249.269 395.579 249.269 402.709 Q249.269 409.816 251.074 413.38 Q252.903 416.922 256.514 416.922 Q260.148 416.922 261.954 413.38 Q263.783 409.816 263.783 402.709 Q263.783 395.579 261.954 392.038 Q260.148 388.473 256.514 388.473 M256.514 384.769 Q262.324 384.769 265.38 389.376 Q268.458 393.959 268.458 402.709 Q268.458 411.436 265.38 416.042 Q262.324 420.626 256.514 420.626 Q250.704 420.626 247.625 416.042 Q244.57 411.436 244.57 402.709 Q244.57 393.959 247.625 389.376 Q250.704 384.769 256.514 384.769 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M142.279 205.541 Q138.667 205.541 136.839 209.106 Q135.033 212.647 135.033 219.777 Q135.033 226.884 136.839 230.448 Q138.667 233.99 142.279 233.99 Q145.913 233.99 147.718 230.448 Q149.547 226.884 149.547 219.777 Q149.547 212.647 147.718 209.106 Q145.913 205.541 142.279 205.541 M142.279 201.837 Q148.089 201.837 151.144 206.444 Q154.223 211.027 154.223 219.777 Q154.223 228.504 151.144 233.11 Q148.089 237.694 142.279 237.694 Q136.468 237.694 133.39 233.11 Q130.334 228.504 130.334 219.777 Q130.334 211.027 133.39 206.444 Q136.468 201.837 142.279 201.837 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M159.292 231.143 L164.177 231.143 L164.177 237.022 L159.292 237.022 L159.292 231.143 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M169.385 236.305 L169.385 232.046 Q171.144 232.879 172.95 233.319 Q174.755 233.758 176.491 233.758 Q181.121 233.758 183.552 230.657 Q186.005 227.532 186.352 221.189 Q185.01 223.18 182.95 224.245 Q180.889 225.309 178.39 225.309 Q173.204 225.309 170.172 222.184 Q167.163 219.036 167.163 213.597 Q167.163 208.272 170.311 205.055 Q173.459 201.837 178.69 201.837 Q184.686 201.837 187.834 206.444 Q191.005 211.027 191.005 219.777 Q191.005 227.948 187.116 232.833 Q183.251 237.694 176.7 237.694 Q174.94 237.694 173.135 237.346 Q171.329 236.999 169.385 236.305 M178.69 221.652 Q181.839 221.652 183.667 219.499 Q185.519 217.347 185.519 213.597 Q185.519 209.87 183.667 207.717 Q181.839 205.541 178.69 205.541 Q175.542 205.541 173.69 207.717 Q171.862 209.87 171.862 213.597 Q171.862 217.347 173.69 219.499 Q175.542 221.652 178.69 221.652 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M196.121 202.462 L214.477 202.462 L214.477 206.398 L200.403 206.398 L200.403 214.87 Q201.422 214.522 202.44 214.36 Q203.459 214.175 204.477 214.175 Q210.264 214.175 213.644 217.347 Q217.024 220.518 217.024 225.934 Q217.024 231.513 213.551 234.615 Q210.079 237.694 203.76 237.694 Q201.584 237.694 199.315 237.323 Q197.07 236.953 194.663 236.212 L194.663 231.513 Q196.746 232.647 198.968 233.203 Q201.19 233.758 203.667 233.758 Q207.672 233.758 210.01 231.652 Q212.348 229.546 212.348 225.934 Q212.348 222.323 210.01 220.217 Q207.672 218.11 203.667 218.11 Q201.792 218.11 199.917 218.527 Q198.065 218.944 196.121 219.823 L196.121 202.462 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M226.121 233.087 L242.44 233.087 L242.44 237.022 L220.496 237.022 L220.496 233.087 Q223.158 230.333 227.741 225.703 Q232.348 221.05 233.528 219.708 Q235.773 217.184 236.653 215.448 Q237.556 213.689 237.556 211.999 Q237.556 209.245 235.611 207.509 Q233.69 205.773 230.588 205.773 Q228.389 205.773 225.936 206.536 Q223.505 207.3 220.727 208.851 L220.727 204.129 Q223.551 202.995 226.005 202.416 Q228.459 201.837 230.496 201.837 Q235.866 201.837 239.06 204.523 Q242.255 207.208 242.255 211.698 Q242.255 213.828 241.445 215.749 Q240.658 217.647 238.551 220.24 Q237.973 220.911 234.871 224.129 Q231.769 227.323 226.121 233.087 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M247.556 202.462 L265.912 202.462 L265.912 206.398 L251.838 206.398 L251.838 214.87 Q252.857 214.522 253.875 214.36 Q254.894 214.175 255.912 214.175 Q261.699 214.175 265.079 217.347 Q268.458 220.518 268.458 225.934 Q268.458 231.513 264.986 234.615 Q261.514 237.694 255.195 237.694 Q253.019 237.694 250.75 237.323 Q248.505 236.953 246.097 236.212 L246.097 231.513 Q248.181 232.647 250.403 233.203 Q252.625 233.758 255.102 233.758 Q259.107 233.758 261.445 231.652 Q263.783 229.546 263.783 225.934 Q263.783 222.323 261.445 220.217 Q259.107 218.11 255.102 218.11 Q253.227 218.11 251.352 218.527 Q249.5 218.944 247.556 219.823 L247.556 202.462 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M1341.77 1518.33 L1341.77 1525.11 Q1338.52 1522.08 1334.83 1520.59 Q1331.17 1519.09 1327.03 1519.09 Q1318.88 1519.09 1314.55 1524.09 Q1310.23 1529.05 1310.23 1538.47 Q1310.23 1547.86 1314.55 1552.86 Q1318.88 1557.82 1327.03 1557.82 Q1331.17 1557.82 1334.83 1556.33 Q1338.52 1554.83 1341.77 1551.81 L1341.77 1558.53 Q1338.39 1560.82 1334.61 1561.96 Q1330.85 1563.11 1326.65 1563.11 Q1315.86 1563.11 1309.65 1556.52 Q1303.45 1549.9 1303.45 1538.47 Q1303.45 1527.01 1309.65 1520.43 Q1315.86 1513.81 1326.65 1513.81 Q1330.91 1513.81 1334.67 1514.95 Q1338.46 1516.07 1341.77 1518.33 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M44.1444 1020.68 L50.9239 1020.68 Q47.9002 1023.93 46.4043 1027.62 Q44.9083 1031.28 44.9083 1035.42 Q44.9083 1043.57 49.9054 1047.9 Q54.8707 1052.23 64.2919 1052.23 Q73.6813 1052.23 78.6784 1047.9 Q83.6436 1043.57 83.6436 1035.42 Q83.6436 1031.28 82.1477 1027.62 Q80.6518 1023.93 77.6281 1020.68 L84.3439 1020.68 Q86.6355 1024.06 87.7814 1027.84 Q88.9272 1031.6 88.9272 1035.8 Q88.9272 1046.59 82.3387 1052.8 Q75.7183 1059 64.2919 1059 Q52.8336 1059 46.2451 1052.8 Q39.6248 1046.59 39.6248 1035.8 Q39.6248 1031.54 40.7706 1027.78 Q41.8846 1023.99 44.1444 1020.68 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M38.479 1014.54 L38.479 1008.68 L88.0042 1008.68 L88.0042 1014.54 L38.479 1014.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M70.0847 986.34 Q70.0847 993.438 71.7079 996.175 Q73.3312 998.913 77.2461 998.913 Q80.3653 998.913 82.2114 996.876 Q84.0256 994.807 84.0256 991.274 Q84.0256 986.404 80.5881 983.476 Q77.1188 980.516 71.3897 980.516 L70.0847 980.516 L70.0847 986.34 M67.6657 974.659 L88.0042 974.659 L88.0042 980.516 L82.5933 980.516 Q85.8398 982.521 87.3994 985.513 Q88.9272 988.505 88.9272 992.833 Q88.9272 998.308 85.8716 1001.55 Q82.7843 1004.77 77.6281 1004.77 Q71.6125 1004.77 68.5569 1000.76 Q65.5014 996.717 65.5014 988.728 L65.5014 980.516 L64.9285 980.516 Q60.8862 980.516 58.6901 983.189 Q56.4621 985.831 56.4621 990.637 Q56.4621 993.693 57.1941 996.589 Q57.9262 999.486 59.3903 1002.16 L53.9795 1002.16 Q52.7381 998.945 52.1334 995.921 Q51.4968 992.897 51.4968 990.033 Q51.4968 982.298 55.5072 978.479 Q59.5176 974.659 67.6657 974.659 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M53.4065 945.791 L58.9447 945.791 Q57.6716 948.273 57.035 950.947 Q56.3984 953.621 56.3984 956.485 Q56.3984 960.846 57.7352 963.042 Q59.072 965.206 61.7456 965.206 Q63.7826 965.206 64.9603 963.647 Q66.1061 962.087 67.1565 957.376 L67.6021 955.371 Q68.9389 949.133 71.3897 946.523 Q73.8086 943.881 78.1691 943.881 Q83.1344 943.881 86.0308 947.828 Q88.9272 951.743 88.9272 958.618 Q88.9272 961.482 88.3543 964.602 Q87.8132 967.689 86.6992 971.126 L80.6518 971.126 Q82.3387 967.88 83.198 964.729 Q84.0256 961.578 84.0256 958.49 Q84.0256 954.353 82.6251 952.125 Q81.1929 949.897 78.6147 949.897 Q76.2276 949.897 74.9545 951.52 Q73.6813 953.111 72.5037 958.554 L72.0262 960.591 Q70.8804 966.034 68.5251 968.453 Q66.138 970.872 62.0002 970.872 Q56.9713 970.872 54.2341 967.307 Q51.4968 963.742 51.4968 957.185 Q51.4968 953.939 51.9743 951.074 Q52.4517 948.21 53.4065 945.791 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M53.4065 915.013 L58.9447 915.013 Q57.6716 917.495 57.035 920.169 Q56.3984 922.842 56.3984 925.707 Q56.3984 930.068 57.7352 932.264 Q59.072 934.428 61.7456 934.428 Q63.7826 934.428 64.9603 932.868 Q66.1061 931.309 67.1565 926.598 L67.6021 924.593 Q68.9389 918.355 71.3897 915.745 Q73.8086 913.103 78.1691 913.103 Q83.1344 913.103 86.0308 917.05 Q88.9272 920.965 88.9272 927.84 Q88.9272 930.704 88.3543 933.823 Q87.8132 936.911 86.6992 940.348 L80.6518 940.348 Q82.3387 937.102 83.198 933.951 Q84.0256 930.8 84.0256 927.712 Q84.0256 923.575 82.6251 921.347 Q81.1929 919.119 78.6147 919.119 Q76.2276 919.119 74.9545 920.742 Q73.6813 922.333 72.5037 927.776 L72.0262 929.813 Q70.8804 935.256 68.5251 937.675 Q66.138 940.094 62.0002 940.094 Q56.9713 940.094 54.2341 936.529 Q51.4968 932.964 51.4968 926.407 Q51.4968 923.161 51.9743 920.296 Q52.4517 917.432 53.4065 915.013 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M52.3562 906.96 L52.3562 901.104 L88.0042 901.104 L88.0042 906.96 L52.3562 906.96 M38.479 906.96 L38.479 901.104 L45.895 901.104 L45.895 906.96 L38.479 906.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M38.479 876.914 L43.3487 876.914 L43.3487 882.516 Q43.3487 885.667 44.6219 886.908 Q45.895 888.118 49.2052 888.118 L52.3562 888.118 L52.3562 878.474 L56.9077 878.474 L56.9077 888.118 L88.0042 888.118 L88.0042 894.006 L56.9077 894.006 L56.9077 899.608 L52.3562 899.608 L52.3562 894.006 L49.8736 894.006 Q43.9216 894.006 41.2162 891.237 Q38.479 888.468 38.479 882.452 L38.479 876.914 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M52.3562 870.771 L52.3562 864.915 L88.0042 864.915 L88.0042 870.771 L52.3562 870.771 M38.479 870.771 L38.479 864.915 L45.895 864.915 L45.895 870.771 L38.479 870.771 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M53.7248 833.118 L59.1993 833.118 Q57.8307 835.6 57.1623 838.115 Q56.4621 840.598 56.4621 843.144 Q56.4621 848.841 60.0905 851.992 Q63.6872 855.143 70.212 855.143 Q76.7369 855.143 80.3653 851.992 Q83.9619 848.841 83.9619 843.144 Q83.9619 840.598 83.2935 838.115 Q82.5933 835.6 81.2247 833.118 L86.6355 833.118 Q87.7814 835.569 88.3543 838.21 Q88.9272 840.82 88.9272 843.78 Q88.9272 851.833 83.8664 856.575 Q78.8057 861.318 70.212 861.318 Q61.491 861.318 56.4939 856.544 Q51.4968 851.738 51.4968 843.398 Q51.4968 840.693 52.0697 838.115 Q52.6108 835.537 53.7248 833.118 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M70.0847 810.774 Q70.0847 817.872 71.7079 820.609 Q73.3312 823.346 77.2461 823.346 Q80.3653 823.346 82.2114 821.309 Q84.0256 819.241 84.0256 815.708 Q84.0256 810.838 80.5881 807.91 Q77.1188 804.95 71.3897 804.95 L70.0847 804.95 L70.0847 810.774 M67.6657 799.093 L88.0042 799.093 L88.0042 804.95 L82.5933 804.95 Q85.8398 806.955 87.3994 809.947 Q88.9272 812.939 88.9272 817.267 Q88.9272 822.742 85.8716 825.988 Q82.7843 829.203 77.6281 829.203 Q71.6125 829.203 68.5569 825.193 Q65.5014 821.15 65.5014 813.161 L65.5014 804.95 L64.9285 804.95 Q60.8862 804.95 58.6901 807.623 Q56.4621 810.265 56.4621 815.071 Q56.4621 818.127 57.1941 821.023 Q57.9262 823.919 59.3903 826.593 L53.9795 826.593 Q52.7381 823.378 52.1334 820.355 Q51.4968 817.331 51.4968 814.466 Q51.4968 806.732 55.5072 802.913 Q59.5176 799.093 67.6657 799.093 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M42.2347 787.157 L52.3562 787.157 L52.3562 775.094 L56.9077 775.094 L56.9077 787.157 L76.2594 787.157 Q80.6199 787.157 81.8613 785.98 Q83.1026 784.77 83.1026 781.11 L83.1026 775.094 L88.0042 775.094 L88.0042 781.11 Q88.0042 787.889 85.4897 790.468 Q82.9434 793.046 76.2594 793.046 L56.9077 793.046 L56.9077 797.343 L52.3562 797.343 L52.3562 793.046 L42.2347 793.046 L42.2347 787.157 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M52.3562 768.952 L52.3562 763.095 L88.0042 763.095 L88.0042 768.952 L52.3562 768.952 M38.479 768.952 L38.479 763.095 L45.895 763.095 L45.895 768.952 L38.479 768.952 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M56.4621 743.139 Q56.4621 747.849 60.1542 750.586 Q63.8145 753.324 70.212 753.324 Q76.6095 753.324 80.3017 750.618 Q83.9619 747.881 83.9619 743.139 Q83.9619 738.46 80.2698 735.723 Q76.5777 732.985 70.212 732.985 Q63.8781 732.985 60.186 735.723 Q56.4621 738.46 56.4621 743.139 M51.4968 743.139 Q51.4968 735.5 56.4621 731.139 Q61.4273 726.779 70.212 726.779 Q78.9649 726.779 83.9619 731.139 Q88.9272 735.5 88.9272 743.139 Q88.9272 750.809 83.9619 755.17 Q78.9649 759.498 70.212 759.498 Q61.4273 759.498 56.4621 755.17 Q51.4968 750.809 51.4968 743.139 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M66.4881 691.003 L88.0042 691.003 L88.0042 696.86 L66.679 696.86 Q61.6183 696.86 59.1038 698.833 Q56.5894 700.807 56.5894 704.753 Q56.5894 709.496 59.6131 712.233 Q62.6368 714.97 67.8567 714.97 L88.0042 714.97 L88.0042 720.859 L52.3562 720.859 L52.3562 714.97 L57.8944 714.97 Q54.6797 712.87 53.0883 710.037 Q51.4968 707.172 51.4968 703.448 Q51.4968 697.305 55.3163 694.154 Q59.1038 691.003 66.4881 691.003 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M46.818 648.003 L70.4666 656.724 L70.4666 639.25 L46.818 648.003 M40.4842 651.632 L40.4842 644.343 L88.0042 626.232 L88.0042 632.916 L75.8138 637.245 L75.8138 658.666 L88.0042 662.994 L88.0042 669.774 L40.4842 651.632 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M53.7248 595.582 L59.1993 595.582 Q57.8307 598.064 57.1623 600.579 Q56.4621 603.061 56.4621 605.607 Q56.4621 611.305 60.0905 614.456 Q63.6872 617.607 70.212 617.607 Q76.7369 617.607 80.3653 614.456 Q83.9619 611.305 83.9619 605.607 Q83.9619 603.061 83.2935 600.579 Q82.5933 598.064 81.2247 595.582 L86.6355 595.582 Q87.7814 598.032 88.3543 600.674 Q88.9272 603.284 88.9272 606.244 Q88.9272 614.297 83.8664 619.039 Q78.8057 623.782 70.212 623.782 Q61.491 623.782 56.4939 619.007 Q51.4968 614.201 51.4968 605.862 Q51.4968 603.157 52.0697 600.579 Q52.6108 598 53.7248 595.582 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M53.7248 563.785 L59.1993 563.785 Q57.8307 566.267 57.1623 568.782 Q56.4621 571.265 56.4621 573.811 Q56.4621 579.508 60.0905 582.659 Q63.6872 585.81 70.212 585.81 Q76.7369 585.81 80.3653 582.659 Q83.9619 579.508 83.9619 573.811 Q83.9619 571.265 83.2935 568.782 Q82.5933 566.267 81.2247 563.785 L86.6355 563.785 Q87.7814 566.236 88.3543 568.877 Q88.9272 571.487 88.9272 574.447 Q88.9272 582.5 83.8664 587.242 Q78.8057 591.985 70.212 591.985 Q61.491 591.985 56.4939 587.211 Q51.4968 582.404 51.4968 574.065 Q51.4968 571.36 52.0697 568.782 Q52.6108 566.204 53.7248 563.785 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M73.9359 558.247 L52.3562 558.247 L52.3562 552.39 L73.7131 552.39 Q78.7739 552.39 81.3202 550.417 Q83.8346 548.443 83.8346 544.497 Q83.8346 539.754 80.8109 537.017 Q77.7872 534.248 72.5673 534.248 L52.3562 534.248 L52.3562 528.391 L88.0042 528.391 L88.0042 534.248 L82.5296 534.248 Q85.7762 536.38 87.3676 539.213 Q88.9272 542.014 88.9272 545.738 Q88.9272 551.881 85.1078 555.064 Q81.2883 558.247 73.9359 558.247 M51.4968 543.51 L51.4968 543.51 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M57.8307 501.592 Q57.2578 502.579 57.0032 503.756 Q56.7167 504.902 56.7167 506.302 Q56.7167 511.268 59.9632 513.941 Q63.1779 516.583 69.2253 516.583 L88.0042 516.583 L88.0042 522.471 L52.3562 522.471 L52.3562 516.583 L57.8944 516.583 Q54.6479 514.737 53.0883 511.777 Q51.4968 508.817 51.4968 504.584 Q51.4968 503.979 51.5923 503.247 Q51.656 502.515 51.8151 501.624 L57.8307 501.592 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M70.0847 479.248 Q70.0847 486.346 71.7079 489.083 Q73.3312 491.82 77.2461 491.82 Q80.3653 491.82 82.2114 489.783 Q84.0256 487.715 84.0256 484.182 Q84.0256 479.312 80.5881 476.384 Q77.1188 473.424 71.3897 473.424 L70.0847 473.424 L70.0847 479.248 M67.6657 467.567 L88.0042 467.567 L88.0042 473.424 L82.5933 473.424 Q85.8398 475.429 87.3994 478.421 Q88.9272 481.413 88.9272 485.741 Q88.9272 491.216 85.8716 494.462 Q82.7843 497.677 77.6281 497.677 Q71.6125 497.677 68.5569 493.667 Q65.5014 489.624 65.5014 481.635 L65.5014 473.424 L64.9285 473.424 Q60.8862 473.424 58.6901 476.097 Q56.4621 478.739 56.4621 483.545 Q56.4621 486.601 57.1941 489.497 Q57.9262 492.393 59.3903 495.067 L53.9795 495.067 Q52.7381 491.852 52.1334 488.829 Q51.4968 485.805 51.4968 482.94 Q51.4968 475.206 55.5072 471.387 Q59.5176 467.567 67.6657 467.567 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M53.7248 435.77 L59.1993 435.77 Q57.8307 438.253 57.1623 440.768 Q56.4621 443.25 56.4621 445.796 Q56.4621 451.494 60.0905 454.645 Q63.6872 457.796 70.212 457.796 Q76.7369 457.796 80.3653 454.645 Q83.9619 451.494 83.9619 445.796 Q83.9619 443.25 83.2935 440.768 Q82.5933 438.253 81.2247 435.77 L86.6355 435.77 Q87.7814 438.221 88.3543 440.863 Q88.9272 443.473 88.9272 446.433 Q88.9272 454.486 83.8664 459.228 Q78.8057 463.971 70.212 463.971 Q61.491 463.971 56.4939 459.196 Q51.4968 454.39 51.4968 446.051 Q51.4968 443.346 52.0697 440.768 Q52.6108 438.189 53.7248 435.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M91.3143 414.795 Q97.68 417.278 99.6216 419.633 Q101.563 421.989 101.563 425.935 L101.563 430.614 L96.6615 430.614 L96.6615 427.177 Q96.6615 424.758 95.5157 423.421 Q94.3699 422.084 90.1048 420.461 L87.4312 419.411 L52.3562 433.829 L52.3562 427.622 L80.238 416.482 L52.3562 405.342 L52.3562 399.136 L91.3143 414.795 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip832)\" style=\"stroke:#009af9; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  350.769,1116.92 383.712,1116.92 416.656,1245.3 449.6,987.183 482.543,1115.56 515.487,1115.56 548.431,1115.56 581.374,985.817 614.318,1114.19 647.262,1242.56 \n",
       "  680.206,1372.3 713.149,987.183 746.093,730.436 779.037,345.316 811.98,345.316 844.924,345.316 877.868,345.316 910.811,215.577 943.755,343.95 976.699,216.943 \n",
       "  1009.64,345.316 1042.59,345.316 1075.53,87.2037 1108.47,216.943 1141.42,343.95 1174.36,214.211 1207.3,214.211 1240.25,342.585 1273.19,343.95 1306.14,85.838 \n",
       "  1339.08,214.211 1372.02,85.838 1404.97,729.07 1437.91,985.817 1470.85,856.078 1503.8,1114.19 1536.74,727.705 1569.68,342.585 1602.63,472.324 1635.57,214.211 \n",
       "  1668.52,600.697 1701.46,730.436 1734.4,602.063 1767.35,214.211 1800.29,214.211 1833.23,214.211 1866.18,214.211 1899.12,342.585 1932.07,214.211 1965.01,342.585 \n",
       "  1997.95,470.958 2030.9,470.958 2063.84,599.331 2096.78,85.838 2129.73,214.211 2162.67,214.211 2195.61,343.95 2228.56,599.331 2261.5,214.211 2294.45,342.585 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip830)\" d=\"\n",
       "M1855.83 213.659 L2284.08 213.659 L2284.08 92.6992 L1855.83 92.6992  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1855.83,213.659 2284.08,213.659 2284.08,92.6992 1855.83,92.6992 1855.83,213.659 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip830)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1878.72,153.179 2016.08,153.179 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip830)\" d=\"M 0 0 M2052.16 170.459 L2038.97 135.899 L2043.85 135.899 L2054.8 164.996 L2065.77 135.899 L2070.63 135.899 L2057.46 170.459 L2052.16 170.459 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M2083.2 157.427 Q2078.04 157.427 2076.05 158.607 Q2074.06 159.788 2074.06 162.635 Q2074.06 164.904 2075.54 166.246 Q2077.05 167.566 2079.62 167.566 Q2083.16 167.566 2085.29 165.066 Q2087.44 162.543 2087.44 158.376 L2087.44 157.427 L2083.2 157.427 M2091.7 155.668 L2091.7 170.459 L2087.44 170.459 L2087.44 166.524 Q2085.98 168.885 2083.81 170.019 Q2081.63 171.13 2078.48 171.13 Q2074.5 171.13 2072.14 168.908 Q2069.8 166.663 2069.8 162.913 Q2069.8 158.538 2072.72 156.316 Q2075.66 154.094 2081.47 154.094 L2087.44 154.094 L2087.44 153.677 Q2087.44 150.737 2085.49 149.14 Q2083.57 147.519 2080.08 147.519 Q2077.86 147.519 2075.75 148.052 Q2073.64 148.584 2071.7 149.649 L2071.7 145.714 Q2074.04 144.811 2076.24 144.371 Q2078.43 143.908 2080.52 143.908 Q2086.14 143.908 2088.92 146.825 Q2091.7 149.742 2091.7 155.668 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M2096.17 134.441 L2100.43 134.441 L2100.43 170.459 L2096.17 170.459 L2096.17 134.441 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M2104.89 144.533 L2109.15 144.533 L2109.15 170.459 L2104.89 170.459 L2104.89 144.533 M2104.89 134.441 L2109.15 134.441 L2109.15 139.834 L2104.89 139.834 L2104.89 134.441 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M2130.68 148.469 L2130.68 134.441 L2134.94 134.441 L2134.94 170.459 L2130.68 170.459 L2130.68 166.57 Q2129.34 168.885 2127.28 170.019 Q2125.24 171.13 2122.37 171.13 Q2117.67 171.13 2114.71 167.38 Q2111.77 163.631 2111.77 157.519 Q2111.77 151.408 2114.71 147.658 Q2117.67 143.908 2122.37 143.908 Q2125.24 143.908 2127.28 145.043 Q2129.34 146.154 2130.68 148.469 M2116.17 157.519 Q2116.17 162.218 2118.09 164.904 Q2120.03 167.566 2123.41 167.566 Q2126.79 167.566 2128.74 164.904 Q2130.68 162.218 2130.68 157.519 Q2130.68 152.82 2128.74 150.158 Q2126.79 147.473 2123.41 147.473 Q2120.03 147.473 2118.09 150.158 Q2116.17 152.82 2116.17 157.519 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M2151.19 157.427 Q2146.03 157.427 2144.04 158.607 Q2142.05 159.788 2142.05 162.635 Q2142.05 164.904 2143.53 166.246 Q2145.03 167.566 2147.6 167.566 Q2151.14 167.566 2153.27 165.066 Q2155.43 162.543 2155.43 158.376 L2155.43 157.427 L2151.19 157.427 M2159.68 155.668 L2159.68 170.459 L2155.43 170.459 L2155.43 166.524 Q2153.97 168.885 2151.79 170.019 Q2149.61 171.13 2146.47 171.13 Q2142.49 171.13 2140.12 168.908 Q2137.79 166.663 2137.79 162.913 Q2137.79 158.538 2140.7 156.316 Q2143.64 154.094 2149.45 154.094 L2155.43 154.094 L2155.43 153.677 Q2155.43 150.737 2153.48 149.14 Q2151.56 147.519 2148.06 147.519 Q2145.84 147.519 2143.74 148.052 Q2141.63 148.584 2139.68 149.649 L2139.68 145.714 Q2142.02 144.811 2144.22 144.371 Q2146.42 143.908 2148.5 143.908 Q2154.13 143.908 2156.91 146.825 Q2159.68 149.742 2159.68 155.668 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M2168.36 137.172 L2168.36 144.533 L2177.14 144.533 L2177.14 147.844 L2168.36 147.844 L2168.36 161.918 Q2168.36 165.089 2169.22 165.992 Q2170.1 166.894 2172.76 166.894 L2177.14 166.894 L2177.14 170.459 L2172.76 170.459 Q2167.83 170.459 2165.96 168.63 Q2164.08 166.779 2164.08 161.918 L2164.08 147.844 L2160.96 147.844 L2160.96 144.533 L2164.08 144.533 L2164.08 137.172 L2168.36 137.172 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M2181.61 144.533 L2185.86 144.533 L2185.86 170.459 L2181.61 170.459 L2181.61 144.533 M2181.61 134.441 L2185.86 134.441 L2185.86 139.834 L2181.61 139.834 L2181.61 134.441 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M2200.38 147.519 Q2196.95 147.519 2194.96 150.205 Q2192.97 152.867 2192.97 157.519 Q2192.97 162.172 2194.94 164.857 Q2196.93 167.519 2200.38 167.519 Q2203.78 167.519 2205.77 164.834 Q2207.76 162.149 2207.76 157.519 Q2207.76 152.913 2205.77 150.228 Q2203.78 147.519 2200.38 147.519 M2200.38 143.908 Q2205.93 143.908 2209.11 147.519 Q2212.28 151.131 2212.28 157.519 Q2212.28 163.885 2209.11 167.519 Q2205.93 171.13 2200.38 171.13 Q2194.8 171.13 2191.63 167.519 Q2188.48 163.885 2188.48 157.519 Q2188.48 151.131 2191.63 147.519 Q2194.8 143.908 2200.38 143.908 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip830)\" d=\"M 0 0 M2238.29 154.811 L2238.29 170.459 L2234.04 170.459 L2234.04 154.95 Q2234.04 151.269 2232.6 149.441 Q2231.17 147.612 2228.29 147.612 Q2224.85 147.612 2222.86 149.811 Q2220.86 152.01 2220.86 155.806 L2220.86 170.459 L2216.58 170.459 L2216.58 144.533 L2220.86 144.533 L2220.86 148.561 Q2222.39 146.223 2224.45 145.066 Q2226.54 143.908 2229.24 143.908 Q2233.71 143.908 2236 146.686 Q2238.29 149.441 2238.29 154.811 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(curve.parameter_values,\n",
    "     curve.measurements,\n",
    "     xscale=curve.parameter_scale,\n",
    "     xlab=curve.parameter_name,\n",
    "     ylab=\"Classification Accuracy\",\n",
    "     label=\"Validation\", lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95433"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = round(maximum(curve.measurements), digits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.447567486554116"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_C = curve.parameter_values[argmax(curve.measurements)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second look at `rbf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJBase.NumericRange(Float64, :gamma, ... )"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1 = range(Float64, :C, lower=5*10^-0, upper=5*10^7, scale=:log10)\n",
    "r2 = range(Float64, :gamma, lower=10^-10, upper=10^-1, scale=:log10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeterministicTunedModel(\n",
       "    model = SVMClassifier(\n",
       "            C = 1.0,\n",
       "            kernel = \"rbf\",\n",
       "            degree = 3,\n",
       "            gamma = \"auto\",\n",
       "            coef0 = 0.0,\n",
       "            shrinking = true,\n",
       "            tol = 0.001,\n",
       "            cache_size = 1000,\n",
       "            max_iter = -1,\n",
       "            decision_function_shape = \"ovr\",\n",
       "            random_state = nothing),\n",
       "    tuning = Grid(\n",
       "            goal = 100,\n",
       "            resolution = 10,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = CV(\n",
       "            nfolds = 6,\n",
       "            shuffle = false,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = accuracy(),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = MLJBase.NumericRange{Float64,MLJBase.Bounded,Symbol}[\u001b[34mNumericRange{Float64,…} @565\u001b[39m, \u001b[34mNumericRange{Float64,…} @534\u001b[39m],\n",
       "    train_best = true,\n",
       "    repeats = 1,\n",
       "    n = nothing,\n",
       "    acceleration = CPUThreads{Int64}(1),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true)\u001b[34m @075\u001b[39m"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_rbf_model = SVMClassifier(kernel=\"rbf\", cache_size=1000)\n",
    "self_tuning_svm_model = TunedModel(model=svm_rbf_model,\n",
    "                                    tuning=Grid(goal=100),\n",
    "                                    resampling=CV(), \n",
    "                                    measure=accuracy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1,r2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DeterministicTunedModel{Grid,…}} @016\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @639\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @038\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_svm_mach = machine(self_tuning_svm_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{DeterministicTunedModel{Grid,…}} @016\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      "┌ Info: Attempting to evaluate 100 models.\n",
      "└ @ MLJTuning /home/andrew/.julia/packages/MLJTuning/Bbgvk/src/tuned_models.jl:494\n",
      "\u001b[33mEvaluating over 100 metamodels: 100%[=========================] Time: 0:00:14\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DeterministicTunedModel{Grid,…}} @016\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @639\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @038\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = fit!(self_tuning_svm_mach, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip910\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip910)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip911\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip910)\" d=\"\n",
       "M264.865 1410.9 L2112.76 1410.9 L2112.76 47.2441 L264.865 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip912\">\n",
       "    <rect x=\"264\" y=\"47\" width=\"1849\" height=\"1365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  392.132,1410.9 392.132,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  641.174,1410.9 641.174,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  890.216,1410.9 890.216,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1139.26,1410.9 1139.26,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1388.3,1410.9 1388.3,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1637.34,1410.9 1637.34,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1886.38,1410.9 1886.38,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  264.865,1372.3 2112.76,1372.3 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  264.865,1086.42 2112.76,1086.42 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  264.865,800.541 2112.76,800.541 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  264.865,514.66 2112.76,514.66 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  264.865,228.779 2112.76,228.779 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  264.865,1410.9 2112.76,1410.9 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  264.865,1410.9 264.865,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  392.132,1410.9 392.132,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  641.174,1410.9 641.174,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  890.216,1410.9 890.216,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1139.26,1410.9 1139.26,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1388.3,1410.9 1388.3,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1637.34,1410.9 1637.34,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1886.38,1410.9 1886.38,1394.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  264.865,1372.3 287.039,1372.3 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  264.865,1086.42 287.039,1086.42 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  264.865,800.541 287.039,800.541 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  264.865,514.66 287.039,514.66 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  264.865,228.779 287.039,228.779 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip910)\" d=\"M 0 0 M360.648 1481.97 L368.286 1481.97 L368.286 1455.6 L359.976 1457.27 L359.976 1453.01 L368.24 1451.34 L372.916 1451.34 L372.916 1481.97 L380.555 1481.97 L380.555 1485.9 L360.648 1485.9 L360.648 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M395.624 1454.42 Q392.013 1454.42 390.184 1457.99 Q388.379 1461.53 388.379 1468.66 Q388.379 1475.77 390.184 1479.33 Q392.013 1482.87 395.624 1482.87 Q399.258 1482.87 401.064 1479.33 Q402.893 1475.77 402.893 1468.66 Q402.893 1461.53 401.064 1457.99 Q399.258 1454.42 395.624 1454.42 M395.624 1450.72 Q401.434 1450.72 404.49 1455.33 Q407.569 1459.91 407.569 1468.66 Q407.569 1477.39 404.49 1481.99 Q401.434 1486.58 395.624 1486.58 Q389.814 1486.58 386.735 1481.99 Q383.68 1477.39 383.68 1468.66 Q383.68 1459.91 386.735 1455.33 Q389.814 1450.72 395.624 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M408.114 1455.3 L414.321 1455.3 L414.321 1433.87 L407.569 1435.23 L407.569 1431.77 L414.283 1430.41 L418.082 1430.41 L418.082 1455.3 L424.289 1455.3 L424.289 1458.49 L408.114 1458.49 L408.114 1455.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M609.135 1481.97 L616.774 1481.97 L616.774 1455.6 L608.463 1457.27 L608.463 1453.01 L616.727 1451.34 L621.403 1451.34 L621.403 1481.97 L629.042 1481.97 L629.042 1485.9 L609.135 1485.9 L609.135 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M644.111 1454.42 Q640.5 1454.42 638.672 1457.99 Q636.866 1461.53 636.866 1468.66 Q636.866 1475.77 638.672 1479.33 Q640.5 1482.87 644.111 1482.87 Q647.746 1482.87 649.551 1479.33 Q651.38 1475.77 651.38 1468.66 Q651.38 1461.53 649.551 1457.99 Q647.746 1454.42 644.111 1454.42 M644.111 1450.72 Q649.922 1450.72 652.977 1455.33 Q656.056 1459.91 656.056 1468.66 Q656.056 1477.39 652.977 1481.99 Q649.922 1486.58 644.111 1486.58 Q638.301 1486.58 635.223 1481.99 Q632.167 1477.39 632.167 1468.66 Q632.167 1459.91 635.223 1455.33 Q638.301 1450.72 644.111 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M660.626 1455.3 L673.885 1455.3 L673.885 1458.49 L656.056 1458.49 L656.056 1455.3 Q658.219 1453.06 661.943 1449.3 Q665.685 1445.52 666.645 1444.43 Q668.469 1442.38 669.184 1440.96 Q669.917 1439.54 669.917 1438.16 Q669.917 1435.92 668.337 1434.51 Q666.776 1433.1 664.256 1433.1 Q662.469 1433.1 660.476 1433.72 Q658.501 1434.34 656.244 1435.6 L656.244 1431.77 Q658.538 1430.85 660.532 1430.38 Q662.526 1429.91 664.181 1429.91 Q668.544 1429.91 671.14 1432.09 Q673.735 1434.27 673.735 1437.92 Q673.735 1439.65 673.077 1441.21 Q672.437 1442.75 670.726 1444.86 Q670.256 1445.4 667.735 1448.02 Q665.215 1450.61 660.626 1455.3 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M857.848 1481.97 L865.486 1481.97 L865.486 1455.6 L857.176 1457.27 L857.176 1453.01 L865.44 1451.34 L870.116 1451.34 L870.116 1481.97 L877.755 1481.97 L877.755 1485.9 L857.848 1485.9 L857.848 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M892.824 1454.42 Q889.213 1454.42 887.384 1457.99 Q885.579 1461.53 885.579 1468.66 Q885.579 1475.77 887.384 1479.33 Q889.213 1482.87 892.824 1482.87 Q896.458 1482.87 898.264 1479.33 Q900.093 1475.77 900.093 1468.66 Q900.093 1461.53 898.264 1457.99 Q896.458 1454.42 892.824 1454.42 M892.824 1450.72 Q898.634 1450.72 901.69 1455.33 Q904.769 1459.91 904.769 1468.66 Q904.769 1477.39 901.69 1481.99 Q898.634 1486.58 892.824 1486.58 Q887.014 1486.58 883.935 1481.99 Q880.88 1477.39 880.88 1468.66 Q880.88 1459.91 883.935 1455.33 Q887.014 1450.72 892.824 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M917.464 1443.35 Q920.191 1443.94 921.714 1445.78 Q923.257 1447.62 923.257 1450.33 Q923.257 1454.49 920.398 1456.76 Q917.539 1459.04 912.273 1459.04 Q910.505 1459.04 908.624 1458.68 Q906.762 1458.34 904.769 1457.65 L904.769 1453.98 Q906.348 1454.9 908.229 1455.37 Q910.11 1455.84 912.16 1455.84 Q915.733 1455.84 917.595 1454.43 Q919.476 1453.02 919.476 1450.33 Q919.476 1447.85 917.727 1446.46 Q915.997 1445.05 912.894 1445.05 L909.621 1445.05 L909.621 1441.92 L913.044 1441.92 Q915.846 1441.92 917.332 1440.81 Q918.818 1439.69 918.818 1437.58 Q918.818 1435.42 917.276 1434.27 Q915.752 1433.1 912.894 1433.1 Q911.332 1433.1 909.546 1433.44 Q907.759 1433.78 905.615 1434.49 L905.615 1431.11 Q907.778 1430.51 909.659 1430.21 Q911.558 1429.91 913.232 1429.91 Q917.558 1429.91 920.078 1431.88 Q922.598 1433.84 922.598 1437.18 Q922.598 1439.52 921.263 1441.13 Q919.928 1442.73 917.464 1443.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1105.9 1481.97 L1113.54 1481.97 L1113.54 1455.6 L1105.23 1457.27 L1105.23 1453.01 L1113.49 1451.34 L1118.17 1451.34 L1118.17 1481.97 L1125.81 1481.97 L1125.81 1485.9 L1105.9 1485.9 L1105.9 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1140.88 1454.42 Q1137.27 1454.42 1135.44 1457.99 Q1133.63 1461.53 1133.63 1468.66 Q1133.63 1475.77 1135.44 1479.33 Q1137.27 1482.87 1140.88 1482.87 Q1144.51 1482.87 1146.32 1479.33 Q1148.15 1475.77 1148.15 1468.66 Q1148.15 1461.53 1146.32 1457.99 Q1144.51 1454.42 1140.88 1454.42 M1140.88 1450.72 Q1146.69 1450.72 1149.74 1455.33 Q1152.82 1459.91 1152.82 1468.66 Q1152.82 1477.39 1149.74 1481.99 Q1146.69 1486.58 1140.88 1486.58 Q1135.07 1486.58 1131.99 1481.99 Q1128.93 1477.39 1128.93 1468.66 Q1128.93 1459.91 1131.99 1455.33 Q1135.07 1450.72 1140.88 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1165.5 1433.72 L1155.91 1448.71 L1165.5 1448.71 L1165.5 1433.72 M1164.5 1430.41 L1169.28 1430.41 L1169.28 1448.71 L1173.29 1448.71 L1173.29 1451.87 L1169.28 1451.87 L1169.28 1458.49 L1165.5 1458.49 L1165.5 1451.87 L1152.82 1451.87 L1152.82 1448.21 L1164.5 1430.41 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1356.09 1481.97 L1363.73 1481.97 L1363.73 1455.6 L1355.42 1457.27 L1355.42 1453.01 L1363.68 1451.34 L1368.36 1451.34 L1368.36 1481.97 L1376 1481.97 L1376 1485.9 L1356.09 1485.9 L1356.09 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1391.07 1454.42 Q1387.46 1454.42 1385.63 1457.99 Q1383.82 1461.53 1383.82 1468.66 Q1383.82 1475.77 1385.63 1479.33 Q1387.46 1482.87 1391.07 1482.87 Q1394.7 1482.87 1396.51 1479.33 Q1398.34 1475.77 1398.34 1468.66 Q1398.34 1461.53 1396.51 1457.99 Q1394.7 1454.42 1391.07 1454.42 M1391.07 1450.72 Q1396.88 1450.72 1399.93 1455.33 Q1403.01 1459.91 1403.01 1468.66 Q1403.01 1477.39 1399.93 1481.99 Q1396.88 1486.58 1391.07 1486.58 Q1385.26 1486.58 1382.18 1481.99 Q1379.12 1477.39 1379.12 1468.66 Q1379.12 1459.91 1382.18 1455.33 Q1385.26 1450.72 1391.07 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1404.2 1430.41 L1419.11 1430.41 L1419.11 1433.61 L1407.68 1433.61 L1407.68 1440.49 Q1408.5 1440.21 1409.33 1440.08 Q1410.16 1439.93 1410.99 1439.93 Q1415.69 1439.93 1418.43 1442.51 Q1421.18 1445.08 1421.18 1449.48 Q1421.18 1454.02 1418.36 1456.54 Q1415.54 1459.04 1410.4 1459.04 Q1408.64 1459.04 1406.79 1458.74 Q1404.97 1458.44 1403.01 1457.84 L1403.01 1454.02 Q1404.71 1454.94 1406.51 1455.39 Q1408.32 1455.84 1410.33 1455.84 Q1413.58 1455.84 1415.48 1454.13 Q1417.38 1452.42 1417.38 1449.48 Q1417.38 1446.55 1415.48 1444.84 Q1413.58 1443.13 1410.33 1443.13 Q1408.81 1443.13 1407.28 1443.47 Q1405.78 1443.8 1404.2 1444.52 L1404.2 1430.41 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1604.52 1481.97 L1612.16 1481.97 L1612.16 1455.6 L1603.85 1457.27 L1603.85 1453.01 L1612.11 1451.34 L1616.79 1451.34 L1616.79 1481.97 L1624.43 1481.97 L1624.43 1485.9 L1604.52 1485.9 L1604.52 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1639.5 1454.42 Q1635.89 1454.42 1634.06 1457.99 Q1632.25 1461.53 1632.25 1468.66 Q1632.25 1475.77 1634.06 1479.33 Q1635.89 1482.87 1639.5 1482.87 Q1643.13 1482.87 1644.94 1479.33 Q1646.77 1475.77 1646.77 1468.66 Q1646.77 1461.53 1644.94 1457.99 Q1643.13 1454.42 1639.5 1454.42 M1639.5 1450.72 Q1645.31 1450.72 1648.36 1455.33 Q1651.44 1459.91 1651.44 1468.66 Q1651.44 1477.39 1648.36 1481.99 Q1645.31 1486.58 1639.5 1486.58 Q1633.69 1486.58 1630.61 1481.99 Q1627.55 1477.39 1627.55 1468.66 Q1627.55 1459.91 1630.61 1455.33 Q1633.69 1450.72 1639.5 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1661.47 1442.94 Q1658.91 1442.94 1657.41 1444.69 Q1655.92 1446.44 1655.92 1449.48 Q1655.92 1452.51 1657.41 1454.28 Q1658.91 1456.03 1661.47 1456.03 Q1664.03 1456.03 1665.51 1454.28 Q1667.02 1452.51 1667.02 1449.48 Q1667.02 1446.44 1665.51 1444.69 Q1664.03 1442.94 1661.47 1442.94 M1669.01 1431.03 L1669.01 1434.49 Q1667.58 1433.82 1666.11 1433.46 Q1664.66 1433.1 1663.24 1433.1 Q1659.47 1433.1 1657.48 1435.64 Q1655.51 1438.18 1655.22 1443.32 Q1656.33 1441.68 1658.01 1440.81 Q1659.68 1439.93 1661.69 1439.93 Q1665.93 1439.93 1668.37 1442.51 Q1670.83 1445.06 1670.83 1449.48 Q1670.83 1453.81 1668.28 1456.42 Q1665.72 1459.04 1661.47 1459.04 Q1656.6 1459.04 1654.02 1455.31 Q1651.44 1451.57 1651.44 1444.48 Q1651.44 1437.82 1654.6 1433.87 Q1657.76 1429.91 1663.09 1429.91 Q1664.51 1429.91 1665.96 1430.19 Q1667.43 1430.47 1669.01 1431.03 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1854.23 1481.97 L1861.87 1481.97 L1861.87 1455.6 L1853.56 1457.27 L1853.56 1453.01 L1861.82 1451.34 L1866.5 1451.34 L1866.5 1481.97 L1874.14 1481.97 L1874.14 1485.9 L1854.23 1485.9 L1854.23 1481.97 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1889.21 1454.42 Q1885.6 1454.42 1883.77 1457.99 Q1881.96 1461.53 1881.96 1468.66 Q1881.96 1475.77 1883.77 1479.33 Q1885.6 1482.87 1889.21 1482.87 Q1892.84 1482.87 1894.65 1479.33 Q1896.48 1475.77 1896.48 1468.66 Q1896.48 1461.53 1894.65 1457.99 Q1892.84 1454.42 1889.21 1454.42 M1889.21 1450.72 Q1895.02 1450.72 1898.07 1455.33 Q1901.15 1459.91 1901.15 1468.66 Q1901.15 1477.39 1898.07 1481.99 Q1895.02 1486.58 1889.21 1486.58 Q1883.4 1486.58 1880.32 1481.99 Q1877.26 1477.39 1877.26 1468.66 Q1877.26 1459.91 1880.32 1455.33 Q1883.4 1450.72 1889.21 1450.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1901.15 1430.41 L1919.21 1430.41 L1919.21 1432.03 L1909.01 1458.49 L1905.05 1458.49 L1914.64 1433.61 L1901.15 1433.61 L1901.15 1430.41 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M126.931 1392.1 L134.57 1392.1 L134.57 1365.73 L126.26 1367.4 L126.26 1363.14 L134.524 1361.47 L139.2 1361.47 L139.2 1392.1 L146.839 1392.1 L146.839 1396.03 L126.931 1396.03 L126.931 1392.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M161.908 1364.55 Q158.297 1364.55 156.468 1368.11 Q154.663 1371.66 154.663 1378.79 Q154.663 1385.89 156.468 1389.46 Q158.297 1393 161.908 1393 Q165.542 1393 167.348 1389.46 Q169.177 1385.89 169.177 1378.79 Q169.177 1371.66 167.348 1368.11 Q165.542 1364.55 161.908 1364.55 M161.908 1360.85 Q167.718 1360.85 170.774 1365.45 Q173.852 1370.04 173.852 1378.79 Q173.852 1387.51 170.774 1392.12 Q167.718 1396.7 161.908 1396.7 Q156.098 1396.7 153.019 1392.12 Q149.964 1387.51 149.964 1378.79 Q149.964 1370.04 153.019 1365.45 Q156.098 1360.85 161.908 1360.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M173.852 1354.95 L197.964 1354.95 L197.964 1358.14 L173.852 1358.14 L173.852 1354.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M202.741 1365.42 L208.948 1365.42 L208.948 1344 L202.196 1345.35 L202.196 1341.89 L208.91 1340.54 L212.709 1340.54 L212.709 1365.42 L218.916 1365.42 L218.916 1368.62 L202.741 1368.62 L202.741 1365.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M231.16 1343.04 Q228.226 1343.04 226.74 1345.94 Q225.273 1348.82 225.273 1354.61 Q225.273 1360.38 226.74 1363.28 Q228.226 1366.16 231.16 1366.16 Q234.113 1366.16 235.58 1363.28 Q237.065 1360.38 237.065 1354.61 Q237.065 1348.82 235.58 1345.94 Q234.113 1343.04 231.16 1343.04 M231.16 1340.03 Q235.881 1340.03 238.363 1343.77 Q240.865 1347.5 240.865 1354.61 Q240.865 1361.7 238.363 1365.44 Q235.881 1369.17 231.16 1369.17 Q226.439 1369.17 223.938 1365.44 Q221.455 1361.7 221.455 1354.61 Q221.455 1347.5 223.938 1343.77 Q226.439 1340.03 231.16 1340.03 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M147.959 1106.21 L155.597 1106.21 L155.597 1079.85 L147.287 1081.52 L147.287 1077.26 L155.551 1075.59 L160.227 1075.59 L160.227 1106.21 L167.866 1106.21 L167.866 1110.15 L147.959 1110.15 L147.959 1106.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M182.935 1078.67 Q179.324 1078.67 177.495 1082.23 Q175.69 1085.77 175.69 1092.9 Q175.69 1100.01 177.495 1103.58 Q179.324 1107.12 182.935 1107.12 Q186.569 1107.12 188.375 1103.58 Q190.204 1100.01 190.204 1092.9 Q190.204 1085.77 188.375 1082.23 Q186.569 1078.67 182.935 1078.67 M182.935 1074.96 Q188.745 1074.96 191.801 1079.57 Q194.88 1084.15 194.88 1092.9 Q194.88 1101.63 191.801 1106.24 Q188.745 1110.82 182.935 1110.82 Q177.125 1110.82 174.046 1106.24 Q170.991 1101.63 170.991 1092.9 Q170.991 1084.15 174.046 1079.57 Q177.125 1074.96 182.935 1074.96 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M194.88 1069.07 L218.991 1069.07 L218.991 1072.26 L194.88 1072.26 L194.88 1069.07 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M231.235 1069.4 Q228.527 1069.4 226.966 1070.85 Q225.423 1072.3 225.423 1074.84 Q225.423 1077.38 226.966 1078.83 Q228.527 1080.28 231.235 1080.28 Q233.943 1080.28 235.504 1078.83 Q237.065 1077.36 237.065 1074.84 Q237.065 1072.3 235.504 1070.85 Q233.962 1069.4 231.235 1069.4 M227.436 1067.79 Q224.991 1067.18 223.618 1065.51 Q222.264 1063.84 222.264 1061.43 Q222.264 1058.06 224.652 1056.11 Q227.06 1054.15 231.235 1054.15 Q235.429 1054.15 237.818 1056.11 Q240.206 1058.06 240.206 1061.43 Q240.206 1063.84 238.833 1065.51 Q237.479 1067.18 235.053 1067.79 Q237.799 1068.43 239.322 1070.29 Q240.865 1072.15 240.865 1074.84 Q240.865 1078.92 238.363 1081.1 Q235.881 1083.28 231.235 1083.28 Q226.589 1083.28 224.088 1081.1 Q221.605 1078.92 221.605 1074.84 Q221.605 1072.15 223.148 1070.29 Q224.69 1068.43 227.436 1067.79 M226.044 1061.79 Q226.044 1063.97 227.398 1065.19 Q228.771 1066.41 231.235 1066.41 Q233.68 1066.41 235.053 1065.19 Q236.445 1063.97 236.445 1061.79 Q236.445 1059.61 235.053 1058.38 Q233.68 1057.16 231.235 1057.16 Q228.771 1057.16 227.398 1058.38 Q226.044 1059.61 226.044 1061.79 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M147.752 820.333 L155.39 820.333 L155.39 793.967 L147.08 795.634 L147.08 791.375 L155.344 789.708 L160.02 789.708 L160.02 820.333 L167.659 820.333 L167.659 824.268 L147.752 824.268 L147.752 820.333 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M182.728 792.787 Q179.117 792.787 177.289 796.352 Q175.483 799.893 175.483 807.023 Q175.483 814.129 177.289 817.694 Q179.117 821.236 182.728 821.236 Q186.363 821.236 188.168 817.694 Q189.997 814.129 189.997 807.023 Q189.997 799.893 188.168 796.352 Q186.363 792.787 182.728 792.787 M182.728 789.083 Q188.538 789.083 191.594 793.69 Q194.673 798.273 194.673 807.023 Q194.673 815.75 191.594 820.356 Q188.538 824.94 182.728 824.94 Q176.918 824.94 173.839 820.356 Q170.784 815.75 170.784 807.023 Q170.784 798.273 173.839 793.69 Q176.918 789.083 182.728 789.083 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M194.673 783.185 L218.784 783.185 L218.784 786.382 L194.673 786.382 L194.673 783.185 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M231.498 781.304 Q228.94 781.304 227.436 783.053 Q225.95 784.802 225.95 787.849 Q225.95 790.877 227.436 792.645 Q228.94 794.394 231.498 794.394 Q234.056 794.394 235.542 792.645 Q237.047 790.877 237.047 787.849 Q237.047 784.802 235.542 783.053 Q234.056 781.304 231.498 781.304 M239.04 769.398 L239.04 772.859 Q237.611 772.182 236.144 771.825 Q234.696 771.467 233.266 771.467 Q229.505 771.467 227.511 774.006 Q225.536 776.545 225.254 781.68 Q226.364 780.044 228.038 779.179 Q229.712 778.295 231.724 778.295 Q235.956 778.295 238.401 780.871 Q240.865 783.429 240.865 787.849 Q240.865 792.175 238.307 794.789 Q235.749 797.403 231.498 797.403 Q226.627 797.403 224.05 793.679 Q221.474 789.937 221.474 782.846 Q221.474 776.188 224.633 772.238 Q227.793 768.27 233.116 768.27 Q234.545 768.27 235.993 768.552 Q237.46 768.834 239.04 769.398 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M147.488 534.452 L155.127 534.452 L155.127 508.086 L146.817 509.753 L146.817 505.494 L155.081 503.827 L159.757 503.827 L159.757 534.452 L167.396 534.452 L167.396 538.387 L147.488 538.387 L147.488 534.452 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M182.465 506.906 Q178.854 506.906 177.025 510.471 Q175.22 514.012 175.22 521.142 Q175.22 528.248 177.025 531.813 Q178.854 535.355 182.465 535.355 Q186.099 535.355 187.905 531.813 Q189.733 528.248 189.733 521.142 Q189.733 514.012 187.905 510.471 Q186.099 506.906 182.465 506.906 M182.465 503.202 Q188.275 503.202 191.331 507.809 Q194.409 512.392 194.409 521.142 Q194.409 529.869 191.331 534.475 Q188.275 539.058 182.465 539.058 Q176.655 539.058 173.576 534.475 Q170.521 529.869 170.521 521.142 Q170.521 512.392 173.576 507.809 Q176.655 503.202 182.465 503.202 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M194.409 497.304 L218.521 497.304 L218.521 500.501 L194.409 500.501 L194.409 497.304 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M233.078 486.207 L223.486 501.197 L233.078 501.197 L233.078 486.207 M232.081 482.897 L236.859 482.897 L236.859 501.197 L240.865 501.197 L240.865 504.356 L236.859 504.356 L236.859 510.977 L233.078 510.977 L233.078 504.356 L220.402 504.356 L220.402 500.689 L232.081 482.897 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M149.181 248.571 L156.82 248.571 L156.82 222.205 L148.51 223.872 L148.51 219.613 L156.774 217.946 L161.449 217.946 L161.449 248.571 L169.088 248.571 L169.088 252.506 L149.181 252.506 L149.181 248.571 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M184.158 221.025 Q180.547 221.025 178.718 224.59 Q176.912 228.131 176.912 235.261 Q176.912 242.367 178.718 245.932 Q180.547 249.474 184.158 249.474 Q187.792 249.474 189.597 245.932 Q191.426 242.367 191.426 235.261 Q191.426 228.131 189.597 224.59 Q187.792 221.025 184.158 221.025 M184.158 217.321 Q189.968 217.321 193.023 221.928 Q196.102 226.511 196.102 235.261 Q196.102 243.988 193.023 248.594 Q189.968 253.177 184.158 253.177 Q178.348 253.177 175.269 248.594 Q172.213 243.988 172.213 235.261 Q172.213 226.511 175.269 221.928 Q178.348 217.321 184.158 217.321 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M196.102 211.423 L220.214 211.423 L220.214 214.62 L196.102 214.62 L196.102 211.423 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M227.605 221.898 L240.865 221.898 L240.865 225.096 L223.035 225.096 L223.035 221.898 Q225.198 219.66 228.922 215.899 Q232.664 212.118 233.624 211.028 Q235.448 208.977 236.163 207.567 Q236.896 206.138 236.896 204.765 Q236.896 202.526 235.316 201.116 Q233.755 199.705 231.235 199.705 Q229.448 199.705 227.455 200.326 Q225.48 200.947 223.223 202.207 L223.223 198.37 Q225.517 197.448 227.511 196.978 Q229.505 196.508 231.16 196.508 Q235.523 196.508 238.119 198.69 Q240.714 200.871 240.714 204.52 Q240.714 206.25 240.056 207.811 Q239.416 209.354 237.705 211.46 Q237.235 212.006 234.714 214.62 Q232.194 217.215 227.605 221.898 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1207.97 1518.33 L1207.97 1525.11 Q1204.72 1522.08 1201.03 1520.59 Q1197.37 1519.09 1193.23 1519.09 Q1185.09 1519.09 1180.76 1524.09 Q1176.43 1529.05 1176.43 1538.47 Q1176.43 1547.86 1180.76 1552.86 Q1185.09 1557.82 1193.23 1557.82 Q1197.37 1557.82 1201.03 1556.33 Q1204.72 1554.83 1207.97 1551.81 L1207.97 1558.53 Q1204.6 1560.82 1200.81 1561.96 Q1197.05 1563.11 1192.85 1563.11 Q1182.06 1563.11 1175.86 1556.52 Q1169.65 1549.9 1169.65 1538.47 Q1169.65 1527.01 1175.86 1520.43 Q1182.06 1513.81 1192.85 1513.81 Q1197.12 1513.81 1200.87 1514.95 Q1204.66 1516.07 1207.97 1518.33 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M69.7664 810.981 Q63.4007 810.981 59.8996 813.623 Q56.3984 816.233 56.3984 820.975 Q56.3984 825.686 59.8996 828.328 Q63.4007 830.938 69.7664 830.938 Q76.1003 830.938 79.6014 828.328 Q83.1026 825.686 83.1026 820.975 Q83.1026 816.233 79.6014 813.623 Q76.1003 810.981 69.7664 810.981 M83.58 805.125 Q92.683 805.125 97.1071 809.167 Q101.563 813.209 101.563 821.548 Q101.563 824.636 101.086 827.373 Q100.64 830.11 99.6852 832.688 L93.9879 832.688 Q95.3884 830.11 96.0568 827.596 Q96.7252 825.081 96.7252 822.471 Q96.7252 816.71 93.7015 813.846 Q90.7096 810.981 84.6303 810.981 L81.7339 810.981 Q84.885 812.795 86.4446 815.628 Q88.0042 818.461 88.0042 822.408 Q88.0042 828.964 83.0071 832.975 Q78.01 836.985 69.7664 836.985 Q61.491 836.985 56.4939 832.975 Q51.4968 828.964 51.4968 822.408 Q51.4968 818.461 53.0564 815.628 Q54.616 812.795 57.7671 810.981 L52.3562 810.981 L52.3562 805.125 L83.58 805.125 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M70.0847 782.781 Q70.0847 789.879 71.7079 792.616 Q73.3312 795.353 77.2461 795.353 Q80.3653 795.353 82.2114 793.316 Q84.0256 791.247 84.0256 787.714 Q84.0256 782.845 80.5881 779.916 Q77.1188 776.956 71.3897 776.956 L70.0847 776.956 L70.0847 782.781 M67.6657 771.1 L88.0042 771.1 L88.0042 776.956 L82.5933 776.956 Q85.8398 778.962 87.3994 781.953 Q88.9272 784.945 88.9272 789.274 Q88.9272 794.749 85.8716 797.995 Q82.7843 801.21 77.6281 801.21 Q71.6125 801.21 68.5569 797.199 Q65.5014 793.157 65.5014 785.168 L65.5014 776.956 L64.9285 776.956 Q60.8862 776.956 58.6901 779.63 Q56.4621 782.272 56.4621 787.078 Q56.4621 790.133 57.1941 793.03 Q57.9262 795.926 59.3903 798.6 L53.9795 798.6 Q52.7381 795.385 52.1334 792.361 Q51.4968 789.338 51.4968 786.473 Q51.4968 778.739 55.5072 774.919 Q59.5176 771.1 67.6657 771.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M59.1993 737.203 Q55.2526 735.006 53.3747 731.951 Q51.4968 728.895 51.4968 724.758 Q51.4968 719.188 55.4117 716.164 Q59.2948 713.14 66.4881 713.14 L88.0042 713.14 L88.0042 719.028 L66.679 719.028 Q61.5546 719.028 59.072 720.843 Q56.5894 722.657 56.5894 726.381 Q56.5894 730.932 59.6131 733.574 Q62.6368 736.216 67.8567 736.216 L88.0042 736.216 L88.0042 742.104 L66.679 742.104 Q61.5228 742.104 59.072 743.918 Q56.5894 745.733 56.5894 749.52 Q56.5894 754.008 59.6449 756.65 Q62.6686 759.292 67.8567 759.292 L88.0042 759.292 L88.0042 765.18 L52.3562 765.18 L52.3562 759.292 L57.8944 759.292 Q54.616 757.286 53.0564 754.485 Q51.4968 751.685 51.4968 747.833 Q51.4968 743.95 53.4702 741.245 Q55.4436 738.508 59.1993 737.203 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M59.1993 679.243 Q55.2526 677.047 53.3747 673.991 Q51.4968 670.936 51.4968 666.798 Q51.4968 661.228 55.4117 658.204 Q59.2948 655.18 66.4881 655.18 L88.0042 655.18 L88.0042 661.069 L66.679 661.069 Q61.5546 661.069 59.072 662.883 Q56.5894 664.697 56.5894 668.421 Q56.5894 672.973 59.6131 675.614 Q62.6368 678.256 67.8567 678.256 L88.0042 678.256 L88.0042 684.144 L66.679 684.144 Q61.5228 684.144 59.072 685.959 Q56.5894 687.773 56.5894 691.56 Q56.5894 696.048 59.6449 698.69 Q62.6686 701.332 67.8567 701.332 L88.0042 701.332 L88.0042 707.22 L52.3562 707.22 L52.3562 701.332 L57.8944 701.332 Q54.616 699.327 53.0564 696.526 Q51.4968 693.725 51.4968 689.874 Q51.4968 685.99 53.4702 683.285 Q55.4436 680.548 59.1993 679.243 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M70.0847 632.837 Q70.0847 639.935 71.7079 642.672 Q73.3312 645.409 77.2461 645.409 Q80.3653 645.409 82.2114 643.372 Q84.0256 641.303 84.0256 637.77 Q84.0256 632.9 80.5881 629.972 Q77.1188 627.012 71.3897 627.012 L70.0847 627.012 L70.0847 632.837 M67.6657 621.156 L88.0042 621.156 L88.0042 627.012 L82.5933 627.012 Q85.8398 629.017 87.3994 632.009 Q88.9272 635.001 88.9272 639.33 Q88.9272 644.804 85.8716 648.051 Q82.7843 651.266 77.6281 651.266 Q71.6125 651.266 68.5569 647.255 Q65.5014 643.213 65.5014 635.224 L65.5014 627.012 L64.9285 627.012 Q60.8862 627.012 58.6901 629.686 Q56.4621 632.328 56.4621 637.134 Q56.4621 640.189 57.1941 643.086 Q57.9262 645.982 59.3903 648.656 L53.9795 648.656 Q52.7381 645.441 52.1334 642.417 Q51.4968 639.393 51.4968 636.529 Q51.4968 628.795 55.5072 624.975 Q59.5176 621.156 67.6657 621.156 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><circle clip-path=\"url(#clip912)\" cx=\"704.562\" cy=\"1229.36\" r=\"53\" fill=\"#fabe24\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"2060.46\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"2060.46\" cy=\"943.481\" r=\"57\" fill=\"#f1e865\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"317.163\" cy=\"800.541\" r=\"56\" fill=\"#f4dd4f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"2060.46\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1673.06\" cy=\"1372.3\" r=\"57\" fill=\"#f3e156\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1866.76\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"704.562\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"898.261\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1091.96\" cy=\"1086.42\" r=\"57\" fill=\"#f3e056\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1479.36\" cy=\"1372.3\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1091.96\" cy=\"657.6\" r=\"58\" fill=\"#f1ef75\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"898.261\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"317.163\" cy=\"514.66\" r=\"57\" fill=\"#f1e864\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"2060.46\" cy=\"1372.3\" r=\"59\" fill=\"#f4f78e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1866.76\" cy=\"1372.3\" r=\"57\" fill=\"#f1e865\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"898.261\" cy=\"943.481\" r=\"56\" fill=\"#f4dd4f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1673.06\" cy=\"657.6\" r=\"57\" fill=\"#f3e156\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1866.76\" cy=\"800.541\" r=\"57\" fill=\"#f2e45e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1285.66\" cy=\"943.481\" r=\"60\" fill=\"#f9fc9d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1285.66\" cy=\"1229.36\" r=\"56\" fill=\"#f4dd4f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"704.562\" cy=\"1372.3\" r=\"48\" fill=\"#fa9406\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"510.863\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"704.562\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1866.76\" cy=\"1229.36\" r=\"61\" fill=\"#fcfea4\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1673.06\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"898.261\" cy=\"514.66\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1285.66\" cy=\"1086.42\" r=\"59\" fill=\"#f1f27d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1091.96\" cy=\"800.541\" r=\"61\" fill=\"#fbfea4\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"898.261\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"2060.46\" cy=\"1086.42\" r=\"60\" fill=\"#f8fc9d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1091.96\" cy=\"943.481\" r=\"59\" fill=\"#f4f78d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"704.562\" cy=\"657.6\" r=\"59\" fill=\"#f4f78d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"898.261\" cy=\"657.6\" r=\"60\" fill=\"#f6fa95\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1673.06\" cy=\"1229.36\" r=\"58\" fill=\"#f1ef75\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"704.562\" cy=\"514.66\" r=\"56\" fill=\"#f4dd4f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"510.863\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"2060.46\" cy=\"657.6\" r=\"57\" fill=\"#f3e156\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"704.562\" cy=\"943.481\" r=\"55\" fill=\"#f7d13d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"317.163\" cy=\"1229.36\" r=\"32\" fill=\"#b33259\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1091.96\" cy=\"1229.36\" r=\"55\" fill=\"#f7d13d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1866.76\" cy=\"943.481\" r=\"57\" fill=\"#f1e865\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1285.66\" cy=\"514.66\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1479.36\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"898.261\" cy=\"1086.42\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1091.96\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"317.163\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1479.36\" cy=\"514.66\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1285.66\" cy=\"800.541\" r=\"59\" fill=\"#f1f27d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1479.36\" cy=\"1229.36\" r=\"57\" fill=\"#f2e45d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"510.863\" cy=\"943.481\" r=\"55\" fill=\"#f8cd37\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"317.163\" cy=\"657.6\" r=\"56\" fill=\"#f4dd4f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"898.261\" cy=\"800.541\" r=\"60\" fill=\"#f8fc9d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1091.96\" cy=\"1372.3\" r=\"53\" fill=\"#fac228\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"510.863\" cy=\"800.541\" r=\"57\" fill=\"#f2e45d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1866.76\" cy=\"1086.42\" r=\"60\" fill=\"#f8fc9d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"704.562\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1479.36\" cy=\"1086.42\" r=\"60\" fill=\"#f6fa95\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"510.863\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1673.06\" cy=\"943.481\" r=\"59\" fill=\"#f4f78e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"704.562\" cy=\"800.541\" r=\"59\" fill=\"#f1f27d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1479.36\" cy=\"657.6\" r=\"57\" fill=\"#f3e156\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1285.66\" cy=\"657.6\" r=\"59\" fill=\"#f1f27d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1479.36\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1866.76\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"510.863\" cy=\"1229.36\" r=\"49\" fill=\"#fb9c06\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"2060.46\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"2060.46\" cy=\"514.66\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1285.66\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1673.06\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"317.163\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1479.36\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1285.66\" cy=\"1372.3\" r=\"56\" fill=\"#f5d949\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1866.76\" cy=\"85.838\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1866.76\" cy=\"657.6\" r=\"57\" fill=\"#f3e156\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"317.163\" cy=\"1086.42\" r=\"51\" fill=\"#fbab0f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"510.863\" cy=\"514.66\" r=\"57\" fill=\"#f1e865\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"510.863\" cy=\"1372.3\" r=\"14\" fill=\"#34095f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1091.96\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"2060.46\" cy=\"800.541\" r=\"57\" fill=\"#f2e45e\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"898.261\" cy=\"1229.36\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"898.261\" cy=\"1372.3\" r=\"52\" fill=\"#fbba1f\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"510.863\" cy=\"1086.42\" r=\"53\" fill=\"#fac228\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"317.163\" cy=\"371.719\" r=\"53\" fill=\"#fac228\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1091.96\" cy=\"514.66\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1673.06\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"704.562\" cy=\"1086.42\" r=\"56\" fill=\"#f5d949\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1479.36\" cy=\"800.541\" r=\"57\" fill=\"#f2e45d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1673.06\" cy=\"1086.42\" r=\"61\" fill=\"#fbfea4\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1091.96\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"317.163\" cy=\"943.481\" r=\"53\" fill=\"#fac228\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"510.863\" cy=\"657.6\" r=\"59\" fill=\"#f4f78d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1673.06\" cy=\"514.66\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"317.163\" cy=\"1372.3\" r=\"3\" fill=\"#000003\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1673.06\" cy=\"800.541\" r=\"57\" fill=\"#f1e865\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1866.76\" cy=\"514.66\" r=\"55\" fill=\"#f6d543\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1285.66\" cy=\"228.779\" r=\"4\" fill=\"#010109\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1285.66\" cy=\"371.719\" r=\"53\" fill=\"#fabe23\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"1479.36\" cy=\"943.481\" r=\"60\" fill=\"#f9fc9d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<circle clip-path=\"url(#clip912)\" cx=\"2060.46\" cy=\"1229.36\" r=\"60\" fill=\"#f9fc9d\" fill-rule=\"evenodd\" fill-opacity=\"1\" stroke=\"#000000\" stroke-opacity=\"1\" stroke-width=\"3.2\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip913\">\n",
       "    <rect x=\"2160\" y=\"47\" width=\"73\" height=\"1365\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<g clip-path=\"url(#clip913)\">\n",
       "<image width=\"72\" height=\"1364\" xlink:href=\"data:image/png;base64,\n",
       "iVBORw0KGgoAAAANSUhEUgAAAEgAAAVUCAYAAABzwV4AAAAL0klEQVR4nO3dwY3sRhAFQY5Q/lsh\n",
       "L6XfLQtUeSQPERYsEg+zZJM7+/v3/H0f/tdfb/8AXydQECgIFAQKc+6/b/8Mn2ZBQaAgUBAozL1/\n",
       "3v4ZPs2CgkBBoCBQcCUdLCgIFAQKAgWBwly/xVYWFAQKAgWBwtzjQ3pjQUGgIFAQKLiSDhYUBAoC\n",
       "BYGCQMFvsWBBQaAgUBAoOA8KFhQECgIFgcI8rqRXFhQECgIFgYJAwXlQsKAgUBAoCBTmcR60sqAg\n",
       "UBAoCBTmOf+8/TN8mgUFgYJAQaAgUPBUI1hQECgIFAQKzoOCBQWBgkBBoOBDOlhQECgIFAQKAgXn\n",
       "QcGCgkBBoCBQmJ9bjZUFBYGCQEGgMM/xpf8bCwoCBYGCQEGg4FYjWFAQKAgUBApuNYIFBYGCQEGg\n",
       "4P2gYEFBoCBQECgIFObnVmNlQUGgIFAQKHiJM1hQECgIFAQKrqSDBQWBgkBBoCBQ8Og5WFAQKAgU\n",
       "BApuNYIFBYGCQEGg4Eo6WFAQKAgUBAoCBbcawYKCQEGgIFCY55y3f4ZPs6AgUBAoCBRcSQcLCgIF\n",
       "gYJAQaDgqUawoCBQECgIFHxIBwsKAgWBgkBhfg7tVxYUBAoCBYGCQMGtRrCgIFAQKAgUvB8ULCgI\n",
       "FAQKAgXnQcGCgkBBoCBQECg4DwoWFAQKAgWBgvOgYEFBoCBQECj4kA4WFAQKAgWBgkDBXz0HCwoC\n",
       "BYGCQMGtRrCgIFAQKAgUfEgHCwoCBYGCQEGg4LdYsKAgUBAoCBR8SAcLCgIFgYJAYZ7rQ3pjQUGg\n",
       "IFAQKAgU3GoECwoCBYGCQMGHdLCgIFAQKAgUfEgHCwoCBYGCQEGgMM+5b/8Mn2ZBQaAgUBAouNUI\n",
       "FhQECgIFgYIr6WBBQaAgUBAoCBTcagQLCgIFgYJAwa1GsKAgUBAoCBTmcSG9sqAgUBAoCBQECvNc\n",
       "txobCwoCBYGCQMGtRrCgIFAQKAgUfEgHCwoCBYGCQEGgML4Eb2dBQaAgUBAouNUIFhQECgIFgYIP\n",
       "6WBBQaAgUBAoCBTmOb+3f4ZPs6AgUBAoCBTmXh/SGwsKAgWBgkDBeVCwoCBQECgIFBx3BAsKAgWB\n",
       "gkBBoDD3aLRRJwgUBAoCBbcawYKCQEGgIFCYx5PVlQUFgYJAQaAgUJjrVmNlQUGgIFAQKMzj0H6l\n",
       "ThAoCBQECq6kgwUFgYJAQaAgUPBUI1hQECgIFAQKXuIM6gSBgkBBoOAlzmBBQaAgUBAoCBQ81QgW\n",
       "FAQKAgWBwtyr0UadIFAQKAgUnAcFCwoCBYGCQEGg4DwoWFAQKAgUBArzOA9aqRMECgIFgYIr6WBB\n",
       "QaAgUBAoCBT8FgsWFAQKAgWBgj/qDRYUBAoCBYGCP+oN6gSBgkBBoCBQ8P/mgwUFgYJAQaDg0D5Y\n",
       "UBAoCBQECr70P6gTBAoCBYGCQMF5ULCgIFAQKAgUnAcFCwoCBYGCQMGVdLCgIFAQKAgUBArecg3q\n",
       "BIGCQEGg4FYjWFAQKAgUBAoO7YMFBYGCQEGgIFBwqxEsKAgUBAoCBf+6JlhQECgIFAQK/glkUCcI\n",
       "FAQKAgWBgvOgYEFBoCBQECh49BwsKAgUBAoCBVfSwYKCQEGgIFAQKMzxW2xlQUGgIFAQKPij3qBO\n",
       "ECgIFAQKzoOCBQWBgkBBoCBQ8FssWFAQKAgUBAreDwoWFAQKAgWBgivpYEFBoCBQECgIFPwWCxYU\n",
       "BAoCBYGCr6YI6gSBgkBBoDDHof3KgoJAQaAgUBAoOA8KFhQECgIFgYIP6WBBQaAgUBAo+JAOFhQE\n",
       "CgIFgYJAwfcHBQsKAgWBgkDBrUawoCBQECgIFHxIBwsKAgWBgkBBoOBPEYI6QaAgUBAo+FOEYEFB\n",
       "oCBQECg4DwoWFAQKAgWBgkDBb7FgQUGgIFAQKHiJM1hQECgIFAQKrqSDBQWBgkBBoCBQmPv4Lbax\n",
       "oCBQECgIFJwHBQsKAgWBgkDBeVCwoCBQECgIFAQKbjWCBQWBgkBBoOBWI1hQECgIFAQKrqSDBQWB\n",
       "gkBBoCBQcKsRLCgIFAQKAgUf0sGCgkBBoCBQmONN+5UFBYGCQEGg4Eo6WFAQKAgUBAoCBY+egwUF\n",
       "gYJAQaAw9779I3ybBQWBgkBBoOBKOlhQECgIFAQKAgVf8hYsKAgUBAoCBbcawYKCQEGgIFDwflCw\n",
       "oCBQECgIFAQK/hQhWFAQKAgUBAreDwoWFAQKAgWBgkP7YEFBoCBQECgIFDzVCBYUBAoCBYGCQ/tg\n",
       "QUGgIFAQKDgPChYUBAoCBYGCQMGj52BBQaAgUBAoOA8KFhQECgIFgYInq8GCgkBBoCBQECjMefsn\n",
       "+DgLCgIFgYJAwa1GsKAgUBAoCBS8HxQsKAgUBAoCBYGC86BgQUGgIFAQKDgPChYUBAoCBYGCK+lg\n",
       "QUGgIFAQKAgU3GoECwoCBYGCQMGj52BBQaAgUBAojC9e2FlQECgIFAQKAgW3GsGCgkBBoCBQ8Og5\n",
       "WFAQKAgUBAqerAYLCgIFgYJAQaDgViNYUBAoCBQECm41ggUFgYJAQaAwxwtCKwsKAgWBgkBBoOA8\n",
       "KFhQECgIFAQKzoOCBQWBgkBBoOBKOlhQECgIFAQKAgX/bz5YUBAoCBQECv7ffLCgIFAQKAgUvB8U\n",
       "LCgIFAQKAgWBgm/BCxYUBAoCBYGCL3kLFhQECgIFgYInq8GCgkBBoCBQECh4yzVYUBAoCBQECv5e\n",
       "LFhQECgIFAQKrqSDBQWBgkBBoCBQ8JZrsKAgUBAoCBS8xBksKAgUBAoCBVfSwYKCQEGgIFAQKHjL\n",
       "NVhQECgIFAQKvj8oWFAQKAgUBArOg4IFBYGCQEGgIFDw6DlYUBAoCBQECm41ggUFgYJAQaDgSjpY\n",
       "UBAoCBQECgIFtxrBgoJAQaAgUPAhHSwoCBQECgIF50HBgoJAQaAgUBAouNUIFhQECgIFgYJbjWBB\n",
       "QaAgUBAouJIOFhQECgIFgYJAwfcHBQsKAgWBgkDBv64JFhQECgIFgYLzoGBBQaAgUBAoCBTmPn6N\n",
       "bSwoCBQECgIFtxrBgoJAQaAgUPBkNVhQECgIFAQKnqwGCwoCBYGCQEGg4DwoWFAQKAgUBAqerAYL\n",
       "CgIFgYJAwZV0sKAgUBAoCBQECr4/KFhQECgIFAQKc7wgtLKgIFAQKAgUvB8ULCgIFAQKAgWBwly3\n",
       "GisLCgIFgYJAwa1GsKAgUBAoCBQc2gcLCgIFgYJAQaDg/aBgQUGgIFAQKMzx92IrCwoCBYGCQMF5\n",
       "ULCgIFAQKAgUBAq+miJYUBAoCBQECt4PChYUBAoCBYGCQ/tgQUGgIFAQKAgUnAcFCwoCBYGCQGHO\n",
       "40RoY0FBoCBQECg4DwoWFAQKAgWBgkDBb7FgQUGgIFAQKDi0DxYUBAoCBYGCQ/tgQUGgIFAQKAgU\n",
       "5vzcamwsKAgUBAoCBbcawYKCQEGgIFCY60N6ZUFBoCBQECgIFNxqBAsKAgWBgkBhzs+H9MaCgkBB\n",
       "oCBQcCUdLCgIFAQKAgWBgqcawYKCQEGgIFBwqxEsKAgUBAoCBR/SwYKCQEGgIFAQKHj0HCwoCBQE\n",
       "CgKFuc+ft3+GT7OgIFAQKAgUnAcFCwoCBYGCQEGg4P2gYEFBoCBQECjMuc6DNhYUBAoCBYGCK+lg\n",
       "QUGgIFAQKAgU5nj0vLKgIFAQKAgU3GoECwoCBYGCQMGVdLCgIFAQKAgUBApzr1uNjQUFgYJAQaDg\n",
       "PChYUBAoCBQECl7iDBYUBAoCBYGCQMGtRrCgIFAQKAgU5rrVWFlQECgIFAQKrqSDBQWBgkBBoCBQ\n",
       "mOP9oJUFBYGCQEGg4PukgwUFgYJAQaDgTftgQUGgIFAQKAgUPNUIFhQECgIFgYJbjWBBQaAgUBAo\n",
       "+JAOFhQECgIFgYJAwaPnYEFBoCBQECi41QgWFAQKAgWBwjyerK4sKAgUBAoCBYGCW41gQUGgIFAQ\n",
       "KHiJM1hQECgIFAQK87iSXllQECgIFAQKAgXvBwULCgIFgYJAwaPnYEFBoCBQECg4DwoWFAQKAgWB\n",
       "gkBh7nPf/hk+zYKCQEGgIFBwHhQsKAgUBAoChXlcSa8sKAgUBAoChf8AJ8ztt7m0cDkAAAAASUVO\n",
       "RK5CYII=\n",
       "\" transform=\"translate(2161, 47)\"/>\n",
       "</g>\n",
       "<path clip-path=\"url(#clip910)\" d=\"M 0 0 M2280.7 1301.14 Q2277.09 1301.14 2275.26 1304.71 Q2273.45 1308.25 2273.45 1315.38 Q2273.45 1322.48 2275.26 1326.05 Q2277.09 1329.59 2280.7 1329.59 Q2284.33 1329.59 2286.14 1326.05 Q2287.97 1322.48 2287.97 1315.38 Q2287.97 1308.25 2286.14 1304.71 Q2284.33 1301.14 2280.7 1301.14 M2280.7 1297.44 Q2286.51 1297.44 2289.57 1302.04 Q2292.64 1306.63 2292.64 1315.38 Q2292.64 1324.1 2289.57 1328.71 Q2286.51 1333.29 2280.7 1333.29 Q2274.89 1333.29 2271.81 1328.71 Q2268.76 1324.1 2268.76 1315.38 Q2268.76 1306.63 2271.81 1302.04 Q2274.89 1297.44 2280.7 1297.44 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2297.71 1326.74 L2302.6 1326.74 L2302.6 1332.62 L2297.71 1332.62 L2297.71 1326.74 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2318.25 1313.48 Q2315.1 1313.48 2313.25 1315.63 Q2311.42 1317.79 2311.42 1321.54 Q2311.42 1325.26 2313.25 1327.44 Q2315.1 1329.59 2318.25 1329.59 Q2321.39 1329.59 2323.22 1327.44 Q2325.08 1325.26 2325.08 1321.54 Q2325.08 1317.79 2323.22 1315.63 Q2321.39 1313.48 2318.25 1313.48 M2327.53 1298.83 L2327.53 1303.09 Q2325.77 1302.25 2323.96 1301.81 Q2322.18 1301.37 2320.42 1301.37 Q2315.79 1301.37 2313.34 1304.5 Q2310.91 1307.62 2310.56 1313.94 Q2311.93 1311.93 2313.99 1310.86 Q2316.05 1309.78 2318.52 1309.78 Q2323.73 1309.78 2326.74 1312.95 Q2329.77 1316.1 2329.77 1321.54 Q2329.77 1326.86 2326.63 1330.08 Q2323.48 1333.29 2318.25 1333.29 Q2312.25 1333.29 2309.08 1328.71 Q2305.91 1324.1 2305.91 1315.38 Q2305.91 1307.18 2309.8 1302.32 Q2313.69 1297.44 2320.24 1297.44 Q2322 1297.44 2323.78 1297.79 Q2325.58 1298.13 2327.53 1298.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2334.89 1298.06 L2353.25 1298.06 L2353.25 1302 L2339.17 1302 L2339.17 1310.47 Q2340.19 1310.12 2341.21 1309.96 Q2342.23 1309.78 2343.25 1309.78 Q2349.03 1309.78 2352.41 1312.95 Q2355.79 1316.12 2355.79 1321.54 Q2355.79 1327.11 2352.32 1330.22 Q2348.85 1333.29 2342.53 1333.29 Q2340.35 1333.29 2338.08 1332.92 Q2335.84 1332.55 2333.43 1331.81 L2333.43 1327.11 Q2335.51 1328.25 2337.74 1328.8 Q2339.96 1329.36 2342.44 1329.36 Q2346.44 1329.36 2348.78 1327.25 Q2351.12 1325.15 2351.12 1321.54 Q2351.12 1317.92 2348.78 1315.82 Q2346.44 1313.71 2342.44 1313.71 Q2340.56 1313.71 2338.69 1314.13 Q2336.83 1314.54 2334.89 1315.42 L2334.89 1298.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2280.7 1090.81 Q2277.09 1090.81 2275.26 1094.38 Q2273.45 1097.92 2273.45 1105.05 Q2273.45 1112.16 2275.26 1115.72 Q2277.09 1119.26 2280.7 1119.26 Q2284.33 1119.26 2286.14 1115.72 Q2287.97 1112.16 2287.97 1105.05 Q2287.97 1097.92 2286.14 1094.38 Q2284.33 1090.81 2280.7 1090.81 M2280.7 1087.11 Q2286.51 1087.11 2289.57 1091.72 Q2292.64 1096.3 2292.64 1105.05 Q2292.64 1113.78 2289.57 1118.38 Q2286.51 1122.97 2280.7 1122.97 Q2274.89 1122.97 2271.81 1118.38 Q2268.76 1113.78 2268.76 1105.05 Q2268.76 1096.3 2271.81 1091.72 Q2274.89 1087.11 2280.7 1087.11 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2297.71 1116.42 L2302.6 1116.42 L2302.6 1122.3 L2297.71 1122.3 L2297.71 1116.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2306.49 1087.74 L2328.71 1087.74 L2328.71 1089.73 L2316.16 1122.3 L2311.28 1122.3 L2323.08 1091.67 L2306.49 1091.67 L2306.49 1087.74 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2343.78 1090.81 Q2340.17 1090.81 2338.34 1094.38 Q2336.53 1097.92 2336.53 1105.05 Q2336.53 1112.16 2338.34 1115.72 Q2340.17 1119.26 2343.78 1119.26 Q2347.41 1119.26 2349.22 1115.72 Q2351.05 1112.16 2351.05 1105.05 Q2351.05 1097.92 2349.22 1094.38 Q2347.41 1090.81 2343.78 1090.81 M2343.78 1087.11 Q2349.59 1087.11 2352.64 1091.72 Q2355.72 1096.3 2355.72 1105.05 Q2355.72 1113.78 2352.64 1118.38 Q2349.59 1122.97 2343.78 1122.97 Q2337.97 1122.97 2334.89 1118.38 Q2331.83 1113.78 2331.83 1105.05 Q2331.83 1096.3 2334.89 1091.72 Q2337.97 1087.11 2343.78 1087.11 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2280.7 880.488 Q2277.09 880.488 2275.26 884.053 Q2273.45 887.594 2273.45 894.724 Q2273.45 901.83 2275.26 905.395 Q2277.09 908.937 2280.7 908.937 Q2284.33 908.937 2286.14 905.395 Q2287.97 901.83 2287.97 894.724 Q2287.97 887.594 2286.14 884.053 Q2284.33 880.488 2280.7 880.488 M2280.7 876.784 Q2286.51 876.784 2289.57 881.391 Q2292.64 885.974 2292.64 894.724 Q2292.64 903.451 2289.57 908.057 Q2286.51 912.641 2280.7 912.641 Q2274.89 912.641 2271.81 908.057 Q2268.76 903.451 2268.76 894.724 Q2268.76 885.974 2271.81 881.391 Q2274.89 876.784 2280.7 876.784 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2297.71 906.09 L2302.6 906.09 L2302.6 911.969 L2297.71 911.969 L2297.71 906.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2306.49 877.409 L2328.71 877.409 L2328.71 879.4 L2316.16 911.969 L2311.28 911.969 L2323.08 881.344 L2306.49 881.344 L2306.49 877.409 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2333.83 877.409 L2352.18 877.409 L2352.18 881.344 L2338.11 881.344 L2338.11 889.817 Q2339.13 889.469 2340.14 889.307 Q2341.16 889.122 2342.18 889.122 Q2347.97 889.122 2351.35 892.293 Q2354.73 895.465 2354.73 900.881 Q2354.73 906.46 2351.26 909.562 Q2347.78 912.641 2341.46 912.641 Q2339.29 912.641 2337.02 912.27 Q2334.77 911.9 2332.37 911.159 L2332.37 906.46 Q2334.45 907.594 2336.67 908.15 Q2338.89 908.705 2341.37 908.705 Q2345.38 908.705 2347.71 906.599 Q2350.05 904.492 2350.05 900.881 Q2350.05 897.27 2347.71 895.164 Q2345.38 893.057 2341.37 893.057 Q2339.5 893.057 2337.62 893.474 Q2335.77 893.891 2333.83 894.77 L2333.83 877.409 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2280.7 670.161 Q2277.09 670.161 2275.26 673.726 Q2273.45 677.267 2273.45 684.397 Q2273.45 691.504 2275.26 695.068 Q2277.09 698.61 2280.7 698.61 Q2284.33 698.61 2286.14 695.068 Q2287.97 691.504 2287.97 684.397 Q2287.97 677.267 2286.14 673.726 Q2284.33 670.161 2280.7 670.161 M2280.7 666.457 Q2286.51 666.457 2289.57 671.064 Q2292.64 675.647 2292.64 684.397 Q2292.64 693.124 2289.57 697.73 Q2286.51 702.314 2280.7 702.314 Q2274.89 702.314 2271.81 697.73 Q2268.76 693.124 2268.76 684.397 Q2268.76 675.647 2271.81 671.064 Q2274.89 666.457 2280.7 666.457 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2297.71 695.763 L2302.6 695.763 L2302.6 701.642 L2297.71 701.642 L2297.71 695.763 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2317.67 685.23 Q2314.33 685.23 2312.41 687.013 Q2310.51 688.795 2310.51 691.92 Q2310.51 695.045 2312.41 696.828 Q2314.33 698.61 2317.67 698.61 Q2321 698.61 2322.92 696.828 Q2324.84 695.022 2324.84 691.92 Q2324.84 688.795 2322.92 687.013 Q2321.02 685.23 2317.67 685.23 M2312.99 683.24 Q2309.98 682.499 2308.29 680.439 Q2306.63 678.379 2306.63 675.416 Q2306.63 671.272 2309.57 668.865 Q2312.53 666.457 2317.67 666.457 Q2322.83 666.457 2325.77 668.865 Q2328.71 671.272 2328.71 675.416 Q2328.71 678.379 2327.02 680.439 Q2325.35 682.499 2322.37 683.24 Q2325.75 684.027 2327.62 686.318 Q2329.52 688.61 2329.52 691.92 Q2329.52 696.943 2326.44 699.628 Q2323.39 702.314 2317.67 702.314 Q2311.95 702.314 2308.87 699.628 Q2305.82 696.943 2305.82 691.92 Q2305.82 688.61 2307.71 686.318 Q2309.61 684.027 2312.99 683.24 M2311.28 675.855 Q2311.28 678.541 2312.95 680.045 Q2314.64 681.55 2317.67 681.55 Q2320.68 681.55 2322.37 680.045 Q2324.08 678.541 2324.08 675.855 Q2324.08 673.17 2322.37 671.666 Q2320.68 670.161 2317.67 670.161 Q2314.64 670.161 2312.95 671.666 Q2311.28 673.17 2311.28 675.855 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2344.59 670.161 Q2340.98 670.161 2339.15 673.726 Q2337.34 677.267 2337.34 684.397 Q2337.34 691.504 2339.15 695.068 Q2340.98 698.61 2344.59 698.61 Q2348.22 698.61 2350.03 695.068 Q2351.86 691.504 2351.86 684.397 Q2351.86 677.267 2350.03 673.726 Q2348.22 670.161 2344.59 670.161 M2344.59 666.457 Q2350.4 666.457 2353.45 671.064 Q2356.53 675.647 2356.53 684.397 Q2356.53 693.124 2353.45 697.73 Q2350.4 702.314 2344.59 702.314 Q2338.78 702.314 2335.7 697.73 Q2332.64 693.124 2332.64 684.397 Q2332.64 675.647 2335.7 671.064 Q2338.78 666.457 2344.59 666.457 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2280.7 459.834 Q2277.09 459.834 2275.26 463.399 Q2273.45 466.941 2273.45 474.07 Q2273.45 481.177 2275.26 484.741 Q2277.09 488.283 2280.7 488.283 Q2284.33 488.283 2286.14 484.741 Q2287.97 481.177 2287.97 474.07 Q2287.97 466.941 2286.14 463.399 Q2284.33 459.834 2280.7 459.834 M2280.7 456.13 Q2286.51 456.13 2289.57 460.737 Q2292.64 465.32 2292.64 474.07 Q2292.64 482.797 2289.57 487.403 Q2286.51 491.987 2280.7 491.987 Q2274.89 491.987 2271.81 487.403 Q2268.76 482.797 2268.76 474.07 Q2268.76 465.32 2271.81 460.737 Q2274.89 456.13 2280.7 456.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2297.71 485.436 L2302.6 485.436 L2302.6 491.315 L2297.71 491.315 L2297.71 485.436 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2317.67 474.904 Q2314.33 474.904 2312.41 476.686 Q2310.51 478.468 2310.51 481.593 Q2310.51 484.718 2312.41 486.501 Q2314.33 488.283 2317.67 488.283 Q2321 488.283 2322.92 486.501 Q2324.84 484.695 2324.84 481.593 Q2324.84 478.468 2322.92 476.686 Q2321.02 474.904 2317.67 474.904 M2312.99 472.913 Q2309.98 472.172 2308.29 470.112 Q2306.63 468.052 2306.63 465.089 Q2306.63 460.945 2309.57 458.538 Q2312.53 456.13 2317.67 456.13 Q2322.83 456.13 2325.77 458.538 Q2328.71 460.945 2328.71 465.089 Q2328.71 468.052 2327.02 470.112 Q2325.35 472.172 2322.37 472.913 Q2325.75 473.7 2327.62 475.991 Q2329.52 478.283 2329.52 481.593 Q2329.52 486.616 2326.44 489.302 Q2323.39 491.987 2317.67 491.987 Q2311.95 491.987 2308.87 489.302 Q2305.82 486.616 2305.82 481.593 Q2305.82 478.283 2307.71 475.991 Q2309.61 473.7 2312.99 472.913 M2311.28 465.529 Q2311.28 468.214 2312.95 469.718 Q2314.64 471.223 2317.67 471.223 Q2320.68 471.223 2322.37 469.718 Q2324.08 468.214 2324.08 465.529 Q2324.08 462.843 2322.37 461.339 Q2320.68 459.834 2317.67 459.834 Q2314.64 459.834 2312.95 461.339 Q2311.28 462.843 2311.28 465.529 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2334.64 456.755 L2352.99 456.755 L2352.99 460.691 L2338.92 460.691 L2338.92 469.163 Q2339.94 468.816 2340.95 468.654 Q2341.97 468.468 2342.99 468.468 Q2348.78 468.468 2352.16 471.64 Q2355.54 474.811 2355.54 480.228 Q2355.54 485.806 2352.07 488.908 Q2348.59 491.987 2342.27 491.987 Q2340.1 491.987 2337.83 491.616 Q2335.58 491.246 2333.18 490.505 L2333.18 485.806 Q2335.26 486.94 2337.48 487.496 Q2339.7 488.052 2342.18 488.052 Q2346.19 488.052 2348.52 485.945 Q2350.86 483.839 2350.86 480.228 Q2350.86 476.616 2348.52 474.51 Q2346.19 472.404 2342.18 472.404 Q2340.31 472.404 2338.43 472.82 Q2336.58 473.237 2334.64 474.116 L2334.64 456.755 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2280.7 249.507 Q2277.09 249.507 2275.26 253.072 Q2273.45 256.614 2273.45 263.743 Q2273.45 270.85 2275.26 274.415 Q2277.09 277.956 2280.7 277.956 Q2284.33 277.956 2286.14 274.415 Q2287.97 270.85 2287.97 263.743 Q2287.97 256.614 2286.14 253.072 Q2284.33 249.507 2280.7 249.507 M2280.7 245.804 Q2286.51 245.804 2289.57 250.41 Q2292.64 254.993 2292.64 263.743 Q2292.64 272.47 2289.57 277.077 Q2286.51 281.66 2280.7 281.66 Q2274.89 281.66 2271.81 277.077 Q2268.76 272.47 2268.76 263.743 Q2268.76 254.993 2271.81 250.41 Q2274.89 245.804 2280.7 245.804 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2297.71 275.109 L2302.6 275.109 L2302.6 280.989 L2297.71 280.989 L2297.71 275.109 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2307.81 280.271 L2307.81 276.012 Q2309.57 276.845 2311.37 277.285 Q2313.18 277.725 2314.91 277.725 Q2319.54 277.725 2321.97 274.623 Q2324.43 271.498 2324.77 265.155 Q2323.43 267.146 2321.37 268.211 Q2319.31 269.276 2316.81 269.276 Q2311.63 269.276 2308.59 266.151 Q2305.58 263.003 2305.58 257.563 Q2305.58 252.239 2308.73 249.021 Q2311.88 245.804 2317.11 245.804 Q2323.11 245.804 2326.26 250.41 Q2329.43 254.993 2329.43 263.743 Q2329.43 271.915 2325.54 276.799 Q2321.67 281.66 2315.12 281.66 Q2313.36 281.66 2311.56 281.313 Q2309.75 280.965 2307.81 280.271 M2317.11 265.618 Q2320.26 265.618 2322.09 263.466 Q2323.94 261.313 2323.94 257.563 Q2323.94 253.836 2322.09 251.683 Q2320.26 249.507 2317.11 249.507 Q2313.96 249.507 2312.11 251.683 Q2310.28 253.836 2310.28 257.563 Q2310.28 261.313 2312.11 263.466 Q2313.96 265.618 2317.11 265.618 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2344.5 249.507 Q2340.89 249.507 2339.06 253.072 Q2337.25 256.614 2337.25 263.743 Q2337.25 270.85 2339.06 274.415 Q2340.89 277.956 2344.5 277.956 Q2348.13 277.956 2349.94 274.415 Q2351.76 270.85 2351.76 263.743 Q2351.76 256.614 2349.94 253.072 Q2348.13 249.507 2344.5 249.507 M2344.5 245.804 Q2350.31 245.804 2353.36 250.41 Q2356.44 254.993 2356.44 263.743 Q2356.44 272.47 2353.36 277.077 Q2350.31 281.66 2344.5 281.66 Q2338.69 281.66 2335.61 277.077 Q2332.55 272.47 2332.55 263.743 Q2332.55 254.993 2335.61 250.41 Q2338.69 245.804 2344.5 245.804 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2280.7 39.1804 Q2277.09 39.1804 2275.26 42.7452 Q2273.45 46.2868 2273.45 53.4164 Q2273.45 60.5229 2275.26 64.0877 Q2277.09 67.6293 2280.7 67.6293 Q2284.33 67.6293 2286.14 64.0877 Q2287.97 60.5229 2287.97 53.4164 Q2287.97 46.2868 2286.14 42.7452 Q2284.33 39.1804 2280.7 39.1804 M2280.7 35.4767 Q2286.51 35.4767 2289.57 40.0832 Q2292.64 44.6665 2292.64 53.4164 Q2292.64 62.1432 2289.57 66.7497 Q2286.51 71.333 2280.7 71.333 Q2274.89 71.333 2271.81 66.7497 Q2268.76 62.1432 2268.76 53.4164 Q2268.76 44.6665 2271.81 40.0832 Q2274.89 35.4767 2280.7 35.4767 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2297.71 64.7821 L2302.6 64.7821 L2302.6 70.6617 L2297.71 70.6617 L2297.71 64.7821 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2307.81 69.9441 L2307.81 65.6849 Q2309.57 66.5182 2311.37 66.958 Q2313.18 67.3978 2314.91 67.3978 Q2319.54 67.3978 2321.97 64.296 Q2324.43 61.171 2324.77 54.8285 Q2323.43 56.8192 2321.37 57.884 Q2319.31 58.9488 2316.81 58.9488 Q2311.63 58.9488 2308.59 55.8238 Q2305.58 52.6757 2305.58 47.2359 Q2305.58 41.9119 2308.73 38.6943 Q2311.88 35.4767 2317.11 35.4767 Q2323.11 35.4767 2326.26 40.0832 Q2329.43 44.6665 2329.43 53.4164 Q2329.43 61.5877 2325.54 66.4719 Q2321.67 71.333 2315.12 71.333 Q2313.36 71.333 2311.56 70.9858 Q2309.75 70.6386 2307.81 69.9441 M2317.11 55.2914 Q2320.26 55.2914 2322.09 53.1387 Q2323.94 50.9859 2323.94 47.2359 Q2323.94 43.5091 2322.09 41.3563 Q2320.26 39.1804 2317.11 39.1804 Q2313.96 39.1804 2312.11 41.3563 Q2310.28 43.5091 2310.28 47.2359 Q2310.28 50.9859 2312.11 53.1387 Q2313.96 55.2914 2317.11 55.2914 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2334.54 36.1017 L2352.9 36.1017 L2352.9 40.0369 L2338.82 40.0369 L2338.82 48.5091 Q2339.84 48.1618 2340.86 47.9998 Q2341.88 47.8146 2342.9 47.8146 Q2348.69 47.8146 2352.07 50.9859 Q2355.45 54.1572 2355.45 59.5738 Q2355.45 65.1525 2351.97 68.2543 Q2348.5 71.333 2342.18 71.333 Q2340.01 71.333 2337.74 70.9626 Q2335.49 70.5923 2333.08 69.8515 L2333.08 65.1525 Q2335.17 66.2867 2337.39 66.8423 Q2339.61 67.3978 2342.09 67.3978 Q2346.09 67.3978 2348.43 65.2914 Q2350.77 63.1849 2350.77 59.5738 Q2350.77 55.9627 2348.43 53.8563 Q2346.09 51.7498 2342.09 51.7498 Q2340.21 51.7498 2338.34 52.1664 Q2336.49 52.5831 2334.54 53.4627 L2334.54 36.1017 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2232.76,1410.9 2232.76,1318.97 2256.76,1318.97 2232.76,1318.97 2232.76,1108.64 2256.76,1108.64 2232.76,1108.64 2232.76,898.318 2256.76,898.318 2232.76,898.318 \n",
       "  2232.76,687.991 2256.76,687.991 2232.76,687.991 2232.76,477.664 2256.76,477.664 2232.76,477.664 2232.76,267.337 2256.76,267.337 2232.76,267.337 2232.76,57.0105 \n",
       "  2256.76,57.0105 2232.76,57.0105 2232.76,47.2441 \n",
       "  \"/>\n",
       "</svg>\n"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(self_tuning_svm_mach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(best_model = \u001b[34mSVMClassifier @445\u001b[39m,\n",
       " best_fitted_params = (support = Int32[12, 14, 22, 48, 70, 105, 112, 125, 131, 150  …  228, 234, 240, 262, 317, 363, 380, 385, 394, 395],\n",
       "                       support_vectors = [14.2 20.53 … 0.2534 0.07858; 11.75 20.18 … 0.3168 0.07987; … ; 12.45 15.7 … 0.3985 0.1244; 16.46 20.11 … 0.3054 0.09519],\n",
       "                       n_support = Int32[23, 21],\n",
       "                       dual_coef = [-8.340502686000295e6 -2.060000331555374e6 … 8.340502686000295e6 8.340502686000295e6],\n",
       "                       coef = nothing,\n",
       "                       intercept = [2797.6162307287295],\n",
       "                       fit_status = 0,\n",
       "                       classes = UInt32[0x00000001, 0x00000002],),)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rbf = fitted_params(self_tuning_svm_mach)\n",
    "best_rbf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVMClassifier(\n",
       "    C = 8.340502686000295e6,\n",
       "    kernel = \"rbf\",\n",
       "    degree = 3,\n",
       "    gamma = 1.0e-9,\n",
       "    coef0 = 0.0,\n",
       "    shrinking = true,\n",
       "    tol = 0.001,\n",
       "    cache_size = 1000,\n",
       "    max_iter = -1,\n",
       "    decision_function_shape = \"ovr\",\n",
       "    random_state = nothing)\u001b[34m @445\u001b[39m"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_rbf.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95232"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss = round(z.report.best_result.measurement[1],digits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fn = \"Figures/LearningCurve_DT_merge_purity_thresh:$(best_mpt)_loss:$(best_loss)\"\n",
    "# png(replace(fn,'.' => ','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{SVMClassifier} @594\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  17%[====>                    ]  ETA: 0:00:18\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:10\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:07\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:05\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:02\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:11\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m─────────────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┬\u001b[0m────────────────────────────────────────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.measure           \u001b[0m│\u001b[0m\u001b[22m _.measurement \u001b[0m│\u001b[0m\u001b[22m _.per_fold                                     \u001b[0m│\u001b[0m\n",
       "├\u001b[0m─────────────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┼\u001b[0m────────────────────────────────────────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m accuracy            \u001b[0m│\u001b[0m 0.954         \u001b[0m│\u001b[0m [0.958, 0.989, 0.958, 0.958, 0.905, 0.957]     \u001b[0m│\u001b[0m\n",
       "│\u001b[0m false_negative_rate \u001b[0m│\u001b[0m 0.0765        \u001b[0m│\u001b[0m [0.075, 0.0312, 0.0312, 0.0938, 0.171, 0.0571] \u001b[0m│\u001b[0m\n",
       "│\u001b[0m false_positive_rate \u001b[0m│\u001b[0m 0.0254        \u001b[0m│\u001b[0m [0.0182, 0.0, 0.0476, 0.0159, 0.037, 0.0339]   \u001b[0m│\u001b[0m\n",
       "│\u001b[0m true_negative_rate  \u001b[0m│\u001b[0m 0.975         \u001b[0m│\u001b[0m [0.982, 1.0, 0.952, 0.984, 0.963, 0.966]       \u001b[0m│\u001b[0m\n",
       "│\u001b[0m true_positive_rate  \u001b[0m│\u001b[0m 0.923         \u001b[0m│\u001b[0m [0.925, 0.969, 0.969, 0.906, 0.829, 0.943]     \u001b[0m│\u001b[0m\n",
       "└\u001b[0m─────────────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┴\u001b[0m────────────────────────────────────────────────\u001b[0m┘\u001b[0m\n",
       "_.per_observation = [missing, missing, missing, missing, missing]\n",
       "_.fitted_params_per_fold = [ … ]\n",
       "_.report_per_fold = [ … ]\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVMClassifier(kernel=\"linear\", C = best_C)\n",
    "svm_mach = machine(svm_model, X, y)\n",
    "fit!(svm_mach, rows=train, verbosity=2)\n",
    "svm_acc = evaluate!(svm_mach, resampling=CV(shuffle=true), measure=[accuracy, fnr, fpr, tnr, tpr], \n",
    "                        verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{SVMClassifier} @630\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  17%[====>                    ]  ETA: 0:00:00\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  33%[========>                ]  ETA: 0:00:00\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  50%[============>            ]  ETA: 0:00:00\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  67%[================>        ]  ETA: 0:00:00\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds:  83%[====================>    ]  ETA: 0:00:00\u001b[39m┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n",
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m─────────────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┬\u001b[0m───────────────────────────────────────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.measure           \u001b[0m│\u001b[0m\u001b[22m _.measurement \u001b[0m│\u001b[0m\u001b[22m _.per_fold                                    \u001b[0m│\u001b[0m\n",
       "├\u001b[0m─────────────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┼\u001b[0m───────────────────────────────────────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m accuracy            \u001b[0m│\u001b[0m 0.946         \u001b[0m│\u001b[0m [0.905, 0.979, 0.947, 0.937, 0.968, 0.936]    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m false_negative_rate \u001b[0m│\u001b[0m 0.0968        \u001b[0m│\u001b[0m [0.147, 0.0323, 0.108, 0.182, 0.0233, 0.0882] \u001b[0m│\u001b[0m\n",
       "│\u001b[0m false_positive_rate \u001b[0m│\u001b[0m 0.0312        \u001b[0m│\u001b[0m [0.0656, 0.0156, 0.0172, 0.0, 0.0385, 0.05]   \u001b[0m│\u001b[0m\n",
       "│\u001b[0m true_negative_rate  \u001b[0m│\u001b[0m 0.969         \u001b[0m│\u001b[0m [0.934, 0.984, 0.983, 1.0, 0.962, 0.95]       \u001b[0m│\u001b[0m\n",
       "│\u001b[0m true_positive_rate  \u001b[0m│\u001b[0m 0.903         \u001b[0m│\u001b[0m [0.853, 0.968, 0.892, 0.818, 0.977, 0.912]    \u001b[0m│\u001b[0m\n",
       "└\u001b[0m─────────────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┴\u001b[0m───────────────────────────────────────────────\u001b[0m┘\u001b[0m\n",
       "_.per_observation = [missing, missing, missing, missing, missing]\n",
       "_.fitted_params_per_fold = [ … ]\n",
       "_.report_per_fold = [ … ]\n"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_model = SVMClassifier(kernel=\"rbf\", C=best_rbf.best_model.C, gamma=best_rbf.best_model.gamma)\n",
    "svm_mach = machine(svm_model, X, y)\n",
    "fit!(svm_mach, rows=train, verbosity=2)\n",
    "svm_acc = evaluate!(svm_mach, resampling=CV(shuffle=true), measure=[accuracy, fnr, fpr, tnr, tpr], \n",
    "                        verbosity=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function learn_curve(model, X_train, y_train, data_schedule)\n",
    "    training_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    #split training data into training and holdout\n",
    "    #for data_size in data_schedule\n",
    "        #train model \n",
    "        #add loss to training_losses\n",
    "        #test against holdout\n",
    "        #add loss to valid_losses\n",
    "    \n",
    "    return training_losses, valid_losses    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVMClassifier(\n",
       "    C = 178.11239451312218,\n",
       "    kernel = \"linear\",\n",
       "    degree = 3,\n",
       "    gamma = \"auto\",\n",
       "    coef0 = 0.0,\n",
       "    shrinking = true,\n",
       "    tol = 0.001,\n",
       "    cache_size = 1000,\n",
       "    max_iter = -1,\n",
       "    decision_function_shape = \"ovr\",\n",
       "    random_state = nothing)\u001b[34m @623\u001b[39m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_svm = SVMClassifier(kernel=\"linear\", C = best_C, cache_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{SVMClassifier} @136\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @337\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @114\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_SVM = machine(final_svm, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{SVMClassifier} @136\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{SVMClassifier} @136\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @337\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @114\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(Final_SVM, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ2 = MLJ.predict(Final_SVM, X[test,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766081871345029"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(ŷ2, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "              ┌───────────────────────────┐\n",
       "              │       Ground Truth        │\n",
       "┌─────────────┼─────────────┬─────────────┤\n",
       "│  Predicted  │      B      │      M      │\n",
       "├─────────────┼─────────────┼─────────────┤\n",
       "│      B      │     105     │      2      │\n",
       "├─────────────┼─────────────┼─────────────┤\n",
       "│      M      │      2      │     62      │\n",
       "└─────────────┴─────────────┴─────────────┘\n"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ŷ2, y[test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVMClassifier(\n",
       "    C = 23207.944168063907,\n",
       "    kernel = \"rbf\",\n",
       "    degree = 3,\n",
       "    gamma = 1.0e-7,\n",
       "    coef0 = 0.0,\n",
       "    shrinking = true,\n",
       "    tol = 0.001,\n",
       "    cache_size = 1000,\n",
       "    max_iter = -1,\n",
       "    decision_function_shape = \"ovr\",\n",
       "    random_state = nothing)\u001b[34m @055\u001b[39m"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_svm_rbf = best_rbf.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{SVMClassifier} @319\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @547\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @319\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_SVM = machine(final_svm, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{SVMClassifier} @319\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{SVMClassifier} @319\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @547\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @319\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(Final_SVM, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ2 = MLJ.predict(Final_SVM, X[test,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9766081871345029"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(ŷ2, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='B' and positive='M'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "              ┌───────────────────────────┐\n",
       "              │       Ground Truth        │\n",
       "┌─────────────┼─────────────┬─────────────┤\n",
       "│  Predicted  │      B      │      M      │\n",
       "├─────────────┼─────────────┼─────────────┤\n",
       "│      B      │     105     │      2      │\n",
       "├─────────────┼─────────────┼─────────────┤\n",
       "│      M      │      2      │     62      │\n",
       "└─────────────┴─────────────┴─────────────┘\n"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(ŷ2, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
