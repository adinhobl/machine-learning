{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using DataFrames\n",
    "using CSV\n",
    "using MLJ\n",
    "using DecisionTree: print_tree\n",
    "using Plots\n",
    "using StatsBase\n",
    "\n",
    "include(\"../../lib.jl\")\n",
    "\n",
    "ENV[\"LINES\"]=30;\n",
    "random = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "IOError: mkdir: file already exists (EEXIST)",
     "output_type": "error",
     "traceback": [
      "IOError: mkdir: file already exists (EEXIST)",
      "",
      "Stacktrace:",
      " [1] uv_error at ./libuv.jl:97 [inlined]",
      " [2] mkdir(::String; mode::UInt16) at ./file.jl:177",
      " [3] mkdir(::String) at ./file.jl:170",
      " [4] top-level scope at In[2]:1",
      " [5] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "mkdir(\"Figures/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Class_Name</th><th>Left_Weight</th><th>Left_Distance</th><th>Right_Weight</th><th>Right_Distance</th></tr><tr><th></th><th>String</th><th>Int64</th><th>Int64</th><th>Int64</th><th>Int64</th></tr></thead><tbody><p>625 rows × 5 columns</p><tr><th>1</th><td>B</td><td>1</td><td>1</td><td>1</td><td>1</td></tr><tr><th>2</th><td>R</td><td>1</td><td>1</td><td>1</td><td>2</td></tr><tr><th>3</th><td>R</td><td>1</td><td>1</td><td>1</td><td>3</td></tr><tr><th>4</th><td>R</td><td>1</td><td>1</td><td>1</td><td>4</td></tr><tr><th>5</th><td>R</td><td>1</td><td>1</td><td>1</td><td>5</td></tr><tr><th>6</th><td>R</td><td>1</td><td>1</td><td>2</td><td>1</td></tr><tr><th>7</th><td>R</td><td>1</td><td>1</td><td>2</td><td>2</td></tr><tr><th>8</th><td>R</td><td>1</td><td>1</td><td>2</td><td>3</td></tr><tr><th>9</th><td>R</td><td>1</td><td>1</td><td>2</td><td>4</td></tr><tr><th>10</th><td>R</td><td>1</td><td>1</td><td>2</td><td>5</td></tr><tr><th>11</th><td>R</td><td>1</td><td>1</td><td>3</td><td>1</td></tr><tr><th>12</th><td>R</td><td>1</td><td>1</td><td>3</td><td>2</td></tr><tr><th>13</th><td>R</td><td>1</td><td>1</td><td>3</td><td>3</td></tr><tr><th>14</th><td>R</td><td>1</td><td>1</td><td>3</td><td>4</td></tr><tr><th>15</th><td>R</td><td>1</td><td>1</td><td>3</td><td>5</td></tr><tr><th>16</th><td>R</td><td>1</td><td>1</td><td>4</td><td>1</td></tr><tr><th>17</th><td>R</td><td>1</td><td>1</td><td>4</td><td>2</td></tr><tr><th>18</th><td>R</td><td>1</td><td>1</td><td>4</td><td>3</td></tr><tr><th>19</th><td>R</td><td>1</td><td>1</td><td>4</td><td>4</td></tr><tr><th>20</th><td>R</td><td>1</td><td>1</td><td>4</td><td>5</td></tr><tr><th>21</th><td>R</td><td>1</td><td>1</td><td>5</td><td>1</td></tr><tr><th>22</th><td>R</td><td>1</td><td>1</td><td>5</td><td>2</td></tr><tr><th>23</th><td>R</td><td>1</td><td>1</td><td>5</td><td>3</td></tr><tr><th>24</th><td>R</td><td>1</td><td>1</td><td>5</td><td>4</td></tr><tr><th>25</th><td>R</td><td>1</td><td>1</td><td>5</td><td>5</td></tr><tr><th>26</th><td>L</td><td>1</td><td>2</td><td>1</td><td>1</td></tr><tr><th>27</th><td>B</td><td>1</td><td>2</td><td>1</td><td>2</td></tr><tr><th>28</th><td>R</td><td>1</td><td>2</td><td>1</td><td>3</td></tr><tr><th>29</th><td>R</td><td>1</td><td>2</td><td>1</td><td>4</td></tr><tr><th>30</th><td>R</td><td>1</td><td>2</td><td>1</td><td>5</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccccc}\n",
       "\t& Class\\_Name & Left\\_Weight & Left\\_Distance & Right\\_Weight & Right\\_Distance\\\\\n",
       "\t\\hline\n",
       "\t& String & Int64 & Int64 & Int64 & Int64\\\\\n",
       "\t\\hline\n",
       "\t1 & B & 1 & 1 & 1 & 1 \\\\\n",
       "\t2 & R & 1 & 1 & 1 & 2 \\\\\n",
       "\t3 & R & 1 & 1 & 1 & 3 \\\\\n",
       "\t4 & R & 1 & 1 & 1 & 4 \\\\\n",
       "\t5 & R & 1 & 1 & 1 & 5 \\\\\n",
       "\t6 & R & 1 & 1 & 2 & 1 \\\\\n",
       "\t7 & R & 1 & 1 & 2 & 2 \\\\\n",
       "\t8 & R & 1 & 1 & 2 & 3 \\\\\n",
       "\t9 & R & 1 & 1 & 2 & 4 \\\\\n",
       "\t10 & R & 1 & 1 & 2 & 5 \\\\\n",
       "\t11 & R & 1 & 1 & 3 & 1 \\\\\n",
       "\t12 & R & 1 & 1 & 3 & 2 \\\\\n",
       "\t13 & R & 1 & 1 & 3 & 3 \\\\\n",
       "\t14 & R & 1 & 1 & 3 & 4 \\\\\n",
       "\t15 & R & 1 & 1 & 3 & 5 \\\\\n",
       "\t16 & R & 1 & 1 & 4 & 1 \\\\\n",
       "\t17 & R & 1 & 1 & 4 & 2 \\\\\n",
       "\t18 & R & 1 & 1 & 4 & 3 \\\\\n",
       "\t19 & R & 1 & 1 & 4 & 4 \\\\\n",
       "\t20 & R & 1 & 1 & 4 & 5 \\\\\n",
       "\t21 & R & 1 & 1 & 5 & 1 \\\\\n",
       "\t22 & R & 1 & 1 & 5 & 2 \\\\\n",
       "\t23 & R & 1 & 1 & 5 & 3 \\\\\n",
       "\t24 & R & 1 & 1 & 5 & 4 \\\\\n",
       "\t25 & R & 1 & 1 & 5 & 5 \\\\\n",
       "\t26 & L & 1 & 2 & 1 & 1 \\\\\n",
       "\t27 & B & 1 & 2 & 1 & 2 \\\\\n",
       "\t28 & R & 1 & 2 & 1 & 3 \\\\\n",
       "\t29 & R & 1 & 2 & 1 & 4 \\\\\n",
       "\t30 & R & 1 & 2 & 1 & 5 \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "625×5 DataFrame. Omitted printing of 1 columns\n",
       "│ Row │ Class_Name │ Left_Weight │ Left_Distance │ Right_Weight │\n",
       "│     │ \u001b[90mString\u001b[39m     │ \u001b[90mInt64\u001b[39m       │ \u001b[90mInt64\u001b[39m         │ \u001b[90mInt64\u001b[39m        │\n",
       "├─────┼────────────┼─────────────┼───────────────┼──────────────┤\n",
       "│ 1   │ B          │ 1           │ 1             │ 1            │\n",
       "│ 2   │ R          │ 1           │ 1             │ 1            │\n",
       "│ 3   │ R          │ 1           │ 1             │ 1            │\n",
       "│ 4   │ R          │ 1           │ 1             │ 1            │\n",
       "│ 5   │ R          │ 1           │ 1             │ 1            │\n",
       "│ 6   │ R          │ 1           │ 1             │ 2            │\n",
       "│ 7   │ R          │ 1           │ 1             │ 2            │\n",
       "│ 8   │ R          │ 1           │ 1             │ 2            │\n",
       "│ 9   │ R          │ 1           │ 1             │ 2            │\n",
       "│ 10  │ R          │ 1           │ 1             │ 2            │\n",
       "⋮\n",
       "│ 615 │ L          │ 5           │ 5             │ 3            │\n",
       "│ 616 │ L          │ 5           │ 5             │ 4            │\n",
       "│ 617 │ L          │ 5           │ 5             │ 4            │\n",
       "│ 618 │ L          │ 5           │ 5             │ 4            │\n",
       "│ 619 │ L          │ 5           │ 5             │ 4            │\n",
       "│ 620 │ L          │ 5           │ 5             │ 4            │\n",
       "│ 621 │ L          │ 5           │ 5             │ 5            │\n",
       "│ 622 │ L          │ 5           │ 5             │ 5            │\n",
       "│ 623 │ L          │ 5           │ 5             │ 5            │\n",
       "│ 624 │ L          │ 5           │ 5             │ 5            │\n",
       "│ 625 │ B          │ 5           │ 5             │ 5            │"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CSV.read(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at class labels to see if dataset is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Int64} with 3 entries:\n",
       "  \"B\" => 49\n",
       "  \"L\" => 288\n",
       "  \"R\" => 288"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = countmap(data[:(Class_Name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.0784\n",
       " 0.4608\n",
       " 0.4608"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(label_counts[i] / size(data)[1] for i in keys(label_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m────────────────\u001b[0m┬\u001b[0m─────────\u001b[0m┬\u001b[0m────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.names        \u001b[0m│\u001b[0m\u001b[22m _.types \u001b[0m│\u001b[0m\u001b[22m _.scitypes \u001b[0m│\u001b[0m\n",
       "├\u001b[0m────────────────\u001b[0m┼\u001b[0m─────────\u001b[0m┼\u001b[0m────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m Class_Name     \u001b[0m│\u001b[0m String  \u001b[0m│\u001b[0m Textual    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Left_Weight    \u001b[0m│\u001b[0m Int64   \u001b[0m│\u001b[0m Count      \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Left_Distance  \u001b[0m│\u001b[0m Int64   \u001b[0m│\u001b[0m Count      \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Right_Weight   \u001b[0m│\u001b[0m Int64   \u001b[0m│\u001b[0m Count      \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Right_Distance \u001b[0m│\u001b[0m Int64   \u001b[0m│\u001b[0m Count      \u001b[0m│\u001b[0m\n",
       "└\u001b[0m────────────────\u001b[0m┴\u001b[0m─────────\u001b[0m┴\u001b[0m────────────\u001b[0m┘\u001b[0m\n",
       "_.nrows = 625\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m────────────────\u001b[0m┬\u001b[0m─────────────────────────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.names        \u001b[0m│\u001b[0m\u001b[22m _.types                         \u001b[0m│\u001b[0m\u001b[22m _.scitypes    \u001b[0m│\u001b[0m\n",
       "├\u001b[0m────────────────\u001b[0m┼\u001b[0m─────────────────────────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m Class_Name     \u001b[0m│\u001b[0m CategoricalValue{String,UInt32} \u001b[0m│\u001b[0m Multiclass{3} \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Left_Weight    \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Left_Distance  \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Right_Weight   \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Right_Distance \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "└\u001b[0m────────────────\u001b[0m┴\u001b[0m─────────────────────────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┘\u001b[0m\n",
       "_.nrows = 625\n"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coerce!(data, :Class_Name=>Multiclass,\n",
    "              :Left_Weight=>Continuous,\n",
    "              :Right_Weight=>Continuous,\n",
    "              :Left_Distance=>Continuous,\n",
    "              :Right_Distance=>Continuous)\n",
    "schema(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CategoricalValue{String,UInt32}[\"B\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\"  …  \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"B\"], 625×4 DataFrame\n",
       "│ Row │ Left_Weight │ Left_Distance │ Right_Weight │ Right_Distance │\n",
       "│     │ \u001b[90mFloat64\u001b[39m     │ \u001b[90mFloat64\u001b[39m       │ \u001b[90mFloat64\u001b[39m      │ \u001b[90mFloat64\u001b[39m        │\n",
       "├─────┼─────────────┼───────────────┼──────────────┼────────────────┤\n",
       "│ 1   │ 1.0         │ 1.0           │ 1.0          │ 1.0            │\n",
       "│ 2   │ 1.0         │ 1.0           │ 1.0          │ 2.0            │\n",
       "│ 3   │ 1.0         │ 1.0           │ 1.0          │ 3.0            │\n",
       "│ 4   │ 1.0         │ 1.0           │ 1.0          │ 4.0            │\n",
       "│ 5   │ 1.0         │ 1.0           │ 1.0          │ 5.0            │\n",
       "│ 6   │ 1.0         │ 1.0           │ 2.0          │ 1.0            │\n",
       "│ 7   │ 1.0         │ 1.0           │ 2.0          │ 2.0            │\n",
       "│ 8   │ 1.0         │ 1.0           │ 2.0          │ 3.0            │\n",
       "│ 9   │ 1.0         │ 1.0           │ 2.0          │ 4.0            │\n",
       "│ 10  │ 1.0         │ 1.0           │ 2.0          │ 5.0            │\n",
       "⋮\n",
       "│ 615 │ 5.0         │ 5.0           │ 3.0          │ 5.0            │\n",
       "│ 616 │ 5.0         │ 5.0           │ 4.0          │ 1.0            │\n",
       "│ 617 │ 5.0         │ 5.0           │ 4.0          │ 2.0            │\n",
       "│ 618 │ 5.0         │ 5.0           │ 4.0          │ 3.0            │\n",
       "│ 619 │ 5.0         │ 5.0           │ 4.0          │ 4.0            │\n",
       "│ 620 │ 5.0         │ 5.0           │ 4.0          │ 5.0            │\n",
       "│ 621 │ 5.0         │ 5.0           │ 5.0          │ 1.0            │\n",
       "│ 622 │ 5.0         │ 5.0           │ 5.0          │ 2.0            │\n",
       "│ 623 │ 5.0         │ 5.0           │ 5.0          │ 3.0            │\n",
       "│ 624 │ 5.0         │ 5.0           │ 5.0          │ 4.0            │\n",
       "│ 625 │ 5.0         │ 5.0           │ 5.0          │ 5.0            │)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, X = unpack(data, ==(:Class_Name), colname->true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition train and test data accoring to class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([595, 102, 55, 568, 425, 389, 146, 63, 372, 250  …  195, 500, 571, 533, 112, 396, 297, 106, 303, 261], [444, 144, 546, 43, 19, 173, 365, 423, 27, 218  …  293, 614, 90, 471, 13, 134, 296, 79, 395, 415])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data to use when trying to fit a single validation set\n",
    "train, test = partition(eachindex(y), 0.7, shuffle=true, rng=123, stratify=values(data[:Class_Name])) # gives 70:30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.0776255707762557\n",
       " 0.4611872146118721\n",
       " 0.4611872146118721"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts = countmap(data[train,:Class_Name])\n",
    "collect(train_counts[i] / size(train)[1] for i in keys(train_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.08021390374331551\n",
       " 0.45989304812834225\n",
       " 0.45989304812834225"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_counts = countmap(data[test,:Class_Name])\n",
    "collect(test_counts[i] / size(test)[1] for i in keys(test_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Left_Weight</th><th>Left_Distance</th><th>Right_Weight</th><th>Right_Distance</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>5 rows × 4 columns</p><tr><th>1</th><td>1.0</td><td>1.0</td><td>1.0</td><td>1.0</td></tr><tr><th>2</th><td>1.0</td><td>1.0</td><td>1.0</td><td>2.0</td></tr><tr><th>3</th><td>1.0</td><td>1.0</td><td>1.0</td><td>3.0</td></tr><tr><th>4</th><td>1.0</td><td>1.0</td><td>1.0</td><td>4.0</td></tr><tr><th>5</th><td>1.0</td><td>1.0</td><td>1.0</td><td>5.0</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& Left\\_Weight & Left\\_Distance & Right\\_Weight & Right\\_Distance\\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64\\\\\n",
       "\t\\hline\n",
       "\t1 & 1.0 & 1.0 & 1.0 & 1.0 \\\\\n",
       "\t2 & 1.0 & 1.0 & 1.0 & 2.0 \\\\\n",
       "\t3 & 1.0 & 1.0 & 1.0 & 3.0 \\\\\n",
       "\t4 & 1.0 & 1.0 & 1.0 & 4.0 \\\\\n",
       "\t5 & 1.0 & 1.0 & 1.0 & 5.0 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "5×4 DataFrame\n",
       "│ Row │ Left_Weight │ Left_Distance │ Right_Weight │ Right_Distance │\n",
       "│     │ \u001b[90mFloat64\u001b[39m     │ \u001b[90mFloat64\u001b[39m       │ \u001b[90mFloat64\u001b[39m      │ \u001b[90mFloat64\u001b[39m        │\n",
       "├─────┼─────────────┼───────────────┼──────────────┼────────────────┤\n",
       "│ 1   │ 1.0         │ 1.0           │ 1.0          │ 1.0            │\n",
       "│ 2   │ 1.0         │ 1.0           │ 1.0          │ 2.0            │\n",
       "│ 3   │ 1.0         │ 1.0           │ 1.0          │ 3.0            │\n",
       "│ 4   │ 1.0         │ 1.0           │ 1.0          │ 4.0            │\n",
       "│ 5   │ 1.0         │ 1.0           │ 1.0          │ 5.0            │"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first(X,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five Learning Algorithms\n",
    "\n",
    "* Decision trees with some form of pruning\n",
    "* Neural networks\n",
    "* Boosting\n",
    "* Support Vector Machines\n",
    "* k-nearest neighbors\n",
    "\n",
    "\n",
    "##### Testing\n",
    "* Implement the algorithms\n",
    "* Design two *interesting* classification problems. For the purposes of this assignment, a classification problem is just a set of training examples and a set of test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42-element Array{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype),T} where T<:Tuple,1}:\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianLDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianQDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = ScikitLearn, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " (name = ExtraTreesClassifier, package_name = ScikitLearn, ... )\n",
       " ⋮\n",
       " (name = ProbabilisticSGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVC, package_name = LIBSVM, ... )\n",
       " (name = SVMClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(matching(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJModels ✔\n",
      "import DecisionTree ✔\n",
      "import MLJModels.DecisionTree_ ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading into module \"Main\": \n",
      "└ @ MLJModels /home/andrew/.julia/packages/MLJModels/mUBFt/src/loading.jl:70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = false,\n",
       "    merge_purity_threshold = 1.0,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 5)\u001b[34m @108\u001b[39m"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load DecisionTreeClassifier verbosity=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees\n",
    "* Be sure to use some form of pruning. \n",
    "* You are not required to use information gain (for example, there is something called the GINI index that is sometimes used) to split attributes, but you should describe whatever it is that you do use.\n",
    "\n",
    "1. https://alan-turing-institute.github.io/MLJ.jl/dev/transformers/#MLJModels.UnivariateDiscretizer\n",
    "1. https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/#Getting-Started-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No post-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = false,\n",
       "    merge_purity_threshold = 1.0,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 14)\u001b[34m @846\u001b[39m"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(post_prune=false, display_depth=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @968\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @069\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @739\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tree = machine(dt, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{DecisionTreeClassifier} @968\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 4, Threshold 2.5\n",
      "L-> Feature 2, Threshold 2.5\n",
      "    L-> Feature 1, Threshold 1.5\n",
      "        L-> Feature 3, Threshold 2.5\n",
      "            L-> Feature 4, Threshold 1.5\n",
      "                L-> Feature 2, Threshold 1.5\n",
      "                    L-> Feature 3, Threshold 1.5\n",
      "                        L-> 1 : 1/1\n",
      "                        R-> 3 : 1/1\n",
      "                    R-> 1 : 1/1\n",
      "                R-> 3 : 3/3\n",
      "            R-> 3 : 11/11\n",
      "        R-> Feature 3, Threshold 2.5\n",
      "            L-> Feature 1, Threshold 2.5\n",
      "                L-> Feature 4, Threshold 1.5\n",
      "                    L-> Feature 2, Threshold 1.5\n",
      "                        L-> Feature 3, Threshold 1.5\n",
      "                            L-> 2 : 1/1\n",
      "                            R-> 1 : 1/1\n",
      "                        R-> 2 : 2/2\n",
      "                    R-> 1 : 2/2\n",
      "                R-> Feature 2, Threshold 1.5\n",
      "                    L-> Feature 3, Threshold 1.5\n",
      "                        L-> 2 : 4/4\n",
      "                        R-> Feature 1, Threshold 4.5\n",
      "                            L-> Feature 4, Threshold 1.5\n",
      "                                L-> 2 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> 2 : 1/1\n",
      "                    R-> 2 : 9/9\n",
      "            R-> Feature 2, Threshold 1.5\n",
      "                L-> Feature 4, Threshold 1.5\n",
      "                    L-> Feature 1, Threshold 3.5\n",
      "                        L-> Feature 3, Threshold 3.5\n",
      "                            L-> Feature 1, Threshold 2.5\n",
      "                                L-> 3 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> 3 : 4/4\n",
      "                        R-> Feature 3, Threshold 4.5\n",
      "                            L-> 2 : 3/3\n",
      "                            R-> 3 : 1/1\n",
      "                    R-> 3 : 9/9\n",
      "                R-> Feature 3, Threshold 4.5\n",
      "                    L-> Feature 1, Threshold 4.5\n",
      "                        L-> Feature 4, Threshold 1.5\n",
      "                            L-> Feature 1, Threshold 2.5\n",
      "                                L-> Feature 3, Threshold 3.5\n",
      "                                    L-> 2 : 1/1\n",
      "                                    R-> 1 : 1/1\n",
      "                                R-> 2 : 3/3\n",
      "                            R-> Feature 3, Threshold 3.5\n",
      "                                L-> Feature 1, Threshold 3.5\n",
      "                                    L-> 1 : 1/1\n",
      "                                    R-> 2 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                        R-> 2 : 3/3\n",
      "                    R-> Feature 1, Threshold 4.0\n",
      "                        L-> 3 : 3/3\n",
      "                        R-> 1 : 1/1\n",
      "    R-> Feature 1, Threshold 2.5\n",
      "        L-> Feature 3, Threshold 2.5\n",
      "            L-> Feature 1, Threshold 1.5\n",
      "                L-> Feature 3, Threshold 1.5\n",
      "                    L-> 2 : 5/5\n",
      "                    R-> Feature 2, Threshold 4.5\n",
      "                        L-> Feature 2, Threshold 3.5\n",
      "                            L-> 2 : 1/1\n",
      "                            R-> 1 : 1/1\n",
      "                        R-> 2 : 2/2\n",
      "                R-> 2 : 11/11\n",
      "            R-> Feature 1, Threshold 1.5\n",
      "                L-> Feature 4, Threshold 1.5\n",
      "                    L-> Feature 2, Threshold 4.5\n",
      "                        L-> Feature 3, Threshold 3.5\n",
      "                            L-> 2 : 1/1\n",
      "                            R-> 3 : 3/3\n",
      "                        R-> Feature 3, Threshold 4.5\n",
      "                            L-> 2 : 2/2\n",
      "                            R-> 1 : 1/1\n",
      "                    R-> 3 : 6/6\n",
      "                R-> Feature 4, Threshold 1.5\n",
      "                    L-> 2 : 8/8\n",
      "                    R-> Feature 2, Threshold 3.5\n",
      "                        L-> Feature 3, Threshold 3.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> 3 : 2/2\n",
      "                        R-> Feature 3, Threshold 4.5\n",
      "                            L-> 2 : 3/3\n",
      "                            R-> 1 : 1/1\n",
      "        R-> 2 : 66/66\n",
      "R-> Feature 1, Threshold 2.5\n",
      "    L-> Feature 3, Threshold 2.5\n",
      "        L-> Feature 2, Threshold 3.5\n",
      "            L-> Feature 1, Threshold 1.5\n",
      "                L-> 3 : 13/13\n",
      "                R-> Feature 2, Threshold 1.5\n",
      "                    L-> 3 : 5/5\n",
      "                    R-> Feature 4, Threshold 4.5\n",
      "                        L-> Feature 3, Threshold 1.5\n",
      "                            L-> Feature 4, Threshold 3.5\n",
      "                                L-> 2 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> Feature 2, Threshold 2.5\n",
      "                                L-> 3 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                        R-> 3 : 2/2\n",
      "            R-> Feature 1, Threshold 1.5\n",
      "                L-> Feature 3, Threshold 1.5\n",
      "                    L-> Feature 4, Threshold 4.5\n",
      "                        L-> 2 : 2/2\n",
      "                        R-> 3 : 1/1\n",
      "                    R-> 3 : 4/4\n",
      "                R-> Feature 2, Threshold 4.5\n",
      "                    L-> Feature 4, Threshold 4.5\n",
      "                        L-> 1 : 1/1\n",
      "                        R-> Feature 3, Threshold 1.5\n",
      "                            L-> 2 : 1/1\n",
      "                            R-> 3 : 1/1\n",
      "                    R-> 2 : 4/4\n",
      "        R-> Feature 2, Threshold 4.5\n",
      "            L-> 3 : 47/47\n",
      "            R-> Feature 4, Threshold 3.5\n",
      "                L-> Feature 3, Threshold 3.5\n",
      "                    L-> Feature 1, Threshold 1.5\n",
      "                        L-> 3 : 1/1\n",
      "                        R-> 2 : 1/1\n",
      "                    R-> 3 : 3/3\n",
      "                R-> 3 : 8/8\n",
      "    R-> Feature 3, Threshold 2.5\n",
      "        L-> Feature 2, Threshold 1.5\n",
      "            L-> Feature 3, Threshold 1.5\n",
      "                L-> Feature 1, Threshold 3.5\n",
      "                    L-> Feature 4, Threshold 3.5\n",
      "                        L-> 1 : 1/1\n",
      "                        R-> 3 : 2/2\n",
      "                    R-> Feature 4, Threshold 3.5\n",
      "                        L-> 2 : 1/1\n",
      "                        R-> Feature 1, Threshold 4.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> Feature 4, Threshold 4.5\n",
      "                                L-> 2 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                R-> 3 : 7/7\n",
      "            R-> Feature 3, Threshold 1.5\n",
      "                L-> 2 : 25/25\n",
      "                R-> Feature 2, Threshold 2.5\n",
      "                    L-> Feature 4, Threshold 3.5\n",
      "                        L-> 2 : 2/2\n",
      "                        R-> Feature 4, Threshold 4.5\n",
      "                            L-> Feature 1, Threshold 3.5\n",
      "                                L-> 3 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> 3 : 1/1\n",
      "                    R-> Feature 4, Threshold 4.5\n",
      "                        L-> 2 : 10/10\n",
      "                        R-> Feature 1, Threshold 3.5\n",
      "                            L-> 3 : 1/1\n",
      "                            R-> 2 : 4/4\n",
      "        R-> Feature 2, Threshold 2.5\n",
      "            L-> Feature 4, Threshold 3.5\n",
      "                L-> Feature 1, Threshold 4.5\n",
      "                    L-> 3 : 7/7\n",
      "                    R-> Feature 3, Threshold 3.5\n",
      "                        L-> 2 : 1/1\n",
      "                        R-> 3 : 2/2\n",
      "                R-> 3 : 26/26\n",
      "            R-> Feature 4, Threshold 3.5\n",
      "                L-> Feature 3, Threshold 4.5\n",
      "                    L-> Feature 1, Threshold 3.5\n",
      "                        L-> Feature 2, Threshold 4.5\n",
      "                            L-> Feature 3, Threshold 3.5\n",
      "                                L-> Feature 2, Threshold 3.5\n",
      "                                    L-> 1 : 1/1\n",
      "                                    R-> 2 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> 2 : 2/2\n",
      "                        R-> 2 : 9/9\n",
      "                    R-> Feature 2, Threshold 3.5\n",
      "                        L-> Feature 1, Threshold 4.5\n",
      "                            L-> 3 : 2/2\n",
      "                            R-> 1 : 1/1\n",
      "                        R-> Feature 1, Threshold 3.5\n",
      "                            L-> Feature 2, Threshold 4.5\n",
      "                                L-> 3 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> 2 : 2/2\n",
      "                R-> Feature 1, Threshold 3.5\n",
      "                    L-> Feature 3, Threshold 3.5\n",
      "                        L-> Feature 4, Threshold 4.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> 3 : 1/1\n",
      "                        R-> 3 : 11/11\n",
      "                    R-> Feature 3, Threshold 3.5\n",
      "                        L-> Feature 2, Threshold 3.5\n",
      "                            L-> Feature 4, Threshold 4.5\n",
      "                                L-> Feature 1, Threshold 4.5\n",
      "                                    L-> 1 : 1/1\n",
      "                                    R-> 2 : 1/1\n",
      "                                R-> Feature 1, Threshold 4.5\n",
      "                                    L-> 3 : 1/1\n",
      "                                    R-> 1 : 1/1\n",
      "                            R-> 2 : 4/4\n",
      "                        R-> Feature 2, Threshold 3.5\n",
      "                            L-> 3 : 6/6\n",
      "                            R-> Feature 2, Threshold 4.5\n",
      "                                L-> Feature 3, Threshold 4.5\n",
      "                                    L-> 1 : 2/2\n",
      "                                    R-> Feature 1, Threshold 4.5\n",
      "                                        L-> 3 : 1/1\n",
      "                                        R-> Feature 4, Threshold 4.5\n",
      "                                            L-> 1 : 1/1\n",
      "                                            R-> 3 : 1/1\n",
      "                                R-> Feature 1, Threshold 4.5\n",
      "                                    L-> Feature 4, Threshold 4.5\n",
      "                                        L-> 1 : 1/1\n",
      "                                        R-> 3 : 1/1\n",
      "                                    R-> Feature 3, Threshold 4.5\n",
      "                                        L-> 2 : 1/1\n",
      "                                        R-> Feature 4, Threshold 4.5\n",
      "                                            L-> 2 : 1/1\n",
      "                                            R-> 1 : 1/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @968\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @069\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @739\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(Tree, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:05\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m───────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┬\u001b[0m─────────────────────────────────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.measure     \u001b[0m│\u001b[0m\u001b[22m _.measurement \u001b[0m│\u001b[0m\u001b[22m _.per_fold                              \u001b[0m│\u001b[0m\n",
       "├\u001b[0m───────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┼\u001b[0m─────────────────────────────────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m cross_entropy \u001b[0m│\u001b[0m 7.73          \u001b[0m│\u001b[0m [7.21, 7.97, 7.28, 7.28, 7.97, 8.66]    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m acc           \u001b[0m│\u001b[0m 0.786         \u001b[0m│\u001b[0m [0.8, 0.779, 0.798, 0.798, 0.779, 0.76] \u001b[0m│\u001b[0m\n",
       "└\u001b[0m───────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┴\u001b[0m─────────────────────────────────────────\u001b[0m┘\u001b[0m\n",
       "_.per_observation = [[[2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 36.0], [2.22e-16, 2.22e-16, ..., 2.22e-16], [36.0, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16]], missing]\n",
       "_.fitted_params_per_fold = [ … ]\n",
       "_.report_per_fold = [ … ]\n"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_acc = evaluate!(Tree, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 1, Threshold 2.5\n",
      "L-> Feature 3, Threshold 2.5\n",
      "    L-> Feature 2, Threshold 3.5\n",
      "        L-> Feature 4, Threshold 1.5\n",
      "            L-> Feature 2, Threshold 1.5\n",
      "                L-> Feature 1, Threshold 1.5\n",
      "                    L-> Feature 3, Threshold 1.5\n",
      "                        L-> 1 : 1/1\n",
      "                        R-> 3 : 1/1\n",
      "                    R-> Feature 3, Threshold 1.5\n",
      "                        L-> 2 : 1/1\n",
      "                        R-> 1 : 1/1\n",
      "                R-> 2 : 4/4\n",
      "            R-> Feature 2, Threshold 1.5\n",
      "                L-> 3 : 12/12\n",
      "                R-> Feature 1, Threshold 1.5\n",
      "                    L-> Feature 4, Threshold 3.5\n",
      "                        L-> Feature 3, Threshold 1.5\n",
      "                            L-> Feature 2, Threshold 2.5\n",
      "                                L-> Feature 4, Threshold 2.5\n",
      "                                    L-> 1 : 1/1\n",
      "                                    R-> 3 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> 3 : 3/3\n",
      "                        R-> 3 : 7/7\n",
      "                    R-> Feature 3, Threshold 1.5\n",
      "                        L-> Feature 2, Threshold 2.5\n",
      "                            L-> Feature 4, Threshold 3.5\n",
      "                                L-> 2 : 1/1\n",
      "                                R-> Feature 4, Threshold 4.5\n",
      "                                    L-> 1 : 1/1\n",
      "                                    R-> 3 : 1/1\n",
      "                            R-> 2 : 4/4\n",
      "                        R-> Feature 4, Threshold 2.5\n",
      "                            L-> Feature 2, Threshold 2.5\n",
      "                                L-> 1 : 1/1\n",
      "                                R-> 2 : 1/1\n",
      "                            R-> Feature 4, Threshold 3.5\n",
      "                                L-> Feature 2, Threshold 2.5\n",
      "                                    L-> 3 : 1/1\n",
      "                                    R-> 1 : 1/1\n",
      "                                R-> 3 : 3/3\n",
      "        R-> Feature 4, Threshold 2.5\n",
      "            L-> Feature 1, Threshold 1.5\n",
      "                L-> Feature 2, Threshold 4.5\n",
      "                    L-> Feature 4, Threshold 1.5\n",
      "                        L-> 2 : 2/2\n",
      "                        R-> 1 : 1/1\n",
      "                    R-> 2 : 2/2\n",
      "                R-> 2 : 8/8\n",
      "            R-> Feature 1, Threshold 1.5\n",
      "                L-> Feature 3, Threshold 1.5\n",
      "                    L-> Feature 4, Threshold 4.5\n",
      "                        L-> Feature 2, Threshold 4.5\n",
      "                            L-> Feature 4, Threshold 3.5\n",
      "                                L-> 2 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> 2 : 1/1\n",
      "                        R-> Feature 2, Threshold 4.5\n",
      "                            L-> 3 : 1/1\n",
      "                            R-> 1 : 1/1\n",
      "                    R-> 3 : 5/5\n",
      "                R-> Feature 3, Threshold 1.5\n",
      "                    L-> 2 : 5/5\n",
      "                    R-> Feature 4, Threshold 4.5\n",
      "                        L-> Feature 4, Threshold 3.5\n",
      "                            L-> 2 : 2/2\n",
      "                            R-> Feature 2, Threshold 4.5\n",
      "                                L-> 1 : 1/1\n",
      "                                R-> 2 : 1/1\n",
      "                        R-> Feature 2, Threshold 4.5\n",
      "                            L-> 3 : 1/1\n",
      "                            R-> 1 : 1/1\n",
      "    R-> Feature 4, Threshold 1.5\n",
      "        L-> Feature 2, Threshold 2.5\n",
      "            L-> Feature 2, Threshold 1.5\n",
      "                L-> 3 : 5/5\n",
      "                R-> Feature 1, Threshold 1.5\n",
      "                    L-> 3 : 2/2\n",
      "                    R-> Feature 3, Threshold 3.5\n",
      "                        L-> 2 : 1/1\n",
      "                        R-> Feature 3, Threshold 4.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> 3 : 1/1\n",
      "            R-> Feature 1, Threshold 1.5\n",
      "                L-> Feature 3, Threshold 3.5\n",
      "                    L-> 2 : 2/2\n",
      "                    R-> Feature 2, Threshold 4.5\n",
      "                        L-> Feature 3, Threshold 4.5\n",
      "                            L-> Feature 2, Threshold 3.5\n",
      "                                L-> 3 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> 3 : 2/2\n",
      "                        R-> Feature 3, Threshold 4.5\n",
      "                            L-> 2 : 1/1\n",
      "                            R-> 1 : 1/1\n",
      "                R-> 2 : 8/8\n",
      "        R-> Feature 4, Threshold 2.5\n",
      "            L-> Feature 2, Threshold 4.5\n",
      "                L-> Feature 2, Threshold 2.5\n",
      "                    L-> 3 : 10/10\n",
      "                    R-> Feature 1, Threshold 1.5\n",
      "                        L-> 3 : 5/5\n",
      "                        R-> Feature 3, Threshold 4.5\n",
      "                            L-> Feature 3, Threshold 3.5\n",
      "                                L-> 1 : 1/1\n",
      "                                R-> Feature 2, Threshold 3.5\n",
      "                                    L-> 3 : 1/1\n",
      "                                    R-> 1 : 1/1\n",
      "                            R-> 3 : 2/2\n",
      "                R-> Feature 1, Threshold 1.5\n",
      "                    L-> 3 : 2/2\n",
      "                    R-> 2 : 1/1\n",
      "            R-> 3 : 77/77\n",
      "R-> Feature 2, Threshold 2.5\n",
      "    L-> Feature 4, Threshold 1.5\n",
      "        L-> Feature 2, Threshold 1.5\n",
      "            L-> Feature 3, Threshold 3.5\n",
      "                L-> Feature 3, Threshold 2.5\n",
      "                    L-> 2 : 5/5\n",
      "                    R-> Feature 1, Threshold 3.5\n",
      "                        L-> 1 : 1/1\n",
      "                        R-> 2 : 2/2\n",
      "                R-> Feature 1, Threshold 4.5\n",
      "                    L-> Feature 1, Threshold 3.5\n",
      "                        L-> 3 : 2/2\n",
      "                        R-> Feature 3, Threshold 4.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> 3 : 1/1\n",
      "                    R-> 2 : 1/1\n",
      "            R-> 2 : 12/12\n",
      "        R-> Feature 3, Threshold 1.5\n",
      "            L-> Feature 2, Threshold 1.5\n",
      "                L-> Feature 4, Threshold 4.5\n",
      "                    L-> Feature 1, Threshold 3.5\n",
      "                        L-> 1 : 1/1\n",
      "                        R-> Feature 4, Threshold 3.5\n",
      "                            L-> 2 : 3/3\n",
      "                            R-> Feature 1, Threshold 4.5\n",
      "                                L-> 1 : 1/1\n",
      "                                R-> 2 : 1/1\n",
      "                    R-> Feature 1, Threshold 4.5\n",
      "                        L-> 3 : 2/2\n",
      "                        R-> 1 : 1/1\n",
      "                R-> 2 : 10/10\n",
      "            R-> Feature 2, Threshold 1.5\n",
      "                L-> Feature 4, Threshold 2.5\n",
      "                    L-> Feature 3, Threshold 2.5\n",
      "                        L-> Feature 1, Threshold 4.0\n",
      "                            L-> 3 : 1/1\n",
      "                            R-> 2 : 1/1\n",
      "                        R-> 3 : 8/8\n",
      "                    R-> 3 : 28/28\n",
      "                R-> Feature 4, Threshold 2.5\n",
      "                    L-> Feature 1, Threshold 4.5\n",
      "                        L-> Feature 3, Threshold 3.5\n",
      "                            L-> Feature 1, Threshold 3.5\n",
      "                                L-> 1 : 1/1\n",
      "                                R-> 2 : 2/2\n",
      "                            R-> Feature 1, Threshold 3.5\n",
      "                                L-> 3 : 1/1\n",
      "                                R-> Feature 3, Threshold 4.5\n",
      "                                    L-> 1 : 1/1\n",
      "                                    R-> 3 : 1/1\n",
      "                        R-> 2 : 3/3\n",
      "                    R-> Feature 3, Threshold 2.5\n",
      "                        L-> Feature 1, Threshold 3.5\n",
      "                            L-> Feature 4, Threshold 3.5\n",
      "                                L-> 1 : 1/1\n",
      "                                R-> 3 : 2/2\n",
      "                            R-> Feature 4, Threshold 3.5\n",
      "                                L-> 2 : 1/1\n",
      "                                R-> Feature 4, Threshold 4.5\n",
      "                                    L-> Feature 1, Threshold 4.5\n",
      "                                        L-> 1 : 1/1\n",
      "                                        R-> 2 : 1/1\n",
      "                                    R-> 1 : 1/1\n",
      "                        R-> 3 : 26/26\n",
      "    R-> Feature 3, Threshold 2.5\n",
      "        L-> 2 : 80/80\n",
      "        R-> Feature 4, Threshold 3.5\n",
      "            L-> Feature 4, Threshold 2.5\n",
      "                L-> Feature 3, Threshold 4.5\n",
      "                    L-> 2 : 30/30\n",
      "                    R-> Feature 1, Threshold 3.5\n",
      "                        L-> Feature 2, Threshold 3.5\n",
      "                            L-> Feature 4, Threshold 1.5\n",
      "                                L-> 2 : 1/1\n",
      "                                R-> 3 : 1/1\n",
      "                            R-> 2 : 4/4\n",
      "                        R-> 2 : 8/8\n",
      "                R-> Feature 2, Threshold 3.5\n",
      "                    L-> Feature 1, Threshold 3.5\n",
      "                        L-> 3 : 2/2\n",
      "                        R-> Feature 3, Threshold 4.5\n",
      "                            L-> Feature 1, Threshold 4.5\n",
      "                                L-> Feature 3, Threshold 3.5\n",
      "                                    L-> 2 : 1/1\n",
      "                                    R-> 1 : 1/1\n",
      "                                R-> 2 : 2/2\n",
      "                            R-> Feature 1, Threshold 4.5\n",
      "                                L-> 3 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                    R-> Feature 3, Threshold 4.5\n",
      "                        L-> 2 : 10/10\n",
      "                        R-> Feature 1, Threshold 3.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> 2 : 2/2\n",
      "            R-> Feature 2, Threshold 3.5\n",
      "                L-> Feature 3, Threshold 3.5\n",
      "                    L-> Feature 1, Threshold 4.5\n",
      "                        L-> 3 : 3/3\n",
      "                        R-> Feature 4, Threshold 4.5\n",
      "                            L-> 2 : 1/1\n",
      "                            R-> 1 : 1/1\n",
      "                    R-> 3 : 12/12\n",
      "                R-> Feature 1, Threshold 3.5\n",
      "                    L-> Feature 3, Threshold 3.5\n",
      "                        L-> Feature 2, Threshold 4.5\n",
      "                            L-> Feature 4, Threshold 4.5\n",
      "                                L-> 1 : 1/1\n",
      "                                R-> 3 : 1/1\n",
      "                            R-> 1 : 1/1\n",
      "                        R-> 3 : 5/5\n",
      "                    R-> Feature 3, Threshold 3.5\n",
      "                        L-> 2 : 7/7\n",
      "                        R-> Feature 3, Threshold 4.5\n",
      "                            L-> Feature 2, Threshold 4.5\n",
      "                                L-> Feature 4, Threshold 4.5\n",
      "                                    L-> Feature 1, Threshold 4.5\n",
      "                                        L-> 1 : 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        R-> 2 : 1/1\n",
      "                                    R-> 1 : 1/1\n",
      "                                R-> Feature 4, Threshold 4.5\n",
      "                                    L-> 2 : 2/2\n",
      "                                    R-> Feature 1, Threshold 4.5\n",
      "                                        L-> 1 : 1/1\n",
      "                                        R-> 2 : 1/1\n",
      "                            R-> Feature 2, Threshold 4.5\n",
      "                                L-> Feature 1, Threshold 4.5\n",
      "                                    L-> 3 : 2/2\n",
      "                                    R-> 1 : 1/1\n",
      "                                R-> Feature 1, Threshold 4.5\n",
      "                                    L-> 1 : 1/1\n",
      "                                    R-> Feature 4, Threshold 4.5\n",
      "                                        L-> 2 : 1/1\n",
      "                                        R-> 1 : 1/1\n"
     ]
    }
   ],
   "source": [
    "fitted_params(Tree) \n",
    "print_tree(Tree.fitresult[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tree = Decision Tree\n",
       "Leaves: 124\n",
       "Depth:  11,\n",
       " encoding = Dict{CategoricalValue{String,UInt32},UInt32}(\"B\" => 0x00000001,\"L\" => 0x00000002,\"R\" => 0x00000003),)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_params(Tree) \n",
    "# print_tree(Tree.fitresult[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(classes_seen = CategoricalValue{String,UInt32}[\"B\", \"L\", \"R\"],\n",
       " print_tree = TreePrinter object (call with display depth),)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Post-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = true,\n",
       "    merge_purity_threshold = 0.7,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 14)\u001b[34m @803\u001b[39m"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt2 = DecisionTreeClassifier(post_prune=true, display_depth=14, merge_purity_threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @043\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @602\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @973\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tree2 = machine(dt2, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{DecisionTreeClassifier} @043\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 4, Threshold 2.5\n",
      "L-> Feature 2, Threshold 2.5\n",
      "    L-> Feature 1, Threshold 1.5\n",
      "        L-> Feature 3, Threshold 2.5\n",
      "            L-> Feature 4, Threshold 1.5\n",
      "                L-> Feature 3, Threshold 1.5\n",
      "                    L-> 1 : 1/1\n",
      "                    R-> Feature 2, Threshold 1.5\n",
      "                        L-> 3 : 1/1\n",
      "                        R-> 1 : 1/1\n",
      "                R-> 3 : 3/3\n",
      "            R-> 3 : 11/11\n",
      "        R-> Feature 3, Threshold 2.5\n",
      "            L-> Feature 1, Threshold 2.5\n",
      "                L-> Feature 4, Threshold 1.5\n",
      "                    L-> Feature 2, Threshold 1.5\n",
      "                        L-> Feature 3, Threshold 1.5\n",
      "                            L-> 2 : 1/1\n",
      "                            R-> 1 : 1/1\n",
      "                        R-> 2 : 2/2\n",
      "                    R-> 1 : 2/2\n",
      "                R-> Feature 2, Threshold 1.5\n",
      "                    L-> Feature 3, Threshold 1.5\n",
      "                        L-> 2 : 4/4\n",
      "                        R-> Feature 1, Threshold 4.5\n",
      "                            L-> Feature 4, Threshold 1.5\n",
      "                                L-> 2 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> 2 : 1/1\n",
      "                    R-> 2 : 9/9\n",
      "            R-> Feature 2, Threshold 1.5\n",
      "                L-> Feature 4, Threshold 1.5\n",
      "                    L-> Feature 1, Threshold 3.5\n",
      "                        L-> Feature 3, Threshold 3.5\n",
      "                            L-> Feature 1, Threshold 2.5\n",
      "                                L-> 3 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> 3 : 4/4\n",
      "                        R-> 2 : 3/4\n",
      "                    R-> 3 : 9/9\n",
      "                R-> Feature 3, Threshold 4.5\n",
      "                    L-> Feature 1, Threshold 4.5\n",
      "                        L-> Feature 4, Threshold 1.5\n",
      "                            L-> Feature 1, Threshold 2.5\n",
      "                                L-> Feature 3, Threshold 3.5\n",
      "                                    L-> 2 : 1/1\n",
      "                                    R-> 1 : 1/1\n",
      "                                R-> 2 : 3/3\n",
      "                            R-> Feature 3, Threshold 3.5\n",
      "                                L-> Feature 1, Threshold 3.5\n",
      "                                    L-> 1 : 1/1\n",
      "                                    R-> 2 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                        R-> 2 : 3/3\n",
      "                    R-> 3 : 3/4\n",
      "    R-> Feature 1, Threshold 2.5\n",
      "        L-> Feature 3, Threshold 2.5\n",
      "            L-> Feature 1, Threshold 1.5\n",
      "                L-> Feature 3, Threshold 1.5\n",
      "                    L-> 2 : 5/5\n",
      "                    R-> Feature 2, Threshold 4.5\n",
      "                        L-> Feature 2, Threshold 3.5\n",
      "                            L-> 2 : 1/1\n",
      "                            R-> 1 : 1/1\n",
      "                        R-> 2 : 2/2\n",
      "                R-> 2 : 11/11\n",
      "            R-> Feature 1, Threshold 1.5\n",
      "                L-> Feature 4, Threshold 1.5\n",
      "                    L-> Feature 2, Threshold 4.5\n",
      "                        L-> 3 : 3/4\n",
      "                        R-> Feature 3, Threshold 4.5\n",
      "                            L-> 2 : 2/2\n",
      "                            R-> 1 : 1/1\n",
      "                    R-> 3 : 6/6\n",
      "                R-> Feature 4, Threshold 1.5\n",
      "                    L-> 2 : 8/8\n",
      "                    R-> Feature 2, Threshold 3.5\n",
      "                        L-> Feature 3, Threshold 3.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> 3 : 2/2\n",
      "                        R-> 2 : 3/4\n",
      "        R-> 2 : 66/66\n",
      "R-> Feature 1, Threshold 2.5\n",
      "    L-> Feature 3, Threshold 2.5\n",
      "        L-> Feature 2, Threshold 3.5\n",
      "            L-> Feature 1, Threshold 1.5\n",
      "                L-> 3 : 13/13\n",
      "                R-> Feature 2, Threshold 1.5\n",
      "                    L-> 3 : 5/5\n",
      "                    R-> Feature 4, Threshold 4.5\n",
      "                        L-> Feature 3, Threshold 1.5\n",
      "                            L-> Feature 4, Threshold 3.5\n",
      "                                L-> 2 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> Feature 2, Threshold 2.5\n",
      "                                L-> 3 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                        R-> 3 : 2/2\n",
      "            R-> Feature 1, Threshold 1.5\n",
      "                L-> Feature 3, Threshold 1.5\n",
      "                    L-> Feature 4, Threshold 4.5\n",
      "                        L-> 2 : 2/2\n",
      "                        R-> 3 : 1/1\n",
      "                    R-> 3 : 4/4\n",
      "                R-> Feature 2, Threshold 4.5\n",
      "                    L-> Feature 4, Threshold 4.5\n",
      "                        L-> 1 : 1/1\n",
      "                        R-> Feature 3, Threshold 1.5\n",
      "                            L-> 2 : 1/1\n",
      "                            R-> 3 : 1/1\n",
      "                    R-> 2 : 4/4\n",
      "        R-> Feature 2, Threshold 4.5\n",
      "            L-> 3 : 47/47\n",
      "            R-> Feature 4, Threshold 3.5\n",
      "                L-> Feature 3, Threshold 3.5\n",
      "                    L-> Feature 1, Threshold 1.5\n",
      "                        L-> 3 : 1/1\n",
      "                        R-> 2 : 1/1\n",
      "                    R-> 3 : 3/3\n",
      "                R-> 3 : 8/8\n",
      "    R-> Feature 3, Threshold 2.5\n",
      "        L-> Feature 2, Threshold 1.5\n",
      "            L-> Feature 3, Threshold 1.5\n",
      "                L-> Feature 1, Threshold 3.5\n",
      "                    L-> Feature 4, Threshold 3.5\n",
      "                        L-> 1 : 1/1\n",
      "                        R-> 3 : 2/2\n",
      "                    R-> Feature 1, Threshold 4.5\n",
      "                        L-> 1 : 1/1\n",
      "                        R-> Feature 4, Threshold 4.5\n",
      "                            L-> 2 : 2/2\n",
      "                            R-> 1 : 1/1\n",
      "                R-> 3 : 7/7\n",
      "            R-> Feature 3, Threshold 1.5\n",
      "                L-> 2 : 25/25\n",
      "                R-> Feature 2, Threshold 2.5\n",
      "                    L-> Feature 4, Threshold 3.5\n",
      "                        L-> 2 : 2/2\n",
      "                        R-> Feature 1, Threshold 3.5\n",
      "                            L-> 3 : 1/1\n",
      "                            R-> Feature 4, Threshold 4.5\n",
      "                                L-> 1 : 1/1\n",
      "                                R-> 3 : 1/1\n",
      "                    R-> 2 : 14/15\n",
      "        R-> Feature 2, Threshold 2.5\n",
      "            L-> Feature 1, Threshold 4.5\n",
      "                L-> 3 : 26/26\n",
      "                R-> Feature 4, Threshold 3.5\n",
      "                    L-> Feature 3, Threshold 3.5\n",
      "                        L-> 2 : 1/1\n",
      "                        R-> 3 : 2/2\n",
      "                    R-> 3 : 7/7\n",
      "            R-> Feature 4, Threshold 3.5\n",
      "                L-> Feature 3, Threshold 4.5\n",
      "                    L-> Feature 1, Threshold 3.5\n",
      "                        L-> Feature 2, Threshold 4.5\n",
      "                            L-> Feature 2, Threshold 3.5\n",
      "                                L-> 1 : 1/1\n",
      "                                R-> Feature 3, Threshold 3.5\n",
      "                                    L-> 2 : 1/1\n",
      "                                    R-> 1 : 1/1\n",
      "                            R-> 2 : 2/2\n",
      "                        R-> 2 : 9/9\n",
      "                    R-> Feature 2, Threshold 3.5\n",
      "                        L-> Feature 1, Threshold 4.5\n",
      "                            L-> 3 : 2/2\n",
      "                            R-> 1 : 1/1\n",
      "                        R-> Feature 1, Threshold 3.5\n",
      "                            L-> Feature 2, Threshold 4.5\n",
      "                                L-> 3 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> 2 : 2/2\n",
      "                R-> Feature 1, Threshold 3.5\n",
      "                    L-> Feature 3, Threshold 3.5\n",
      "                        L-> Feature 4, Threshold 4.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> 3 : 1/1\n",
      "                        R-> 3 : 11/11\n",
      "                    R-> Feature 3, Threshold 3.5\n",
      "                        L-> Feature 2, Threshold 3.5\n",
      "                            L-> Feature 4, Threshold 4.5\n",
      "                                L-> Feature 1, Threshold 4.5\n",
      "                                    L-> 1 : 1/1\n",
      "                                    R-> 2 : 1/1\n",
      "                                R-> Feature 1, Threshold 4.5\n",
      "                                    L-> 3 : 1/1\n",
      "                                    R-> 1 : 1/1\n",
      "                            R-> 2 : 4/4\n",
      "                        R-> Feature 2, Threshold 3.5\n",
      "                            L-> 3 : 6/6\n",
      "                            R-> Feature 2, Threshold 4.5\n",
      "                                L-> Feature 3, Threshold 4.5\n",
      "                                    L-> 1 : 2/2\n",
      "                                    R-> Feature 1, Threshold 4.5\n",
      "                                        L-> 3 : 1/1\n",
      "                                        R-> Feature 4, Threshold 4.5\n",
      "                                            L-> 1 : 1/1\n",
      "                                            R-> 3 : 1/1\n",
      "                                R-> Feature 1, Threshold 4.5\n",
      "                                    L-> Feature 4, Threshold 4.5\n",
      "                                        L-> 1 : 1/1\n",
      "                                        R-> 3 : 1/1\n",
      "                                    R-> Feature 3, Threshold 4.5\n",
      "                                        L-> 2 : 1/1\n",
      "                                        R-> Feature 4, Threshold 4.5\n",
      "                                            L-> 2 : 1/1\n",
      "                                            R-> 1 : 1/1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @043\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @602\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @973\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(Tree2, rows=train, verbosity=2, force=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m───────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┬\u001b[0m──────────────────────────────────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.measure     \u001b[0m│\u001b[0m\u001b[22m _.measurement \u001b[0m│\u001b[0m\u001b[22m _.per_fold                               \u001b[0m│\u001b[0m\n",
       "├\u001b[0m───────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┼\u001b[0m──────────────────────────────────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m cross_entropy \u001b[0m│\u001b[0m 7.11          \u001b[0m│\u001b[0m [7.21, 5.89, 8.73, 6.94, 6.24, 7.63]     \u001b[0m│\u001b[0m\n",
       "│\u001b[0m acc           \u001b[0m│\u001b[0m 0.797         \u001b[0m│\u001b[0m [0.8, 0.837, 0.731, 0.798, 0.827, 0.788] \u001b[0m│\u001b[0m\n",
       "└\u001b[0m───────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┴\u001b[0m──────────────────────────────────────────\u001b[0m┘\u001b[0m\n",
       "_.per_observation = [[[2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 36.0], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16]], missing]\n",
       "_.fitted_params_per_fold = [ … ]\n",
       "_.report_per_fold = [ … ]\n"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_acc = evaluate!(Tree2, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate!(Tree2, resampling=CV(shuffle=true), measure=[tnr,tpr,fnr,fpr], verbosity=1, operation=predict_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tree = Decision Tree\n",
       "Leaves: 133\n",
       "Depth:  12,\n",
       " encoding = Dict{CategoricalValue{String,UInt32},UInt32}(\"B\" => 0x00000001,\"L\" => 0x00000002,\"R\" => 0x00000003),)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_params(Tree2) \n",
    "# print_tree(Tree.fitresult[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(classes_seen = CategoricalValue{String,UInt32}[\"B\", \"L\", \"R\"],\n",
       " print_tree = TreePrinter object (call with display depth),)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(Tree2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch / RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @580\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @489\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @642\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(post_prune=true, display_depth=14)\n",
    "Tree = machine(dt, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @287\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      "┌ Info: Attempting to evaluate 1000 models.\n",
      "└ @ MLJTuning /home/andrew/.julia/packages/MLJTuning/Bbgvk/src/tuned_models.jl:494\n",
      "\u001b[33mEvaluating over 1000 metamodels: 100%[=========================] Time: 0:00:07\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(parameter_name = \"merge_purity_threshold\",\n",
       " parameter_scale = :linear,\n",
       " parameter_values = [0.50001, 0.5005104904904905, 0.501010980980981, 0.5015114714714715, 0.502011961961962, 0.5025124524524525, 0.503012942942943, 0.5035134334334335, 0.504013923923924, 0.5045144144144145  …  0.9954955855855856, 0.9959960760760761, 0.9964965665665666, 0.9969970570570571, 0.9974975475475476, 0.997998038038038, 0.9984985285285285, 0.998999019019019, 0.9994995095095095, 1.0],\n",
       " measurements = [7.28811137799705, 7.28511185077465, 7.17780747365816, 7.132649102456171, 7.352456636097592, 7.132640531028236, 6.734462556460055, 7.006175980516129, 7.116124242539272, 7.247344016027882  …  7.668628330956854, 8.129626217710213, 8.016302154856765, 7.898027040713598, 7.7841528610501785, 8.07571477033333, 8.479500508849997, 8.710549569036644, 8.476749924800156, 8.192339534046589],)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = range(dt, :merge_purity_threshold, lower=0.50001, upper=1)\n",
    "curve = learning_curve(Tree, \n",
    "                        range=r, \n",
    "#                         resampling=Holdout(fraction_train=0.7), \n",
    "                        resampling=CV(nfolds=6, shuffle=true), \n",
    "                        measure=cross_entropy, \n",
    "                        acceleration=CPUThreads(),\n",
    "                        resolution = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip910\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip910)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip911\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip910)\" d=\"\n",
       "M211.07 1423.18 L2352.76 1423.18 L2352.76 47.2441 L211.07 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip912\">\n",
       "    <rect x=\"211\" y=\"47\" width=\"2143\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  271.643,1423.18 271.643,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  675.743,1423.18 675.743,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1079.84,1423.18 1079.84,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1483.94,1423.18 1483.94,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1888.04,1423.18 1888.04,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2292.14,1423.18 2292.14,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.07,1250.91 2352.76,1250.91 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.07,1022.53 2352.76,1022.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.07,794.149 2352.76,794.149 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.07,565.77 2352.76,565.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.07,337.39 2352.76,337.39 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip912)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.07,109.011 2352.76,109.011 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.07,1423.18 2352.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.07,1423.18 211.07,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  271.643,1423.18 271.643,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  675.743,1423.18 675.743,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1079.84,1423.18 1079.84,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1483.94,1423.18 1483.94,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1888.04,1423.18 1888.04,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2292.14,1423.18 2292.14,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.07,1250.91 236.77,1250.91 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.07,1022.53 236.77,1022.53 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.07,794.149 236.77,794.149 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.07,565.77 236.77,565.77 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.07,337.39 236.77,337.39 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.07,109.011 236.77,109.011 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip910)\" d=\"M 0 0 M253.657 1445.17 Q250.046 1445.17 248.218 1448.74 Q246.412 1452.28 246.412 1459.41 Q246.412 1466.51 248.218 1470.08 Q250.046 1473.62 253.657 1473.62 Q257.292 1473.62 259.097 1470.08 Q260.926 1466.51 260.926 1459.41 Q260.926 1452.28 259.097 1448.74 Q257.292 1445.17 253.657 1445.17 M253.657 1441.47 Q259.467 1441.47 262.523 1446.07 Q265.602 1450.66 265.602 1459.41 Q265.602 1468.13 262.523 1472.74 Q259.467 1477.32 253.657 1477.32 Q247.847 1477.32 244.769 1472.74 Q241.713 1468.13 241.713 1459.41 Q241.713 1450.66 244.769 1446.07 Q247.847 1441.47 253.657 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M270.671 1470.77 L275.555 1470.77 L275.555 1476.65 L270.671 1476.65 L270.671 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M280.671 1442.09 L299.027 1442.09 L299.027 1446.03 L284.953 1446.03 L284.953 1454.5 Q285.972 1454.15 286.99 1453.99 Q288.009 1453.8 289.028 1453.8 Q294.815 1453.8 298.194 1456.98 Q301.574 1460.15 301.574 1465.56 Q301.574 1471.14 298.102 1474.24 Q294.629 1477.32 288.31 1477.32 Q286.134 1477.32 283.866 1476.95 Q281.62 1476.58 279.213 1475.84 L279.213 1471.14 Q281.296 1472.28 283.518 1472.83 Q285.741 1473.39 288.217 1473.39 Q292.222 1473.39 294.56 1471.28 Q296.898 1469.18 296.898 1465.56 Q296.898 1461.95 294.56 1459.85 Q292.222 1457.74 288.217 1457.74 Q286.342 1457.74 284.467 1458.16 Q282.616 1458.57 280.671 1459.45 L280.671 1442.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M657.178 1445.17 Q653.567 1445.17 651.739 1448.74 Q649.933 1452.28 649.933 1459.41 Q649.933 1466.51 651.739 1470.08 Q653.567 1473.62 657.178 1473.62 Q660.813 1473.62 662.618 1470.08 Q664.447 1466.51 664.447 1459.41 Q664.447 1452.28 662.618 1448.74 Q660.813 1445.17 657.178 1445.17 M657.178 1441.47 Q662.989 1441.47 666.044 1446.07 Q669.123 1450.66 669.123 1459.41 Q669.123 1468.13 666.044 1472.74 Q662.989 1477.32 657.178 1477.32 Q651.368 1477.32 648.29 1472.74 Q645.234 1468.13 645.234 1459.41 Q645.234 1450.66 648.29 1446.07 Q651.368 1441.47 657.178 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M674.192 1470.77 L679.076 1470.77 L679.076 1476.65 L674.192 1476.65 L674.192 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M694.724 1457.51 Q691.576 1457.51 689.725 1459.66 Q687.896 1461.81 687.896 1465.56 Q687.896 1469.29 689.725 1471.47 Q691.576 1473.62 694.724 1473.62 Q697.873 1473.62 699.701 1471.47 Q701.553 1469.29 701.553 1465.56 Q701.553 1461.81 699.701 1459.66 Q697.873 1457.51 694.724 1457.51 M704.007 1442.86 L704.007 1447.11 Q702.248 1446.28 700.442 1445.84 Q698.66 1445.4 696.9 1445.4 Q692.271 1445.4 689.817 1448.53 Q687.387 1451.65 687.039 1457.97 Q688.405 1455.96 690.465 1454.89 Q692.525 1453.8 695.002 1453.8 Q700.211 1453.8 703.22 1456.98 Q706.252 1460.12 706.252 1465.56 Q706.252 1470.89 703.104 1474.11 Q699.956 1477.32 694.724 1477.32 Q688.729 1477.32 685.558 1472.74 Q682.387 1468.13 682.387 1459.41 Q682.387 1451.21 686.275 1446.35 Q690.164 1441.47 696.715 1441.47 Q698.474 1441.47 700.257 1441.81 Q702.062 1442.16 704.007 1442.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1061.81 1445.17 Q1058.2 1445.17 1056.37 1448.74 Q1054.57 1452.28 1054.57 1459.41 Q1054.57 1466.51 1056.37 1470.08 Q1058.2 1473.62 1061.81 1473.62 Q1065.44 1473.62 1067.25 1470.08 Q1069.08 1466.51 1069.08 1459.41 Q1069.08 1452.28 1067.25 1448.74 Q1065.44 1445.17 1061.81 1445.17 M1061.81 1441.47 Q1067.62 1441.47 1070.68 1446.07 Q1073.75 1450.66 1073.75 1459.41 Q1073.75 1468.13 1070.68 1472.74 Q1067.62 1477.32 1061.81 1477.32 Q1056 1477.32 1052.92 1472.74 Q1049.87 1468.13 1049.87 1459.41 Q1049.87 1450.66 1052.92 1446.07 Q1056 1441.47 1061.81 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1078.82 1470.77 L1083.71 1470.77 L1083.71 1476.65 L1078.82 1476.65 L1078.82 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1087.6 1442.09 L1109.82 1442.09 L1109.82 1444.08 L1097.27 1476.65 L1092.39 1476.65 L1104.19 1446.03 L1087.6 1446.03 L1087.6 1442.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1465.51 1445.17 Q1461.89 1445.17 1460.07 1448.74 Q1458.26 1452.28 1458.26 1459.41 Q1458.26 1466.51 1460.07 1470.08 Q1461.89 1473.62 1465.51 1473.62 Q1469.14 1473.62 1470.95 1470.08 Q1472.77 1466.51 1472.77 1459.41 Q1472.77 1452.28 1470.95 1448.74 Q1469.14 1445.17 1465.51 1445.17 M1465.51 1441.47 Q1471.32 1441.47 1474.37 1446.07 Q1477.45 1450.66 1477.45 1459.41 Q1477.45 1468.13 1474.37 1472.74 Q1471.32 1477.32 1465.51 1477.32 Q1459.7 1477.32 1456.62 1472.74 Q1453.56 1468.13 1453.56 1459.41 Q1453.56 1450.66 1456.62 1446.07 Q1459.7 1441.47 1465.51 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1482.52 1470.77 L1487.4 1470.77 L1487.4 1476.65 L1482.52 1476.65 L1482.52 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1502.47 1460.24 Q1499.14 1460.24 1497.22 1462.02 Q1495.32 1463.8 1495.32 1466.93 Q1495.32 1470.05 1497.22 1471.84 Q1499.14 1473.62 1502.47 1473.62 Q1505.81 1473.62 1507.73 1471.84 Q1509.65 1470.03 1509.65 1466.93 Q1509.65 1463.8 1507.73 1462.02 Q1505.83 1460.24 1502.47 1460.24 M1497.8 1458.25 Q1494.79 1457.51 1493.1 1455.45 Q1491.43 1453.39 1491.43 1450.43 Q1491.43 1446.28 1494.37 1443.87 Q1497.33 1441.47 1502.47 1441.47 Q1507.63 1441.47 1510.57 1443.87 Q1513.51 1446.28 1513.51 1450.43 Q1513.51 1453.39 1511.82 1455.45 Q1510.16 1457.51 1507.17 1458.25 Q1510.55 1459.04 1512.43 1461.33 Q1514.32 1463.62 1514.32 1466.93 Q1514.32 1471.95 1511.25 1474.64 Q1508.19 1477.32 1502.47 1477.32 Q1496.76 1477.32 1493.68 1474.64 Q1490.62 1471.95 1490.62 1466.93 Q1490.62 1463.62 1492.52 1461.33 Q1494.42 1459.04 1497.8 1458.25 M1496.08 1450.86 Q1496.08 1453.55 1497.75 1455.05 Q1499.44 1456.56 1502.47 1456.56 Q1505.48 1456.56 1507.17 1455.05 Q1508.88 1453.55 1508.88 1450.86 Q1508.88 1448.18 1507.17 1446.68 Q1505.48 1445.17 1502.47 1445.17 Q1499.44 1445.17 1497.75 1446.68 Q1496.08 1448.18 1496.08 1450.86 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1869.65 1445.17 Q1866.04 1445.17 1864.21 1448.74 Q1862.41 1452.28 1862.41 1459.41 Q1862.41 1466.51 1864.21 1470.08 Q1866.04 1473.62 1869.65 1473.62 Q1873.29 1473.62 1875.09 1470.08 Q1876.92 1466.51 1876.92 1459.41 Q1876.92 1452.28 1875.09 1448.74 Q1873.29 1445.17 1869.65 1445.17 M1869.65 1441.47 Q1875.46 1441.47 1878.52 1446.07 Q1881.6 1450.66 1881.6 1459.41 Q1881.6 1468.13 1878.52 1472.74 Q1875.46 1477.32 1869.65 1477.32 Q1863.84 1477.32 1860.76 1472.74 Q1857.71 1468.13 1857.71 1459.41 Q1857.71 1450.66 1860.76 1446.07 Q1863.84 1441.47 1869.65 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1886.67 1470.77 L1891.55 1470.77 L1891.55 1476.65 L1886.67 1476.65 L1886.67 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1896.76 1475.93 L1896.76 1471.68 Q1898.52 1472.51 1900.32 1472.95 Q1902.13 1473.39 1903.86 1473.39 Q1908.49 1473.39 1910.92 1470.29 Q1913.38 1467.16 1913.73 1460.82 Q1912.38 1462.81 1910.32 1463.87 Q1908.26 1464.94 1905.76 1464.94 Q1900.58 1464.94 1897.54 1461.81 Q1894.54 1458.67 1894.54 1453.23 Q1894.54 1447.9 1897.68 1444.68 Q1900.83 1441.47 1906.06 1441.47 Q1912.06 1441.47 1915.21 1446.07 Q1918.38 1450.66 1918.38 1459.41 Q1918.38 1467.58 1914.49 1472.46 Q1910.62 1477.32 1904.07 1477.32 Q1902.31 1477.32 1900.51 1476.98 Q1898.7 1476.63 1896.76 1475.93 M1906.06 1461.28 Q1909.21 1461.28 1911.04 1459.13 Q1912.89 1456.98 1912.89 1453.23 Q1912.89 1449.5 1911.04 1447.35 Q1909.21 1445.17 1906.06 1445.17 Q1902.92 1445.17 1901.06 1447.35 Q1899.23 1449.5 1899.23 1453.23 Q1899.23 1456.98 1901.06 1459.13 Q1902.92 1461.28 1906.06 1461.28 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2264.04 1472.72 L2271.68 1472.72 L2271.68 1446.35 L2263.37 1448.02 L2263.37 1443.76 L2271.63 1442.09 L2276.31 1442.09 L2276.31 1472.72 L2283.95 1472.72 L2283.95 1476.65 L2264.04 1476.65 L2264.04 1472.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2289.02 1470.77 L2293.9 1470.77 L2293.9 1476.65 L2289.02 1476.65 L2289.02 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2308.97 1445.17 Q2305.36 1445.17 2303.53 1448.74 Q2301.73 1452.28 2301.73 1459.41 Q2301.73 1466.51 2303.53 1470.08 Q2305.36 1473.62 2308.97 1473.62 Q2312.61 1473.62 2314.41 1470.08 Q2316.24 1466.51 2316.24 1459.41 Q2316.24 1452.28 2314.41 1448.74 Q2312.61 1445.17 2308.97 1445.17 M2308.97 1441.47 Q2314.78 1441.47 2317.84 1446.07 Q2320.92 1450.66 2320.92 1459.41 Q2320.92 1468.13 2317.84 1472.74 Q2314.78 1477.32 2308.97 1477.32 Q2303.16 1477.32 2300.08 1472.74 Q2297.03 1468.13 2297.03 1459.41 Q2297.03 1450.66 2300.08 1446.07 Q2303.16 1441.47 2308.97 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M139.57 1249.05 Q136.422 1249.05 134.57 1251.2 Q132.742 1253.35 132.742 1257.1 Q132.742 1260.83 134.57 1263 Q136.422 1265.16 139.57 1265.16 Q142.718 1265.16 144.547 1263 Q146.399 1260.83 146.399 1257.1 Q146.399 1253.35 144.547 1251.2 Q142.718 1249.05 139.57 1249.05 M148.853 1234.39 L148.853 1238.65 Q147.093 1237.82 145.288 1237.38 Q143.505 1236.94 141.746 1236.94 Q137.117 1236.94 134.663 1240.06 Q132.232 1243.19 131.885 1249.51 Q133.251 1247.49 135.311 1246.43 Q137.371 1245.34 139.848 1245.34 Q145.056 1245.34 148.066 1248.51 Q151.098 1251.66 151.098 1257.1 Q151.098 1262.42 147.95 1265.64 Q144.802 1268.86 139.57 1268.86 Q133.575 1268.86 130.404 1264.28 Q127.232 1259.67 127.232 1250.94 Q127.232 1242.75 131.121 1237.89 Q135.01 1233 141.561 1233 Q143.32 1233 145.103 1233.35 Q146.908 1233.7 148.853 1234.39 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M156.167 1262.31 L161.052 1262.31 L161.052 1268.19 L156.167 1268.19 L156.167 1262.31 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M166.167 1233.63 L184.524 1233.63 L184.524 1237.56 L170.45 1237.56 L170.45 1246.04 Q171.468 1245.69 172.487 1245.53 Q173.505 1245.34 174.524 1245.34 Q180.311 1245.34 183.69 1248.51 Q187.07 1251.68 187.07 1257.1 Q187.07 1262.68 183.598 1265.78 Q180.126 1268.86 173.806 1268.86 Q171.63 1268.86 169.362 1268.49 Q167.116 1268.12 164.709 1267.38 L164.709 1262.68 Q166.792 1263.81 169.015 1264.37 Q171.237 1264.92 173.714 1264.92 Q177.718 1264.92 180.056 1262.82 Q182.394 1260.71 182.394 1257.1 Q182.394 1253.49 180.056 1251.38 Q177.718 1249.28 173.714 1249.28 Q171.839 1249.28 169.964 1249.69 Q168.112 1250.11 166.167 1250.99 L166.167 1233.63 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M127.881 1005.25 L150.103 1005.25 L150.103 1007.24 L137.556 1039.81 L132.672 1039.81 L144.478 1009.18 L127.881 1009.18 L127.881 1005.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M155.172 1033.93 L160.056 1033.93 L160.056 1039.81 L155.172 1039.81 L155.172 1033.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M175.126 1008.33 Q171.515 1008.33 169.686 1011.89 Q167.88 1015.43 167.88 1022.56 Q167.88 1029.67 169.686 1033.23 Q171.515 1036.78 175.126 1036.78 Q178.76 1036.78 180.565 1033.23 Q182.394 1029.67 182.394 1022.56 Q182.394 1015.43 180.565 1011.89 Q178.76 1008.33 175.126 1008.33 M175.126 1004.62 Q180.936 1004.62 183.991 1009.23 Q187.07 1013.81 187.07 1022.56 Q187.07 1031.29 183.991 1035.9 Q180.936 1040.48 175.126 1040.48 Q169.315 1040.48 166.237 1035.9 Q163.181 1031.29 163.181 1022.56 Q163.181 1013.81 166.237 1009.23 Q169.315 1004.62 175.126 1004.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M128.876 776.869 L151.098 776.869 L151.098 778.86 L138.552 811.429 L133.668 811.429 L145.473 780.805 L128.876 780.805 L128.876 776.869 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M156.167 805.55 L161.052 805.55 L161.052 811.429 L156.167 811.429 L156.167 805.55 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M166.167 776.869 L184.524 776.869 L184.524 780.805 L170.45 780.805 L170.45 789.277 Q171.468 788.93 172.487 788.767 Q173.505 788.582 174.524 788.582 Q180.311 788.582 183.69 791.754 Q187.07 794.925 187.07 800.341 Q187.07 805.92 183.598 809.022 Q180.126 812.101 173.806 812.101 Q171.63 812.101 169.362 811.73 Q167.116 811.36 164.709 810.619 L164.709 805.92 Q166.792 807.054 169.015 807.61 Q171.237 808.166 173.714 808.166 Q177.718 808.166 180.056 806.059 Q182.394 803.953 182.394 800.341 Q182.394 796.73 180.056 794.624 Q177.718 792.517 173.714 792.517 Q171.839 792.517 169.964 792.934 Q168.112 793.351 166.167 794.23 L166.167 776.869 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M138.251 566.638 Q134.918 566.638 132.996 568.42 Q131.098 570.203 131.098 573.328 Q131.098 576.453 132.996 578.235 Q134.918 580.017 138.251 580.017 Q141.584 580.017 143.505 578.235 Q145.427 576.429 145.427 573.328 Q145.427 570.203 143.505 568.42 Q141.607 566.638 138.251 566.638 M133.575 564.647 Q130.566 563.906 128.876 561.846 Q127.209 559.786 127.209 556.823 Q127.209 552.68 130.149 550.272 Q133.112 547.865 138.251 547.865 Q143.413 547.865 146.353 550.272 Q149.292 552.68 149.292 556.823 Q149.292 559.786 147.603 561.846 Q145.936 563.906 142.95 564.647 Q146.329 565.434 148.204 567.726 Q150.103 570.017 150.103 573.328 Q150.103 578.351 147.024 581.036 Q143.968 583.721 138.251 583.721 Q132.533 583.721 129.455 581.036 Q126.399 578.351 126.399 573.328 Q126.399 570.017 128.297 567.726 Q130.195 565.434 133.575 564.647 M131.862 557.263 Q131.862 559.948 133.529 561.453 Q135.218 562.957 138.251 562.957 Q141.26 562.957 142.95 561.453 Q144.663 559.948 144.663 557.263 Q144.663 554.578 142.95 553.073 Q141.26 551.568 138.251 551.568 Q135.218 551.568 133.529 553.073 Q131.862 554.578 131.862 557.263 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M155.172 577.17 L160.056 577.17 L160.056 583.05 L155.172 583.05 L155.172 577.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M175.126 551.568 Q171.515 551.568 169.686 555.133 Q167.88 558.675 167.88 565.805 Q167.88 572.911 169.686 576.476 Q171.515 580.017 175.126 580.017 Q178.76 580.017 180.565 576.476 Q182.394 572.911 182.394 565.805 Q182.394 558.675 180.565 555.133 Q178.76 551.568 175.126 551.568 M175.126 547.865 Q180.936 547.865 183.991 552.471 Q187.07 557.055 187.07 565.805 Q187.07 574.531 183.991 579.138 Q180.936 583.721 175.126 583.721 Q169.315 583.721 166.237 579.138 Q163.181 574.531 163.181 565.805 Q163.181 557.055 166.237 552.471 Q169.315 547.865 175.126 547.865 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M139.246 338.258 Q135.913 338.258 133.992 340.041 Q132.093 341.823 132.093 344.948 Q132.093 348.073 133.992 349.855 Q135.913 351.638 139.246 351.638 Q142.58 351.638 144.501 349.855 Q146.422 348.05 146.422 344.948 Q146.422 341.823 144.501 340.041 Q142.603 338.258 139.246 338.258 M134.57 336.268 Q131.561 335.527 129.871 333.467 Q128.205 331.406 128.205 328.444 Q128.205 324.3 131.144 321.893 Q134.107 319.485 139.246 319.485 Q144.408 319.485 147.348 321.893 Q150.288 324.3 150.288 328.444 Q150.288 331.406 148.598 333.467 Q146.931 335.527 143.945 336.268 Q147.325 337.055 149.2 339.346 Q151.098 341.638 151.098 344.948 Q151.098 349.971 148.019 352.656 Q144.964 355.342 139.246 355.342 Q133.529 355.342 130.45 352.656 Q127.394 349.971 127.394 344.948 Q127.394 341.638 129.293 339.346 Q131.191 337.055 134.57 336.268 M132.857 328.883 Q132.857 331.569 134.524 333.073 Q136.214 334.578 139.246 334.578 Q142.255 334.578 143.945 333.073 Q145.658 331.569 145.658 328.883 Q145.658 326.198 143.945 324.694 Q142.255 323.189 139.246 323.189 Q136.214 323.189 134.524 324.694 Q132.857 326.198 132.857 328.883 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M156.167 348.791 L161.052 348.791 L161.052 354.67 L156.167 354.67 L156.167 348.791 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M166.167 320.11 L184.524 320.11 L184.524 324.045 L170.45 324.045 L170.45 332.518 Q171.468 332.17 172.487 332.008 Q173.505 331.823 174.524 331.823 Q180.311 331.823 183.69 334.994 Q187.07 338.166 187.07 343.582 Q187.07 349.161 183.598 352.263 Q180.126 355.342 173.806 355.342 Q171.63 355.342 169.362 354.971 Q167.116 354.601 164.709 353.86 L164.709 349.161 Q166.792 350.295 169.015 350.851 Q171.237 351.406 173.714 351.406 Q177.718 351.406 180.056 349.3 Q182.394 347.193 182.394 343.582 Q182.394 339.971 180.056 337.865 Q177.718 335.758 173.714 335.758 Q171.839 335.758 169.964 336.175 Q168.112 336.592 166.167 337.471 L166.167 320.11 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M128.482 125.573 L128.482 121.314 Q130.242 122.147 132.047 122.587 Q133.853 123.027 135.589 123.027 Q140.218 123.027 142.649 119.925 Q145.103 116.8 145.45 110.457 Q144.107 112.448 142.047 113.513 Q139.987 114.578 137.487 114.578 Q132.302 114.578 129.269 111.453 Q126.26 108.305 126.26 102.865 Q126.26 97.5408 129.408 94.3233 Q132.556 91.1057 137.788 91.1057 Q143.783 91.1057 146.931 95.7121 Q150.103 100.295 150.103 109.045 Q150.103 117.217 146.214 122.101 Q142.348 126.962 135.797 126.962 Q134.038 126.962 132.232 126.615 Q130.427 126.268 128.482 125.573 M137.788 110.92 Q140.936 110.92 142.765 108.768 Q144.617 106.615 144.617 102.865 Q144.617 99.138 142.765 96.9853 Q140.936 94.8094 137.788 94.8094 Q134.64 94.8094 132.788 96.9853 Q130.959 99.138 130.959 102.865 Q130.959 106.615 132.788 108.768 Q134.64 110.92 137.788 110.92 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M155.172 120.411 L160.056 120.411 L160.056 126.291 L155.172 126.291 L155.172 120.411 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M175.126 94.8094 Q171.515 94.8094 169.686 98.3742 Q167.88 101.916 167.88 109.045 Q167.88 116.152 169.686 119.717 Q171.515 123.258 175.126 123.258 Q178.76 123.258 180.565 119.717 Q182.394 116.152 182.394 109.045 Q182.394 101.916 180.565 98.3742 Q178.76 94.8094 175.126 94.8094 M175.126 91.1057 Q180.936 91.1057 183.991 95.7121 Q187.07 100.295 187.07 109.045 Q187.07 117.772 183.991 122.379 Q180.936 126.962 175.126 126.962 Q169.315 126.962 166.237 122.379 Q163.181 117.772 163.181 109.045 Q163.181 100.295 166.237 95.7121 Q169.315 91.1057 175.126 91.1057 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M961.081 1527.24 Q963.278 1523.29 966.333 1521.41 Q969.389 1519.54 973.526 1519.54 Q979.096 1519.54 982.12 1523.45 Q985.144 1527.33 985.144 1534.53 L985.144 1556.04 L979.255 1556.04 L979.255 1534.72 Q979.255 1529.59 977.441 1527.11 Q975.627 1524.63 971.903 1524.63 Q967.352 1524.63 964.71 1527.65 Q962.068 1530.68 962.068 1535.9 L962.068 1556.04 L956.18 1556.04 L956.18 1534.72 Q956.18 1529.56 954.366 1527.11 Q952.551 1524.63 948.764 1524.63 Q944.276 1524.63 941.634 1527.68 Q938.992 1530.71 938.992 1535.9 L938.992 1556.04 L933.104 1556.04 L933.104 1520.4 L938.992 1520.4 L938.992 1525.93 Q940.998 1522.66 943.798 1521.1 Q946.599 1519.54 950.451 1519.54 Q954.334 1519.54 957.039 1521.51 Q959.776 1523.48 961.081 1527.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1021.78 1536.76 L1021.78 1539.62 L994.851 1539.62 Q995.233 1545.67 998.48 1548.85 Q1001.76 1552 1007.58 1552 Q1010.96 1552 1014.11 1551.17 Q1017.29 1550.35 1020.41 1548.69 L1020.41 1554.23 Q1017.26 1555.57 1013.95 1556.27 Q1010.64 1556.97 1007.23 1556.97 Q998.703 1556.97 993.706 1552 Q988.74 1547.04 988.74 1538.57 Q988.74 1529.82 993.451 1524.69 Q998.193 1519.54 1006.21 1519.54 Q1013.41 1519.54 1017.58 1524.18 Q1021.78 1528.8 1021.78 1536.76 M1015.92 1535.04 Q1015.86 1530.23 1013.22 1527.37 Q1010.61 1524.5 1006.28 1524.5 Q1001.38 1524.5 998.416 1527.27 Q995.488 1530.04 995.042 1535.07 L1015.92 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1048.58 1525.87 Q1047.59 1525.3 1046.41 1525.04 Q1045.27 1524.76 1043.87 1524.76 Q1038.9 1524.76 1036.23 1528 Q1033.59 1531.22 1033.59 1537.27 L1033.59 1556.04 L1027.7 1556.04 L1027.7 1520.4 L1033.59 1520.4 L1033.59 1525.93 Q1035.43 1522.69 1038.39 1521.13 Q1041.35 1519.54 1045.59 1519.54 Q1046.19 1519.54 1046.92 1519.63 Q1047.65 1519.7 1048.55 1519.85 L1048.58 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1077.03 1537.81 Q1077.03 1531.44 1074.39 1527.94 Q1071.78 1524.44 1067.04 1524.44 Q1062.33 1524.44 1059.69 1527.94 Q1057.08 1531.44 1057.08 1537.81 Q1057.08 1544.14 1059.69 1547.64 Q1062.33 1551.14 1067.04 1551.14 Q1071.78 1551.14 1074.39 1547.64 Q1077.03 1544.14 1077.03 1537.81 M1082.89 1551.62 Q1082.89 1560.72 1078.85 1565.15 Q1074.8 1569.6 1066.47 1569.6 Q1063.38 1569.6 1060.64 1569.13 Q1057.9 1568.68 1055.33 1567.72 L1055.33 1562.03 Q1057.9 1563.43 1060.42 1564.1 Q1062.93 1564.76 1065.54 1564.76 Q1071.3 1564.76 1074.17 1561.74 Q1077.03 1558.75 1077.03 1552.67 L1077.03 1549.77 Q1075.22 1552.92 1072.39 1554.48 Q1069.55 1556.04 1065.61 1556.04 Q1059.05 1556.04 1055.04 1551.05 Q1051.03 1546.05 1051.03 1537.81 Q1051.03 1529.53 1055.04 1524.53 Q1059.05 1519.54 1065.61 1519.54 Q1069.55 1519.54 1072.39 1521.1 Q1075.22 1522.66 1077.03 1525.81 L1077.03 1520.4 L1082.89 1520.4 L1082.89 1551.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1119.52 1536.76 L1119.52 1539.62 L1092.6 1539.62 Q1092.98 1545.67 1096.23 1548.85 Q1099.5 1552 1105.33 1552 Q1108.7 1552 1111.85 1551.17 Q1115.04 1550.35 1118.16 1548.69 L1118.16 1554.23 Q1115 1555.57 1111.69 1556.27 Q1108.38 1556.97 1104.98 1556.97 Q1096.45 1556.97 1091.45 1552 Q1086.49 1547.04 1086.49 1538.57 Q1086.49 1529.82 1091.2 1524.69 Q1095.94 1519.54 1103.96 1519.54 Q1111.15 1519.54 1115.32 1524.18 Q1119.52 1528.8 1119.52 1536.76 M1113.67 1535.04 Q1113.6 1530.23 1110.96 1527.37 Q1108.35 1524.5 1104.02 1524.5 Q1099.12 1524.5 1096.16 1527.27 Q1093.23 1530.04 1092.79 1535.07 L1113.67 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1152.75 1566.87 L1152.75 1571.42 L1118.89 1571.42 L1118.89 1566.87 L1152.75 1566.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1164.56 1550.7 L1164.56 1569.6 L1158.67 1569.6 L1158.67 1520.4 L1164.56 1520.4 L1164.56 1525.81 Q1166.41 1522.62 1169.21 1521.1 Q1172.04 1519.54 1175.96 1519.54 Q1182.45 1519.54 1186.49 1524.69 Q1190.57 1529.85 1190.57 1538.25 Q1190.57 1546.65 1186.49 1551.81 Q1182.45 1556.97 1175.96 1556.97 Q1172.04 1556.97 1169.21 1555.44 Q1166.41 1553.88 1164.56 1550.7 M1184.49 1538.25 Q1184.49 1531.79 1181.81 1528.13 Q1179.17 1524.44 1174.52 1524.44 Q1169.88 1524.44 1167.2 1528.13 Q1164.56 1531.79 1164.56 1538.25 Q1164.56 1544.71 1167.2 1548.4 Q1169.88 1552.07 1174.52 1552.07 Q1179.17 1552.07 1181.81 1548.4 Q1184.49 1544.71 1184.49 1538.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1196.1 1541.98 L1196.1 1520.4 L1201.96 1520.4 L1201.96 1541.75 Q1201.96 1546.81 1203.93 1549.36 Q1205.91 1551.87 1209.85 1551.87 Q1214.6 1551.87 1217.33 1548.85 Q1220.1 1545.83 1220.1 1540.61 L1220.1 1520.4 L1225.96 1520.4 L1225.96 1556.04 L1220.1 1556.04 L1220.1 1550.57 Q1217.97 1553.82 1215.14 1555.41 Q1212.34 1556.97 1208.61 1556.97 Q1202.47 1556.97 1199.29 1553.15 Q1196.1 1549.33 1196.1 1541.98 M1210.84 1519.54 L1210.84 1519.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1252.76 1525.87 Q1251.77 1525.3 1250.59 1525.04 Q1249.45 1524.76 1248.05 1524.76 Q1243.08 1524.76 1240.41 1528 Q1237.77 1531.22 1237.77 1537.27 L1237.77 1556.04 L1231.88 1556.04 L1231.88 1520.4 L1237.77 1520.4 L1237.77 1525.93 Q1239.61 1522.69 1242.57 1521.13 Q1245.53 1519.54 1249.77 1519.54 Q1250.37 1519.54 1251.1 1519.63 Q1251.83 1519.7 1252.73 1519.85 L1252.76 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1258.9 1520.4 L1264.76 1520.4 L1264.76 1556.04 L1258.9 1556.04 L1258.9 1520.4 M1258.9 1506.52 L1264.76 1506.52 L1264.76 1513.93 L1258.9 1513.93 L1258.9 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1276.69 1510.27 L1276.69 1520.4 L1288.76 1520.4 L1288.76 1524.95 L1276.69 1524.95 L1276.69 1544.3 Q1276.69 1548.66 1277.87 1549.9 Q1279.08 1551.14 1282.74 1551.14 L1288.76 1551.14 L1288.76 1556.04 L1282.74 1556.04 Q1275.96 1556.04 1273.38 1553.53 Q1270.8 1550.98 1270.8 1544.3 L1270.8 1524.95 L1266.51 1524.95 L1266.51 1520.4 L1270.8 1520.4 L1270.8 1510.27 L1276.69 1510.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1309.73 1559.35 Q1307.25 1565.72 1304.89 1567.66 Q1302.54 1569.6 1298.59 1569.6 L1293.91 1569.6 L1293.91 1564.7 L1297.35 1564.7 Q1299.77 1564.7 1301.11 1563.56 Q1302.44 1562.41 1304.07 1558.14 L1305.12 1555.47 L1290.7 1520.4 L1296.9 1520.4 L1308.04 1548.28 L1319.18 1520.4 L1325.39 1520.4 L1309.73 1559.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1358.62 1566.87 L1358.62 1571.42 L1324.75 1571.42 L1324.75 1566.87 L1358.62 1566.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1370.56 1510.27 L1370.56 1520.4 L1382.62 1520.4 L1382.62 1524.95 L1370.56 1524.95 L1370.56 1544.3 Q1370.56 1548.66 1371.73 1549.9 Q1372.94 1551.14 1376.6 1551.14 L1382.62 1551.14 L1382.62 1556.04 L1376.6 1556.04 Q1369.82 1556.04 1367.25 1553.53 Q1364.67 1550.98 1364.67 1544.3 L1364.67 1524.95 L1360.37 1524.95 L1360.37 1520.4 L1364.67 1520.4 L1364.67 1510.27 L1370.56 1510.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1418.39 1534.53 L1418.39 1556.04 L1412.54 1556.04 L1412.54 1534.72 Q1412.54 1529.66 1410.56 1527.14 Q1408.59 1524.63 1404.64 1524.63 Q1399.9 1524.63 1397.16 1527.65 Q1394.43 1530.68 1394.43 1535.9 L1394.43 1556.04 L1388.54 1556.04 L1388.54 1506.52 L1394.43 1506.52 L1394.43 1525.93 Q1396.53 1522.72 1399.36 1521.13 Q1402.22 1519.54 1405.95 1519.54 Q1412.09 1519.54 1415.24 1523.36 Q1418.39 1527.14 1418.39 1534.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1445.19 1525.87 Q1444.21 1525.3 1443.03 1525.04 Q1441.88 1524.76 1440.48 1524.76 Q1435.52 1524.76 1432.84 1528 Q1430.2 1531.22 1430.2 1537.27 L1430.2 1556.04 L1424.31 1556.04 L1424.31 1520.4 L1430.2 1520.4 L1430.2 1525.93 Q1432.05 1522.69 1435.01 1521.13 Q1437.97 1519.54 1442.2 1519.54 Q1442.81 1519.54 1443.54 1519.63 Q1444.27 1519.7 1445.16 1519.85 L1445.19 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1480.4 1536.76 L1480.4 1539.62 L1453.47 1539.62 Q1453.85 1545.67 1457.1 1548.85 Q1460.38 1552 1466.2 1552 Q1469.57 1552 1472.73 1551.17 Q1475.91 1550.35 1479.03 1548.69 L1479.03 1554.23 Q1475.88 1555.57 1472.57 1556.27 Q1469.26 1556.97 1465.85 1556.97 Q1457.32 1556.97 1452.32 1552 Q1447.36 1547.04 1447.36 1538.57 Q1447.36 1529.82 1452.07 1524.69 Q1456.81 1519.54 1464.83 1519.54 Q1472.02 1519.54 1476.19 1524.18 Q1480.4 1528.8 1480.4 1536.76 M1474.54 1535.04 Q1474.48 1530.23 1471.83 1527.37 Q1469.22 1524.5 1464.9 1524.5 Q1459.99 1524.5 1457.03 1527.27 Q1454.11 1530.04 1453.66 1535.07 L1474.54 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1509.26 1521.45 L1509.26 1526.98 Q1506.78 1525.71 1504.11 1525.07 Q1501.43 1524.44 1498.57 1524.44 Q1494.21 1524.44 1492.01 1525.77 Q1489.85 1527.11 1489.85 1529.79 Q1489.85 1531.82 1491.41 1533 Q1492.97 1534.15 1497.68 1535.2 L1499.68 1535.64 Q1505.92 1536.98 1508.53 1539.43 Q1511.17 1541.85 1511.17 1546.21 Q1511.17 1551.17 1507.23 1554.07 Q1503.31 1556.97 1496.44 1556.97 Q1493.57 1556.97 1490.45 1556.39 Q1487.37 1555.85 1483.93 1554.74 L1483.93 1548.69 Q1487.18 1550.38 1490.33 1551.24 Q1493.48 1552.07 1496.56 1552.07 Q1500.7 1552.07 1502.93 1550.66 Q1505.16 1549.23 1505.16 1546.65 Q1505.16 1544.27 1503.54 1542.99 Q1501.94 1541.72 1496.5 1540.54 L1494.46 1540.07 Q1489.02 1538.92 1486.6 1536.56 Q1484.18 1534.18 1484.18 1530.04 Q1484.18 1525.01 1487.75 1522.27 Q1491.31 1519.54 1497.87 1519.54 Q1501.12 1519.54 1503.98 1520.01 Q1506.85 1520.49 1509.26 1521.45 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1546.95 1534.53 L1546.95 1556.04 L1541.09 1556.04 L1541.09 1534.72 Q1541.09 1529.66 1539.12 1527.14 Q1537.15 1524.63 1533.2 1524.63 Q1528.46 1524.63 1525.72 1527.65 Q1522.98 1530.68 1522.98 1535.9 L1522.98 1556.04 L1517.09 1556.04 L1517.09 1506.52 L1522.98 1506.52 L1522.98 1525.93 Q1525.08 1522.72 1527.92 1521.13 Q1530.78 1519.54 1534.5 1519.54 Q1540.65 1519.54 1543.8 1523.36 Q1546.95 1527.14 1546.95 1534.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1566.91 1524.5 Q1562.2 1524.5 1559.46 1528.19 Q1556.72 1531.85 1556.72 1538.25 Q1556.72 1544.65 1559.43 1548.34 Q1562.16 1552 1566.91 1552 Q1571.58 1552 1574.32 1548.31 Q1577.06 1544.62 1577.06 1538.25 Q1577.06 1531.92 1574.32 1528.23 Q1571.58 1524.5 1566.91 1524.5 M1566.91 1519.54 Q1574.54 1519.54 1578.91 1524.5 Q1583.27 1529.47 1583.27 1538.25 Q1583.27 1547 1578.91 1552 Q1574.54 1556.97 1566.91 1556.97 Q1559.23 1556.97 1554.87 1552 Q1550.55 1547 1550.55 1538.25 Q1550.55 1529.47 1554.87 1524.5 Q1559.23 1519.54 1566.91 1519.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1589.41 1506.52 L1595.26 1506.52 L1595.26 1556.04 L1589.41 1556.04 L1589.41 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M1624.87 1525.81 L1624.87 1506.52 L1630.72 1506.52 L1630.72 1556.04 L1624.87 1556.04 L1624.87 1550.7 Q1623.02 1553.88 1620.19 1555.44 Q1617.39 1556.97 1613.44 1556.97 Q1606.98 1556.97 1602.9 1551.81 Q1598.86 1546.65 1598.86 1538.25 Q1598.86 1529.85 1602.9 1524.69 Q1606.98 1519.54 1613.44 1519.54 Q1617.39 1519.54 1620.19 1521.1 Q1623.02 1522.62 1624.87 1525.81 M1604.91 1538.25 Q1604.91 1544.71 1607.55 1548.4 Q1610.22 1552.07 1614.87 1552.07 Q1619.52 1552.07 1622.19 1548.4 Q1624.87 1544.71 1624.87 1538.25 Q1624.87 1531.79 1622.19 1528.13 Q1619.52 1524.44 1614.87 1524.44 Q1610.22 1524.44 1607.55 1528.13 Q1604.91 1531.79 1604.91 1538.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M44.1444 904.492 L50.9239 904.492 Q47.9002 907.739 46.4043 911.431 Q44.9083 915.091 44.9083 919.229 Q44.9083 927.377 49.9054 931.705 Q54.8707 936.034 64.2919 936.034 Q73.6813 936.034 78.6784 931.705 Q83.6436 927.377 83.6436 919.229 Q83.6436 915.091 82.1477 911.431 Q80.6518 907.739 77.6281 904.492 L84.3439 904.492 Q86.6355 907.866 87.7814 911.653 Q88.9272 915.409 88.9272 919.611 Q88.9272 930.4 82.3387 936.607 Q75.7183 942.814 64.2919 942.814 Q52.8336 942.814 46.2451 936.607 Q39.6248 930.4 39.6248 919.611 Q39.6248 915.346 40.7706 911.59 Q41.8846 907.802 44.1444 904.492 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M57.8307 877.692 Q57.2578 878.679 57.0032 879.857 Q56.7167 881.003 56.7167 882.403 Q56.7167 887.368 59.9632 890.042 Q63.1779 892.684 69.2253 892.684 L88.0042 892.684 L88.0042 898.572 L52.3562 898.572 L52.3562 892.684 L57.8944 892.684 Q54.6479 890.838 53.0883 887.878 Q51.4968 884.917 51.4968 880.684 Q51.4968 880.08 51.5923 879.347 Q51.656 878.615 51.8151 877.724 L57.8307 877.692 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M56.4621 859.168 Q56.4621 863.879 60.1542 866.616 Q63.8145 869.353 70.212 869.353 Q76.6095 869.353 80.3017 866.648 Q83.9619 863.911 83.9619 859.168 Q83.9619 854.489 80.2698 851.752 Q76.5777 849.015 70.212 849.015 Q63.8781 849.015 60.186 851.752 Q56.4621 854.489 56.4621 859.168 M51.4968 859.168 Q51.4968 851.529 56.4621 847.169 Q61.4273 842.808 70.212 842.808 Q78.9649 842.808 83.9619 847.169 Q88.9272 851.529 88.9272 859.168 Q88.9272 866.839 83.9619 871.199 Q78.9649 875.528 70.212 875.528 Q61.4273 875.528 56.4621 871.199 Q51.4968 866.839 51.4968 859.168 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M53.4065 813.94 L58.9447 813.94 Q57.6716 816.422 57.035 819.096 Q56.3984 821.77 56.3984 824.634 Q56.3984 828.995 57.7352 831.191 Q59.072 833.355 61.7456 833.355 Q63.7826 833.355 64.9603 831.796 Q66.1061 830.236 67.1565 825.525 L67.6021 823.52 Q68.9389 817.282 71.3897 814.672 Q73.8086 812.03 78.1691 812.03 Q83.1344 812.03 86.0308 815.977 Q88.9272 819.892 88.9272 826.767 Q88.9272 829.631 88.3543 832.751 Q87.8132 835.838 86.6992 839.275 L80.6518 839.275 Q82.3387 836.029 83.198 832.878 Q84.0256 829.727 84.0256 826.639 Q84.0256 822.502 82.6251 820.274 Q81.1929 818.046 78.6147 818.046 Q76.2276 818.046 74.9545 819.669 Q73.6813 821.26 72.5037 826.703 L72.0262 828.74 Q70.8804 834.183 68.5251 836.602 Q66.138 839.021 62.0002 839.021 Q56.9713 839.021 54.2341 835.456 Q51.4968 831.891 51.4968 825.334 Q51.4968 822.088 51.9743 819.223 Q52.4517 816.359 53.4065 813.94 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M53.4065 783.162 L58.9447 783.162 Q57.6716 785.644 57.035 788.318 Q56.3984 790.991 56.3984 793.856 Q56.3984 798.217 57.7352 800.413 Q59.072 802.577 61.7456 802.577 Q63.7826 802.577 64.9603 801.017 Q66.1061 799.458 67.1565 794.747 L67.6021 792.742 Q68.9389 786.504 71.3897 783.894 Q73.8086 781.252 78.1691 781.252 Q83.1344 781.252 86.0308 785.199 Q88.9272 789.114 88.9272 795.989 Q88.9272 798.853 88.3543 801.972 Q87.8132 805.06 86.6992 808.497 L80.6518 808.497 Q82.3387 805.251 83.198 802.1 Q84.0256 798.949 84.0256 795.861 Q84.0256 791.724 82.6251 789.496 Q81.1929 787.268 78.6147 787.268 Q76.2276 787.268 74.9545 788.891 Q73.6813 790.482 72.5037 795.925 L72.0262 797.962 Q70.8804 803.405 68.5251 805.824 Q66.138 808.243 62.0002 808.243 Q56.9713 808.243 54.2341 804.678 Q51.4968 801.113 51.4968 794.556 Q51.4968 791.31 51.9743 788.445 Q52.4517 785.581 53.4065 783.162 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M40.4842 754.134 L40.4842 724.088 L45.895 724.088 L45.895 747.705 L59.9632 747.705 L59.9632 725.075 L65.3741 725.075 L65.3741 747.705 L82.5933 747.705 L82.5933 723.515 L88.0042 723.515 L88.0042 754.134 L40.4842 754.134 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M66.4881 687.74 L88.0042 687.74 L88.0042 693.596 L66.679 693.596 Q61.6183 693.596 59.1038 695.57 Q56.5894 697.543 56.5894 701.49 Q56.5894 706.232 59.6131 708.969 Q62.6368 711.707 67.8567 711.707 L88.0042 711.707 L88.0042 717.595 L52.3562 717.595 L52.3562 711.707 L57.8944 711.707 Q54.6797 709.606 53.0883 706.773 Q51.4968 703.909 51.4968 700.185 Q51.4968 694.042 55.3163 690.891 Q59.1038 687.74 66.4881 687.74 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M42.2347 675.804 L52.3562 675.804 L52.3562 663.741 L56.9077 663.741 L56.9077 675.804 L76.2594 675.804 Q80.6199 675.804 81.8613 674.626 Q83.1026 673.417 83.1026 669.757 L83.1026 663.741 L88.0042 663.741 L88.0042 669.757 Q88.0042 676.536 85.4897 679.114 Q82.9434 681.692 76.2594 681.692 L56.9077 681.692 L56.9077 685.989 L52.3562 685.989 L52.3562 681.692 L42.2347 681.692 L42.2347 675.804 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M57.8307 636.941 Q57.2578 637.928 57.0032 639.106 Q56.7167 640.252 56.7167 641.652 Q56.7167 646.617 59.9632 649.291 Q63.1779 651.933 69.2253 651.933 L88.0042 651.933 L88.0042 657.821 L52.3562 657.821 L52.3562 651.933 L57.8944 651.933 Q54.6479 650.087 53.0883 647.127 Q51.4968 644.166 51.4968 639.933 Q51.4968 639.329 51.5923 638.596 Q51.656 637.864 51.8151 636.973 L57.8307 636.941 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M56.4621 618.417 Q56.4621 623.128 60.1542 625.865 Q63.8145 628.602 70.212 628.602 Q76.6095 628.602 80.3017 625.897 Q83.9619 623.16 83.9619 618.417 Q83.9619 613.738 80.2698 611.001 Q76.5777 608.264 70.212 608.264 Q63.8781 608.264 60.186 611.001 Q56.4621 613.738 56.4621 618.417 M51.4968 618.417 Q51.4968 610.778 56.4621 606.418 Q61.4273 602.057 70.212 602.057 Q78.9649 602.057 83.9619 606.418 Q88.9272 610.778 88.9272 618.417 Q88.9272 626.088 83.9619 630.448 Q78.9649 634.777 70.212 634.777 Q61.4273 634.777 56.4621 630.448 Q51.4968 626.088 51.4968 618.417 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M82.657 590.249 L101.563 590.249 L101.563 596.137 L52.3562 596.137 L52.3562 590.249 L57.7671 590.249 Q54.5842 588.403 53.0564 585.602 Q51.4968 582.769 51.4968 578.854 Q51.4968 572.361 56.6531 568.319 Q61.8093 564.245 70.212 564.245 Q78.6147 564.245 83.771 568.319 Q88.9272 572.361 88.9272 578.854 Q88.9272 582.769 87.3994 585.602 Q85.8398 588.403 82.657 590.249 M70.212 570.324 Q63.7508 570.324 60.0905 572.998 Q56.3984 575.64 56.3984 580.287 Q56.3984 584.934 60.0905 587.607 Q63.7508 590.249 70.212 590.249 Q76.6732 590.249 80.3653 587.607 Q84.0256 584.934 84.0256 580.287 Q84.0256 575.64 80.3653 572.998 Q76.6732 570.324 70.212 570.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M91.3143 543.27 Q97.68 545.753 99.6216 548.108 Q101.563 550.463 101.563 554.41 L101.563 559.089 L96.6615 559.089 L96.6615 555.651 Q96.6615 553.232 95.5157 551.896 Q94.3699 550.559 90.1048 548.935 L87.4312 547.885 L52.3562 562.303 L52.3562 556.097 L80.238 544.957 L52.3562 533.817 L52.3562 527.61 L91.3143 543.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip912)\" style=\"stroke:#009af9; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  271.684,890.931 273.706,892.302 275.729,941.314 277.751,961.94 279.774,861.541 281.796,961.944 283.819,1143.82 285.841,1019.71 287.864,969.488 289.886,909.552 \n",
       "  291.909,860.206 293.931,706.358 295.954,859.581 297.976,999.246 299.999,940.876 302.021,1212.06 304.043,1293.12 306.066,807.326 308.088,1332.32 310.111,1120.81 \n",
       "  312.133,995.016 314.156,917.314 316.178,1025.26 318.201,934.044 320.223,1177.64 322.246,1077.59 324.268,838.298 326.291,1067.26 328.313,1099.17 330.336,1017.67 \n",
       "  332.358,964.597 334.381,973.361 336.403,1254.47 338.426,866.545 340.448,1296.85 342.471,1022.48 344.493,1124.84 346.516,1039.23 348.538,1329.39 350.561,1095.49 \n",
       "  352.583,811.103 354.605,833.393 356.628,1072.87 358.65,1018.72 360.673,860.672 362.695,877.651 364.718,887.265 366.74,786.793 368.763,964.453 370.785,866.051 \n",
       "  372.808,864.485 374.83,1101.56 376.853,1099.24 378.875,837.347 380.898,913.736 382.92,1051.23 384.943,1141.89 386.965,1093.5 388.988,868.325 391.01,1070.45 \n",
       "  393.033,811.059 395.055,887.039 397.078,1040.24 399.1,780.631 401.123,970.228 403.145,1044.83 405.168,969.052 407.19,1020.93 409.212,964.093 411.235,1074.24 \n",
       "  413.257,865.523 415.28,1154.74 417.302,948.66 419.325,945.235 421.347,837.769 423.37,1010.42 425.392,835.076 427.415,1023.17 429.437,1073.12 431.46,1075.36 \n",
       "  433.482,812.039 435.505,811.656 437.527,894.359 439.55,1273.64 441.572,1154.61 443.595,1289.18 445.617,1124.19 447.64,863.92 449.662,966.371 451.685,1116.06 \n",
       "  453.707,838.43 455.73,653.568 457.752,944.263 459.774,892.843 461.797,753.342 463.819,871.334 465.842,778.309 467.864,759.502 469.887,622.292 471.909,1092.89 \n",
       "  473.932,939.185 475.954,986.687 477.977,1102.39 479.999,908.153 482.022,886.32 484.044,1143.57 486.067,888.977 488.089,906.697 490.112,643.021 492.134,1036.83 \n",
       "  494.157,938.849 496.179,785.58 498.202,1063.14 500.224,1061.65 502.247,733.678 504.269,808.053 506.292,1075.99 508.314,810.786 510.337,1173.63 512.359,966.037 \n",
       "  514.381,940.707 516.404,990.74 518.426,1181.94 520.449,757.815 522.471,861.881 524.494,1010.71 526.516,1090.23 528.539,861.622 530.561,950.136 532.584,830.394 \n",
       "  534.606,827.065 536.629,1030.41 538.651,1066.03 540.674,945.974 542.696,837.772 544.719,913.815 546.741,809.324 548.764,1206 550.786,915.924 552.809,1227.58 \n",
       "  554.831,1150.26 556.854,1070.99 558.876,1048.28 560.899,1298.62 562.921,1130.6 564.943,1009.42 566.966,656.404 568.988,1098.77 571.011,1035.78 573.033,1138 \n",
       "  575.056,964.561 577.078,918.826 579.101,835.737 581.123,707.492 583.146,1039.32 585.168,987.891 587.191,1039.11 589.213,1043.82 591.236,914.093 593.258,998.645 \n",
       "  595.281,935.909 597.303,1146.94 599.326,962.864 601.348,880.91 603.371,997.703 605.393,1148.57 607.416,945.643 609.438,1023.68 611.461,1228.96 613.483,1173.92 \n",
       "  615.506,909.982 617.528,1121.12 619.55,1303.78 621.573,1098.41 623.595,1069.67 625.618,1048.71 627.64,1022.34 629.663,760.241 631.685,936.394 633.708,914.349 \n",
       "  635.73,1192.09 637.753,735.946 639.775,1042.17 641.798,900.292 643.82,996.191 645.843,963.072 647.865,1065.19 649.888,1384.24 651.91,964.76 653.933,1122.41 \n",
       "  655.955,826.557 657.978,547.455 660,943.467 662.023,864.014 664.045,1094.12 666.068,940.464 668.09,1010.33 670.112,868.879 672.135,923.728 674.157,1052.32 \n",
       "  676.18,969.798 678.202,1285.45 680.225,891.199 682.247,927.906 684.27,988.784 686.292,1116.51 688.315,1069.54 690.337,1252.03 692.36,939.31 694.382,968.447 \n",
       "  696.405,967.633 698.427,1093.99 700.45,962.466 702.472,1149.53 704.495,1018.83 706.517,790.151 708.54,892.11 710.562,1021.57 712.585,786.262 714.607,1015.93 \n",
       "  716.63,731.646 718.652,1194.54 720.675,837.265 722.697,1009.26 724.719,914.463 726.742,948.443 728.764,1017.71 730.787,685.33 732.809,1067.73 734.832,1018.62 \n",
       "  736.854,992.144 738.877,836.241 740.899,763.558 742.922,1127.55 744.944,899.837 746.967,922.506 748.989,626.243 751.012,996.986 753.034,957.967 755.057,1047.23 \n",
       "  757.079,916.607 759.102,855.402 761.124,841.769 763.147,914.989 765.169,1114.44 767.192,840.921 769.214,1077.24 771.237,1023.55 773.259,787.455 775.282,1074.57 \n",
       "  777.304,738.502 779.326,943.395 781.349,1201.38 783.371,761.113 785.394,995.297 787.416,859.441 789.439,861.928 791.461,863.282 793.484,1178.58 795.506,997.771 \n",
       "  797.529,858.689 799.551,674.856 801.574,1122.06 803.596,998.342 805.619,1158.34 807.641,961.003 809.664,1096.43 811.686,969.45 813.709,892.803 815.731,967.054 \n",
       "  817.754,965.357 819.776,1175.2 821.799,1046.22 823.821,776.572 825.844,837.61 827.866,969.892 829.888,942.532 831.911,788.801 833.933,1173.96 835.956,1150.42 \n",
       "  837.978,856.875 840.001,1019.51 842.023,973.232 844.046,1064.18 846.068,888.386 848.091,889.37 850.113,967.611 852.136,961.656 854.158,834.005 856.181,1124.58 \n",
       "  858.203,686.018 860.226,887.81 862.248,858.887 864.271,1178.29 866.293,1018.23 868.316,1040.76 870.338,1105.48 872.361,822.763 874.383,727.855 876.406,993.223 \n",
       "  878.428,1073.87 880.451,1103.14 882.473,776.355 884.495,1125.76 886.518,992.195 888.54,1209.39 890.563,938.697 892.585,630.735 894.608,855.471 896.63,911.654 \n",
       "  898.653,965.043 900.675,965.22 902.698,966.576 904.72,1074.05 906.743,995.084 908.765,1169.47 910.788,757.216 912.81,921.662 914.833,1131.39 916.855,1065.92 \n",
       "  918.878,1032.99 920.9,704.627 922.923,673.983 924.945,753.766 926.968,1152.83 928.99,1050.17 931.013,1072.06 933.035,735.051 935.057,911.139 937.08,733.283 \n",
       "  939.102,862.746 941.125,1073.22 943.147,1173.23 945.17,398.079 947.192,687.06 949.215,716 951.237,740.528 953.26,767.212 955.282,687.685 957.305,317.733 \n",
       "  959.327,661.258 961.35,817.146 963.372,764.596 965.395,741.124 967.417,634.632 969.44,685.493 971.462,998.691 973.485,737.31 975.507,610.573 977.53,874.246 \n",
       "  979.552,631.663 981.575,872.34 983.597,584.678 985.62,658.708 987.642,845.409 989.664,662.268 991.687,711.214 993.709,504.621 995.732,919.872 997.754,821.415 \n",
       "  999.777,713.95 1001.8,872.122 1003.82,816.543 1005.84,634.264 1007.87,790.318 1009.89,794.933 1011.91,632.377 1013.93,842.148 1015.96,660.763 1017.98,551.444 \n",
       "  1020,736.213 1022.02,610.419 1024.05,816.432 1026.07,736.555 1028.09,610.184 1030.11,318.377 1032.14,766.683 1034.16,555.736 1036.18,664.228 1038.2,715.267 \n",
       "  1040.23,819.364 1042.25,842.644 1044.27,580.649 1046.29,844.267 1048.32,684.47 1050.34,739.913 1052.36,661.746 1054.38,687.735 1056.41,657.785 1058.43,637.331 \n",
       "  1060.45,708.954 1062.47,581.636 1064.5,709.051 1066.52,716.408 1068.54,605.062 1070.56,765.661 1072.59,529.582 1074.61,656.633 1076.63,786.63 1078.65,765.561 \n",
       "  1080.68,604.717 1082.7,794.081 1084.72,764.855 1086.74,763.641 1088.77,501.667 1090.79,976.348 1092.81,734.418 1094.83,607.49 1096.86,685.77 1098.88,845.505 \n",
       "  1100.9,864.736 1102.92,635.967 1104.95,635.479 1106.97,686.176 1108.99,924.264 1111.01,967.697 1113.04,634.805 1115.06,999.187 1117.08,1020.26 1119.1,739.5 \n",
       "  1121.13,685.635 1123.15,711.29 1125.17,661.013 1127.19,653.996 1129.22,635.687 1131.24,844.279 1133.26,839.823 1135.28,659.325 1137.31,869.277 1139.33,632.024 \n",
       "  1141.35,553.867 1143.37,449.387 1145.4,816.921 1147.42,552.573 1149.44,898.455 1151.46,662.653 1153.49,319.925 1155.51,767.54 1157.53,842.785 1159.55,608.62 \n",
       "  1161.58,688.112 1163.6,872.134 1165.62,555.668 1167.64,580.875 1169.67,633.962 1171.69,925.815 1173.71,792.354 1175.73,631.477 1177.76,580.615 1179.78,764.19 \n",
       "  1181.8,687.005 1183.82,475.165 1185.85,788.923 1187.87,713.556 1189.89,848.361 1191.91,371.851 1193.94,579.551 1195.96,555.314 1197.98,557.583 1200,710.131 \n",
       "  1202.02,792.266 1204.05,682.514 1206.07,868.941 1208.09,845.741 1210.11,743.56 1212.14,732.556 1214.16,557.486 1216.18,950.577 1218.2,791.597 1220.23,840.089 \n",
       "  1222.25,712.547 1224.27,580.615 1226.29,767.263 1228.32,498.806 1230.34,685.885 1232.36,738.886 1234.38,682.77 1236.41,529.187 1238.43,656.59 1240.45,739.367 \n",
       "  1242.47,368.615 1244.5,662.187 1246.52,815.122 1248.54,555.927 1250.56,713.538 1252.59,662.644 1254.61,738.189 1256.63,656.507 1258.65,580.413 1260.68,579.529 \n",
       "  1262.7,820.032 1264.72,611.251 1266.74,475.361 1268.77,738.785 1270.79,760.486 1272.81,763.326 1274.83,475.622 1276.86,946.544 1278.88,894.638 1280.9,663.338 \n",
       "  1282.92,689.515 1284.95,453.384 1286.97,507.168 1288.99,317.943 1291.01,452.14 1293.04,265.956 1295.06,743.349 1297.08,871.853 1299.1,613.266 1301.13,582.961 \n",
       "  1303.15,400.016 1305.17,319.235 1307.19,819.37 1309.22,558.013 1311.24,558.751 1313.26,687.406 1315.28,663.006 1317.31,531.438 1319.33,449.809 1321.35,583.509 \n",
       "  1323.37,505.659 1325.4,764.92 1327.42,527.205 1329.44,424.598 1331.46,847.082 1333.49,611.342 1335.51,661.525 1337.53,479.927 1339.55,743.6 1341.58,611.342 \n",
       "  1343.6,344.836 1345.62,637.815 1347.64,661.006 1349.67,531.002 1351.69,479.676 1353.71,712.671 1355.73,715.224 1357.76,399.183 1359.78,713.877 1361.8,559.165 \n",
       "  1363.82,532.734 1365.85,506.088 1367.87,502.871 1369.89,397.45 1371.91,609.815 1373.94,586.939 1375.96,505.33 1377.98,426.949 1380,479.184 1382.03,478.733 \n",
       "  1384.05,796.39 1386.07,399.712 1388.09,714.426 1390.12,608.947 1392.14,610.893 1394.16,453.38 1396.18,716.501 1398.21,741.513 1400.23,555.724 1402.25,477.77 \n",
       "  1404.27,477.997 1406.3,690.325 1408.32,764.619 1410.34,506.059 1412.36,507.239 1414.39,687.677 1416.41,400.023 1418.43,560.037 1420.45,715.294 1422.48,427.149 \n",
       "  1424.5,741.908 1426.52,845.175 1428.54,664.36 1430.57,742.182 1432.59,637.474 1434.61,769.391 1436.63,500.303 1438.66,505.95 1440.68,423.455 1442.7,790.027 \n",
       "  1444.72,609.779 1446.75,632.039 1448.77,636.264 1450.79,427.914 1452.81,687.041 1454.84,478.922 1456.86,531.327 1458.88,585.134 1460.9,868.223 1462.92,555.742 \n",
       "  1464.95,637.149 1466.97,663.686 1468.99,688.067 1471.01,584.146 1473.04,635.57 1475.06,584.958 1477.08,715.368 1479.1,688.319 1481.13,713.603 1483.15,637.223 \n",
       "  1485.17,559.58 1487.19,558.324 1489.22,427.412 1491.24,636.72 1493.26,585.21 1495.28,610.92 1497.31,768.637 1499.33,637.325 1501.35,793.829 1503.37,688.664 \n",
       "  1505.4,662.55 1507.42,769.391 1509.44,427.412 1511.46,530.377 1513.49,505.305 1515.51,426.006 1517.53,689.513 1519.55,479.425 1521.58,400.023 1523.6,480.681 \n",
       "  1525.62,688.969 1527.64,717.629 1529.67,558.072 1531.69,454.298 1533.71,372.755 1535.73,610.043 1537.76,586.585 1539.78,268.224 1541.8,478.019 1543.82,690.743 \n",
       "  1545.85,661.432 1547.87,636.72 1549.89,376.906 1551.91,610.362 1553.94,663.551 1555.96,504.889 1557.98,321.878 1560,664.535 1562.03,504.167 1564.05,507.064 \n",
       "  1566.07,531.502 1568.09,479.425 1570.12,478.671 1572.14,530.433 1574.16,478.168 1576.18,873.669 1578.21,374.379 1580.23,556.243 1582.25,478.922 1584.27,690.241 \n",
       "  1586.3,478.809 1588.32,373.891 1590.34,664.75 1592.36,504.803 1594.39,584.052 1596.41,584.924 1598.43,613.101 1600.45,504.645 1602.48,559.329 1604.5,426.406 \n",
       "  1606.52,400.777 1608.54,531.186 1610.57,295.721 1612.59,400.628 1614.61,532.191 1616.63,400.274 1618.66,638.479 1620.68,400.526 1622.7,636.52 1624.72,531.438 \n",
       "  1626.75,531.438 1628.77,504.3 1630.79,400.777 1632.81,346 1634.84,560.585 1636.86,453.292 1638.88,586.969 1640.9,796.606 1642.93,612.849 1644.95,453.041 \n",
       "  1646.97,477.917 1648.99,505.054 1651.02,346.754 1653.04,583.903 1655.06,531.689 1657.08,268.106 1659.11,506.233 1661.13,505.557 1663.15,375.398 1665.17,450.78 \n",
       "  1667.2,479.173 1669.22,426.155 1671.24,716.373 1673.26,637.223 1675.29,611.87 1677.31,637.725 1679.33,453.18 1681.35,479.449 1683.38,505.808 1685.4,453.683 \n",
       "  1687.42,556.565 1689.44,584.958 1691.47,661.751 1693.49,716.373 1695.51,558.575 1697.53,268.86 1699.56,581.444 1701.58,716.624 1703.6,637.223 1705.62,453.041 \n",
       "  1707.65,346.251 1709.67,584.205 1711.69,585.963 1713.71,374.393 1715.74,374.393 1717.76,532.694 1719.78,506.059 1721.8,637.474 1723.83,503.044 1725.85,582.697 \n",
       "  1727.87,716.876 1729.89,692.502 1731.91,480.353 1733.94,665.365 1735.96,557.319 1737.98,452.287 1740,611.593 1742.03,585.963 1744.05,821.153 1746.07,584.456 \n",
       "  1748.09,532.618 1750.12,346.754 1752.14,478.671 1754.16,688.812 1756.18,609.08 1758.21,530.433 1760.23,557.067 1762.25,426.909 1764.27,716.956 1766.3,638.479 \n",
       "  1768.32,506.361 1770.34,638.437 1772.36,533.448 1774.39,532.945 1776.41,400.777 1778.43,372.886 1780.45,479.927 1782.48,532.191 1784.5,506.956 1786.52,478.168 \n",
       "  1788.54,426.658 1790.57,374.645 1792.59,506.059 1794.61,400.526 1796.63,530.684 1798.66,559.077 1800.68,767.175 1802.7,581.943 1804.72,610.839 1806.75,375.147 \n",
       "  1808.77,86.1857 1810.79,612.347 1812.81,190.212 1814.84,556.816 1816.86,637.725 1818.88,454.298 1820.9,372.634 1822.93,374.142 1824.95,427.412 1826.97,637.223 \n",
       "  1828.99,610.839 1831.02,399.52 1833.04,372.383 1835.06,741.599 1837.08,609.834 1839.11,531.016 1841.13,611.342 1843.15,190.463 1845.17,531.186 1847.2,558.324 \n",
       "  1849.22,293.987 1851.24,505.305 1853.26,532.443 1855.29,715.619 1857.31,506.562 1859.33,691.748 1861.35,689.99 1863.38,479.676 1865.4,662.852 1867.42,241.22 \n",
       "  1869.44,293.735 1871.47,480.43 1873.49,611.091 1875.51,610.588 1877.53,479.927 1879.56,400.951 1881.58,637.977 1883.6,664.109 1885.62,689.99 1887.65,663.606 \n",
       "  1889.67,426.909 1891.69,610.588 1893.71,770.145 1895.74,717.127 1897.76,636.72 1899.78,611.593 1901.8,610.588 1903.83,690.743 1905.85,612.849 1907.87,610.839 \n",
       "  1909.89,375.65 1911.92,717.881 1913.94,454.046 1915.96,452.79 1917.98,743.259 1920.01,637.977 1922.03,585.21 1924.05,742.505 1926.07,373.64 1928.1,531.94 \n",
       "  1930.12,241.22 1932.14,716.876 1934.16,558.826 1936.19,533.197 1938.21,610.588 1940.23,742.756 1942.25,505.054 1944.28,374.393 1946.3,452.036 1948.32,611.844 \n",
       "  1950.34,716.624 1952.37,662.601 1954.39,636.971 1956.41,610.085 1958.43,504.552 1960.46,506.059 1962.48,585.712 1964.5,768.637 1966.52,478.671 1968.55,638.479 \n",
       "  1970.57,610.337 1972.59,584.958 1974.61,480.932 1976.64,585.461 1978.66,585.461 1980.68,398.767 1982.7,504.803 1984.73,373.388 1986.75,530.935 1988.77,742.003 \n",
       "  1990.79,505.808 1992.81,504.803 1994.84,241.974 1996.86,503.547 1998.88,795.775 2000.9,452.79 2002.93,241.974 2004.95,612.347 2006.97,321.375 2008.99,399.772 \n",
       "  2011.02,453.544 2013.04,347.256 2015.06,506.311 2017.08,558.826 2019.11,663.857 2021.13,216.093 2023.15,531.438 2025.17,585.461 2027.2,480.178 2029.22,374.393 \n",
       "  2031.24,718.132 2033.26,426.909 2035.29,558.575 2037.31,637.223 2039.33,637.223 2041.35,215.59 2043.38,585.21 2045.4,506.813 2047.42,557.319 2049.44,267.855 \n",
       "  2051.47,319.868 2053.49,636.72 2055.51,505.557 2057.53,425.15 2059.56,479.927 2061.58,505.808 2063.6,609.583 2065.62,612.096 2067.65,428.165 2069.67,609.08 \n",
       "  2071.69,426.406 2073.71,427.663 2075.74,529.93 2077.76,611.091 2079.78,477.917 2081.8,559.831 2083.83,743.259 2085.85,347.759 2087.87,636.971 2089.89,584.707 \n",
       "  2091.92,402.033 2093.94,164.08 2095.96,478.42 2097.98,584.707 2100.01,688.985 2102.03,636.72 2104.05,584.707 2106.07,504.803 2108.1,742.505 2110.12,505.557 \n",
       "  2112.14,715.368 2114.16,505.054 2116.19,664.109 2118.21,267.352 2120.23,374.896 2122.25,768.386 2124.28,266.096 2126.3,295.746 2128.32,584.958 2130.34,665.114 \n",
       "  2132.37,453.292 2134.39,610.085 2136.41,453.544 2138.43,533.197 2140.46,320.119 2142.48,454.046 2144.5,479.425 2146.52,374.645 2148.55,741.5 2150.57,426.406 \n",
       "  2152.59,479.173 2154.61,533.95 2156.64,505.557 2158.66,637.725 2160.68,373.891 2162.7,505.305 2164.73,427.16 2166.75,584.707 2168.77,533.448 2170.79,585.461 \n",
       "  2172.82,558.826 2174.84,478.671 2176.86,691.246 2178.88,531.438 2180.91,296.751 2182.93,506.311 2184.95,638.73 2186.97,454.298 2189,769.14 2191.02,532.945 \n",
       "  2193.04,584.456 2195.06,610.839 2197.09,531.438 2199.11,663.606 2201.13,505.557 2203.15,372.634 2205.18,665.114 2207.2,375.147 2209.22,586.969 2211.24,453.795 \n",
       "  2213.27,480.932 2215.29,399.018 2217.31,427.914 2219.33,557.821 2221.36,426.909 2223.38,505.808 2225.4,557.57 2227.42,663.606 2229.45,557.57 2231.47,637.725 \n",
       "  2233.49,663.355 2235.51,478.42 2237.54,320.873 2239.56,638.982 2241.58,530.684 2243.6,505.305 2245.63,478.42 2247.65,136.44 2249.67,425.653 2251.69,532.443 \n",
       "  2253.72,531.94 2255.74,796.026 2257.76,400.777 2259.78,401.028 2261.8,688.985 2263.83,611.844 2265.85,531.689 2267.87,453.544 2269.89,610.588 2271.92,506.311 \n",
       "  2273.94,717.127 2275.96,506.562 2277.98,558.324 2280.01,612.347 2282.03,664.36 2284.05,531.186 2286.07,346.754 2288.1,241.22 2290.12,348.01 2292.14,477.917 \n",
       "  \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip910)\" d=\"\n",
       "M1844.07 1377.32 L2281.37 1377.32 L2281.37 1256.36 L1844.07 1256.36  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1844.07,1377.32 2281.37,1377.32 2281.37,1256.36 1844.07,1256.36 1844.07,1377.32 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip910)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1867.87,1316.84 2010.65,1316.84 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip910)\" d=\"M 0 0 M2047.64 1334.12 L2034.45 1299.56 L2039.33 1299.56 L2050.28 1328.65 L2061.25 1299.56 L2066.11 1299.56 L2052.94 1334.12 L2047.64 1334.12 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2078.68 1321.08 Q2073.52 1321.08 2071.53 1322.26 Q2069.54 1323.44 2069.54 1326.29 Q2069.54 1328.56 2071.02 1329.9 Q2072.52 1331.22 2075.09 1331.22 Q2078.64 1331.22 2080.76 1328.72 Q2082.92 1326.2 2082.92 1322.03 L2082.92 1321.08 L2078.68 1321.08 M2087.18 1319.32 L2087.18 1334.12 L2082.92 1334.12 L2082.92 1330.18 Q2081.46 1332.54 2079.28 1333.68 Q2077.11 1334.79 2073.96 1334.79 Q2069.98 1334.79 2067.62 1332.56 Q2065.28 1330.32 2065.28 1326.57 Q2065.28 1322.19 2068.2 1319.97 Q2071.14 1317.75 2076.95 1317.75 L2082.92 1317.75 L2082.92 1317.33 Q2082.92 1314.39 2080.97 1312.8 Q2079.05 1311.18 2075.56 1311.18 Q2073.33 1311.18 2071.23 1311.71 Q2069.12 1312.24 2067.18 1313.31 L2067.18 1309.37 Q2069.51 1308.47 2071.71 1308.03 Q2073.91 1307.56 2076 1307.56 Q2081.62 1307.56 2084.4 1310.48 Q2087.18 1313.4 2087.18 1319.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2091.64 1298.1 L2095.9 1298.1 L2095.9 1334.12 L2091.64 1334.12 L2091.64 1298.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2100.37 1308.19 L2104.63 1308.19 L2104.63 1334.12 L2100.37 1334.12 L2100.37 1308.19 M2100.37 1298.1 L2104.63 1298.1 L2104.63 1303.49 L2100.37 1303.49 L2100.37 1298.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2126.16 1312.12 L2126.16 1298.1 L2130.42 1298.1 L2130.42 1334.12 L2126.16 1334.12 L2126.16 1330.23 Q2124.82 1332.54 2122.76 1333.68 Q2120.72 1334.79 2117.85 1334.79 Q2113.15 1334.79 2110.19 1331.04 Q2107.25 1327.29 2107.25 1321.18 Q2107.25 1315.06 2110.19 1311.31 Q2113.15 1307.56 2117.85 1307.56 Q2120.72 1307.56 2122.76 1308.7 Q2124.82 1309.81 2126.16 1312.12 M2111.64 1321.18 Q2111.64 1325.87 2113.57 1328.56 Q2115.51 1331.22 2118.89 1331.22 Q2122.27 1331.22 2124.21 1328.56 Q2126.16 1325.87 2126.16 1321.18 Q2126.16 1316.48 2124.21 1313.81 Q2122.27 1311.13 2118.89 1311.13 Q2115.51 1311.13 2113.57 1313.81 Q2111.64 1316.48 2111.64 1321.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2146.67 1321.08 Q2141.51 1321.08 2139.51 1322.26 Q2137.52 1323.44 2137.52 1326.29 Q2137.52 1328.56 2139.01 1329.9 Q2140.51 1331.22 2143.08 1331.22 Q2146.62 1331.22 2148.75 1328.72 Q2150.9 1326.2 2150.9 1322.03 L2150.9 1321.08 L2146.67 1321.08 M2155.16 1319.32 L2155.16 1334.12 L2150.9 1334.12 L2150.9 1330.18 Q2149.45 1332.54 2147.27 1333.68 Q2145.09 1334.79 2141.95 1334.79 Q2137.96 1334.79 2135.6 1332.56 Q2133.26 1330.32 2133.26 1326.57 Q2133.26 1322.19 2136.18 1319.97 Q2139.12 1317.75 2144.93 1317.75 L2150.9 1317.75 L2150.9 1317.33 Q2150.9 1314.39 2148.96 1312.8 Q2147.04 1311.18 2143.54 1311.18 Q2141.32 1311.18 2139.21 1311.71 Q2137.11 1312.24 2135.16 1313.31 L2135.16 1309.37 Q2137.5 1308.47 2139.7 1308.03 Q2141.9 1307.56 2143.98 1307.56 Q2149.61 1307.56 2152.38 1310.48 Q2155.16 1313.4 2155.16 1319.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2163.84 1300.83 L2163.84 1308.19 L2172.62 1308.19 L2172.62 1311.5 L2163.84 1311.5 L2163.84 1325.57 Q2163.84 1328.74 2164.7 1329.65 Q2165.58 1330.55 2168.24 1330.55 L2172.62 1330.55 L2172.62 1334.12 L2168.24 1334.12 Q2163.31 1334.12 2161.44 1332.29 Q2159.56 1330.43 2159.56 1325.57 L2159.56 1311.5 L2156.44 1311.5 L2156.44 1308.19 L2159.56 1308.19 L2159.56 1300.83 L2163.84 1300.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2177.08 1308.19 L2181.34 1308.19 L2181.34 1334.12 L2177.08 1334.12 L2177.08 1308.19 M2177.08 1298.1 L2181.34 1298.1 L2181.34 1303.49 L2177.08 1303.49 L2177.08 1298.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2195.86 1311.18 Q2192.43 1311.18 2190.44 1313.86 Q2188.45 1316.52 2188.45 1321.18 Q2188.45 1325.83 2190.42 1328.51 Q2192.41 1331.18 2195.86 1331.18 Q2199.26 1331.18 2201.25 1328.49 Q2203.24 1325.81 2203.24 1321.18 Q2203.24 1316.57 2201.25 1313.88 Q2199.26 1311.18 2195.86 1311.18 M2195.86 1307.56 Q2201.41 1307.56 2204.58 1311.18 Q2207.75 1314.79 2207.75 1321.18 Q2207.75 1327.54 2204.58 1331.18 Q2201.41 1334.79 2195.86 1334.79 Q2190.28 1334.79 2187.11 1331.18 Q2183.96 1327.54 2183.96 1321.18 Q2183.96 1314.79 2187.11 1311.18 Q2190.28 1307.56 2195.86 1307.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip910)\" d=\"M 0 0 M2233.77 1318.47 L2233.77 1334.12 L2229.51 1334.12 L2229.51 1318.61 Q2229.51 1314.93 2228.08 1313.1 Q2226.64 1311.27 2223.77 1311.27 Q2220.32 1311.27 2218.33 1313.47 Q2216.34 1315.67 2216.34 1319.46 L2216.34 1334.12 L2212.06 1334.12 L2212.06 1308.19 L2216.34 1308.19 L2216.34 1312.22 Q2217.87 1309.88 2219.93 1308.72 Q2222.01 1307.56 2224.72 1307.56 Q2229.19 1307.56 2231.48 1310.34 Q2233.77 1313.1 2233.77 1318.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(curve.parameter_values,\n",
    "     curve.measurements,\n",
    "     xlab=curve.parameter_name,\n",
    "     ylab=\"Cross Entropy\",\n",
    "     label=\"Validation\", lw=2, legend=:bottomright)\n",
    "# plot!(Net2.report.training_losses, label=\"Training\", lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.2081"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = round(minimum(curve.measurements), digits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5936017217217218"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = curve.parameter_values[argmin(curve.measurements)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJBase.NumericRange(Float64, :merge_purity_threshold, ... )"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param1 = :merge_purity_threshold\n",
    "\n",
    "r1 = range(dt, param1, lower=0.50001, upper=1, scale=:linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "    model = DecisionTreeClassifier(\n",
       "            max_depth = -1,\n",
       "            min_samples_leaf = 1,\n",
       "            min_samples_split = 2,\n",
       "            min_purity_increase = 0.0,\n",
       "            n_subfeatures = 0,\n",
       "            post_prune = true,\n",
       "            merge_purity_threshold = 1.0,\n",
       "            pdf_smoothing = 0.0,\n",
       "            display_depth = 14),\n",
       "    tuning = Grid(\n",
       "            goal = 1000,\n",
       "            resolution = 10,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = CV(\n",
       "            nfolds = 6,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = cross_entropy(\n",
       "            eps = 2.220446049250313e-16),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = MLJBase.NumericRange{Float64,MLJBase.Bounded,Symbol}[\u001b[34mNumericRange{Float64,…} @828\u001b[39m],\n",
       "    train_best = true,\n",
       "    repeats = 1,\n",
       "    n = nothing,\n",
       "    acceleration = CPUThreads{Int64}(1),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true)\u001b[34m @880\u001b[39m"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_dt_model = TunedModel(model=dt,\n",
    "                                    tuning=Grid(goal=1000),\n",
    "                                    resampling=CV(nfolds=6, shuffle=true), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @139\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @977\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @999\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_dt = machine(self_tuning_dt_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @139\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      "┌ Info: Attempting to evaluate 1000 models.\n",
      "└ @ MLJTuning /home/andrew/.julia/packages/MLJTuning/Bbgvk/src/tuned_models.jl:494\n",
      "\u001b[33mEvaluating over 1000 metamodels: 100%[=========================] Time: 0:00:05\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @139\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @977\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @999\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = fit!(self_tuning_dt, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(best_model = \u001b[34mDecisionTreeClassifier @124\u001b[39m,\n",
       " best_fitted_params = (tree = Decision Tree\n",
       "Leaves: 93\n",
       "Depth:  12,\n",
       "                       encoding = Dict{CategoricalValue{String,UInt32},UInt32}(\"B\" => 0x00000001,\"L\" => 0x00000002,\"R\" => 0x00000003),),)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = fitted_params(self_tuning_dt)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = true,\n",
       "    merge_purity_threshold = 0.526535995995996,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 14)\u001b[34m @124\u001b[39m"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.67149"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss = round(z.report.best_result.measurement[1],digits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52654"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_mpt = round(best.best_model.merge_purity_threshold,digits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"Figures/DT_MCC\"\n",
    "png(replace(fn,'.' => ','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = true,\n",
       "    merge_purity_threshold = 0.52654,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 5)\u001b[34m @070\u001b[39m"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(post_prune=true, merge_purity_threshold=best_mpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (10, 0.7, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (15, 1.0, 0.5568181818181818)\n",
      "(d, train_metric, valid_metric) = (20, 0.85, 0.5227272727272727)\n",
      "(d, train_metric, valid_metric) = (25, 1.0, 0.6477272727272727)\n",
      "(d, train_metric, valid_metric) = (30, 0.7666666666666667, 0.5227272727272727)\n",
      "(d, train_metric, valid_metric) = (35, 1.0, 0.7613636363636364)\n",
      "(d, train_metric, valid_metric) = (40, 0.95, 0.8181818181818182)\n",
      "(d, train_metric, valid_metric) = (45, 0.9777777777777777, 0.6590909090909091)\n",
      "(d, train_metric, valid_metric) = (50, 0.98, 0.6477272727272727)\n",
      "(d, train_metric, valid_metric) = (55, 0.9636363636363636, 0.7272727272727273)\n",
      "(d, train_metric, valid_metric) = (60, 1.0, 0.7613636363636364)\n",
      "(d, train_metric, valid_metric) = (65, 1.0, 0.7159090909090909)\n",
      "(d, train_metric, valid_metric) = (70, 0.9857142857142858, 0.6931818181818182)\n",
      "(d, train_metric, valid_metric) = (75, 0.96, 0.75)\n",
      "(d, train_metric, valid_metric) = (80, 0.95, 0.7613636363636364)\n",
      "(d, train_metric, valid_metric) = (85, 0.9764705882352941, 0.7272727272727273)\n",
      "(d, train_metric, valid_metric) = (90, 0.9555555555555556, 0.7159090909090909)\n",
      "(d, train_metric, valid_metric) = (95, 0.9473684210526315, 0.7954545454545454)\n",
      "(d, train_metric, valid_metric) = (100, 0.95, 0.7727272727272727)\n",
      "(d, train_metric, valid_metric) = (105, 0.9428571428571428, 0.7727272727272727)\n",
      "(d, train_metric, valid_metric) = (110, 0.9363636363636364, 0.8068181818181818)\n",
      "(d, train_metric, valid_metric) = (115, 0.9391304347826087, 0.8295454545454546)\n",
      "(d, train_metric, valid_metric) = (120, 0.95, 0.8068181818181818)\n",
      "(d, train_metric, valid_metric) = (125, 0.96, 0.7954545454545454)\n",
      "(d, train_metric, valid_metric) = (130, 0.9230769230769231, 0.8295454545454546)\n",
      "(d, train_metric, valid_metric) = (135, 0.9555555555555556, 0.8409090909090909)\n",
      "(d, train_metric, valid_metric) = (140, 0.95, 0.8409090909090909)\n",
      "(d, train_metric, valid_metric) = (145, 0.9448275862068966, 0.7954545454545454)\n",
      "(d, train_metric, valid_metric) = (150, 0.9466666666666667, 0.7840909090909091)\n",
      "(d, train_metric, valid_metric) = (155, 0.9741935483870968, 0.7954545454545454)\n",
      "(d, train_metric, valid_metric) = (160, 0.95625, 0.8181818181818182)\n",
      "(d, train_metric, valid_metric) = (165, 0.9454545454545454, 0.8409090909090909)\n",
      "(d, train_metric, valid_metric) = (170, 0.9235294117647059, 0.8068181818181818)\n",
      "(d, train_metric, valid_metric) = (175, 0.9314285714285714, 0.8409090909090909)\n",
      "(d, train_metric, valid_metric) = (180, 0.9277777777777778, 0.8295454545454546)\n",
      "(d, train_metric, valid_metric) = (185, 0.9243243243243243, 0.8295454545454546)\n",
      "(d, train_metric, valid_metric) = (190, 0.9315789473684211, 0.8522727272727273)\n",
      "(d, train_metric, valid_metric) = (195, 0.9538461538461539, 0.8068181818181818)\n",
      "(d, train_metric, valid_metric) = (200, 0.945, 0.7954545454545454)\n",
      "(d, train_metric, valid_metric) = (205, 0.9609756097560975, 0.7727272727272727)\n",
      "(d, train_metric, valid_metric) = (210, 0.9, 0.7386363636363636)\n",
      "(d, train_metric, valid_metric) = (215, 0.9627906976744186, 0.7840909090909091)\n",
      "(d, train_metric, valid_metric) = (220, 0.9454545454545454, 0.8068181818181818)\n",
      "(d, train_metric, valid_metric) = (225, 0.9644444444444444, 0.7840909090909091)\n",
      "(d, train_metric, valid_metric) = (230, 0.9478260869565217, 0.7727272727272727)\n",
      "(d, train_metric, valid_metric) = (235, 0.9446808510638298, 0.7727272727272727)\n",
      "(d, train_metric, valid_metric) = (240, 0.9791666666666666, 0.7386363636363636)\n",
      "(d, train_metric, valid_metric) = (245, 0.9795918367346939, 0.75)\n",
      "(d, train_metric, valid_metric) = (250, 0.972, 0.75)\n",
      "(d, train_metric, valid_metric) = (255, 0.9529411764705882, 0.8068181818181818)\n",
      "(d, train_metric, valid_metric) = (260, 0.926923076923077, 0.7613636363636364)\n",
      "(d, train_metric, valid_metric) = (265, 0.9207547169811321, 0.7613636363636364)\n",
      "(d, train_metric, valid_metric) = (270, 0.9259259259259259, 0.7727272727272727)\n",
      "(d, train_metric, valid_metric) = (275, 0.9527272727272728, 0.7727272727272727)\n",
      "(d, train_metric, valid_metric) = (280, 0.9428571428571428, 0.7727272727272727)\n",
      "(d, train_metric, valid_metric) = (285, 0.9403508771929825, 0.7727272727272727)\n",
      "(d, train_metric, valid_metric) = (290, 0.9137931034482759, 0.7840909090909091)\n",
      "(d, train_metric, valid_metric) = (295, 0.9694915254237289, 0.7727272727272727)\n",
      "(d, train_metric, valid_metric) = (300, 0.97, 0.7840909090909091)\n",
      "(d, train_metric, valid_metric) = (305, 0.9704918032786886, 0.7840909090909091)\n",
      "(d, train_metric, valid_metric) = (310, 0.9483870967741935, 0.8522727272727273)\n",
      "(d, train_metric, valid_metric) = (315, 0.9555555555555556, 0.7727272727272727)\n",
      "(d, train_metric, valid_metric) = (320, 0.925, 0.7840909090909091)\n",
      "(d, train_metric, valid_metric) = (325, 0.963076923076923, 0.8068181818181818)\n",
      "(d, train_metric, valid_metric) = (330, 0.9696969696969697, 0.8295454545454546)\n",
      "(d, train_metric, valid_metric) = (335, 0.9462686567164179, 0.7727272727272727)\n",
      "(d, train_metric, valid_metric) = (340, 0.9735294117647059, 0.8068181818181818)\n",
      "(d, train_metric, valid_metric) = (345, 0.9739130434782609, 0.8068181818181818)\n",
      "(d, train_metric, valid_metric) = (350, 0.98, 0.8181818181818182)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10:5:350, Any[0.7, 1.0, 0.85, 1.0, 0.7666666666666667, 1.0, 0.95, 0.9777777777777777, 0.98, 0.9636363636363636  …  0.9704918032786886, 0.9483870967741935, 0.9555555555555556, 0.925, 0.963076923076923, 0.9696969696969697, 0.9462686567164179, 0.9735294117647059, 0.9739130434782609, 0.98], Any[0.42045454545454547, 0.5568181818181818, 0.5227272727272727, 0.6477272727272727, 0.5227272727272727, 0.7613636363636364, 0.8181818181818182, 0.6590909090909091, 0.6477272727272727, 0.7272727272727273  …  0.7840909090909091, 0.8522727272727273, 0.7727272727272727, 0.7840909090909091, 0.8068181818181818, 0.8295454545454546, 0.7727272727272727, 0.8068181818181818, 0.8068181818181818, 0.8181818181818182])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_schedule, training_losses, valid_losses = learn_curve(dt, X[train,:], y[train], acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip960\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip960)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip961\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip960)\" d=\"\n",
       "M211.278 1423.18 L2352.76 1423.18 L2352.76 47.2441 L211.278 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip962\">\n",
       "    <rect x=\"211\" y=\"47\" width=\"2142\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip962)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  212.467,1423.18 212.467,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip962)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  806.661,1423.18 806.661,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip962)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1400.86,1423.18 1400.86,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip962)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1995.05,1423.18 1995.05,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip962)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.278,1206.07 2352.76,1206.07 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip962)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.278,982.096 2352.76,982.096 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip962)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.278,758.119 2352.76,758.119 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip962)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.278,534.141 2352.76,534.141 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip962)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.278,310.163 2352.76,310.163 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip962)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  211.278,86.1857 2352.76,86.1857 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip960)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.278,1423.18 2352.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip960)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.278,1423.18 211.278,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip960)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  212.467,1423.18 212.467,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip960)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  806.661,1423.18 806.661,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip960)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1400.86,1423.18 1400.86,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip960)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1995.05,1423.18 1995.05,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip960)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.278,1206.07 236.976,1206.07 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip960)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.278,982.096 236.976,982.096 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip960)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.278,758.119 236.976,758.119 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip960)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.278,534.141 236.976,534.141 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip960)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.278,310.163 236.976,310.163 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip960)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  211.278,86.1857 236.976,86.1857 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip960)\" d=\"M 0 0 M212.467 1445.17 Q208.856 1445.17 207.027 1448.74 Q205.221 1452.28 205.221 1459.41 Q205.221 1466.51 207.027 1470.08 Q208.856 1473.62 212.467 1473.62 Q216.101 1473.62 217.907 1470.08 Q219.735 1466.51 219.735 1459.41 Q219.735 1452.28 217.907 1448.74 Q216.101 1445.17 212.467 1445.17 M212.467 1441.47 Q218.277 1441.47 221.332 1446.07 Q224.411 1450.66 224.411 1459.41 Q224.411 1468.13 221.332 1472.74 Q218.277 1477.32 212.467 1477.32 Q206.657 1477.32 203.578 1472.74 Q200.522 1468.13 200.522 1459.41 Q200.522 1450.66 203.578 1446.07 Q206.657 1441.47 212.467 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M770.03 1472.72 L777.668 1472.72 L777.668 1446.35 L769.358 1448.02 L769.358 1443.76 L777.622 1442.09 L782.298 1442.09 L782.298 1472.72 L789.937 1472.72 L789.937 1476.65 L770.03 1476.65 L770.03 1472.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M805.006 1445.17 Q801.395 1445.17 799.567 1448.74 Q797.761 1452.28 797.761 1459.41 Q797.761 1466.51 799.567 1470.08 Q801.395 1473.62 805.006 1473.62 Q808.641 1473.62 810.446 1470.08 Q812.275 1466.51 812.275 1459.41 Q812.275 1452.28 810.446 1448.74 Q808.641 1445.17 805.006 1445.17 M805.006 1441.47 Q810.816 1441.47 813.872 1446.07 Q816.951 1450.66 816.951 1459.41 Q816.951 1468.13 813.872 1472.74 Q810.816 1477.32 805.006 1477.32 Q799.196 1477.32 796.117 1472.74 Q793.062 1468.13 793.062 1459.41 Q793.062 1450.66 796.117 1446.07 Q799.196 1441.47 805.006 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M832.02 1445.17 Q828.409 1445.17 826.58 1448.74 Q824.775 1452.28 824.775 1459.41 Q824.775 1466.51 826.58 1470.08 Q828.409 1473.62 832.02 1473.62 Q835.654 1473.62 837.46 1470.08 Q839.289 1466.51 839.289 1459.41 Q839.289 1452.28 837.46 1448.74 Q835.654 1445.17 832.02 1445.17 M832.02 1441.47 Q837.83 1441.47 840.886 1446.07 Q843.964 1450.66 843.964 1459.41 Q843.964 1468.13 840.886 1472.74 Q837.83 1477.32 832.02 1477.32 Q826.21 1477.32 823.131 1472.74 Q820.076 1468.13 820.076 1459.41 Q820.076 1450.66 823.131 1446.07 Q826.21 1441.47 832.02 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1368.5 1472.72 L1384.81 1472.72 L1384.81 1476.65 L1362.87 1476.65 L1362.87 1472.72 Q1365.53 1469.96 1370.12 1465.33 Q1374.72 1460.68 1375.9 1459.34 Q1378.15 1456.81 1379.03 1455.08 Q1379.93 1453.32 1379.93 1451.63 Q1379.93 1448.87 1377.99 1447.14 Q1376.06 1445.4 1372.96 1445.4 Q1370.76 1445.4 1368.31 1446.17 Q1365.88 1446.93 1363.1 1448.48 L1363.1 1443.76 Q1365.93 1442.62 1368.38 1442.05 Q1370.83 1441.47 1372.87 1441.47 Q1378.24 1441.47 1381.43 1444.15 Q1384.63 1446.84 1384.63 1451.33 Q1384.63 1453.46 1383.82 1455.38 Q1383.03 1457.28 1380.93 1459.87 Q1380.35 1460.54 1377.25 1463.76 Q1374.14 1466.95 1368.5 1472.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1399.88 1445.17 Q1396.27 1445.17 1394.44 1448.74 Q1392.64 1452.28 1392.64 1459.41 Q1392.64 1466.51 1394.44 1470.08 Q1396.27 1473.62 1399.88 1473.62 Q1403.52 1473.62 1405.32 1470.08 Q1407.15 1466.51 1407.15 1459.41 Q1407.15 1452.28 1405.32 1448.74 Q1403.52 1445.17 1399.88 1445.17 M1399.88 1441.47 Q1405.69 1441.47 1408.75 1446.07 Q1411.83 1450.66 1411.83 1459.41 Q1411.83 1468.13 1408.75 1472.74 Q1405.69 1477.32 1399.88 1477.32 Q1394.07 1477.32 1391 1472.74 Q1387.94 1468.13 1387.94 1459.41 Q1387.94 1450.66 1391 1446.07 Q1394.07 1441.47 1399.88 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1426.9 1445.17 Q1423.29 1445.17 1421.46 1448.74 Q1419.65 1452.28 1419.65 1459.41 Q1419.65 1466.51 1421.46 1470.08 Q1423.29 1473.62 1426.9 1473.62 Q1430.53 1473.62 1432.34 1470.08 Q1434.17 1466.51 1434.17 1459.41 Q1434.17 1452.28 1432.34 1448.74 Q1430.53 1445.17 1426.9 1445.17 M1426.9 1441.47 Q1432.71 1441.47 1435.76 1446.07 Q1438.84 1450.66 1438.84 1459.41 Q1438.84 1468.13 1435.76 1472.74 Q1432.71 1477.32 1426.9 1477.32 Q1421.09 1477.32 1418.01 1472.74 Q1414.95 1468.13 1414.95 1459.41 Q1414.95 1450.66 1418.01 1446.07 Q1421.09 1441.47 1426.9 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1972.28 1458.02 Q1975.64 1458.74 1977.52 1461 Q1979.41 1463.27 1979.41 1466.61 Q1979.41 1471.72 1975.9 1474.52 Q1972.38 1477.32 1965.9 1477.32 Q1963.72 1477.32 1961.41 1476.88 Q1959.11 1476.47 1956.66 1475.61 L1956.66 1471.1 Q1958.6 1472.23 1960.92 1472.81 Q1963.23 1473.39 1965.76 1473.39 Q1970.16 1473.39 1972.45 1471.65 Q1974.76 1469.92 1974.76 1466.61 Q1974.76 1463.55 1972.61 1461.84 Q1970.48 1460.1 1966.66 1460.1 L1962.63 1460.1 L1962.63 1456.26 L1966.84 1456.26 Q1970.29 1456.26 1972.12 1454.89 Q1973.95 1453.5 1973.95 1450.91 Q1973.95 1448.25 1972.05 1446.84 Q1970.18 1445.4 1966.66 1445.4 Q1964.74 1445.4 1962.54 1445.82 Q1960.34 1446.24 1957.7 1447.11 L1957.7 1442.95 Q1960.36 1442.21 1962.68 1441.84 Q1965.02 1441.47 1967.08 1441.47 Q1972.4 1441.47 1975.5 1443.9 Q1978.6 1446.3 1978.6 1450.43 Q1978.6 1453.3 1976.96 1455.29 Q1975.32 1457.25 1972.28 1458.02 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1994.48 1445.17 Q1990.87 1445.17 1989.04 1448.74 Q1987.24 1452.28 1987.24 1459.41 Q1987.24 1466.51 1989.04 1470.08 Q1990.87 1473.62 1994.48 1473.62 Q1998.12 1473.62 1999.92 1470.08 Q2001.75 1466.51 2001.75 1459.41 Q2001.75 1452.28 1999.92 1448.74 Q1998.12 1445.17 1994.48 1445.17 M1994.48 1441.47 Q2000.29 1441.47 2003.35 1446.07 Q2006.43 1450.66 2006.43 1459.41 Q2006.43 1468.13 2003.35 1472.74 Q2000.29 1477.32 1994.48 1477.32 Q1988.67 1477.32 1985.59 1472.74 Q1982.54 1468.13 1982.54 1459.41 Q1982.54 1450.66 1985.59 1446.07 Q1988.67 1441.47 1994.48 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2021.5 1445.17 Q2017.89 1445.17 2016.06 1448.74 Q2014.25 1452.28 2014.25 1459.41 Q2014.25 1466.51 2016.06 1470.08 Q2017.89 1473.62 2021.5 1473.62 Q2025.13 1473.62 2026.94 1470.08 Q2028.77 1466.51 2028.77 1459.41 Q2028.77 1452.28 2026.94 1448.74 Q2025.13 1445.17 2021.5 1445.17 M2021.5 1441.47 Q2027.31 1441.47 2030.36 1446.07 Q2033.44 1450.66 2033.44 1459.41 Q2033.44 1468.13 2030.36 1472.74 Q2027.31 1477.32 2021.5 1477.32 Q2015.69 1477.32 2012.61 1472.74 Q2009.55 1468.13 2009.55 1459.41 Q2009.55 1450.66 2012.61 1446.07 Q2015.69 1441.47 2021.5 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M139.362 1191.87 Q135.751 1191.87 133.922 1195.44 Q132.117 1198.98 132.117 1206.11 Q132.117 1213.22 133.922 1216.78 Q135.751 1220.32 139.362 1220.32 Q142.996 1220.32 144.802 1216.78 Q146.63 1213.22 146.63 1206.11 Q146.63 1198.98 144.802 1195.44 Q142.996 1191.87 139.362 1191.87 M139.362 1188.17 Q145.172 1188.17 148.228 1192.78 Q151.306 1197.36 151.306 1206.11 Q151.306 1214.84 148.228 1219.44 Q145.172 1224.03 139.362 1224.03 Q133.552 1224.03 130.473 1219.44 Q127.418 1214.84 127.418 1206.11 Q127.418 1197.36 130.473 1192.78 Q133.552 1188.17 139.362 1188.17 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M156.376 1217.47 L161.26 1217.47 L161.26 1223.35 L156.376 1223.35 L156.376 1217.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M166.376 1188.79 L184.732 1188.79 L184.732 1192.73 L170.658 1192.73 L170.658 1201.2 Q171.677 1200.85 172.695 1200.69 Q173.714 1200.51 174.732 1200.51 Q180.519 1200.51 183.899 1203.68 Q187.278 1206.85 187.278 1212.27 Q187.278 1217.84 183.806 1220.95 Q180.334 1224.03 174.015 1224.03 Q171.839 1224.03 169.57 1223.66 Q167.325 1223.28 164.917 1222.54 L164.917 1217.84 Q167.001 1218.98 169.223 1219.53 Q171.445 1220.09 173.922 1220.09 Q177.927 1220.09 180.264 1217.98 Q182.602 1215.88 182.602 1212.27 Q182.602 1208.66 180.264 1206.55 Q177.927 1204.44 173.922 1204.44 Q172.047 1204.44 170.172 1204.86 Q168.32 1205.28 166.376 1206.16 L166.376 1188.79 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M138.205 967.895 Q134.593 967.895 132.765 971.46 Q130.959 975.002 130.959 982.131 Q130.959 989.238 132.765 992.802 Q134.593 996.344 138.205 996.344 Q141.839 996.344 143.644 992.802 Q145.473 989.238 145.473 982.131 Q145.473 975.002 143.644 971.46 Q141.839 967.895 138.205 967.895 M138.205 964.191 Q144.015 964.191 147.07 968.798 Q150.149 973.381 150.149 982.131 Q150.149 990.858 147.07 995.464 Q144.015 1000.05 138.205 1000.05 Q132.394 1000.05 129.316 995.464 Q126.26 990.858 126.26 982.131 Q126.26 973.381 129.316 968.798 Q132.394 964.191 138.205 964.191 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M155.218 993.497 L160.103 993.497 L160.103 999.376 L155.218 999.376 L155.218 993.497 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M175.751 980.233 Q172.603 980.233 170.751 982.386 Q168.922 984.539 168.922 988.289 Q168.922 992.015 170.751 994.191 Q172.603 996.344 175.751 996.344 Q178.899 996.344 180.727 994.191 Q182.579 992.015 182.579 988.289 Q182.579 984.539 180.727 982.386 Q178.899 980.233 175.751 980.233 M185.033 965.58 L185.033 969.84 Q183.274 969.006 181.468 968.566 Q179.686 968.127 177.927 968.127 Q173.297 968.127 170.843 971.252 Q168.413 974.377 168.065 980.696 Q169.431 978.682 171.491 977.617 Q173.552 976.529 176.028 976.529 Q181.237 976.529 184.246 979.701 Q187.278 982.849 187.278 988.289 Q187.278 993.613 184.13 996.83 Q180.982 1000.05 175.751 1000.05 Q169.755 1000.05 166.584 995.464 Q163.413 990.858 163.413 982.131 Q163.413 973.937 167.302 969.076 Q171.19 964.191 177.741 964.191 Q179.501 964.191 181.283 964.539 Q183.089 964.886 185.033 965.58 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M139.269 743.917 Q135.658 743.917 133.83 747.482 Q132.024 751.024 132.024 758.154 Q132.024 765.26 133.83 768.825 Q135.658 772.366 139.269 772.366 Q142.904 772.366 144.709 768.825 Q146.538 765.26 146.538 758.154 Q146.538 751.024 144.709 747.482 Q142.904 743.917 139.269 743.917 M139.269 740.214 Q145.08 740.214 148.135 744.82 Q151.214 749.404 151.214 758.154 Q151.214 766.88 148.135 771.487 Q145.08 776.07 139.269 776.07 Q133.459 776.07 130.381 771.487 Q127.325 766.88 127.325 758.154 Q127.325 749.404 130.381 744.82 Q133.459 740.214 139.269 740.214 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M156.283 769.519 L161.167 769.519 L161.167 775.399 L156.283 775.399 L156.283 769.519 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M165.056 740.839 L187.278 740.839 L187.278 742.83 L174.732 775.399 L169.848 775.399 L181.653 744.774 L165.056 744.774 L165.056 740.839 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M138.459 519.94 Q134.848 519.94 133.019 523.505 Q131.214 527.046 131.214 534.176 Q131.214 541.282 133.019 544.847 Q134.848 548.389 138.459 548.389 Q142.093 548.389 143.899 544.847 Q145.728 541.282 145.728 534.176 Q145.728 527.046 143.899 523.505 Q142.093 519.94 138.459 519.94 M138.459 516.236 Q144.269 516.236 147.325 520.843 Q150.404 525.426 150.404 534.176 Q150.404 542.903 147.325 547.509 Q144.269 552.092 138.459 552.092 Q132.649 552.092 129.57 547.509 Q126.515 542.903 126.515 534.176 Q126.515 525.426 129.57 520.843 Q132.649 516.236 138.459 516.236 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M155.473 545.541 L160.357 545.541 L160.357 551.421 L155.473 551.421 L155.473 545.541 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M175.427 535.009 Q172.093 535.009 170.172 536.792 Q168.274 538.574 168.274 541.699 Q168.274 544.824 170.172 546.606 Q172.093 548.389 175.427 548.389 Q178.76 548.389 180.681 546.606 Q182.602 544.801 182.602 541.699 Q182.602 538.574 180.681 536.792 Q178.783 535.009 175.427 535.009 M170.751 533.018 Q167.741 532.278 166.052 530.217 Q164.385 528.157 164.385 525.194 Q164.385 521.051 167.325 518.643 Q170.288 516.236 175.427 516.236 Q180.589 516.236 183.528 518.643 Q186.468 521.051 186.468 525.194 Q186.468 528.157 184.778 530.217 Q183.112 532.278 180.126 533.018 Q183.505 533.805 185.38 536.097 Q187.278 538.389 187.278 541.699 Q187.278 546.722 184.2 549.407 Q181.144 552.092 175.427 552.092 Q169.709 552.092 166.63 549.407 Q163.575 546.722 163.575 541.699 Q163.575 538.389 165.473 536.097 Q167.371 533.805 170.751 533.018 M169.038 525.634 Q169.038 528.319 170.704 529.824 Q172.394 531.329 175.427 531.329 Q178.436 531.329 180.126 529.824 Q181.839 528.319 181.839 525.634 Q181.839 522.949 180.126 521.444 Q178.436 519.94 175.427 519.94 Q172.394 519.94 170.704 521.444 Q169.038 522.949 169.038 525.634 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M138.552 295.962 Q134.941 295.962 133.112 299.527 Q131.306 303.069 131.306 310.198 Q131.306 317.305 133.112 320.869 Q134.941 324.411 138.552 324.411 Q142.186 324.411 143.992 320.869 Q145.82 317.305 145.82 310.198 Q145.82 303.069 143.992 299.527 Q142.186 295.962 138.552 295.962 M138.552 292.258 Q144.362 292.258 147.417 296.865 Q150.496 301.448 150.496 310.198 Q150.496 318.925 147.417 323.531 Q144.362 328.115 138.552 328.115 Q132.742 328.115 129.663 323.531 Q126.607 318.925 126.607 310.198 Q126.607 301.448 129.663 296.865 Q132.742 292.258 138.552 292.258 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M155.566 321.564 L160.45 321.564 L160.45 327.443 L155.566 327.443 L155.566 321.564 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M165.658 326.726 L165.658 322.467 Q167.417 323.3 169.223 323.74 Q171.028 324.18 172.765 324.18 Q177.394 324.18 179.825 321.078 Q182.278 317.953 182.626 311.61 Q181.283 313.601 179.223 314.666 Q177.163 315.73 174.663 315.73 Q169.478 315.73 166.445 312.605 Q163.436 309.457 163.436 304.018 Q163.436 298.694 166.584 295.476 Q169.732 292.258 174.964 292.258 Q180.959 292.258 184.107 296.865 Q187.278 301.448 187.278 310.198 Q187.278 318.369 183.389 323.254 Q179.524 328.115 172.973 328.115 Q171.214 328.115 169.408 327.767 Q167.603 327.42 165.658 326.726 M174.964 312.073 Q178.112 312.073 179.94 309.92 Q181.792 307.768 181.792 304.018 Q181.792 300.291 179.94 298.138 Q178.112 295.962 174.964 295.962 Q171.815 295.962 169.964 298.138 Q168.135 300.291 168.135 304.018 Q168.135 307.768 169.964 309.92 Q171.815 312.073 174.964 312.073 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M130.404 99.5305 L138.043 99.5305 L138.043 73.1649 L129.732 74.8316 L129.732 70.5723 L137.996 68.9057 L142.672 68.9057 L142.672 99.5305 L150.311 99.5305 L150.311 103.466 L130.404 103.466 L130.404 99.5305 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M155.38 97.5861 L160.265 97.5861 L160.265 103.466 L155.38 103.466 L155.38 97.5861 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M175.334 71.9844 Q171.723 71.9844 169.894 75.5492 Q168.089 79.0908 168.089 86.2204 Q168.089 93.3268 169.894 96.8916 Q171.723 100.433 175.334 100.433 Q178.968 100.433 180.774 96.8916 Q182.602 93.3268 182.602 86.2204 Q182.602 79.0908 180.774 75.5492 Q178.968 71.9844 175.334 71.9844 M175.334 68.2807 Q181.144 68.2807 184.2 72.8871 Q187.278 77.4704 187.278 86.2204 Q187.278 94.9472 184.2 99.5537 Q181.144 104.137 175.334 104.137 Q169.524 104.137 166.445 99.5537 Q163.39 94.9472 163.39 86.2204 Q163.39 77.4704 166.445 72.8871 Q169.524 68.2807 175.334 68.2807 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M870.792 1508.52 L879.45 1508.52 L900.52 1548.28 L900.52 1508.52 L906.759 1508.52 L906.759 1556.04 L898.101 1556.04 L877.031 1516.29 L877.031 1556.04 L870.792 1556.04 L870.792 1508.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M912.297 1541.98 L912.297 1520.4 L918.153 1520.4 L918.153 1541.75 Q918.153 1546.81 920.127 1549.36 Q922.1 1551.87 926.047 1551.87 Q930.789 1551.87 933.527 1548.85 Q936.296 1545.83 936.296 1540.61 L936.296 1520.4 L942.152 1520.4 L942.152 1556.04 L936.296 1556.04 L936.296 1550.57 Q934.163 1553.82 931.33 1555.41 Q928.529 1556.97 924.805 1556.97 Q918.663 1556.97 915.48 1553.15 Q912.297 1549.33 912.297 1541.98 M927.033 1519.54 L927.033 1519.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M976.049 1527.24 Q978.246 1523.29 981.301 1521.41 Q984.357 1519.54 988.494 1519.54 Q994.064 1519.54 997.088 1523.45 Q1000.11 1527.33 1000.11 1534.53 L1000.11 1556.04 L994.224 1556.04 L994.224 1534.72 Q994.224 1529.59 992.409 1527.11 Q990.595 1524.63 986.871 1524.63 Q982.32 1524.63 979.678 1527.65 Q977.036 1530.68 977.036 1535.9 L977.036 1556.04 L971.148 1556.04 L971.148 1534.72 Q971.148 1529.56 969.334 1527.11 Q967.519 1524.63 963.732 1524.63 Q959.244 1524.63 956.602 1527.68 Q953.96 1530.71 953.96 1535.9 L953.96 1556.04 L948.072 1556.04 L948.072 1520.4 L953.96 1520.4 L953.96 1525.93 Q955.966 1522.66 958.767 1521.1 Q961.567 1519.54 965.419 1519.54 Q969.302 1519.54 972.007 1521.51 Q974.744 1523.48 976.049 1527.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1031.84 1538.25 Q1031.84 1531.79 1029.17 1528.13 Q1026.53 1524.44 1021.88 1524.44 Q1017.24 1524.44 1014.56 1528.13 Q1011.92 1531.79 1011.92 1538.25 Q1011.92 1544.71 1014.56 1548.4 Q1017.24 1552.07 1021.88 1552.07 Q1026.53 1552.07 1029.17 1548.4 Q1031.84 1544.71 1031.84 1538.25 M1011.92 1525.81 Q1013.77 1522.62 1016.57 1521.1 Q1019.4 1519.54 1023.31 1519.54 Q1029.81 1519.54 1033.85 1524.69 Q1037.92 1529.85 1037.92 1538.25 Q1037.92 1546.65 1033.85 1551.81 Q1029.81 1556.97 1023.31 1556.97 Q1019.4 1556.97 1016.57 1555.44 Q1013.77 1553.88 1011.92 1550.7 L1011.92 1556.04 L1006.03 1556.04 L1006.03 1506.52 L1011.92 1506.52 L1011.92 1525.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1074.56 1536.76 L1074.56 1539.62 L1047.63 1539.62 Q1048.01 1545.67 1051.26 1548.85 Q1054.54 1552 1060.36 1552 Q1063.74 1552 1066.89 1551.17 Q1070.07 1550.35 1073.19 1548.69 L1073.19 1554.23 Q1070.04 1555.57 1066.73 1556.27 Q1063.42 1556.97 1060.01 1556.97 Q1051.48 1556.97 1046.49 1552 Q1041.52 1547.04 1041.52 1538.57 Q1041.52 1529.82 1046.23 1524.69 Q1050.97 1519.54 1058.99 1519.54 Q1066.19 1519.54 1070.36 1524.18 Q1074.56 1528.8 1074.56 1536.76 M1068.7 1535.04 Q1068.64 1530.23 1066 1527.37 Q1063.39 1524.5 1059.06 1524.5 Q1054.16 1524.5 1051.2 1527.27 Q1048.27 1530.04 1047.82 1535.07 L1068.7 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1101.36 1525.87 Q1100.37 1525.3 1099.19 1525.04 Q1098.05 1524.76 1096.65 1524.76 Q1091.68 1524.76 1089.01 1528 Q1086.37 1531.22 1086.37 1537.27 L1086.37 1556.04 L1080.48 1556.04 L1080.48 1520.4 L1086.37 1520.4 L1086.37 1525.93 Q1088.21 1522.69 1091.17 1521.13 Q1094.13 1519.54 1098.37 1519.54 Q1098.97 1519.54 1099.7 1519.63 Q1100.44 1519.7 1101.33 1519.85 L1101.36 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1142.04 1524.5 Q1137.32 1524.5 1134.59 1528.19 Q1131.85 1531.85 1131.85 1538.25 Q1131.85 1544.65 1134.56 1548.34 Q1137.29 1552 1142.04 1552 Q1146.71 1552 1149.45 1548.31 Q1152.19 1544.62 1152.19 1538.25 Q1152.19 1531.92 1149.45 1528.23 Q1146.71 1524.5 1142.04 1524.5 M1142.04 1519.54 Q1149.67 1519.54 1154.03 1524.5 Q1158.4 1529.47 1158.4 1538.25 Q1158.4 1547 1154.03 1552 Q1149.67 1556.97 1142.04 1556.97 Q1134.36 1556.97 1130 1552 Q1125.68 1547 1125.68 1538.25 Q1125.68 1529.47 1130 1524.5 Q1134.36 1519.54 1142.04 1519.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1182.58 1506.52 L1182.58 1511.39 L1176.98 1511.39 Q1173.83 1511.39 1172.59 1512.66 Q1171.38 1513.93 1171.38 1517.24 L1171.38 1520.4 L1181.03 1520.4 L1181.03 1524.95 L1171.38 1524.95 L1171.38 1556.04 L1165.49 1556.04 L1165.49 1524.95 L1159.89 1524.95 L1159.89 1520.4 L1165.49 1520.4 L1165.49 1517.91 Q1165.49 1511.96 1168.26 1509.26 Q1171.03 1506.52 1177.05 1506.52 L1182.58 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1203.11 1508.52 L1243.31 1508.52 L1243.31 1513.93 L1226.44 1513.93 L1226.44 1556.04 L1219.98 1556.04 L1219.98 1513.93 L1203.11 1513.93 L1203.11 1508.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1260.53 1525.87 Q1259.55 1525.3 1258.37 1525.04 Q1257.22 1524.76 1255.82 1524.76 Q1250.86 1524.76 1248.18 1528 Q1245.54 1531.22 1245.54 1537.27 L1245.54 1556.04 L1239.65 1556.04 L1239.65 1520.4 L1245.54 1520.4 L1245.54 1525.93 Q1247.39 1522.69 1250.35 1521.13 Q1253.31 1519.54 1257.54 1519.54 Q1258.15 1519.54 1258.88 1519.63 Q1259.61 1519.7 1260.5 1519.85 L1260.53 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1282.88 1538.12 Q1275.78 1538.12 1273.04 1539.75 Q1270.3 1541.37 1270.3 1545.29 Q1270.3 1548.4 1272.34 1550.25 Q1274.41 1552.07 1277.94 1552.07 Q1282.81 1552.07 1285.74 1548.63 Q1288.7 1545.16 1288.7 1539.43 L1288.7 1538.12 L1282.88 1538.12 M1294.56 1535.71 L1294.56 1556.04 L1288.7 1556.04 L1288.7 1550.63 Q1286.7 1553.88 1283.7 1555.44 Q1280.71 1556.97 1276.38 1556.97 Q1270.91 1556.97 1267.66 1553.91 Q1264.45 1550.82 1264.45 1545.67 Q1264.45 1539.65 1268.46 1536.6 Q1272.5 1533.54 1280.49 1533.54 L1288.7 1533.54 L1288.7 1532.97 Q1288.7 1528.93 1286.03 1526.73 Q1283.39 1524.5 1278.58 1524.5 Q1275.52 1524.5 1272.63 1525.23 Q1269.73 1525.97 1267.06 1527.43 L1267.06 1522.02 Q1270.27 1520.78 1273.3 1520.17 Q1276.32 1519.54 1279.18 1519.54 Q1286.92 1519.54 1290.74 1523.55 Q1294.56 1527.56 1294.56 1535.71 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1300.7 1520.4 L1306.56 1520.4 L1306.56 1556.04 L1300.7 1556.04 L1300.7 1520.4 M1300.7 1506.52 L1306.56 1506.52 L1306.56 1513.93 L1300.7 1513.93 L1300.7 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1342.33 1534.53 L1342.33 1556.04 L1336.48 1556.04 L1336.48 1534.72 Q1336.48 1529.66 1334.5 1527.14 Q1332.53 1524.63 1328.58 1524.63 Q1323.84 1524.63 1321.1 1527.65 Q1318.37 1530.68 1318.37 1535.9 L1318.37 1556.04 L1312.48 1556.04 L1312.48 1520.4 L1318.37 1520.4 L1318.37 1525.93 Q1320.47 1522.72 1323.3 1521.13 Q1326.16 1519.54 1329.89 1519.54 Q1336.03 1519.54 1339.18 1523.36 Q1342.33 1527.14 1342.33 1534.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1348.48 1520.4 L1354.33 1520.4 L1354.33 1556.04 L1348.48 1556.04 L1348.48 1520.4 M1348.48 1506.52 L1354.33 1506.52 L1354.33 1513.93 L1348.48 1513.93 L1348.48 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1390.11 1534.53 L1390.11 1556.04 L1384.25 1556.04 L1384.25 1534.72 Q1384.25 1529.66 1382.28 1527.14 Q1380.3 1524.63 1376.36 1524.63 Q1371.61 1524.63 1368.88 1527.65 Q1366.14 1530.68 1366.14 1535.9 L1366.14 1556.04 L1360.25 1556.04 L1360.25 1520.4 L1366.14 1520.4 L1366.14 1525.93 Q1368.24 1522.72 1371.07 1521.13 Q1373.94 1519.54 1377.66 1519.54 Q1383.8 1519.54 1386.96 1523.36 Q1390.11 1527.14 1390.11 1534.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1419.71 1537.81 Q1419.71 1531.44 1417.07 1527.94 Q1414.46 1524.44 1409.71 1524.44 Q1405 1524.44 1402.36 1527.94 Q1399.75 1531.44 1399.75 1537.81 Q1399.75 1544.14 1402.36 1547.64 Q1405 1551.14 1409.71 1551.14 Q1414.46 1551.14 1417.07 1547.64 Q1419.71 1544.14 1419.71 1537.81 M1425.56 1551.62 Q1425.56 1560.72 1421.52 1565.15 Q1417.48 1569.6 1409.14 1569.6 Q1406.05 1569.6 1403.32 1569.13 Q1400.58 1568.68 1398 1567.72 L1398 1562.03 Q1400.58 1563.43 1403.09 1564.1 Q1405.61 1564.76 1408.22 1564.76 Q1413.98 1564.76 1416.84 1561.74 Q1419.71 1558.75 1419.71 1552.67 L1419.71 1549.77 Q1417.89 1552.92 1415.06 1554.48 Q1412.23 1556.04 1408.28 1556.04 Q1401.72 1556.04 1397.71 1551.05 Q1393.7 1546.05 1393.7 1537.81 Q1393.7 1529.53 1397.71 1524.53 Q1401.72 1519.54 1408.28 1519.54 Q1412.23 1519.54 1415.06 1521.1 Q1417.89 1522.66 1419.71 1525.81 L1419.71 1520.4 L1425.56 1520.4 L1425.56 1551.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1481.17 1510.08 L1481.17 1516.35 Q1477.51 1514.6 1474.26 1513.74 Q1471.01 1512.88 1467.99 1512.88 Q1462.74 1512.88 1459.87 1514.92 Q1457.04 1516.96 1457.04 1520.71 Q1457.04 1523.87 1458.92 1525.49 Q1460.83 1527.08 1466.11 1528.07 L1470 1528.86 Q1477.19 1530.23 1480.6 1533.7 Q1484.03 1537.14 1484.03 1542.93 Q1484.03 1549.84 1479.39 1553.4 Q1474.77 1556.97 1465.83 1556.97 Q1462.45 1556.97 1458.63 1556.2 Q1454.85 1555.44 1450.77 1553.94 L1450.77 1547.32 Q1454.69 1549.52 1458.44 1550.63 Q1462.2 1551.75 1465.83 1551.75 Q1471.33 1551.75 1474.33 1549.58 Q1477.32 1547.42 1477.32 1543.41 Q1477.32 1539.91 1475.15 1537.93 Q1473.02 1535.96 1468.12 1534.97 L1464.2 1534.21 Q1457.01 1532.78 1453.8 1529.72 Q1450.58 1526.67 1450.58 1521.22 Q1450.58 1514.92 1455.01 1511.29 Q1459.46 1507.66 1467.26 1507.66 Q1470.6 1507.66 1474.07 1508.27 Q1477.54 1508.87 1481.17 1510.08 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1506.38 1538.12 Q1499.28 1538.12 1496.54 1539.75 Q1493.8 1541.37 1493.8 1545.29 Q1493.8 1548.4 1495.84 1550.25 Q1497.91 1552.07 1501.44 1552.07 Q1506.31 1552.07 1509.24 1548.63 Q1512.2 1545.16 1512.2 1539.43 L1512.2 1538.12 L1506.38 1538.12 M1518.06 1535.71 L1518.06 1556.04 L1512.2 1556.04 L1512.2 1550.63 Q1510.2 1553.88 1507.2 1555.44 Q1504.21 1556.97 1499.88 1556.97 Q1494.41 1556.97 1491.16 1553.91 Q1487.95 1550.82 1487.95 1545.67 Q1487.95 1539.65 1491.96 1536.6 Q1496 1533.54 1503.99 1533.54 L1512.2 1533.54 L1512.2 1532.97 Q1512.2 1528.93 1509.53 1526.73 Q1506.89 1524.5 1502.08 1524.5 Q1499.02 1524.5 1496.13 1525.23 Q1493.23 1525.97 1490.56 1527.43 L1490.56 1522.02 Q1493.77 1520.78 1496.8 1520.17 Q1499.82 1519.54 1502.68 1519.54 Q1510.42 1519.54 1514.24 1523.55 Q1518.06 1527.56 1518.06 1535.71 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1551.95 1527.24 Q1554.15 1523.29 1557.21 1521.41 Q1560.26 1519.54 1564.4 1519.54 Q1569.97 1519.54 1572.99 1523.45 Q1576.02 1527.33 1576.02 1534.53 L1576.02 1556.04 L1570.13 1556.04 L1570.13 1534.72 Q1570.13 1529.59 1568.31 1527.11 Q1566.5 1524.63 1562.78 1524.63 Q1558.23 1524.63 1555.58 1527.65 Q1552.94 1530.68 1552.94 1535.9 L1552.94 1556.04 L1547.05 1556.04 L1547.05 1534.72 Q1547.05 1529.56 1545.24 1527.11 Q1543.42 1524.63 1539.64 1524.63 Q1535.15 1524.63 1532.51 1527.68 Q1529.87 1530.71 1529.87 1535.9 L1529.87 1556.04 L1523.98 1556.04 L1523.98 1520.4 L1529.87 1520.4 L1529.87 1525.93 Q1531.87 1522.66 1534.67 1521.1 Q1537.47 1519.54 1541.32 1519.54 Q1545.21 1519.54 1547.91 1521.51 Q1550.65 1523.48 1551.95 1527.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1587.83 1550.7 L1587.83 1569.6 L1581.94 1569.6 L1581.94 1520.4 L1587.83 1520.4 L1587.83 1525.81 Q1589.67 1522.62 1592.47 1521.1 Q1595.31 1519.54 1599.22 1519.54 Q1605.71 1519.54 1609.76 1524.69 Q1613.83 1529.85 1613.83 1538.25 Q1613.83 1546.65 1609.76 1551.81 Q1605.71 1556.97 1599.22 1556.97 Q1595.31 1556.97 1592.47 1555.44 Q1589.67 1553.88 1587.83 1550.7 M1607.75 1538.25 Q1607.75 1531.79 1605.08 1528.13 Q1602.43 1524.44 1597.79 1524.44 Q1593.14 1524.44 1590.47 1528.13 Q1587.83 1531.79 1587.83 1538.25 Q1587.83 1544.71 1590.47 1548.4 Q1593.14 1552.07 1597.79 1552.07 Q1602.43 1552.07 1605.08 1548.4 Q1607.75 1544.71 1607.75 1538.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1619.97 1506.52 L1625.83 1506.52 L1625.83 1556.04 L1619.97 1556.04 L1619.97 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1662.46 1536.76 L1662.46 1539.62 L1635.54 1539.62 Q1635.92 1545.67 1639.17 1548.85 Q1642.44 1552 1648.27 1552 Q1651.64 1552 1654.79 1551.17 Q1657.98 1550.35 1661.09 1548.69 L1661.09 1554.23 Q1657.94 1555.57 1654.63 1556.27 Q1651.32 1556.97 1647.92 1556.97 Q1639.39 1556.97 1634.39 1552 Q1629.43 1547.04 1629.43 1538.57 Q1629.43 1529.82 1634.14 1524.69 Q1638.88 1519.54 1646.9 1519.54 Q1654.09 1519.54 1658.26 1524.18 Q1662.46 1528.8 1662.46 1536.76 M1656.61 1535.04 Q1656.54 1530.23 1653.9 1527.37 Q1651.29 1524.5 1646.96 1524.5 Q1642.06 1524.5 1639.1 1527.27 Q1636.17 1530.04 1635.73 1535.07 L1656.61 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M1691.33 1521.45 L1691.33 1526.98 Q1688.85 1525.71 1686.18 1525.07 Q1683.5 1524.44 1680.64 1524.44 Q1676.28 1524.44 1674.08 1525.77 Q1671.92 1527.11 1671.92 1529.79 Q1671.92 1531.82 1673.48 1533 Q1675.04 1534.15 1679.75 1535.2 L1681.75 1535.64 Q1687.99 1536.98 1690.6 1539.43 Q1693.24 1541.85 1693.24 1546.21 Q1693.24 1551.17 1689.3 1554.07 Q1685.38 1556.97 1678.51 1556.97 Q1675.64 1556.97 1672.52 1556.39 Q1669.43 1555.85 1666 1554.74 L1666 1548.69 Q1669.24 1550.38 1672.39 1551.24 Q1675.55 1552.07 1678.63 1552.07 Q1682.77 1552.07 1685 1550.66 Q1687.23 1549.23 1687.23 1546.65 Q1687.23 1544.27 1685.6 1542.99 Q1684.01 1541.72 1678.57 1540.54 L1676.53 1540.07 Q1671.09 1538.92 1668.67 1536.56 Q1666.25 1534.18 1666.25 1530.04 Q1666.25 1525.01 1669.82 1522.27 Q1673.38 1519.54 1679.94 1519.54 Q1683.18 1519.54 1686.05 1520.01 Q1688.91 1520.49 1691.33 1521.45 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M44.1444 1026.82 L50.9239 1026.82 Q47.9002 1030.07 46.4043 1033.76 Q44.9083 1037.42 44.9083 1041.56 Q44.9083 1049.71 49.9054 1054.04 Q54.8707 1058.37 64.2919 1058.37 Q73.6813 1058.37 78.6784 1054.04 Q83.6436 1049.71 83.6436 1041.56 Q83.6436 1037.42 82.1477 1033.76 Q80.6518 1030.07 77.6281 1026.82 L84.3439 1026.82 Q86.6355 1030.2 87.7814 1033.99 Q88.9272 1037.74 88.9272 1041.94 Q88.9272 1052.73 82.3387 1058.94 Q75.7183 1065.15 64.2919 1065.15 Q52.8336 1065.15 46.2451 1058.94 Q39.6248 1052.73 39.6248 1041.94 Q39.6248 1037.68 40.7706 1033.92 Q41.8846 1030.14 44.1444 1026.82 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M38.479 1020.68 L38.479 1014.83 L88.0042 1014.83 L88.0042 1020.68 L38.479 1020.68 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M70.0847 992.482 Q70.0847 999.58 71.7079 1002.32 Q73.3312 1005.05 77.2461 1005.05 Q80.3653 1005.05 82.2114 1003.02 Q84.0256 1000.95 84.0256 997.415 Q84.0256 992.546 80.5881 989.617 Q77.1188 986.657 71.3897 986.657 L70.0847 986.657 L70.0847 992.482 M67.6657 980.801 L88.0042 980.801 L88.0042 986.657 L82.5933 986.657 Q85.8398 988.663 87.3994 991.654 Q88.9272 994.646 88.9272 998.975 Q88.9272 1004.45 85.8716 1007.7 Q82.7843 1010.91 77.6281 1010.91 Q71.6125 1010.91 68.5569 1006.9 Q65.5014 1002.86 65.5014 994.869 L65.5014 986.657 L64.9285 986.657 Q60.8862 986.657 58.6901 989.331 Q56.4621 991.973 56.4621 996.779 Q56.4621 999.834 57.1941 1002.73 Q57.9262 1005.63 59.3903 1008.3 L53.9795 1008.3 Q52.7381 1005.09 52.1334 1002.06 Q51.4968 999.039 51.4968 996.174 Q51.4968 988.44 55.5072 984.62 Q59.5176 980.801 67.6657 980.801 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M53.4065 951.932 L58.9447 951.932 Q57.6716 954.415 57.035 957.089 Q56.3984 959.762 56.3984 962.627 Q56.3984 966.987 57.7352 969.184 Q59.072 971.348 61.7456 971.348 Q63.7826 971.348 64.9603 969.788 Q66.1061 968.229 67.1565 963.518 L67.6021 961.513 Q68.9389 955.274 71.3897 952.665 Q73.8086 950.023 78.1691 950.023 Q83.1344 950.023 86.0308 953.969 Q88.9272 957.884 88.9272 964.759 Q88.9272 967.624 88.3543 970.743 Q87.8132 973.83 86.6992 977.268 L80.6518 977.268 Q82.3387 974.021 83.198 970.87 Q84.0256 967.719 84.0256 964.632 Q84.0256 960.494 82.6251 958.266 Q81.1929 956.038 78.6147 956.038 Q76.2276 956.038 74.9545 957.662 Q73.6813 959.253 72.5037 964.696 L72.0262 966.733 Q70.8804 972.175 68.5251 974.594 Q66.138 977.013 62.0002 977.013 Q56.9713 977.013 54.2341 973.449 Q51.4968 969.884 51.4968 963.327 Q51.4968 960.081 51.9743 957.216 Q52.4517 954.351 53.4065 951.932 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M53.4065 921.154 L58.9447 921.154 Q57.6716 923.637 57.035 926.31 Q56.3984 928.984 56.3984 931.849 Q56.3984 936.209 57.7352 938.405 Q59.072 940.57 61.7456 940.57 Q63.7826 940.57 64.9603 939.01 Q66.1061 937.45 67.1565 932.74 L67.6021 930.735 Q68.9389 924.496 71.3897 921.886 Q73.8086 919.245 78.1691 919.245 Q83.1344 919.245 86.0308 923.191 Q88.9272 927.106 88.9272 933.981 Q88.9272 936.846 88.3543 939.965 Q87.8132 943.052 86.6992 946.49 L80.6518 946.49 Q82.3387 943.243 83.198 940.092 Q84.0256 936.941 84.0256 933.854 Q84.0256 929.716 82.6251 927.488 Q81.1929 925.26 78.6147 925.26 Q76.2276 925.26 74.9545 926.883 Q73.6813 928.475 72.5037 933.918 L72.0262 935.955 Q70.8804 941.397 68.5251 943.816 Q66.138 946.235 62.0002 946.235 Q56.9713 946.235 54.2341 942.67 Q51.4968 939.106 51.4968 932.549 Q51.4968 929.302 51.9743 926.438 Q52.4517 923.573 53.4065 921.154 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M52.3562 913.102 L52.3562 907.245 L88.0042 907.245 L88.0042 913.102 L52.3562 913.102 M38.479 913.102 L38.479 907.245 L45.895 907.245 L45.895 913.102 L38.479 913.102 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M38.479 883.056 L43.3487 883.056 L43.3487 888.657 Q43.3487 891.808 44.6219 893.05 Q45.895 894.259 49.2052 894.259 L52.3562 894.259 L52.3562 884.615 L56.9077 884.615 L56.9077 894.259 L88.0042 894.259 L88.0042 900.147 L56.9077 900.147 L56.9077 905.749 L52.3562 905.749 L52.3562 900.147 L49.8736 900.147 Q43.9216 900.147 41.2162 897.378 Q38.479 894.609 38.479 888.594 L38.479 883.056 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M52.3562 876.913 L52.3562 871.056 L88.0042 871.056 L88.0042 876.913 L52.3562 876.913 M38.479 876.913 L38.479 871.056 L45.895 871.056 L45.895 876.913 L38.479 876.913 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M53.7248 839.259 L59.1993 839.259 Q57.8307 841.742 57.1623 844.257 Q56.4621 846.739 56.4621 849.285 Q56.4621 854.983 60.0905 858.134 Q63.6872 861.285 70.212 861.285 Q76.7369 861.285 80.3653 858.134 Q83.9619 854.983 83.9619 849.285 Q83.9619 846.739 83.2935 844.257 Q82.5933 841.742 81.2247 839.259 L86.6355 839.259 Q87.7814 841.71 88.3543 844.352 Q88.9272 846.962 88.9272 849.922 Q88.9272 857.975 83.8664 862.717 Q78.8057 867.46 70.212 867.46 Q61.491 867.46 56.4939 862.685 Q51.4968 857.879 51.4968 849.54 Q51.4968 846.835 52.0697 844.257 Q52.6108 841.678 53.7248 839.259 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M70.0847 816.916 Q70.0847 824.014 71.7079 826.751 Q73.3312 829.488 77.2461 829.488 Q80.3653 829.488 82.2114 827.451 Q84.0256 825.382 84.0256 821.849 Q84.0256 816.979 80.5881 814.051 Q77.1188 811.091 71.3897 811.091 L70.0847 811.091 L70.0847 816.916 M67.6657 805.235 L88.0042 805.235 L88.0042 811.091 L82.5933 811.091 Q85.8398 813.096 87.3994 816.088 Q88.9272 819.08 88.9272 823.409 Q88.9272 828.883 85.8716 832.13 Q82.7843 835.345 77.6281 835.345 Q71.6125 835.345 68.5569 831.334 Q65.5014 827.292 65.5014 819.303 L65.5014 811.091 L64.9285 811.091 Q60.8862 811.091 58.6901 813.765 Q56.4621 816.407 56.4621 821.213 Q56.4621 824.268 57.1941 827.165 Q57.9262 830.061 59.3903 832.735 L53.9795 832.735 Q52.7381 829.52 52.1334 826.496 Q51.4968 823.472 51.4968 820.608 Q51.4968 812.874 55.5072 809.054 Q59.5176 805.235 67.6657 805.235 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M42.2347 793.299 L52.3562 793.299 L52.3562 781.236 L56.9077 781.236 L56.9077 793.299 L76.2594 793.299 Q80.6199 793.299 81.8613 792.121 Q83.1026 790.912 83.1026 787.252 L83.1026 781.236 L88.0042 781.236 L88.0042 787.252 Q88.0042 794.031 85.4897 796.609 Q82.9434 799.187 76.2594 799.187 L56.9077 799.187 L56.9077 803.484 L52.3562 803.484 L52.3562 799.187 L42.2347 799.187 L42.2347 793.299 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M52.3562 775.093 L52.3562 769.237 L88.0042 769.237 L88.0042 775.093 L52.3562 775.093 M38.479 775.093 L38.479 769.237 L45.895 769.237 L45.895 775.093 L38.479 775.093 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M56.4621 749.28 Q56.4621 753.991 60.1542 756.728 Q63.8145 759.465 70.212 759.465 Q76.6095 759.465 80.3017 756.76 Q83.9619 754.023 83.9619 749.28 Q83.9619 744.601 80.2698 741.864 Q76.5777 739.127 70.212 739.127 Q63.8781 739.127 60.186 741.864 Q56.4621 744.601 56.4621 749.28 M51.4968 749.28 Q51.4968 741.641 56.4621 737.281 Q61.4273 732.92 70.212 732.92 Q78.9649 732.92 83.9619 737.281 Q88.9272 741.641 88.9272 749.28 Q88.9272 756.951 83.9619 761.311 Q78.9649 765.64 70.212 765.64 Q61.4273 765.64 56.4621 761.311 Q51.4968 756.951 51.4968 749.28 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M66.4881 697.145 L88.0042 697.145 L88.0042 703.001 L66.679 703.001 Q61.6183 703.001 59.1038 704.975 Q56.5894 706.948 56.5894 710.895 Q56.5894 715.637 59.6131 718.375 Q62.6368 721.112 67.8567 721.112 L88.0042 721.112 L88.0042 727 L52.3562 727 L52.3562 721.112 L57.8944 721.112 Q54.6797 719.011 53.0883 716.179 Q51.4968 713.314 51.4968 709.59 Q51.4968 703.447 55.3163 700.296 Q59.1038 697.145 66.4881 697.145 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M46.818 654.145 L70.4666 662.866 L70.4666 645.392 L46.818 654.145 M40.4842 657.773 L40.4842 650.484 L88.0042 632.374 L88.0042 639.058 L75.8138 643.387 L75.8138 664.807 L88.0042 669.136 L88.0042 675.915 L40.4842 657.773 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M53.7248 601.723 L59.1993 601.723 Q57.8307 604.206 57.1623 606.72 Q56.4621 609.203 56.4621 611.749 Q56.4621 617.446 60.0905 620.597 Q63.6872 623.748 70.212 623.748 Q76.7369 623.748 80.3653 620.597 Q83.9619 617.446 83.9619 611.749 Q83.9619 609.203 83.2935 606.72 Q82.5933 604.206 81.2247 601.723 L86.6355 601.723 Q87.7814 604.174 88.3543 606.816 Q88.9272 609.426 88.9272 612.386 Q88.9272 620.438 83.8664 625.181 Q78.8057 629.923 70.212 629.923 Q61.491 629.923 56.4939 625.149 Q51.4968 620.343 51.4968 612.004 Q51.4968 609.298 52.0697 606.72 Q52.6108 604.142 53.7248 601.723 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M53.7248 569.926 L59.1993 569.926 Q57.8307 572.409 57.1623 574.923 Q56.4621 577.406 56.4621 579.952 Q56.4621 585.65 60.0905 588.801 Q63.6872 591.952 70.212 591.952 Q76.7369 591.952 80.3653 588.801 Q83.9619 585.65 83.9619 579.952 Q83.9619 577.406 83.2935 574.923 Q82.5933 572.409 81.2247 569.926 L86.6355 569.926 Q87.7814 572.377 88.3543 575.019 Q88.9272 577.629 88.9272 580.589 Q88.9272 588.642 83.8664 593.384 Q78.8057 598.126 70.212 598.126 Q61.491 598.126 56.4939 593.352 Q51.4968 588.546 51.4968 580.207 Q51.4968 577.502 52.0697 574.923 Q52.6108 572.345 53.7248 569.926 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M73.9359 564.388 L52.3562 564.388 L52.3562 558.532 L73.7131 558.532 Q78.7739 558.532 81.3202 556.558 Q83.8346 554.585 83.8346 550.638 Q83.8346 545.896 80.8109 543.159 Q77.7872 540.39 72.5673 540.39 L52.3562 540.39 L52.3562 534.533 L88.0042 534.533 L88.0042 540.39 L82.5296 540.39 Q85.7762 542.522 87.3676 545.355 Q88.9272 548.156 88.9272 551.88 Q88.9272 558.023 85.1078 561.205 Q81.2883 564.388 73.9359 564.388 M51.4968 549.652 L51.4968 549.652 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M57.8307 507.733 Q57.2578 508.72 57.0032 509.898 Q56.7167 511.044 56.7167 512.444 Q56.7167 517.409 59.9632 520.083 Q63.1779 522.725 69.2253 522.725 L88.0042 522.725 L88.0042 528.613 L52.3562 528.613 L52.3562 522.725 L57.8944 522.725 Q54.6479 520.879 53.0883 517.919 Q51.4968 514.959 51.4968 510.725 Q51.4968 510.121 51.5923 509.389 Q51.656 508.656 51.8151 507.765 L57.8307 507.733 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M70.0847 485.39 Q70.0847 492.488 71.7079 495.225 Q73.3312 497.962 77.2461 497.962 Q80.3653 497.962 82.2114 495.925 Q84.0256 493.856 84.0256 490.323 Q84.0256 485.453 80.5881 482.525 Q77.1188 479.565 71.3897 479.565 L70.0847 479.565 L70.0847 485.39 M67.6657 473.709 L88.0042 473.709 L88.0042 479.565 L82.5933 479.565 Q85.8398 481.57 87.3994 484.562 Q88.9272 487.554 88.9272 491.883 Q88.9272 497.357 85.8716 500.604 Q82.7843 503.819 77.6281 503.819 Q71.6125 503.819 68.5569 499.808 Q65.5014 495.766 65.5014 487.777 L65.5014 479.565 L64.9285 479.565 Q60.8862 479.565 58.6901 482.239 Q56.4621 484.881 56.4621 489.687 Q56.4621 492.742 57.1941 495.639 Q57.9262 498.535 59.3903 501.209 L53.9795 501.209 Q52.7381 497.994 52.1334 494.97 Q51.4968 491.946 51.4968 489.082 Q51.4968 481.348 55.5072 477.528 Q59.5176 473.709 67.6657 473.709 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M53.7248 441.912 L59.1993 441.912 Q57.8307 444.395 57.1623 446.909 Q56.4621 449.392 56.4621 451.938 Q56.4621 457.635 60.0905 460.786 Q63.6872 463.937 70.212 463.937 Q76.7369 463.937 80.3653 460.786 Q83.9619 457.635 83.9619 451.938 Q83.9619 449.392 83.2935 446.909 Q82.5933 444.395 81.2247 441.912 L86.6355 441.912 Q87.7814 444.363 88.3543 447.005 Q88.9272 449.615 88.9272 452.575 Q88.9272 460.627 83.8664 465.37 Q78.8057 470.112 70.212 470.112 Q61.491 470.112 56.4939 465.338 Q51.4968 460.532 51.4968 452.193 Q51.4968 449.487 52.0697 446.909 Q52.6108 444.331 53.7248 441.912 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M91.3143 420.937 Q97.68 423.42 99.6216 425.775 Q101.563 428.13 101.563 432.077 L101.563 436.756 L96.6615 436.756 L96.6615 433.318 Q96.6615 430.899 95.5157 429.563 Q94.3699 428.226 90.1048 426.603 L87.4312 425.552 L52.3562 439.97 L52.3562 433.764 L80.238 422.624 L52.3562 411.484 L52.3562 405.277 L91.3143 420.937 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip962)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  271.886,758.119 301.596,86.1857 331.306,422.152 361.015,86.1857 390.725,608.8 420.435,86.1857 450.145,198.175 479.854,135.958 509.564,130.981 539.274,167.632 \n",
       "  568.984,86.1857 598.693,86.1857 628.403,118.182 658.113,175.777 687.822,198.175 717.532,138.886 747.242,185.731 776.952,204.069 806.661,198.175 836.371,214.173 \n",
       "  866.081,228.717 895.791,222.52 925.5,198.175 955.21,175.777 984.92,258.476 1014.63,185.731 1044.34,198.175 1074.05,209.76 1103.76,205.64 1133.47,143.986 \n",
       "  1163.18,184.176 1192.89,208.355 1222.6,257.463 1252.31,239.77 1282.02,247.947 1311.73,255.682 1341.44,239.434 1371.15,189.56 1400.86,209.373 1430.57,173.592 \n",
       "  1460.28,310.163 1489.99,169.526 1519.69,208.355 1549.4,165.822 1579.11,203.044 1608.82,210.088 1638.53,132.848 1668.24,131.895 1697.95,148.899 1727.66,191.587 \n",
       "  1757.37,249.862 1787.08,263.677 1816.79,252.095 1846.5,192.066 1876.21,214.173 1905.92,219.786 1935.63,279.27 1965.34,154.518 1995.05,153.379 2024.76,152.277 \n",
       "  2054.47,201.787 2084.18,185.731 2113.89,254.169 2143.6,168.885 2173.31,154.058 2203.02,206.532 2232.73,145.474 2262.44,144.615 2292.15,130.981 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip962)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  271.886,1384.24 301.596,1078.81 331.306,1155.17 361.015,875.198 390.725,1155.17 420.435,620.678 450.145,493.418 479.854,849.746 509.564,875.198 539.274,697.034 \n",
       "  568.984,620.678 598.693,722.486 628.403,773.39 658.113,646.13 687.822,620.678 717.532,697.034 747.242,722.486 776.952,544.322 806.661,595.226 836.371,595.226 \n",
       "  866.081,518.87 895.791,467.966 925.5,518.87 955.21,544.322 984.92,467.966 1014.63,442.514 1044.34,442.514 1074.05,544.322 1103.76,569.774 1133.47,544.322 \n",
       "  1163.18,493.418 1192.89,442.514 1222.6,518.87 1252.31,442.514 1282.02,467.966 1311.73,467.966 1341.44,417.062 1371.15,518.87 1400.86,544.322 1430.57,595.226 \n",
       "  1460.28,671.582 1489.99,569.774 1519.69,518.87 1549.4,569.774 1579.11,595.226 1608.82,595.226 1638.53,671.582 1668.24,646.13 1697.95,646.13 1727.66,518.87 \n",
       "  1757.37,620.678 1787.08,620.678 1816.79,595.226 1846.5,595.226 1876.21,595.226 1905.92,595.226 1935.63,569.774 1965.34,595.226 1995.05,569.774 2024.76,569.774 \n",
       "  2054.47,417.062 2084.18,595.226 2113.89,569.774 2143.6,518.87 2173.31,467.966 2203.02,595.226 2232.73,518.87 2262.44,518.87 2292.15,493.418 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip960)\" d=\"\n",
       "M1844.1 1377.32 L2281.37 1377.32 L2281.37 1195.88 L1844.1 1195.88  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip960)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1844.1,1377.32 2281.37,1377.32 2281.37,1195.88 1844.1,1195.88 1844.1,1377.32 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip960)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1867.9,1256.36 2010.66,1256.36 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip960)\" d=\"M 0 0 M2034.46 1239.08 L2063.69 1239.08 L2063.69 1243.01 L2051.42 1243.01 L2051.42 1273.64 L2046.73 1273.64 L2046.73 1243.01 L2034.46 1243.01 L2034.46 1239.08 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2076.22 1251.69 Q2075.5 1251.27 2074.64 1251.09 Q2073.81 1250.88 2072.79 1250.88 Q2069.18 1250.88 2067.23 1253.24 Q2065.31 1255.58 2065.31 1259.98 L2065.31 1273.64 L2061.03 1273.64 L2061.03 1247.71 L2065.31 1247.71 L2065.31 1251.74 Q2066.66 1249.38 2068.81 1248.24 Q2070.96 1247.08 2074.04 1247.08 Q2074.48 1247.08 2075.01 1247.15 Q2075.55 1247.2 2076.19 1247.32 L2076.22 1251.69 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2092.47 1260.6 Q2087.3 1260.6 2085.31 1261.78 Q2083.32 1262.96 2083.32 1265.81 Q2083.32 1268.08 2084.8 1269.42 Q2086.31 1270.74 2088.88 1270.74 Q2092.42 1270.74 2094.55 1268.24 Q2096.7 1265.72 2096.7 1261.55 L2096.7 1260.6 L2092.47 1260.6 M2100.96 1258.84 L2100.96 1273.64 L2096.7 1273.64 L2096.7 1269.7 Q2095.24 1272.06 2093.07 1273.2 Q2090.89 1274.31 2087.74 1274.31 Q2083.76 1274.31 2081.4 1272.08 Q2079.06 1269.84 2079.06 1266.09 Q2079.06 1261.71 2081.98 1259.49 Q2084.92 1257.27 2090.73 1257.27 L2096.7 1257.27 L2096.7 1256.85 Q2096.7 1253.91 2094.76 1252.32 Q2092.84 1250.7 2089.34 1250.7 Q2087.12 1250.7 2085.01 1251.23 Q2082.91 1251.76 2080.96 1252.83 L2080.96 1248.89 Q2083.3 1247.99 2085.5 1247.55 Q2087.7 1247.08 2089.78 1247.08 Q2095.41 1247.08 2098.18 1250 Q2100.96 1252.92 2100.96 1258.84 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2105.43 1247.71 L2109.69 1247.71 L2109.69 1273.64 L2105.43 1273.64 L2105.43 1247.71 M2105.43 1237.62 L2109.69 1237.62 L2109.69 1243.01 L2105.43 1243.01 L2105.43 1237.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2135.71 1257.99 L2135.71 1273.64 L2131.45 1273.64 L2131.45 1258.13 Q2131.45 1254.45 2130.01 1252.62 Q2128.58 1250.79 2125.71 1250.79 Q2122.26 1250.79 2120.27 1252.99 Q2118.28 1255.19 2118.28 1258.98 L2118.28 1273.64 L2113.99 1273.64 L2113.99 1247.71 L2118.28 1247.71 L2118.28 1251.74 Q2119.8 1249.4 2121.86 1248.24 Q2123.95 1247.08 2126.66 1247.08 Q2131.12 1247.08 2133.42 1249.86 Q2135.71 1252.62 2135.71 1257.99 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2140.17 1247.71 L2144.43 1247.71 L2144.43 1273.64 L2140.17 1273.64 L2140.17 1247.71 M2140.17 1237.62 L2144.43 1237.62 L2144.43 1243.01 L2140.17 1243.01 L2140.17 1237.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2170.45 1257.99 L2170.45 1273.64 L2166.19 1273.64 L2166.19 1258.13 Q2166.19 1254.45 2164.76 1252.62 Q2163.32 1250.79 2160.45 1250.79 Q2157 1250.79 2155.01 1252.99 Q2153.02 1255.19 2153.02 1258.98 L2153.02 1273.64 L2148.74 1273.64 L2148.74 1247.71 L2153.02 1247.71 L2153.02 1251.74 Q2154.55 1249.4 2156.61 1248.24 Q2158.69 1247.08 2161.4 1247.08 Q2165.87 1247.08 2168.16 1249.86 Q2170.45 1252.62 2170.45 1257.99 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2191.98 1260.37 Q2191.98 1255.74 2190.06 1253.2 Q2188.16 1250.65 2184.71 1250.65 Q2181.29 1250.65 2179.36 1253.2 Q2177.47 1255.74 2177.47 1260.37 Q2177.47 1264.98 2179.36 1267.52 Q2181.29 1270.07 2184.71 1270.07 Q2188.16 1270.07 2190.06 1267.52 Q2191.98 1264.98 2191.98 1260.37 M2196.24 1270.42 Q2196.24 1277.04 2193.3 1280.26 Q2190.36 1283.5 2184.29 1283.5 Q2182.05 1283.5 2180.06 1283.15 Q2178.07 1282.83 2176.19 1282.13 L2176.19 1277.99 Q2178.07 1279.01 2179.9 1279.49 Q2181.73 1279.98 2183.62 1279.98 Q2187.81 1279.98 2189.9 1277.78 Q2191.98 1275.6 2191.98 1271.18 L2191.98 1269.08 Q2190.66 1271.37 2188.6 1272.5 Q2186.54 1273.64 2183.67 1273.64 Q2178.9 1273.64 2175.98 1270 Q2173.07 1266.37 2173.07 1260.37 Q2173.07 1254.35 2175.98 1250.72 Q2178.9 1247.08 2183.67 1247.08 Q2186.54 1247.08 2188.6 1248.22 Q2190.66 1249.35 2191.98 1251.64 L2191.98 1247.71 L2196.24 1247.71 L2196.24 1270.42 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip960)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1867.9,1316.84 2010.66,1316.84 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip960)\" d=\"M 0 0 M2047.65 1334.12 L2034.46 1299.56 L2039.34 1299.56 L2050.29 1328.65 L2061.26 1299.56 L2066.12 1299.56 L2052.95 1334.12 L2047.65 1334.12 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2078.69 1321.08 Q2073.53 1321.08 2071.54 1322.26 Q2069.55 1323.44 2069.55 1326.29 Q2069.55 1328.56 2071.03 1329.9 Q2072.54 1331.22 2075.11 1331.22 Q2078.65 1331.22 2080.78 1328.72 Q2082.93 1326.2 2082.93 1322.03 L2082.93 1321.08 L2078.69 1321.08 M2087.19 1319.32 L2087.19 1334.12 L2082.93 1334.12 L2082.93 1330.18 Q2081.47 1332.54 2079.3 1333.68 Q2077.12 1334.79 2073.97 1334.79 Q2069.99 1334.79 2067.63 1332.56 Q2065.29 1330.32 2065.29 1326.57 Q2065.29 1322.19 2068.21 1319.97 Q2071.15 1317.75 2076.96 1317.75 L2082.93 1317.75 L2082.93 1317.33 Q2082.93 1314.39 2080.98 1312.8 Q2079.06 1311.18 2075.57 1311.18 Q2073.35 1311.18 2071.24 1311.71 Q2069.13 1312.24 2067.19 1313.31 L2067.19 1309.37 Q2069.53 1308.47 2071.73 1308.03 Q2073.92 1307.56 2076.01 1307.56 Q2081.63 1307.56 2084.41 1310.48 Q2087.19 1313.4 2087.19 1319.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2091.66 1298.1 L2095.92 1298.1 L2095.92 1334.12 L2091.66 1334.12 L2091.66 1298.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2100.38 1308.19 L2104.64 1308.19 L2104.64 1334.12 L2100.38 1334.12 L2100.38 1308.19 M2100.38 1298.1 L2104.64 1298.1 L2104.64 1303.49 L2100.38 1303.49 L2100.38 1298.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2126.17 1312.12 L2126.17 1298.1 L2130.43 1298.1 L2130.43 1334.12 L2126.17 1334.12 L2126.17 1330.23 Q2124.83 1332.54 2122.77 1333.68 Q2120.73 1334.79 2117.86 1334.79 Q2113.16 1334.79 2110.2 1331.04 Q2107.26 1327.29 2107.26 1321.18 Q2107.26 1315.06 2110.2 1311.31 Q2113.16 1307.56 2117.86 1307.56 Q2120.73 1307.56 2122.77 1308.7 Q2124.83 1309.81 2126.17 1312.12 M2111.66 1321.18 Q2111.66 1325.87 2113.58 1328.56 Q2115.52 1331.22 2118.9 1331.22 Q2122.28 1331.22 2124.23 1328.56 Q2126.17 1325.87 2126.17 1321.18 Q2126.17 1316.48 2124.23 1313.81 Q2122.28 1311.13 2118.9 1311.13 Q2115.52 1311.13 2113.58 1313.81 Q2111.66 1316.48 2111.66 1321.18 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2146.68 1321.08 Q2141.52 1321.08 2139.53 1322.26 Q2137.54 1323.44 2137.54 1326.29 Q2137.54 1328.56 2139.02 1329.9 Q2140.52 1331.22 2143.09 1331.22 Q2146.63 1331.22 2148.76 1328.72 Q2150.92 1326.2 2150.92 1322.03 L2150.92 1321.08 L2146.68 1321.08 M2155.17 1319.32 L2155.17 1334.12 L2150.92 1334.12 L2150.92 1330.18 Q2149.46 1332.54 2147.28 1333.68 Q2145.1 1334.79 2141.96 1334.79 Q2137.98 1334.79 2135.61 1332.56 Q2133.28 1330.32 2133.28 1326.57 Q2133.28 1322.19 2136.19 1319.97 Q2139.13 1317.75 2144.94 1317.75 L2150.92 1317.75 L2150.92 1317.33 Q2150.92 1314.39 2148.97 1312.8 Q2147.05 1311.18 2143.55 1311.18 Q2141.33 1311.18 2139.23 1311.71 Q2137.12 1312.24 2135.17 1313.31 L2135.17 1309.37 Q2137.51 1308.47 2139.71 1308.03 Q2141.91 1307.56 2143.99 1307.56 Q2149.62 1307.56 2152.4 1310.48 Q2155.17 1313.4 2155.17 1319.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2163.85 1300.83 L2163.85 1308.19 L2172.63 1308.19 L2172.63 1311.5 L2163.85 1311.5 L2163.85 1325.57 Q2163.85 1328.74 2164.71 1329.65 Q2165.59 1330.55 2168.25 1330.55 L2172.63 1330.55 L2172.63 1334.12 L2168.25 1334.12 Q2163.32 1334.12 2161.45 1332.29 Q2159.57 1330.43 2159.57 1325.57 L2159.57 1311.5 L2156.45 1311.5 L2156.45 1308.19 L2159.57 1308.19 L2159.57 1300.83 L2163.85 1300.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2177.1 1308.19 L2181.35 1308.19 L2181.35 1334.12 L2177.1 1334.12 L2177.1 1308.19 M2177.1 1298.1 L2181.35 1298.1 L2181.35 1303.49 L2177.1 1303.49 L2177.1 1298.1 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2195.87 1311.18 Q2192.44 1311.18 2190.45 1313.86 Q2188.46 1316.52 2188.46 1321.18 Q2188.46 1325.83 2190.43 1328.51 Q2192.42 1331.18 2195.87 1331.18 Q2199.27 1331.18 2201.26 1328.49 Q2203.25 1325.81 2203.25 1321.18 Q2203.25 1316.57 2201.26 1313.88 Q2199.27 1311.18 2195.87 1311.18 M2195.87 1307.56 Q2201.42 1307.56 2204.6 1311.18 Q2207.77 1314.79 2207.77 1321.18 Q2207.77 1327.54 2204.6 1331.18 Q2201.42 1334.79 2195.87 1334.79 Q2190.29 1334.79 2187.12 1331.18 Q2183.97 1327.54 2183.97 1321.18 Q2183.97 1314.79 2187.12 1311.18 Q2190.29 1307.56 2195.87 1307.56 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip960)\" d=\"M 0 0 M2233.78 1318.47 L2233.78 1334.12 L2229.53 1334.12 L2229.53 1318.61 Q2229.53 1314.93 2228.09 1313.1 Q2226.66 1311.27 2223.78 1311.27 Q2220.34 1311.27 2218.35 1313.47 Q2216.35 1315.67 2216.35 1319.46 L2216.35 1334.12 L2212.07 1334.12 L2212.07 1308.19 L2216.35 1308.19 L2216.35 1312.22 Q2217.88 1309.88 2219.94 1308.72 Q2222.03 1307.56 2224.73 1307.56 Q2229.2 1307.56 2231.49 1310.34 Q2233.78 1313.1 2233.78 1318.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(data_schedule, training_losses, label=\"Training\", legend=:bottomright)\n",
    "plot!(data_schedule, valid_losses, label=\"Validation\")\n",
    "xlabel!(\"Number of Training Samples\")\n",
    "ylabel!(\"Classification Accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"Figures/DT_LC\"\n",
    "png(replace(fn,'.' => ','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = true,\n",
       "    merge_purity_threshold = 0.526535995995996,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 14)\u001b[34m @124\u001b[39m"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dt = best.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @560\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @346\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @582\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_Tree = machine(final_dt, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 4, Threshold 2.5\n",
      "L-> Feature 2, Threshold 2.5\n",
      "    L-> Feature 1, Threshold 1.5\n",
      "        L-> Feature 3, Threshold 2.5\n",
      "            L-> Feature 4, Threshold 1.5\n",
      "                L-> Feature 3, Threshold 1.5\n",
      "                    L-> 1 : 1/1\n",
      "                    R-> Feature 2, Threshold 1.5\n",
      "                        L-> 3 : 1/1\n",
      "                        R-> 1 : 1/1\n",
      "                R-> 3 : 3/3\n",
      "            R-> 3 : 11/11\n",
      "        R-> Feature 3, Threshold 2.5\n",
      "            L-> Feature 1, Threshold 2.5\n",
      "                L-> Feature 4, Threshold 1.5\n",
      "                    L-> Feature 3, Threshold 1.5\n",
      "                        L-> 2 : 2/2\n",
      "                        R-> Feature 2, Threshold 1.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> 2 : 1/1\n",
      "                    R-> 1 : 2/2\n",
      "                R-> Feature 2, Threshold 1.5\n",
      "                    L-> Feature 3, Threshold 1.5\n",
      "                        L-> 2 : 4/4\n",
      "                        R-> Feature 1, Threshold 4.5\n",
      "                            L-> Feature 4, Threshold 1.5\n",
      "                                L-> 2 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> 2 : 1/1\n",
      "                    R-> 2 : 9/9\n",
      "            R-> Feature 2, Threshold 1.5\n",
      "                L-> Feature 4, Threshold 1.5\n",
      "                    L-> Feature 1, Threshold 3.5\n",
      "                        L-> Feature 3, Threshold 3.5\n",
      "                            L-> Feature 1, Threshold 2.5\n",
      "                                L-> 3 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> 3 : 4/4\n",
      "                        R-> 2 : 3/4\n",
      "                    R-> 3 : 9/9\n",
      "                R-> Feature 3, Threshold 4.5\n",
      "                    L-> Feature 1, Threshold 4.5\n",
      "                        L-> Feature 4, Threshold 1.5\n",
      "                            L-> Feature 1, Threshold 2.5\n",
      "                                L-> Feature 3, Threshold 3.5\n",
      "                                    L-> 2 : 1/1\n",
      "                                    R-> 1 : 1/1\n",
      "                                R-> 2 : 3/3\n",
      "                            R-> Feature 3, Threshold 3.5\n",
      "                                L-> Feature 1, Threshold 3.5\n",
      "                                    L-> 1 : 1/1\n",
      "                                    R-> 2 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                        R-> 2 : 3/3\n",
      "                    R-> 3 : 3/4\n",
      "    R-> Feature 1, Threshold 2.5\n",
      "        L-> Feature 3, Threshold 2.5\n",
      "            L-> Feature 1, Threshold 1.5\n",
      "                L-> Feature 3, Threshold 1.5\n",
      "                    L-> 2 : 5/5\n",
      "                    R-> Feature 4, Threshold 1.5\n",
      "                        L-> 2 : 2/2\n",
      "                        R-> Feature 2, Threshold 4.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> 2 : 1/1\n",
      "                R-> 2 : 11/11\n",
      "            R-> Feature 1, Threshold 1.5\n",
      "                L-> Feature 4, Threshold 1.5\n",
      "                    L-> Feature 2, Threshold 4.5\n",
      "                        L-> 3 : 3/4\n",
      "                        R-> 2 : 2/3\n",
      "                    R-> 3 : 6/6\n",
      "                R-> Feature 4, Threshold 1.5\n",
      "                    L-> 2 : 8/8\n",
      "                    R-> Feature 2, Threshold 3.5\n",
      "                        L-> 3 : 2/3\n",
      "                        R-> 2 : 3/4\n",
      "        R-> 2 : 66/66\n",
      "R-> Feature 1, Threshold 2.5\n",
      "    L-> Feature 3, Threshold 2.5\n",
      "        L-> Feature 2, Threshold 3.5\n",
      "            L-> Feature 1, Threshold 1.5\n",
      "                L-> 3 : 13/13\n",
      "                R-> Feature 2, Threshold 1.5\n",
      "                    L-> 3 : 5/5\n",
      "                    R-> Feature 4, Threshold 4.5\n",
      "                        L-> Feature 3, Threshold 1.5\n",
      "                            L-> Feature 4, Threshold 3.5\n",
      "                                L-> 2 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> Feature 2, Threshold 2.5\n",
      "                                L-> 3 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                        R-> 3 : 2/2\n",
      "            R-> Feature 1, Threshold 1.5\n",
      "                L-> 3 : 5/7\n",
      "                R-> Feature 2, Threshold 4.5\n",
      "                    L-> Feature 4, Threshold 4.5\n",
      "                        L-> 1 : 1/1\n",
      "                        R-> Feature 3, Threshold 1.5\n",
      "                            L-> 2 : 1/1\n",
      "                            R-> 3 : 1/1\n",
      "                    R-> 2 : 4/4\n",
      "        R-> Feature 2, Threshold 4.5\n",
      "            L-> 3 : 47/47\n",
      "            R-> Feature 4, Threshold 3.5\n",
      "                L-> Feature 3, Threshold 3.5\n",
      "                    L-> Feature 1, Threshold 1.5\n",
      "                        L-> 3 : 1/1\n",
      "                        R-> 2 : 1/1\n",
      "                    R-> 3 : 3/3\n",
      "                R-> 3 : 8/8\n",
      "    R-> Feature 3, Threshold 2.5\n",
      "        L-> Feature 2, Threshold 1.5\n",
      "            L-> Feature 3, Threshold 1.5\n",
      "                L-> Feature 1, Threshold 3.5\n",
      "                    L-> 3 : 2/3\n",
      "                    R-> Feature 4, Threshold 3.5\n",
      "                        L-> 2 : 1/1\n",
      "                        R-> Feature 4, Threshold 4.5\n",
      "                            L-> Feature 1, Threshold 4.5\n",
      "                                L-> 1 : 1/1\n",
      "                                R-> 2 : 1/1\n",
      "                            R-> 1 : 1/1\n",
      "                R-> 3 : 7/7\n",
      "            R-> Feature 3, Threshold 1.5\n",
      "                L-> 2 : 25/25\n",
      "                R-> Feature 2, Threshold 2.5\n",
      "                    L-> Feature 4, Threshold 3.5\n",
      "                        L-> 2 : 2/2\n",
      "                        R-> Feature 1, Threshold 3.5\n",
      "                            L-> 3 : 1/1\n",
      "                            R-> Feature 4, Threshold 4.5\n",
      "                                L-> 1 : 1/1\n",
      "                                R-> 3 : 1/1\n",
      "                    R-> 2 : 14/15\n",
      "        R-> Feature 2, Threshold 2.5\n",
      "            L-> 3 : 35/36\n",
      "            R-> Feature 4, Threshold 3.5\n",
      "                L-> Feature 3, Threshold 4.5\n",
      "                    L-> Feature 1, Threshold 3.5\n",
      "                        L-> Feature 2, Threshold 4.5\n",
      "                            L-> Feature 3, Threshold 3.5\n",
      "                                L-> Feature 2, Threshold 3.5\n",
      "                                    L-> 1 : 1/1\n",
      "                                    R-> 2 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> 2 : 2/2\n",
      "                        R-> 2 : 9/9\n",
      "                    R-> Feature 2, Threshold 3.5\n",
      "                        L-> 3 : 2/3\n",
      "                        R-> Feature 1, Threshold 3.5\n",
      "                            L-> Feature 2, Threshold 4.5\n",
      "                                L-> 3 : 1/1\n",
      "                                R-> 1 : 1/1\n",
      "                            R-> 2 : 2/2\n",
      "                R-> Feature 1, Threshold 3.5\n",
      "                    L-> Feature 3, Threshold 3.5\n",
      "                        L-> Feature 4, Threshold 4.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> 3 : 1/1\n",
      "                        R-> 3 : 11/11\n",
      "                    R-> Feature 3, Threshold 3.5\n",
      "                        L-> Feature 2, Threshold 3.5\n",
      "                            L-> Feature 1, Threshold 4.5\n",
      "                                L-> Feature 4, Threshold 4.5\n",
      "                                    L-> 1 : 1/1\n",
      "                                    R-> 3 : 1/1\n",
      "                                R-> Feature 4, Threshold 4.5\n",
      "                                    L-> 2 : 1/1\n",
      "                                    R-> 1 : 1/1\n",
      "                            R-> 2 : 4/4\n",
      "                        R-> Feature 2, Threshold 3.5\n",
      "                            L-> 3 : 6/6\n",
      "                            R-> Feature 2, Threshold 4.5\n",
      "                                L-> Feature 3, Threshold 4.5\n",
      "                                    L-> 1 : 2/2\n",
      "                                    R-> Feature 1, Threshold 4.5\n",
      "                                        L-> 3 : 1/1\n",
      "                                        R-> Feature 4, Threshold 4.5\n",
      "                                            L-> 1 : 1/1\n",
      "                                            R-> 3 : 1/1\n",
      "                                R-> Feature 1, Threshold 4.5\n",
      "                                    L-> Feature 4, Threshold 4.5\n",
      "                                        L-> 1 : 1/1\n",
      "                                        R-> 3 : 1/1\n",
      "                                    R-> Feature 3, Threshold 4.5\n",
      "                                        L-> 2 : 1/1\n",
      "                                        R-> Feature 4, Threshold 4.5\n",
      "                                            L-> 2 : 1/1\n",
      "                                            R-> 1 : 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{DecisionTreeClassifier} @560\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @560\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @346\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @582\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(Final_Tree, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tree = Decision Tree\n",
       "Leaves: 96\n",
       "Depth:  12,\n",
       " encoding = Dict{CategoricalValue{String,UInt32},UInt32}(\"B\" => 0x00000001,\"L\" => 0x00000002,\"R\" => 0x00000003),)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_params(Final_Tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = MLJ.predict(Final_Tree, X[test,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.929007823916347"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(ŷ, y[test]) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7700534759358288"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(ŷ, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The classes are un-ordered,\n",
      "│ using order: [\"B\", \"L\", \"R\"].\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "              ┌─────────────────────────────────────────┐\n",
       "              │              Ground Truth               │\n",
       "┌─────────────┼─────────────┬─────────────┬─────────────┤\n",
       "│  Predicted  │      B      │      L      │      R      │\n",
       "├─────────────┼─────────────┼─────────────┼─────────────┤\n",
       "│      B      │      0      │     10      │     11      │\n",
       "├─────────────┼─────────────┼─────────────┼─────────────┤\n",
       "│      L      │      6      │     71      │      2      │\n",
       "├─────────────┼─────────────┼─────────────┼─────────────┤\n",
       "│      R      │      9      │      5      │     73      │\n",
       "└─────────────┴─────────────┴─────────────┴─────────────┘\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = confusion_matrix(mode.(ŷ), y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(6+9)/(6+9+0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19767441860465115"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(12+5)/(12+5+69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16279069767441862"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(11+3)/(11+3+72)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6067956130401434"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcc(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mD\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mT\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mC\u001b[22m\u001b[0m\u001b[1ml\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mf\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "DecisionTreeClassifer(; kwargs...)\n",
       "\n",
       "CART decision tree classifier.\n",
       "\n",
       "Inputs are tables with ordinal columns. That is, the element scitype of each column can be \\texttt{Continuous}, \\texttt{Count} or \\texttt{OrderedFactor}. Predictions are probabilistic, but uncalibrated.\n",
       "\n",
       "Instead of predicting the mode class at each leaf, a \\texttt{UnivariateFinite} distribution is fit to the leaf training classes, with smoothing controlled by an additional hyperparameter \\texttt{pdf\\_smoothing}: If \\texttt{n} is the number of observed classes, then each class probability is replaced by \\texttt{pdf\\_smoothing/n}, if it falls below that ratio, and the resulting vector of probabilities is renormalized. Smoothing is only applied to classes actually observed in training. Unseen classes retain zero-probability predictions.\n",
       "\n",
       "To visualize the fitted tree in the REPL, set \\texttt{verbosity=2} when fitting, or call \\texttt{report(mach).print\\_tree(display\\_depth)} where \\texttt{mach} is the fitted machine, and \\texttt{display\\_depth} the desired depth. Interpretting the results will require a knowledge of the internal integer encodings of classes, which are given in \\texttt{fitted\\_params(mach)} (which also stores the raw learned tree object from the DecisionTree.jl algorithm).\n",
       "\n",
       "\\subsection{Hyperparameters}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{max\\_depth=-1}:          max depth of the decision tree (-1=any)\n",
       "\n",
       "\n",
       "\\item \\texttt{min\\_samples\\_leaf=1}:    max number of samples each leaf needs to have\n",
       "\n",
       "\n",
       "\\item \\texttt{min\\_samples\\_split=2}:   min number of samples needed for a split\n",
       "\n",
       "\n",
       "\\item \\texttt{min\\_purity\\_increase=0}: min purity needed for a split\n",
       "\n",
       "\n",
       "\\item \\texttt{n\\_subfeatures=0}: number of features to select at random (0 for all, -1 for square root of number of features)\n",
       "\n",
       "\n",
       "\\item \\texttt{post\\_prune=false}:      set to \\texttt{true} for post-fit pruning\n",
       "\n",
       "\n",
       "\\item \\texttt{merge\\_purity\\_threshold=1.0}:  (post-pruning) merge leaves having \\texttt{>=thresh}                          combined purity\n",
       "\n",
       "\n",
       "\\item \\texttt{pdf\\_smoothing=0.0}:     threshold for smoothing the predicted scores\n",
       "\n",
       "\n",
       "\\item \\texttt{display\\_depth=5}:       max depth to show when displaying the tree\n",
       "\n",
       "\\end{itemize}\n"
      ],
      "text/markdown": [
       "DecisionTreeClassifer(; kwargs...)\n",
       "\n",
       "CART decision tree classifier.\n",
       "\n",
       "Inputs are tables with ordinal columns. That is, the element scitype of each column can be `Continuous`, `Count` or `OrderedFactor`. Predictions are probabilistic, but uncalibrated.\n",
       "\n",
       "Instead of predicting the mode class at each leaf, a `UnivariateFinite` distribution is fit to the leaf training classes, with smoothing controlled by an additional hyperparameter `pdf_smoothing`: If `n` is the number of observed classes, then each class probability is replaced by `pdf_smoothing/n`, if it falls below that ratio, and the resulting vector of probabilities is renormalized. Smoothing is only applied to classes actually observed in training. Unseen classes retain zero-probability predictions.\n",
       "\n",
       "To visualize the fitted tree in the REPL, set `verbosity=2` when fitting, or call `report(mach).print_tree(display_depth)` where `mach` is the fitted machine, and `display_depth` the desired depth. Interpretting the results will require a knowledge of the internal integer encodings of classes, which are given in `fitted_params(mach)` (which also stores the raw learned tree object from the DecisionTree.jl algorithm).\n",
       "\n",
       "## Hyperparameters\n",
       "\n",
       "  * `max_depth=-1`:          max depth of the decision tree (-1=any)\n",
       "  * `min_samples_leaf=1`:    max number of samples each leaf needs to have\n",
       "  * `min_samples_split=2`:   min number of samples needed for a split\n",
       "  * `min_purity_increase=0`: min purity needed for a split\n",
       "  * `n_subfeatures=0`: number of features to select at random (0 for all, -1 for square root of number of features)\n",
       "  * `post_prune=false`:      set to `true` for post-fit pruning\n",
       "  * `merge_purity_threshold=1.0`:  (post-pruning) merge leaves having `>=thresh`                          combined purity\n",
       "  * `pdf_smoothing=0.0`:     threshold for smoothing the predicted scores\n",
       "  * `display_depth=5`:       max depth to show when displaying the tree\n"
      ],
      "text/plain": [
       "  DecisionTreeClassifer(; kwargs...)\n",
       "\n",
       "  CART decision tree classifier.\n",
       "\n",
       "  Inputs are tables with ordinal columns. That is, the element scitype of each\n",
       "  column can be \u001b[36mContinuous\u001b[39m, \u001b[36mCount\u001b[39m or \u001b[36mOrderedFactor\u001b[39m. Predictions are\n",
       "  probabilistic, but uncalibrated.\n",
       "\n",
       "  Instead of predicting the mode class at each leaf, a \u001b[36mUnivariateFinite\u001b[39m\n",
       "  distribution is fit to the leaf training classes, with smoothing controlled\n",
       "  by an additional hyperparameter \u001b[36mpdf_smoothing\u001b[39m: If \u001b[36mn\u001b[39m is the number of\n",
       "  observed classes, then each class probability is replaced by\n",
       "  \u001b[36mpdf_smoothing/n\u001b[39m, if it falls below that ratio, and the resulting vector of\n",
       "  probabilities is renormalized. Smoothing is only applied to classes actually\n",
       "  observed in training. Unseen classes retain zero-probability predictions.\n",
       "\n",
       "  To visualize the fitted tree in the REPL, set \u001b[36mverbosity=2\u001b[39m when fitting, or\n",
       "  call \u001b[36mreport(mach).print_tree(display_depth)\u001b[39m where \u001b[36mmach\u001b[39m is the fitted\n",
       "  machine, and \u001b[36mdisplay_depth\u001b[39m the desired depth. Interpretting the results will\n",
       "  require a knowledge of the internal integer encodings of classes, which are\n",
       "  given in \u001b[36mfitted_params(mach)\u001b[39m (which also stores the raw learned tree object\n",
       "  from the DecisionTree.jl algorithm).\n",
       "\n",
       "\u001b[1m  Hyperparameters\u001b[22m\n",
       "\u001b[1m  =================\u001b[22m\n",
       "\n",
       "    •    \u001b[36mmax_depth=-1\u001b[39m: max depth of the decision tree (-1=any)\n",
       "\n",
       "    •    \u001b[36mmin_samples_leaf=1\u001b[39m: max number of samples each leaf needs to have\n",
       "\n",
       "    •    \u001b[36mmin_samples_split=2\u001b[39m: min number of samples needed for a split\n",
       "\n",
       "    •    \u001b[36mmin_purity_increase=0\u001b[39m: min purity needed for a split\n",
       "\n",
       "    •    \u001b[36mn_subfeatures=0\u001b[39m: number of features to select at random (0 for\n",
       "        all, -1 for square root of number of features)\n",
       "\n",
       "    •    \u001b[36mpost_prune=false\u001b[39m: set to \u001b[36mtrue\u001b[39m for post-fit pruning\n",
       "\n",
       "    •    \u001b[36mmerge_purity_threshold=1.0\u001b[39m: (post-pruning) merge leaves having\n",
       "        \u001b[36m>=thresh\u001b[39m combined purity\n",
       "\n",
       "    •    \u001b[36mpdf_smoothing=0.0\u001b[39m: threshold for smoothing the predicted scores\n",
       "\n",
       "    •    \u001b[36mdisplay_depth=5\u001b[39m: max depth to show when displaying the tree"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
