{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using Distances.colwise in module Main conflicts with an existing identifier.\n"
     ]
    }
   ],
   "source": [
    "using DataFrames\n",
    "using CSV\n",
    "using MLJ\n",
    "using Plots\n",
    "using StatsBase\n",
    "using Distances\n",
    "\n",
    "include(\"../../lib.jl\")\n",
    "\n",
    "ENV[\"LINES\"]=30;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Class</th><th> Instance</th><th>                       Nucleotide_Sequence</th></tr><tr><th></th><th>String</th><th>String</th><th>String</th></tr></thead><tbody><p>3,190 rows × 3 columns</p><tr><th>1</th><td>EI</td><td>    ATRINS-DONOR-521</td><td>               CCAGCTGCATCACAGGAGGCCAGCGAGCAGGTCTGTTCCAAGGGCCTTCGAGCCAGTCTG</td></tr><tr><th>2</th><td>EI</td><td>    ATRINS-DONOR-905</td><td>               AGACCCGCCGGGAGGCGGAGGACCTGCAGGGTGAGCCCCACCGCCCCTCCGTGCCCCCGC</td></tr><tr><th>3</th><td>EI</td><td>    BABAPOE-DONOR-30</td><td>               GAGGTGAAGGACGTCCTTCCCCAGGAGCCGGTGAGAAGCGCAGTCGGGGGCACGGGGATG</td></tr><tr><th>4</th><td>EI</td><td>    BABAPOE-DONOR-867</td><td>              GGGCTGCGTTGCTGGTCACATTCCTGGCAGGTATGGGGCGGGGCTTGCTCGGTTTTCCCC</td></tr><tr><th>5</th><td>EI</td><td>    BABAPOE-DONOR-2817</td><td>             GCTCAGCCCCCAGGTCACCCAGGAACTGACGTGAGTGTCCCCATCCCGGCCCTTGACCCT</td></tr><tr><th>6</th><td>EI</td><td>    CHPIGECA-DONOR-378</td><td>             CAGACTGGGTGGACAACAAAACCTTCAGCGGTAAGAGAGGGCCAAGCTCAGAGACCACAG</td></tr><tr><th>7</th><td>EI</td><td>    CHPIGECA-DONOR-903</td><td>             CCTTTGAGGACAGCACCAAGAAGTGTGCAGGTACGTTCCCACCTGCCCTGGTGGCCGCCA</td></tr><tr><th>8</th><td>EI</td><td>    CHPIGECA-DONOR-1313</td><td>            CCCTCGTGCGGTCCACGACCAAGACCAGCGGTGAGCCACGGGCAGGCCGGGGTCGTGGGG</td></tr><tr><th>9</th><td>EI</td><td>    GCRHBBA1-DONOR-1260</td><td>            TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTCCTGTCCCTGCTCCAGTCC</td></tr><tr><th>10</th><td>EI</td><td>    GCRHBBA1-DONOR-1590</td><td>            AAGCTGACAGTGGACCCGGTCAACTTCAAGGTGAGCCAGGAGTCGGGTGGGAGGGTGAGA</td></tr><tr><th>11</th><td>EI</td><td>    GCRHBBA6-DONOR-461</td><td>             TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTGGTATCCCTGCTGCCAGTC</td></tr><tr><th>12</th><td>EI</td><td>    GCRHBBA6-DONOR-795</td><td>             AAGCTGAGAGTGGACCCTGTCAACTTCAAGGTGAGCCACCAGTCGGGTGGGGAGGGTGAG</td></tr><tr><th>13</th><td>EI</td><td>    GIBHBGGL-DONOR-2278</td><td>            GGAAGATGCTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG</td></tr><tr><th>14</th><td>EI</td><td>    GIBHBGGL-DONOR-2624</td><td>            AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG</td></tr><tr><th>15</th><td>EI</td><td>    GIBHBGGL-DONOR-7198</td><td>            GGAAGATGTTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG</td></tr><tr><th>16</th><td>EI</td><td>    GIBHBGGL-DONOR-7544</td><td>            AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG</td></tr><tr><th>17</th><td>EI</td><td>    HUMA1ATP-DONOR-1972</td><td>            GGCACCACCACTGACCTGGGACAGTGAATCGTAAGTATGCCTTTCACTGCGAGGGGTTCT</td></tr><tr><th>18</th><td>EI</td><td>    HUMA1ATP-DONOR-7932</td><td>            TTGCTCTGGTGAATTACATCTTCTTTAAAGGTAAGGTTGCTCAACCAGCCTGAGCTGTTT</td></tr><tr><th>19</th><td>EI</td><td>    HUMA1ATP-DONOR-9653</td><td>            CACCAAGTTCCTGGAAAATGAAGACAGAAGGTGATTCCCCAACCTGAGGGTGACCAAGAA</td></tr><tr><th>20</th><td>EI</td><td>    HUMA1ATP-DONOR-11057</td><td>           ACAGAGGAGGCACCCCTGAAGCTCTCCAAGGTGAGATCACCCTGACGACCTTGTTGCACC</td></tr><tr><th>21</th><td>EI</td><td>    HUMA1GLY2-DONOR-1693</td><td>           GTGCCCATCACCAACGCCACCCTGGACCGGGTGAGTGCCTGGGCTAGCCCTGTCCTGAGC</td></tr><tr><th>22</th><td>EI</td><td>    HUMA1GLY2-DONOR-2251</td><td>           CACGATCTTTCTCAGAGAGTACCAGACCCGGTGAGAGCCCCCATTCCAATGCACCCCCGA</td></tr><tr><th>23</th><td>EI</td><td>    HUMA1GLY2-DONOR-2540</td><td>           AGCGGGAGAATGGGACCGTCTCCAGATACGGTGAGGGCCAGCCCTCAGGCAGGAGGGTTC</td></tr><tr><th>24</th><td>EI</td><td>    HUMA1GLY2-DONOR-3352</td><td>           ATGAGAAGAACTGGGGGCTGTCTTTCTATGGTAGGCATGCTTAGCAGCCCCAAACTCATG</td></tr><tr><th>25</th><td>EI</td><td>    HUMA1GLY2-DONOR-3606</td><td>           TCAGATGTCATGTACACCGACTGGAAAAAGGTAAACGCAAGGGATTGGACATTGCCCACC</td></tr><tr><th>26</th><td>EI</td><td>    HUMACCYBA-DONOR-289</td><td>            GATCCGCCGCCCGTCCACACCCGCCGCCAGGTAAGCCCGGCCAGCCGACCGGGGCATGCG</td></tr><tr><th>27</th><td>EI</td><td>    HUMACCYBA-DONOR-1250</td><td>           CCCTCCATCGTGGGGCGCCCCAGGCACCAGGTAGGGGAGCTGGCTGGGTGGGGCAGCCCC</td></tr><tr><th>28</th><td>EI</td><td>    HUMACCYBA-DONOR-1624</td><td>           CCCAAGGCCAACCGCGAGAAGATGACCCAGGTGAGTGGCCCGCTACCTCTTCTGGTGGCC</td></tr><tr><th>29</th><td>EI</td><td>    HUMACCYBA-DONOR-2504</td><td>           CTGAGGCACTCTTCCAGCCTTCCTTCCTGGGTGAGTGGAGACTGTCTCCCGGCTCTGCCT</td></tr><tr><th>30</th><td>EI</td><td>    HUMACCYBA-DONOR-2781</td><td>           GCCCTGGCACCCAGCACAATGAAGATCAAGGTGGGTGTCTTTCCTGCCTGAGCTGACCTG</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& Class &  Instance &                        Nucleotide\\_Sequence\\\\\n",
       "\t\\hline\n",
       "\t& String & String & String\\\\\n",
       "\t\\hline\n",
       "\t1 & EI &     ATRINS-DONOR-521 &                CCAGCTGCATCACAGGAGGCCAGCGAGCAGGTCTGTTCCAAGGGCCTTCGAGCCAGTCTG \\\\\n",
       "\t2 & EI &     ATRINS-DONOR-905 &                AGACCCGCCGGGAGGCGGAGGACCTGCAGGGTGAGCCCCACCGCCCCTCCGTGCCCCCGC \\\\\n",
       "\t3 & EI &     BABAPOE-DONOR-30 &                GAGGTGAAGGACGTCCTTCCCCAGGAGCCGGTGAGAAGCGCAGTCGGGGGCACGGGGATG \\\\\n",
       "\t4 & EI &     BABAPOE-DONOR-867 &               GGGCTGCGTTGCTGGTCACATTCCTGGCAGGTATGGGGCGGGGCTTGCTCGGTTTTCCCC \\\\\n",
       "\t5 & EI &     BABAPOE-DONOR-2817 &              GCTCAGCCCCCAGGTCACCCAGGAACTGACGTGAGTGTCCCCATCCCGGCCCTTGACCCT \\\\\n",
       "\t6 & EI &     CHPIGECA-DONOR-378 &              CAGACTGGGTGGACAACAAAACCTTCAGCGGTAAGAGAGGGCCAAGCTCAGAGACCACAG \\\\\n",
       "\t7 & EI &     CHPIGECA-DONOR-903 &              CCTTTGAGGACAGCACCAAGAAGTGTGCAGGTACGTTCCCACCTGCCCTGGTGGCCGCCA \\\\\n",
       "\t8 & EI &     CHPIGECA-DONOR-1313 &             CCCTCGTGCGGTCCACGACCAAGACCAGCGGTGAGCCACGGGCAGGCCGGGGTCGTGGGG \\\\\n",
       "\t9 & EI &     GCRHBBA1-DONOR-1260 &             TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTCCTGTCCCTGCTCCAGTCC \\\\\n",
       "\t10 & EI &     GCRHBBA1-DONOR-1590 &             AAGCTGACAGTGGACCCGGTCAACTTCAAGGTGAGCCAGGAGTCGGGTGGGAGGGTGAGA \\\\\n",
       "\t11 & EI &     GCRHBBA6-DONOR-461 &              TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTGGTATCCCTGCTGCCAGTC \\\\\n",
       "\t12 & EI &     GCRHBBA6-DONOR-795 &              AAGCTGAGAGTGGACCCTGTCAACTTCAAGGTGAGCCACCAGTCGGGTGGGGAGGGTGAG \\\\\n",
       "\t13 & EI &     GIBHBGGL-DONOR-2278 &             GGAAGATGCTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG \\\\\n",
       "\t14 & EI &     GIBHBGGL-DONOR-2624 &             AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG \\\\\n",
       "\t15 & EI &     GIBHBGGL-DONOR-7198 &             GGAAGATGTTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG \\\\\n",
       "\t16 & EI &     GIBHBGGL-DONOR-7544 &             AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG \\\\\n",
       "\t17 & EI &     HUMA1ATP-DONOR-1972 &             GGCACCACCACTGACCTGGGACAGTGAATCGTAAGTATGCCTTTCACTGCGAGGGGTTCT \\\\\n",
       "\t18 & EI &     HUMA1ATP-DONOR-7932 &             TTGCTCTGGTGAATTACATCTTCTTTAAAGGTAAGGTTGCTCAACCAGCCTGAGCTGTTT \\\\\n",
       "\t19 & EI &     HUMA1ATP-DONOR-9653 &             CACCAAGTTCCTGGAAAATGAAGACAGAAGGTGATTCCCCAACCTGAGGGTGACCAAGAA \\\\\n",
       "\t20 & EI &     HUMA1ATP-DONOR-11057 &            ACAGAGGAGGCACCCCTGAAGCTCTCCAAGGTGAGATCACCCTGACGACCTTGTTGCACC \\\\\n",
       "\t21 & EI &     HUMA1GLY2-DONOR-1693 &            GTGCCCATCACCAACGCCACCCTGGACCGGGTGAGTGCCTGGGCTAGCCCTGTCCTGAGC \\\\\n",
       "\t22 & EI &     HUMA1GLY2-DONOR-2251 &            CACGATCTTTCTCAGAGAGTACCAGACCCGGTGAGAGCCCCCATTCCAATGCACCCCCGA \\\\\n",
       "\t23 & EI &     HUMA1GLY2-DONOR-2540 &            AGCGGGAGAATGGGACCGTCTCCAGATACGGTGAGGGCCAGCCCTCAGGCAGGAGGGTTC \\\\\n",
       "\t24 & EI &     HUMA1GLY2-DONOR-3352 &            ATGAGAAGAACTGGGGGCTGTCTTTCTATGGTAGGCATGCTTAGCAGCCCCAAACTCATG \\\\\n",
       "\t25 & EI &     HUMA1GLY2-DONOR-3606 &            TCAGATGTCATGTACACCGACTGGAAAAAGGTAAACGCAAGGGATTGGACATTGCCCACC \\\\\n",
       "\t26 & EI &     HUMACCYBA-DONOR-289 &             GATCCGCCGCCCGTCCACACCCGCCGCCAGGTAAGCCCGGCCAGCCGACCGGGGCATGCG \\\\\n",
       "\t27 & EI &     HUMACCYBA-DONOR-1250 &            CCCTCCATCGTGGGGCGCCCCAGGCACCAGGTAGGGGAGCTGGCTGGGTGGGGCAGCCCC \\\\\n",
       "\t28 & EI &     HUMACCYBA-DONOR-1624 &            CCCAAGGCCAACCGCGAGAAGATGACCCAGGTGAGTGGCCCGCTACCTCTTCTGGTGGCC \\\\\n",
       "\t29 & EI &     HUMACCYBA-DONOR-2504 &            CTGAGGCACTCTTCCAGCCTTCCTTCCTGGGTGAGTGGAGACTGTCTCCCGGCTCTGCCT \\\\\n",
       "\t30 & EI &     HUMACCYBA-DONOR-2781 &            GCCCTGGCACCCAGCACAATGAAGATCAAGGTGGGTGTCTTTCCTGCCTGAGCTGACCTG \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "3190×3 DataFrame. Omitted printing of 1 columns\n",
       "│ Row  │ Class  │  Instance               │\n",
       "│      │ \u001b[90mString\u001b[39m │ \u001b[90mString\u001b[39m                  │\n",
       "├──────┼────────┼─────────────────────────┤\n",
       "│ 1    │ EI     │     ATRINS-DONOR-521    │\n",
       "│ 2    │ EI     │     ATRINS-DONOR-905    │\n",
       "│ 3    │ EI     │     BABAPOE-DONOR-30    │\n",
       "│ 4    │ EI     │     BABAPOE-DONOR-867   │\n",
       "│ 5    │ EI     │     BABAPOE-DONOR-2817  │\n",
       "│ 6    │ EI     │     CHPIGECA-DONOR-378  │\n",
       "│ 7    │ EI     │     CHPIGECA-DONOR-903  │\n",
       "│ 8    │ EI     │     CHPIGECA-DONOR-1313 │\n",
       "│ 9    │ EI     │     GCRHBBA1-DONOR-1260 │\n",
       "│ 10   │ EI     │     GCRHBBA1-DONOR-1590 │\n",
       "⋮\n",
       "│ 3180 │ N      │      MNKHBPSBD-NEG-961  │\n",
       "│ 3181 │ N      │      ORAHBA01-NEG-121   │\n",
       "│ 3182 │ N      │      ORAHBBE-NEG-2581   │\n",
       "│ 3183 │ N      │      ORAHBBPSE-NEG-2101 │\n",
       "│ 3184 │ N      │      ORAHBBPSE-NEG-6661 │\n",
       "│ 3185 │ N      │      ORAHBG2F-NEG-181   │\n",
       "│ 3186 │ N      │      ORAHBPSBD-NEG-2881 │\n",
       "│ 3187 │ N      │      ORAINVOL-NEG-2161  │\n",
       "│ 3188 │ N      │      ORARGIT-NEG-241    │\n",
       "│ 3189 │ N      │      TARHBB-NEG-541     │\n",
       "│ 3190 │ N      │      TARHBD-NEG-1981    │"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = CSV.read(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>x1</th><th>x2</th><th>x3</th><th>x4</th><th>x5</th><th>x6</th><th>x7</th><th>x8</th><th>x9</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>3,190 rows × 60 columns (omitted printing of 51 columns)</p><tr><th>1</th><td>67.0</td><td>67.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>65.0</td></tr><tr><th>2</th><td>65.0</td><td>71.0</td><td>65.0</td><td>67.0</td><td>67.0</td><td>67.0</td><td>71.0</td><td>67.0</td><td>67.0</td></tr><tr><th>3</th><td>71.0</td><td>65.0</td><td>71.0</td><td>71.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>65.0</td><td>71.0</td></tr><tr><th>4</th><td>71.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>71.0</td><td>84.0</td></tr><tr><th>5</th><td>71.0</td><td>67.0</td><td>84.0</td><td>67.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>67.0</td><td>67.0</td></tr><tr><th>6</th><td>67.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>71.0</td><td>71.0</td></tr><tr><th>7</th><td>67.0</td><td>67.0</td><td>84.0</td><td>84.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>71.0</td><td>71.0</td></tr><tr><th>8</th><td>67.0</td><td>67.0</td><td>67.0</td><td>84.0</td><td>67.0</td><td>71.0</td><td>84.0</td><td>71.0</td><td>67.0</td></tr><tr><th>9</th><td>84.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>71.0</td><td>65.0</td><td>67.0</td><td>84.0</td><td>65.0</td></tr><tr><th>10</th><td>65.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>67.0</td><td>65.0</td></tr><tr><th>11</th><td>84.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>71.0</td><td>65.0</td><td>67.0</td><td>84.0</td><td>65.0</td></tr><tr><th>12</th><td>65.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>71.0</td><td>65.0</td></tr><tr><th>13</th><td>71.0</td><td>71.0</td><td>65.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>84.0</td><td>71.0</td><td>67.0</td></tr><tr><th>14</th><td>65.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>65.0</td><td>84.0</td></tr><tr><th>15</th><td>71.0</td><td>71.0</td><td>65.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>84.0</td><td>71.0</td><td>84.0</td></tr><tr><th>16</th><td>65.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>65.0</td><td>84.0</td></tr><tr><th>17</th><td>71.0</td><td>71.0</td><td>67.0</td><td>65.0</td><td>67.0</td><td>67.0</td><td>65.0</td><td>67.0</td><td>67.0</td></tr><tr><th>18</th><td>84.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>71.0</td></tr><tr><th>19</th><td>67.0</td><td>65.0</td><td>67.0</td><td>67.0</td><td>65.0</td><td>65.0</td><td>71.0</td><td>84.0</td><td>84.0</td></tr><tr><th>20</th><td>65.0</td><td>67.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>71.0</td><td>71.0</td><td>65.0</td><td>71.0</td></tr><tr><th>21</th><td>71.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>67.0</td><td>67.0</td><td>65.0</td><td>84.0</td><td>67.0</td></tr><tr><th>22</th><td>67.0</td><td>65.0</td><td>67.0</td><td>71.0</td><td>65.0</td><td>84.0</td><td>67.0</td><td>84.0</td><td>84.0</td></tr><tr><th>23</th><td>65.0</td><td>71.0</td><td>67.0</td><td>71.0</td><td>71.0</td><td>71.0</td><td>65.0</td><td>71.0</td><td>65.0</td></tr><tr><th>24</th><td>65.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>65.0</td><td>71.0</td><td>65.0</td></tr><tr><th>25</th><td>84.0</td><td>67.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>84.0</td><td>71.0</td><td>84.0</td><td>67.0</td></tr><tr><th>26</th><td>71.0</td><td>65.0</td><td>84.0</td><td>67.0</td><td>67.0</td><td>71.0</td><td>67.0</td><td>67.0</td><td>71.0</td></tr><tr><th>27</th><td>67.0</td><td>67.0</td><td>67.0</td><td>84.0</td><td>67.0</td><td>67.0</td><td>65.0</td><td>84.0</td><td>67.0</td></tr><tr><th>28</th><td>67.0</td><td>67.0</td><td>67.0</td><td>65.0</td><td>65.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>67.0</td></tr><tr><th>29</th><td>67.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>65.0</td><td>67.0</td></tr><tr><th>30</th><td>71.0</td><td>67.0</td><td>67.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>65.0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 67.0 & 67.0 & 65.0 & 71.0 & 67.0 & 84.0 & 71.0 & 67.0 & 65.0 & $\\dots$ \\\\\n",
       "\t2 & 65.0 & 71.0 & 65.0 & 67.0 & 67.0 & 67.0 & 71.0 & 67.0 & 67.0 & $\\dots$ \\\\\n",
       "\t3 & 71.0 & 65.0 & 71.0 & 71.0 & 84.0 & 71.0 & 65.0 & 65.0 & 71.0 & $\\dots$ \\\\\n",
       "\t4 & 71.0 & 71.0 & 71.0 & 67.0 & 84.0 & 71.0 & 67.0 & 71.0 & 84.0 & $\\dots$ \\\\\n",
       "\t5 & 71.0 & 67.0 & 84.0 & 67.0 & 65.0 & 71.0 & 67.0 & 67.0 & 67.0 & $\\dots$ \\\\\n",
       "\t6 & 67.0 & 65.0 & 71.0 & 65.0 & 67.0 & 84.0 & 71.0 & 71.0 & 71.0 & $\\dots$ \\\\\n",
       "\t7 & 67.0 & 67.0 & 84.0 & 84.0 & 84.0 & 71.0 & 65.0 & 71.0 & 71.0 & $\\dots$ \\\\\n",
       "\t8 & 67.0 & 67.0 & 67.0 & 84.0 & 67.0 & 71.0 & 84.0 & 71.0 & 67.0 & $\\dots$ \\\\\n",
       "\t9 & 84.0 & 71.0 & 71.0 & 67.0 & 71.0 & 65.0 & 67.0 & 84.0 & 65.0 & $\\dots$ \\\\\n",
       "\t10 & 65.0 & 65.0 & 71.0 & 67.0 & 84.0 & 71.0 & 65.0 & 67.0 & 65.0 & $\\dots$ \\\\\n",
       "\t11 & 84.0 & 71.0 & 71.0 & 67.0 & 71.0 & 65.0 & 67.0 & 84.0 & 65.0 & $\\dots$ \\\\\n",
       "\t12 & 65.0 & 65.0 & 71.0 & 67.0 & 84.0 & 71.0 & 65.0 & 71.0 & 65.0 & $\\dots$ \\\\\n",
       "\t13 & 71.0 & 71.0 & 65.0 & 65.0 & 71.0 & 65.0 & 84.0 & 71.0 & 67.0 & $\\dots$ \\\\\n",
       "\t14 & 65.0 & 65.0 & 71.0 & 67.0 & 84.0 & 71.0 & 67.0 & 65.0 & 84.0 & $\\dots$ \\\\\n",
       "\t15 & 71.0 & 71.0 & 65.0 & 65.0 & 71.0 & 65.0 & 84.0 & 71.0 & 84.0 & $\\dots$ \\\\\n",
       "\t16 & 65.0 & 65.0 & 71.0 & 67.0 & 84.0 & 71.0 & 67.0 & 65.0 & 84.0 & $\\dots$ \\\\\n",
       "\t17 & 71.0 & 71.0 & 67.0 & 65.0 & 67.0 & 67.0 & 65.0 & 67.0 & 67.0 & $\\dots$ \\\\\n",
       "\t18 & 84.0 & 84.0 & 71.0 & 67.0 & 84.0 & 67.0 & 84.0 & 71.0 & 71.0 & $\\dots$ \\\\\n",
       "\t19 & 67.0 & 65.0 & 67.0 & 67.0 & 65.0 & 65.0 & 71.0 & 84.0 & 84.0 & $\\dots$ \\\\\n",
       "\t20 & 65.0 & 67.0 & 65.0 & 71.0 & 65.0 & 71.0 & 71.0 & 65.0 & 71.0 & $\\dots$ \\\\\n",
       "\t21 & 71.0 & 84.0 & 71.0 & 67.0 & 67.0 & 67.0 & 65.0 & 84.0 & 67.0 & $\\dots$ \\\\\n",
       "\t22 & 67.0 & 65.0 & 67.0 & 71.0 & 65.0 & 84.0 & 67.0 & 84.0 & 84.0 & $\\dots$ \\\\\n",
       "\t23 & 65.0 & 71.0 & 67.0 & 71.0 & 71.0 & 71.0 & 65.0 & 71.0 & 65.0 & $\\dots$ \\\\\n",
       "\t24 & 65.0 & 84.0 & 71.0 & 65.0 & 71.0 & 65.0 & 65.0 & 71.0 & 65.0 & $\\dots$ \\\\\n",
       "\t25 & 84.0 & 67.0 & 65.0 & 71.0 & 65.0 & 84.0 & 71.0 & 84.0 & 67.0 & $\\dots$ \\\\\n",
       "\t26 & 71.0 & 65.0 & 84.0 & 67.0 & 67.0 & 71.0 & 67.0 & 67.0 & 71.0 & $\\dots$ \\\\\n",
       "\t27 & 67.0 & 67.0 & 67.0 & 84.0 & 67.0 & 67.0 & 65.0 & 84.0 & 67.0 & $\\dots$ \\\\\n",
       "\t28 & 67.0 & 67.0 & 67.0 & 65.0 & 65.0 & 71.0 & 71.0 & 67.0 & 67.0 & $\\dots$ \\\\\n",
       "\t29 & 67.0 & 84.0 & 71.0 & 65.0 & 71.0 & 71.0 & 67.0 & 65.0 & 67.0 & $\\dots$ \\\\\n",
       "\t30 & 71.0 & 67.0 & 67.0 & 67.0 & 84.0 & 71.0 & 71.0 & 67.0 & 65.0 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "3190×60 DataFrame. Omitted printing of 53 columns\n",
       "│ Row  │ x1      │ x2      │ x3      │ x4      │ x5      │ x6      │ x7      │\n",
       "│      │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├──────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
       "│ 1    │ 67.0    │ 67.0    │ 65.0    │ 71.0    │ 67.0    │ 84.0    │ 71.0    │\n",
       "│ 2    │ 65.0    │ 71.0    │ 65.0    │ 67.0    │ 67.0    │ 67.0    │ 71.0    │\n",
       "│ 3    │ 71.0    │ 65.0    │ 71.0    │ 71.0    │ 84.0    │ 71.0    │ 65.0    │\n",
       "│ 4    │ 71.0    │ 71.0    │ 71.0    │ 67.0    │ 84.0    │ 71.0    │ 67.0    │\n",
       "│ 5    │ 71.0    │ 67.0    │ 84.0    │ 67.0    │ 65.0    │ 71.0    │ 67.0    │\n",
       "│ 6    │ 67.0    │ 65.0    │ 71.0    │ 65.0    │ 67.0    │ 84.0    │ 71.0    │\n",
       "│ 7    │ 67.0    │ 67.0    │ 84.0    │ 84.0    │ 84.0    │ 71.0    │ 65.0    │\n",
       "│ 8    │ 67.0    │ 67.0    │ 67.0    │ 84.0    │ 67.0    │ 71.0    │ 84.0    │\n",
       "│ 9    │ 84.0    │ 71.0    │ 71.0    │ 67.0    │ 71.0    │ 65.0    │ 67.0    │\n",
       "│ 10   │ 65.0    │ 65.0    │ 71.0    │ 67.0    │ 84.0    │ 71.0    │ 65.0    │\n",
       "⋮\n",
       "│ 3180 │ 67.0    │ 67.0    │ 84.0    │ 67.0    │ 65.0    │ 71.0    │ 84.0    │\n",
       "│ 3181 │ 67.0    │ 67.0    │ 84.0    │ 71.0    │ 67.0    │ 67.0    │ 71.0    │\n",
       "│ 3182 │ 67.0    │ 84.0    │ 71.0    │ 71.0    │ 65.0    │ 65.0    │ 71.0    │\n",
       "│ 3183 │ 84.0    │ 71.0    │ 84.0    │ 84.0    │ 84.0    │ 67.0    │ 84.0    │\n",
       "│ 3184 │ 84.0    │ 65.0    │ 65.0    │ 65.0    │ 65.0    │ 65.0    │ 65.0    │\n",
       "│ 3185 │ 65.0    │ 84.0    │ 67.0    │ 65.0    │ 65.0    │ 84.0    │ 65.0    │\n",
       "│ 3186 │ 84.0    │ 67.0    │ 84.0    │ 67.0    │ 84.0    │ 84.0    │ 67.0    │\n",
       "│ 3187 │ 71.0    │ 65.0    │ 71.0    │ 67.0    │ 84.0    │ 67.0    │ 67.0    │\n",
       "│ 3188 │ 84.0    │ 67.0    │ 84.0    │ 67.0    │ 71.0    │ 71.0    │ 71.0    │\n",
       "│ 3189 │ 65.0    │ 84.0    │ 84.0    │ 67.0    │ 84.0    │ 65.0    │ 67.0    │\n",
       "│ 3190 │ 65.0    │ 71.0    │ 71.0    │ 67.0    │ 84.0    │ 71.0    │ 67.0    │"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = separate_bases(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Class</th><th> Instance</th><th>                       Nucleotide_Sequence</th></tr><tr><th></th><th>String</th><th>String</th><th>String</th></tr></thead><tbody><p>3,190 rows × 63 columns (omitted printing of 60 columns)</p><tr><th>1</th><td>EI</td><td>    ATRINS-DONOR-521</td><td>               CCAGCTGCATCACAGGAGGCCAGCGAGCAGGTCTGTTCCAAGGGCCTTCGAGCCAGTCTG</td></tr><tr><th>2</th><td>EI</td><td>    ATRINS-DONOR-905</td><td>               AGACCCGCCGGGAGGCGGAGGACCTGCAGGGTGAGCCCCACCGCCCCTCCGTGCCCCCGC</td></tr><tr><th>3</th><td>EI</td><td>    BABAPOE-DONOR-30</td><td>               GAGGTGAAGGACGTCCTTCCCCAGGAGCCGGTGAGAAGCGCAGTCGGGGGCACGGGGATG</td></tr><tr><th>4</th><td>EI</td><td>    BABAPOE-DONOR-867</td><td>              GGGCTGCGTTGCTGGTCACATTCCTGGCAGGTATGGGGCGGGGCTTGCTCGGTTTTCCCC</td></tr><tr><th>5</th><td>EI</td><td>    BABAPOE-DONOR-2817</td><td>             GCTCAGCCCCCAGGTCACCCAGGAACTGACGTGAGTGTCCCCATCCCGGCCCTTGACCCT</td></tr><tr><th>6</th><td>EI</td><td>    CHPIGECA-DONOR-378</td><td>             CAGACTGGGTGGACAACAAAACCTTCAGCGGTAAGAGAGGGCCAAGCTCAGAGACCACAG</td></tr><tr><th>7</th><td>EI</td><td>    CHPIGECA-DONOR-903</td><td>             CCTTTGAGGACAGCACCAAGAAGTGTGCAGGTACGTTCCCACCTGCCCTGGTGGCCGCCA</td></tr><tr><th>8</th><td>EI</td><td>    CHPIGECA-DONOR-1313</td><td>            CCCTCGTGCGGTCCACGACCAAGACCAGCGGTGAGCCACGGGCAGGCCGGGGTCGTGGGG</td></tr><tr><th>9</th><td>EI</td><td>    GCRHBBA1-DONOR-1260</td><td>            TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTCCTGTCCCTGCTCCAGTCC</td></tr><tr><th>10</th><td>EI</td><td>    GCRHBBA1-DONOR-1590</td><td>            AAGCTGACAGTGGACCCGGTCAACTTCAAGGTGAGCCAGGAGTCGGGTGGGAGGGTGAGA</td></tr><tr><th>11</th><td>EI</td><td>    GCRHBBA6-DONOR-461</td><td>             TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTGGTATCCCTGCTGCCAGTC</td></tr><tr><th>12</th><td>EI</td><td>    GCRHBBA6-DONOR-795</td><td>             AAGCTGAGAGTGGACCCTGTCAACTTCAAGGTGAGCCACCAGTCGGGTGGGGAGGGTGAG</td></tr><tr><th>13</th><td>EI</td><td>    GIBHBGGL-DONOR-2278</td><td>            GGAAGATGCTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG</td></tr><tr><th>14</th><td>EI</td><td>    GIBHBGGL-DONOR-2624</td><td>            AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG</td></tr><tr><th>15</th><td>EI</td><td>    GIBHBGGL-DONOR-7198</td><td>            GGAAGATGTTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG</td></tr><tr><th>16</th><td>EI</td><td>    GIBHBGGL-DONOR-7544</td><td>            AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG</td></tr><tr><th>17</th><td>EI</td><td>    HUMA1ATP-DONOR-1972</td><td>            GGCACCACCACTGACCTGGGACAGTGAATCGTAAGTATGCCTTTCACTGCGAGGGGTTCT</td></tr><tr><th>18</th><td>EI</td><td>    HUMA1ATP-DONOR-7932</td><td>            TTGCTCTGGTGAATTACATCTTCTTTAAAGGTAAGGTTGCTCAACCAGCCTGAGCTGTTT</td></tr><tr><th>19</th><td>EI</td><td>    HUMA1ATP-DONOR-9653</td><td>            CACCAAGTTCCTGGAAAATGAAGACAGAAGGTGATTCCCCAACCTGAGGGTGACCAAGAA</td></tr><tr><th>20</th><td>EI</td><td>    HUMA1ATP-DONOR-11057</td><td>           ACAGAGGAGGCACCCCTGAAGCTCTCCAAGGTGAGATCACCCTGACGACCTTGTTGCACC</td></tr><tr><th>21</th><td>EI</td><td>    HUMA1GLY2-DONOR-1693</td><td>           GTGCCCATCACCAACGCCACCCTGGACCGGGTGAGTGCCTGGGCTAGCCCTGTCCTGAGC</td></tr><tr><th>22</th><td>EI</td><td>    HUMA1GLY2-DONOR-2251</td><td>           CACGATCTTTCTCAGAGAGTACCAGACCCGGTGAGAGCCCCCATTCCAATGCACCCCCGA</td></tr><tr><th>23</th><td>EI</td><td>    HUMA1GLY2-DONOR-2540</td><td>           AGCGGGAGAATGGGACCGTCTCCAGATACGGTGAGGGCCAGCCCTCAGGCAGGAGGGTTC</td></tr><tr><th>24</th><td>EI</td><td>    HUMA1GLY2-DONOR-3352</td><td>           ATGAGAAGAACTGGGGGCTGTCTTTCTATGGTAGGCATGCTTAGCAGCCCCAAACTCATG</td></tr><tr><th>25</th><td>EI</td><td>    HUMA1GLY2-DONOR-3606</td><td>           TCAGATGTCATGTACACCGACTGGAAAAAGGTAAACGCAAGGGATTGGACATTGCCCACC</td></tr><tr><th>26</th><td>EI</td><td>    HUMACCYBA-DONOR-289</td><td>            GATCCGCCGCCCGTCCACACCCGCCGCCAGGTAAGCCCGGCCAGCCGACCGGGGCATGCG</td></tr><tr><th>27</th><td>EI</td><td>    HUMACCYBA-DONOR-1250</td><td>           CCCTCCATCGTGGGGCGCCCCAGGCACCAGGTAGGGGAGCTGGCTGGGTGGGGCAGCCCC</td></tr><tr><th>28</th><td>EI</td><td>    HUMACCYBA-DONOR-1624</td><td>           CCCAAGGCCAACCGCGAGAAGATGACCCAGGTGAGTGGCCCGCTACCTCTTCTGGTGGCC</td></tr><tr><th>29</th><td>EI</td><td>    HUMACCYBA-DONOR-2504</td><td>           CTGAGGCACTCTTCCAGCCTTCCTTCCTGGGTGAGTGGAGACTGTCTCCCGGCTCTGCCT</td></tr><tr><th>30</th><td>EI</td><td>    HUMACCYBA-DONOR-2781</td><td>           GCCCTGGCACCCAGCACAATGAAGATCAAGGTGGGTGTCTTTCCTGCCTGAGCTGACCTG</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccc}\n",
       "\t& Class &  Instance &                        Nucleotide\\_Sequence & \\\\\n",
       "\t\\hline\n",
       "\t& String & String & String & \\\\\n",
       "\t\\hline\n",
       "\t1 & EI &     ATRINS-DONOR-521 &                CCAGCTGCATCACAGGAGGCCAGCGAGCAGGTCTGTTCCAAGGGCCTTCGAGCCAGTCTG & $\\dots$ \\\\\n",
       "\t2 & EI &     ATRINS-DONOR-905 &                AGACCCGCCGGGAGGCGGAGGACCTGCAGGGTGAGCCCCACCGCCCCTCCGTGCCCCCGC & $\\dots$ \\\\\n",
       "\t3 & EI &     BABAPOE-DONOR-30 &                GAGGTGAAGGACGTCCTTCCCCAGGAGCCGGTGAGAAGCGCAGTCGGGGGCACGGGGATG & $\\dots$ \\\\\n",
       "\t4 & EI &     BABAPOE-DONOR-867 &               GGGCTGCGTTGCTGGTCACATTCCTGGCAGGTATGGGGCGGGGCTTGCTCGGTTTTCCCC & $\\dots$ \\\\\n",
       "\t5 & EI &     BABAPOE-DONOR-2817 &              GCTCAGCCCCCAGGTCACCCAGGAACTGACGTGAGTGTCCCCATCCCGGCCCTTGACCCT & $\\dots$ \\\\\n",
       "\t6 & EI &     CHPIGECA-DONOR-378 &              CAGACTGGGTGGACAACAAAACCTTCAGCGGTAAGAGAGGGCCAAGCTCAGAGACCACAG & $\\dots$ \\\\\n",
       "\t7 & EI &     CHPIGECA-DONOR-903 &              CCTTTGAGGACAGCACCAAGAAGTGTGCAGGTACGTTCCCACCTGCCCTGGTGGCCGCCA & $\\dots$ \\\\\n",
       "\t8 & EI &     CHPIGECA-DONOR-1313 &             CCCTCGTGCGGTCCACGACCAAGACCAGCGGTGAGCCACGGGCAGGCCGGGGTCGTGGGG & $\\dots$ \\\\\n",
       "\t9 & EI &     GCRHBBA1-DONOR-1260 &             TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTCCTGTCCCTGCTCCAGTCC & $\\dots$ \\\\\n",
       "\t10 & EI &     GCRHBBA1-DONOR-1590 &             AAGCTGACAGTGGACCCGGTCAACTTCAAGGTGAGCCAGGAGTCGGGTGGGAGGGTGAGA & $\\dots$ \\\\\n",
       "\t11 & EI &     GCRHBBA6-DONOR-461 &              TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTGGTATCCCTGCTGCCAGTC & $\\dots$ \\\\\n",
       "\t12 & EI &     GCRHBBA6-DONOR-795 &              AAGCTGAGAGTGGACCCTGTCAACTTCAAGGTGAGCCACCAGTCGGGTGGGGAGGGTGAG & $\\dots$ \\\\\n",
       "\t13 & EI &     GIBHBGGL-DONOR-2278 &             GGAAGATGCTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG & $\\dots$ \\\\\n",
       "\t14 & EI &     GIBHBGGL-DONOR-2624 &             AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG & $\\dots$ \\\\\n",
       "\t15 & EI &     GIBHBGGL-DONOR-7198 &             GGAAGATGTTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG & $\\dots$ \\\\\n",
       "\t16 & EI &     GIBHBGGL-DONOR-7544 &             AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG & $\\dots$ \\\\\n",
       "\t17 & EI &     HUMA1ATP-DONOR-1972 &             GGCACCACCACTGACCTGGGACAGTGAATCGTAAGTATGCCTTTCACTGCGAGGGGTTCT & $\\dots$ \\\\\n",
       "\t18 & EI &     HUMA1ATP-DONOR-7932 &             TTGCTCTGGTGAATTACATCTTCTTTAAAGGTAAGGTTGCTCAACCAGCCTGAGCTGTTT & $\\dots$ \\\\\n",
       "\t19 & EI &     HUMA1ATP-DONOR-9653 &             CACCAAGTTCCTGGAAAATGAAGACAGAAGGTGATTCCCCAACCTGAGGGTGACCAAGAA & $\\dots$ \\\\\n",
       "\t20 & EI &     HUMA1ATP-DONOR-11057 &            ACAGAGGAGGCACCCCTGAAGCTCTCCAAGGTGAGATCACCCTGACGACCTTGTTGCACC & $\\dots$ \\\\\n",
       "\t21 & EI &     HUMA1GLY2-DONOR-1693 &            GTGCCCATCACCAACGCCACCCTGGACCGGGTGAGTGCCTGGGCTAGCCCTGTCCTGAGC & $\\dots$ \\\\\n",
       "\t22 & EI &     HUMA1GLY2-DONOR-2251 &            CACGATCTTTCTCAGAGAGTACCAGACCCGGTGAGAGCCCCCATTCCAATGCACCCCCGA & $\\dots$ \\\\\n",
       "\t23 & EI &     HUMA1GLY2-DONOR-2540 &            AGCGGGAGAATGGGACCGTCTCCAGATACGGTGAGGGCCAGCCCTCAGGCAGGAGGGTTC & $\\dots$ \\\\\n",
       "\t24 & EI &     HUMA1GLY2-DONOR-3352 &            ATGAGAAGAACTGGGGGCTGTCTTTCTATGGTAGGCATGCTTAGCAGCCCCAAACTCATG & $\\dots$ \\\\\n",
       "\t25 & EI &     HUMA1GLY2-DONOR-3606 &            TCAGATGTCATGTACACCGACTGGAAAAAGGTAAACGCAAGGGATTGGACATTGCCCACC & $\\dots$ \\\\\n",
       "\t26 & EI &     HUMACCYBA-DONOR-289 &             GATCCGCCGCCCGTCCACACCCGCCGCCAGGTAAGCCCGGCCAGCCGACCGGGGCATGCG & $\\dots$ \\\\\n",
       "\t27 & EI &     HUMACCYBA-DONOR-1250 &            CCCTCCATCGTGGGGCGCCCCAGGCACCAGGTAGGGGAGCTGGCTGGGTGGGGCAGCCCC & $\\dots$ \\\\\n",
       "\t28 & EI &     HUMACCYBA-DONOR-1624 &            CCCAAGGCCAACCGCGAGAAGATGACCCAGGTGAGTGGCCCGCTACCTCTTCTGGTGGCC & $\\dots$ \\\\\n",
       "\t29 & EI &     HUMACCYBA-DONOR-2504 &            CTGAGGCACTCTTCCAGCCTTCCTTCCTGGGTGAGTGGAGACTGTCTCCCGGCTCTGCCT & $\\dots$ \\\\\n",
       "\t30 & EI &     HUMACCYBA-DONOR-2781 &            GCCCTGGCACCCAGCACAATGAAGATCAAGGTGGGTGTCTTTCCTGCCTGAGCTGACCTG & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "3190×63 DataFrame. Omitted printing of 61 columns\n",
       "│ Row  │ Class  │  Instance               │\n",
       "│      │ \u001b[90mString\u001b[39m │ \u001b[90mString\u001b[39m                  │\n",
       "├──────┼────────┼─────────────────────────┤\n",
       "│ 1    │ EI     │     ATRINS-DONOR-521    │\n",
       "│ 2    │ EI     │     ATRINS-DONOR-905    │\n",
       "│ 3    │ EI     │     BABAPOE-DONOR-30    │\n",
       "│ 4    │ EI     │     BABAPOE-DONOR-867   │\n",
       "│ 5    │ EI     │     BABAPOE-DONOR-2817  │\n",
       "│ 6    │ EI     │     CHPIGECA-DONOR-378  │\n",
       "│ 7    │ EI     │     CHPIGECA-DONOR-903  │\n",
       "│ 8    │ EI     │     CHPIGECA-DONOR-1313 │\n",
       "│ 9    │ EI     │     GCRHBBA1-DONOR-1260 │\n",
       "│ 10   │ EI     │     GCRHBBA1-DONOR-1590 │\n",
       "⋮\n",
       "│ 3180 │ N      │      MNKHBPSBD-NEG-961  │\n",
       "│ 3181 │ N      │      ORAHBA01-NEG-121   │\n",
       "│ 3182 │ N      │      ORAHBBE-NEG-2581   │\n",
       "│ 3183 │ N      │      ORAHBBPSE-NEG-2101 │\n",
       "│ 3184 │ N      │      ORAHBBPSE-NEG-6661 │\n",
       "│ 3185 │ N      │      ORAHBG2F-NEG-181   │\n",
       "│ 3186 │ N      │      ORAHBPSBD-NEG-2881 │\n",
       "│ 3187 │ N      │      ORAINVOL-NEG-2161  │\n",
       "│ 3188 │ N      │      ORARGIT-NEG-241    │\n",
       "│ 3189 │ N      │      TARHBB-NEG-541     │\n",
       "│ 3190 │ N      │      TARHBD-NEG-1981    │"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = hcat(df,data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[:,3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Class</th><th>x1</th><th>x2</th><th>x3</th><th>x4</th><th>x5</th><th>x6</th><th>x7</th><th>x8</th></tr><tr><th></th><th>String</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>3,190 rows × 61 columns (omitted printing of 52 columns)</p><tr><th>1</th><td>EI</td><td>67.0</td><td>67.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>67.0</td></tr><tr><th>2</th><td>EI</td><td>65.0</td><td>71.0</td><td>65.0</td><td>67.0</td><td>67.0</td><td>67.0</td><td>71.0</td><td>67.0</td></tr><tr><th>3</th><td>EI</td><td>71.0</td><td>65.0</td><td>71.0</td><td>71.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>65.0</td></tr><tr><th>4</th><td>EI</td><td>71.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>71.0</td></tr><tr><th>5</th><td>EI</td><td>71.0</td><td>67.0</td><td>84.0</td><td>67.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>67.0</td></tr><tr><th>6</th><td>EI</td><td>67.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>71.0</td></tr><tr><th>7</th><td>EI</td><td>67.0</td><td>67.0</td><td>84.0</td><td>84.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>71.0</td></tr><tr><th>8</th><td>EI</td><td>67.0</td><td>67.0</td><td>67.0</td><td>84.0</td><td>67.0</td><td>71.0</td><td>84.0</td><td>71.0</td></tr><tr><th>9</th><td>EI</td><td>84.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>71.0</td><td>65.0</td><td>67.0</td><td>84.0</td></tr><tr><th>10</th><td>EI</td><td>65.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>67.0</td></tr><tr><th>11</th><td>EI</td><td>84.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>71.0</td><td>65.0</td><td>67.0</td><td>84.0</td></tr><tr><th>12</th><td>EI</td><td>65.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>71.0</td></tr><tr><th>13</th><td>EI</td><td>71.0</td><td>71.0</td><td>65.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>84.0</td><td>71.0</td></tr><tr><th>14</th><td>EI</td><td>65.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>65.0</td></tr><tr><th>15</th><td>EI</td><td>71.0</td><td>71.0</td><td>65.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>84.0</td><td>71.0</td></tr><tr><th>16</th><td>EI</td><td>65.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>65.0</td></tr><tr><th>17</th><td>EI</td><td>71.0</td><td>71.0</td><td>67.0</td><td>65.0</td><td>67.0</td><td>67.0</td><td>65.0</td><td>67.0</td></tr><tr><th>18</th><td>EI</td><td>84.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>67.0</td><td>84.0</td><td>71.0</td></tr><tr><th>19</th><td>EI</td><td>67.0</td><td>65.0</td><td>67.0</td><td>67.0</td><td>65.0</td><td>65.0</td><td>71.0</td><td>84.0</td></tr><tr><th>20</th><td>EI</td><td>65.0</td><td>67.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>71.0</td><td>71.0</td><td>65.0</td></tr><tr><th>21</th><td>EI</td><td>71.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>67.0</td><td>67.0</td><td>65.0</td><td>84.0</td></tr><tr><th>22</th><td>EI</td><td>67.0</td><td>65.0</td><td>67.0</td><td>71.0</td><td>65.0</td><td>84.0</td><td>67.0</td><td>84.0</td></tr><tr><th>23</th><td>EI</td><td>65.0</td><td>71.0</td><td>67.0</td><td>71.0</td><td>71.0</td><td>71.0</td><td>65.0</td><td>71.0</td></tr><tr><th>24</th><td>EI</td><td>65.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>65.0</td><td>71.0</td></tr><tr><th>25</th><td>EI</td><td>84.0</td><td>67.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>84.0</td><td>71.0</td><td>84.0</td></tr><tr><th>26</th><td>EI</td><td>71.0</td><td>65.0</td><td>84.0</td><td>67.0</td><td>67.0</td><td>71.0</td><td>67.0</td><td>67.0</td></tr><tr><th>27</th><td>EI</td><td>67.0</td><td>67.0</td><td>67.0</td><td>84.0</td><td>67.0</td><td>67.0</td><td>65.0</td><td>84.0</td></tr><tr><th>28</th><td>EI</td><td>67.0</td><td>67.0</td><td>67.0</td><td>65.0</td><td>65.0</td><td>71.0</td><td>71.0</td><td>67.0</td></tr><tr><th>29</th><td>EI</td><td>67.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>65.0</td></tr><tr><th>30</th><td>EI</td><td>71.0</td><td>67.0</td><td>67.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>71.0</td><td>67.0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& Class & x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & \\\\\n",
       "\t\\hline\n",
       "\t& String & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & EI & 67.0 & 67.0 & 65.0 & 71.0 & 67.0 & 84.0 & 71.0 & 67.0 & $\\dots$ \\\\\n",
       "\t2 & EI & 65.0 & 71.0 & 65.0 & 67.0 & 67.0 & 67.0 & 71.0 & 67.0 & $\\dots$ \\\\\n",
       "\t3 & EI & 71.0 & 65.0 & 71.0 & 71.0 & 84.0 & 71.0 & 65.0 & 65.0 & $\\dots$ \\\\\n",
       "\t4 & EI & 71.0 & 71.0 & 71.0 & 67.0 & 84.0 & 71.0 & 67.0 & 71.0 & $\\dots$ \\\\\n",
       "\t5 & EI & 71.0 & 67.0 & 84.0 & 67.0 & 65.0 & 71.0 & 67.0 & 67.0 & $\\dots$ \\\\\n",
       "\t6 & EI & 67.0 & 65.0 & 71.0 & 65.0 & 67.0 & 84.0 & 71.0 & 71.0 & $\\dots$ \\\\\n",
       "\t7 & EI & 67.0 & 67.0 & 84.0 & 84.0 & 84.0 & 71.0 & 65.0 & 71.0 & $\\dots$ \\\\\n",
       "\t8 & EI & 67.0 & 67.0 & 67.0 & 84.0 & 67.0 & 71.0 & 84.0 & 71.0 & $\\dots$ \\\\\n",
       "\t9 & EI & 84.0 & 71.0 & 71.0 & 67.0 & 71.0 & 65.0 & 67.0 & 84.0 & $\\dots$ \\\\\n",
       "\t10 & EI & 65.0 & 65.0 & 71.0 & 67.0 & 84.0 & 71.0 & 65.0 & 67.0 & $\\dots$ \\\\\n",
       "\t11 & EI & 84.0 & 71.0 & 71.0 & 67.0 & 71.0 & 65.0 & 67.0 & 84.0 & $\\dots$ \\\\\n",
       "\t12 & EI & 65.0 & 65.0 & 71.0 & 67.0 & 84.0 & 71.0 & 65.0 & 71.0 & $\\dots$ \\\\\n",
       "\t13 & EI & 71.0 & 71.0 & 65.0 & 65.0 & 71.0 & 65.0 & 84.0 & 71.0 & $\\dots$ \\\\\n",
       "\t14 & EI & 65.0 & 65.0 & 71.0 & 67.0 & 84.0 & 71.0 & 67.0 & 65.0 & $\\dots$ \\\\\n",
       "\t15 & EI & 71.0 & 71.0 & 65.0 & 65.0 & 71.0 & 65.0 & 84.0 & 71.0 & $\\dots$ \\\\\n",
       "\t16 & EI & 65.0 & 65.0 & 71.0 & 67.0 & 84.0 & 71.0 & 67.0 & 65.0 & $\\dots$ \\\\\n",
       "\t17 & EI & 71.0 & 71.0 & 67.0 & 65.0 & 67.0 & 67.0 & 65.0 & 67.0 & $\\dots$ \\\\\n",
       "\t18 & EI & 84.0 & 84.0 & 71.0 & 67.0 & 84.0 & 67.0 & 84.0 & 71.0 & $\\dots$ \\\\\n",
       "\t19 & EI & 67.0 & 65.0 & 67.0 & 67.0 & 65.0 & 65.0 & 71.0 & 84.0 & $\\dots$ \\\\\n",
       "\t20 & EI & 65.0 & 67.0 & 65.0 & 71.0 & 65.0 & 71.0 & 71.0 & 65.0 & $\\dots$ \\\\\n",
       "\t21 & EI & 71.0 & 84.0 & 71.0 & 67.0 & 67.0 & 67.0 & 65.0 & 84.0 & $\\dots$ \\\\\n",
       "\t22 & EI & 67.0 & 65.0 & 67.0 & 71.0 & 65.0 & 84.0 & 67.0 & 84.0 & $\\dots$ \\\\\n",
       "\t23 & EI & 65.0 & 71.0 & 67.0 & 71.0 & 71.0 & 71.0 & 65.0 & 71.0 & $\\dots$ \\\\\n",
       "\t24 & EI & 65.0 & 84.0 & 71.0 & 65.0 & 71.0 & 65.0 & 65.0 & 71.0 & $\\dots$ \\\\\n",
       "\t25 & EI & 84.0 & 67.0 & 65.0 & 71.0 & 65.0 & 84.0 & 71.0 & 84.0 & $\\dots$ \\\\\n",
       "\t26 & EI & 71.0 & 65.0 & 84.0 & 67.0 & 67.0 & 71.0 & 67.0 & 67.0 & $\\dots$ \\\\\n",
       "\t27 & EI & 67.0 & 67.0 & 67.0 & 84.0 & 67.0 & 67.0 & 65.0 & 84.0 & $\\dots$ \\\\\n",
       "\t28 & EI & 67.0 & 67.0 & 67.0 & 65.0 & 65.0 & 71.0 & 71.0 & 67.0 & $\\dots$ \\\\\n",
       "\t29 & EI & 67.0 & 84.0 & 71.0 & 65.0 & 71.0 & 71.0 & 67.0 & 65.0 & $\\dots$ \\\\\n",
       "\t30 & EI & 71.0 & 67.0 & 67.0 & 67.0 & 84.0 & 71.0 & 71.0 & 67.0 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "3190×61 DataFrame. Omitted printing of 54 columns\n",
       "│ Row  │ Class  │ x1      │ x2      │ x3      │ x4      │ x5      │ x6      │\n",
       "│      │ \u001b[90mString\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├──────┼────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
       "│ 1    │ EI     │ 67.0    │ 67.0    │ 65.0    │ 71.0    │ 67.0    │ 84.0    │\n",
       "│ 2    │ EI     │ 65.0    │ 71.0    │ 65.0    │ 67.0    │ 67.0    │ 67.0    │\n",
       "│ 3    │ EI     │ 71.0    │ 65.0    │ 71.0    │ 71.0    │ 84.0    │ 71.0    │\n",
       "│ 4    │ EI     │ 71.0    │ 71.0    │ 71.0    │ 67.0    │ 84.0    │ 71.0    │\n",
       "│ 5    │ EI     │ 71.0    │ 67.0    │ 84.0    │ 67.0    │ 65.0    │ 71.0    │\n",
       "│ 6    │ EI     │ 67.0    │ 65.0    │ 71.0    │ 65.0    │ 67.0    │ 84.0    │\n",
       "│ 7    │ EI     │ 67.0    │ 67.0    │ 84.0    │ 84.0    │ 84.0    │ 71.0    │\n",
       "│ 8    │ EI     │ 67.0    │ 67.0    │ 67.0    │ 84.0    │ 67.0    │ 71.0    │\n",
       "│ 9    │ EI     │ 84.0    │ 71.0    │ 71.0    │ 67.0    │ 71.0    │ 65.0    │\n",
       "│ 10   │ EI     │ 65.0    │ 65.0    │ 71.0    │ 67.0    │ 84.0    │ 71.0    │\n",
       "⋮\n",
       "│ 3180 │ N      │ 67.0    │ 67.0    │ 84.0    │ 67.0    │ 65.0    │ 71.0    │\n",
       "│ 3181 │ N      │ 67.0    │ 67.0    │ 84.0    │ 71.0    │ 67.0    │ 67.0    │\n",
       "│ 3182 │ N      │ 67.0    │ 84.0    │ 71.0    │ 71.0    │ 65.0    │ 65.0    │\n",
       "│ 3183 │ N      │ 84.0    │ 71.0    │ 84.0    │ 84.0    │ 84.0    │ 67.0    │\n",
       "│ 3184 │ N      │ 84.0    │ 65.0    │ 65.0    │ 65.0    │ 65.0    │ 65.0    │\n",
       "│ 3185 │ N      │ 65.0    │ 84.0    │ 67.0    │ 65.0    │ 65.0    │ 84.0    │\n",
       "│ 3186 │ N      │ 84.0    │ 67.0    │ 84.0    │ 67.0    │ 84.0    │ 84.0    │\n",
       "│ 3187 │ N      │ 71.0    │ 65.0    │ 71.0    │ 67.0    │ 84.0    │ 67.0    │\n",
       "│ 3188 │ N      │ 84.0    │ 67.0    │ 84.0    │ 67.0    │ 71.0    │ 71.0    │\n",
       "│ 3189 │ N      │ 65.0    │ 84.0    │ 84.0    │ 67.0    │ 84.0    │ 65.0    │\n",
       "│ 3190 │ N      │ 65.0    │ 71.0    │ 71.0    │ 67.0    │ 84.0    │ 71.0    │"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[:, Not(2:3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at class labels to see if dataset is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Int64} with 3 entries:\n",
       "  \"IE\" => 768\n",
       "  \"EI\" => 767\n",
       "  \"N\"  => 1655"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = countmap(data[:Class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.24075235109717869\n",
       " 0.24043887147335424\n",
       " 0.5188087774294671"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(label_counts[i] / size(data)[1] for i in keys(label_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m─────────\u001b[0m┬\u001b[0m─────────────────────────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.names \u001b[0m│\u001b[0m\u001b[22m _.types                         \u001b[0m│\u001b[0m\u001b[22m _.scitypes    \u001b[0m│\u001b[0m\n",
       "├\u001b[0m─────────\u001b[0m┼\u001b[0m─────────────────────────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m Class   \u001b[0m│\u001b[0m CategoricalValue{String,UInt32} \u001b[0m│\u001b[0m Multiclass{3} \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x1      \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x2      \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x3      \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x4      \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x5      \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x6      \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x7      \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x8      \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x9      \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x10     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x11     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x12     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x13     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x14     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x15     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x16     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x17     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x18     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x19     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x20     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x21     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x22     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x23     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x24     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x25     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x26     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x27     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x28     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x29     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x30     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x31     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x32     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x33     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x34     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x35     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x36     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x37     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x38     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x39     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x40     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x41     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x42     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x43     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x44     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x45     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x46     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x47     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x48     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x49     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x50     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x51     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x52     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x53     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x54     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x55     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x56     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x57     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x58     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x59     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m x60     \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "└\u001b[0m─────────\u001b[0m┴\u001b[0m─────────────────────────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┘\u001b[0m\n",
       "_.nrows = 3190\n"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coerce!(data, :Class=>Multiclass)\n",
    "schema(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CategoricalValue{String,UInt32}[\"EI\", \"EI\", \"EI\", \"EI\", \"EI\", \"EI\", \"EI\", \"EI\", \"EI\", \"EI\"  …  \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\", \"N\"], 3190×60 DataFrame. Omitted printing of 53 columns\n",
       "│ Row  │ x1      │ x2      │ x3      │ x4      │ x5      │ x6      │ x7      │\n",
       "│      │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├──────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
       "│ 1    │ 67.0    │ 67.0    │ 65.0    │ 71.0    │ 67.0    │ 84.0    │ 71.0    │\n",
       "│ 2    │ 65.0    │ 71.0    │ 65.0    │ 67.0    │ 67.0    │ 67.0    │ 71.0    │\n",
       "│ 3    │ 71.0    │ 65.0    │ 71.0    │ 71.0    │ 84.0    │ 71.0    │ 65.0    │\n",
       "│ 4    │ 71.0    │ 71.0    │ 71.0    │ 67.0    │ 84.0    │ 71.0    │ 67.0    │\n",
       "│ 5    │ 71.0    │ 67.0    │ 84.0    │ 67.0    │ 65.0    │ 71.0    │ 67.0    │\n",
       "│ 6    │ 67.0    │ 65.0    │ 71.0    │ 65.0    │ 67.0    │ 84.0    │ 71.0    │\n",
       "│ 7    │ 67.0    │ 67.0    │ 84.0    │ 84.0    │ 84.0    │ 71.0    │ 65.0    │\n",
       "│ 8    │ 67.0    │ 67.0    │ 67.0    │ 84.0    │ 67.0    │ 71.0    │ 84.0    │\n",
       "│ 9    │ 84.0    │ 71.0    │ 71.0    │ 67.0    │ 71.0    │ 65.0    │ 67.0    │\n",
       "│ 10   │ 65.0    │ 65.0    │ 71.0    │ 67.0    │ 84.0    │ 71.0    │ 65.0    │\n",
       "⋮\n",
       "│ 3180 │ 67.0    │ 67.0    │ 84.0    │ 67.0    │ 65.0    │ 71.0    │ 84.0    │\n",
       "│ 3181 │ 67.0    │ 67.0    │ 84.0    │ 71.0    │ 67.0    │ 67.0    │ 71.0    │\n",
       "│ 3182 │ 67.0    │ 84.0    │ 71.0    │ 71.0    │ 65.0    │ 65.0    │ 71.0    │\n",
       "│ 3183 │ 84.0    │ 71.0    │ 84.0    │ 84.0    │ 84.0    │ 67.0    │ 84.0    │\n",
       "│ 3184 │ 84.0    │ 65.0    │ 65.0    │ 65.0    │ 65.0    │ 65.0    │ 65.0    │\n",
       "│ 3185 │ 65.0    │ 84.0    │ 67.0    │ 65.0    │ 65.0    │ 84.0    │ 65.0    │\n",
       "│ 3186 │ 84.0    │ 67.0    │ 84.0    │ 67.0    │ 84.0    │ 84.0    │ 67.0    │\n",
       "│ 3187 │ 71.0    │ 65.0    │ 71.0    │ 67.0    │ 84.0    │ 67.0    │ 67.0    │\n",
       "│ 3188 │ 84.0    │ 67.0    │ 84.0    │ 67.0    │ 71.0    │ 71.0    │ 71.0    │\n",
       "│ 3189 │ 65.0    │ 84.0    │ 84.0    │ 67.0    │ 84.0    │ 65.0    │ 67.0    │\n",
       "│ 3190 │ 65.0    │ 71.0    │ 71.0    │ 67.0    │ 84.0    │ 71.0    │ 67.0    │)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, X = unpack(data, ==(:Class), colname->true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition train and test data accoring to class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1538, 582, 137, 1511, 1964, 2204, 2888, 2375, 2864, 1387  …  656, 625, 283, 1076, 2074, 743, 3177, 530, 592, 2916], [898, 2807, 2307, 2787, 1648, 1390, 3044, 2312, 586, 1612  …  2993, 729, 3006, 958, 2551, 1687, 2792, 2185, 1273, 1616])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data to use when trying to fit a single validation set\n",
    "train, test = partition(eachindex(y), 0.8, shuffle=true, rng=113, stratify=values(data[:Class])) # gives 70:30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.24059561128526646\n",
       " 0.5188087774294671\n",
       " 0.24059561128526646"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts = countmap(data[train,:Class])\n",
    "collect(train_counts[i] / size(train)[1] for i in keys(train_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.2413793103448276\n",
       " 0.5188087774294671\n",
       " 0.23981191222570533"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_counts = countmap(data[test,:Class])\n",
    "collect(test_counts[i] / size(test)[1] for i in keys(test_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five Learning Algorithms\n",
    "\n",
    "* Decision trees with some form of pruning\n",
    "* Neural networks\n",
    "* Boosting\n",
    "* Support Vector Machines\n",
    "* k-nearest neighbors\n",
    "\n",
    "\n",
    "##### Testing\n",
    "* Implement the algorithms\n",
    "* Design two *interesting* classification problems. For the purposes of this assignment, a classification problem is just a set of training examples and a set of test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42-element Array{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype),T} where T<:Tuple,1}:\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianLDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianQDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = ScikitLearn, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " (name = ExtraTreesClassifier, package_name = ScikitLearn, ... )\n",
       " ⋮\n",
       " (name = ProbabilisticSGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVC, package_name = LIBSVM, ... )\n",
       " (name = SVMClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(matching(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 5,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Distances.Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = :uniform)\u001b[34m @471\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load KNNClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors\n",
    "* Use different values of k.\n",
    "\n",
    "1. https://alan-turing-institute.github.io/MLJ.jl/dev/composing_models/\n",
    "1. https://github.com/KristofferC/NearestNeighbors.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 5,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Distances.Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = :uniform)\u001b[34m @868\u001b[39m"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNNClassifier(K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{KNNClassifier} @877\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @238\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @280\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN = machine(knn, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{KNNClassifier} @877\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{KNNClassifier} @877\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @238\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @280\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(KNN, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:04\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m───────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┬\u001b[0m──────────────────────────────────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.measure     \u001b[0m│\u001b[0m\u001b[22m _.measurement \u001b[0m│\u001b[0m\u001b[22m _.per_fold                               \u001b[0m│\u001b[0m\n",
       "├\u001b[0m───────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┼\u001b[0m──────────────────────────────────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m cross_entropy \u001b[0m│\u001b[0m 3.15          \u001b[0m│\u001b[0m [3.59, 2.62, 2.91, 3.58, 2.88, 3.32]     \u001b[0m│\u001b[0m\n",
       "│\u001b[0m acc           \u001b[0m│\u001b[0m 0.654         \u001b[0m│\u001b[0m [0.626, 0.68, 0.65, 0.648, 0.685, 0.633] \u001b[0m│\u001b[0m\n",
       "└\u001b[0m───────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┴\u001b[0m──────────────────────────────────────────\u001b[0m┘\u001b[0m\n",
       "_.per_observation = [[[0.916, 0.223, ..., 2.22e-16], [0.223, 0.223, ..., 1.61], [1.61, 0.223, ..., 0.223], [0.223, 0.916, ..., 36.0], [0.223, 0.511, ..., 0.223], [0.511, 2.22e-16, ..., 0.511]], missing]\n",
       "_.fitted_params_per_fold = [ … ]\n",
       "_.report_per_fold = [ … ]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_acc = evaluate!(KNN, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tree = NearestNeighbors.KDTree{StaticArrays.SArray{Tuple{60},Float64,1,60},Distances.Euclidean,Float64}\n",
       "  Number of points: 2659\n",
       "  Dimensions: 60\n",
       "  Metric: Distances.Euclidean(0.0)\n",
       "  Reordered: true,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_params(KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 5,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Distances.Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = :uniform)\u001b[34m @524\u001b[39m"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNNClassifier(K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{Standardizer} @971\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n"
     ]
    }
   ],
   "source": [
    "standardizer = Standardizer()\n",
    "stand = machine(standardizer, X[train,:])\n",
    "fit!(stand)\n",
    "X_stand = MLJ.transform(stand, X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{KNNClassifier} @998\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @849\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @114\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN = machine(knn, X_stand, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{KNNClassifier} @998\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{KNNClassifier} @998\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @849\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @114\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(KNN, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m───────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┬\u001b[0m────────────────────────────────────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.measure     \u001b[0m│\u001b[0m\u001b[22m _.measurement \u001b[0m│\u001b[0m\u001b[22m _.per_fold                                 \u001b[0m│\u001b[0m\n",
       "├\u001b[0m───────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┼\u001b[0m────────────────────────────────────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m cross_entropy \u001b[0m│\u001b[0m 3.09          \u001b[0m│\u001b[0m [2.72, 3.6, 3.72, 2.45, 2.85, 3.21]        \u001b[0m│\u001b[0m\n",
       "│\u001b[0m acc           \u001b[0m│\u001b[0m 0.644         \u001b[0m│\u001b[0m [0.641, 0.632, 0.648, 0.647, 0.663, 0.633] \u001b[0m│\u001b[0m\n",
       "└\u001b[0m───────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┴\u001b[0m────────────────────────────────────────────\u001b[0m┘\u001b[0m\n",
       "_.per_observation = [[[0.916, 1.61, ..., 0.223], [0.223, 1.61, ..., 0.916], [2.22e-16, 0.916, ..., 36.0], [0.511, 0.223, ..., 2.22e-16], [0.916, 0.223, ..., 2.22e-16], [1.61, 0.511, ..., 1.61]], missing]\n",
       "_.fitted_params_per_fold = [ … ]\n",
       "_.report_per_fold = [ … ]\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_acc = evaluate!(KNN, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate!(KNN, resampling=CV(shuffle=true), measure=[tnr,tpr,fnr,fpr], verbosity=1, operation=predict_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tree = NearestNeighbors.KDTree{StaticArrays.SArray{Tuple{60},Float64,1,60},Distances.Euclidean,Float64}\n",
       "  Number of points: 2659\n",
       "  Dimensions: 60\n",
       "  Metric: Distances.Euclidean(0.0)\n",
       "  Reordered: true,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_params(KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch / RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 5,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Distances.Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = :uniform)\u001b[34m @553\u001b[39m"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_grid = KNNClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJBase.NumericRange(Int64, :K, ... )"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param1 = :K\n",
    "\n",
    "r1 = range(knn_grid, param1, lower=1, upper=10, scale=:linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "    model = KNNClassifier(\n",
       "            K = 5,\n",
       "            algorithm = :kdtree,\n",
       "            metric = Distances.Euclidean(0.0),\n",
       "            leafsize = 10,\n",
       "            reorder = true,\n",
       "            weights = :uniform),\n",
       "    tuning = Grid(\n",
       "            goal = 100,\n",
       "            resolution = 10,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = CV(\n",
       "            nfolds = 6,\n",
       "            shuffle = false,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = cross_entropy(\n",
       "            eps = 2.220446049250313e-16),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = MLJBase.NumericRange{Int64,MLJBase.Bounded,Symbol}[\u001b[34mNumericRange{Int64,…} @741\u001b[39m],\n",
       "    train_best = true,\n",
       "    repeats = 1,\n",
       "    n = nothing,\n",
       "    acceleration = CPUThreads{Int64}(1),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true)\u001b[34m @393\u001b[39m"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_knn_model = TunedModel(model=knn_grid,\n",
    "                                    tuning=Grid(goal=100),\n",
    "                                    resampling=CV(), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @867\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @741\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @820\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_knn = machine(self_tuning_knn_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @867\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      "┌ Info: Attempting to evaluate 10 models.\n",
      "└ @ MLJTuning /home/andrew/.julia/packages/MLJTuning/Bbgvk/src/tuned_models.jl:494\n",
      "\u001b[33mEvaluating over 10 metamodels: 100%[=========================] Time: 0:00:02\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @867\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @741\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @820\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = fit!(self_tuning_knn, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(best_model = \u001b[34mKNNClassifier @848\u001b[39m,\n",
       " best_fitted_params = (tree = NearestNeighbors.KDTree{StaticArrays.SArray{Tuple{60},Float64,1,60},Distances.Euclidean,Float64}\n",
       "  Number of points: 2552\n",
       "  Dimensions: 60\n",
       "  Metric: Distances.Euclidean(0.0)\n",
       "  Reordered: true,),)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = fitted_params(self_tuning_knn)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 10,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Distances.Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = :uniform)\u001b[34m @848\u001b[39m"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 5,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Cityblock(),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = :uniform)\u001b[34m @997\u001b[39m"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_grid = KNNClassifier(metric=Cityblock())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJBase.NumericRange(Int64, :K, ... )"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param1 = :K\n",
    "\n",
    "r1 = range(knn_grid, param1, lower=1, upper=10, scale=:linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "    model = KNNClassifier(\n",
       "            K = 5,\n",
       "            algorithm = :kdtree,\n",
       "            metric = Cityblock(),\n",
       "            leafsize = 10,\n",
       "            reorder = true,\n",
       "            weights = :uniform),\n",
       "    tuning = Grid(\n",
       "            goal = 100,\n",
       "            resolution = 10,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = CV(\n",
       "            nfolds = 6,\n",
       "            shuffle = false,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = cross_entropy(\n",
       "            eps = 2.220446049250313e-16),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = MLJBase.NumericRange{Int64,MLJBase.Bounded,Symbol}[\u001b[34mNumericRange{Int64,…} @741\u001b[39m],\n",
       "    train_best = true,\n",
       "    repeats = 1,\n",
       "    n = nothing,\n",
       "    acceleration = CPUThreads{Int64}(1),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true)\u001b[34m @991\u001b[39m"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_knn_model = TunedModel(model=knn_grid,\n",
    "                                    tuning=Grid(goal=100),\n",
    "                                    resampling=CV(), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @376\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @934\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @667\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_knn = machine(self_tuning_knn_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @376\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      "┌ Info: Attempting to evaluate 10 models.\n",
      "└ @ MLJTuning /home/andrew/.julia/packages/MLJTuning/Bbgvk/src/tuned_models.jl:494\n",
      "\u001b[33mEvaluating over 10 metamodels: 100%[=========================] Time: 0:00:01\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @376\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @934\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @667\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = fit!(self_tuning_knn, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(best_model = \u001b[34mKNNClassifier @636\u001b[39m,\n",
       " best_fitted_params = (tree = NearestNeighbors.KDTree{StaticArrays.SArray{Tuple{60},Float64,1,60},Euclidean,Float64}\n",
       "  Number of points: 2552\n",
       "  Dimensions: 60\n",
       "  Metric: Euclidean(0.0)\n",
       "  Reordered: true,),)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = fitted_params(self_tuning_knn)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 10,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Cityblock(),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = :uniform)\u001b[34m @636\u001b[39m"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (10, 0.6, 0.5509803921568628)\n",
      "(d, train_metric, valid_metric) = (15, 0.6666666666666666, 0.5509803921568628)\n",
      "(d, train_metric, valid_metric) = (20, 0.7, 0.5509803921568628)\n",
      "(d, train_metric, valid_metric) = (25, 0.68, 0.5568627450980392)\n",
      "(d, train_metric, valid_metric) = (30, 0.6666666666666666, 0.5549019607843138)\n",
      "(d, train_metric, valid_metric) = (35, 0.7142857142857143, 0.5490196078431373)\n",
      "(d, train_metric, valid_metric) = (40, 0.7, 0.5490196078431373)\n",
      "(d, train_metric, valid_metric) = (45, 0.7111111111111111, 0.5529411764705883)\n",
      "(d, train_metric, valid_metric) = (50, 0.64, 0.5784313725490197)\n",
      "(d, train_metric, valid_metric) = (55, 0.6909090909090909, 0.5941176470588235)\n",
      "(d, train_metric, valid_metric) = (60, 0.6666666666666666, 0.6098039215686275)\n",
      "(d, train_metric, valid_metric) = (65, 0.6615384615384615, 0.6058823529411764)\n",
      "(d, train_metric, valid_metric) = (70, 0.6285714285714286, 0.5980392156862745)\n",
      "(d, train_metric, valid_metric) = (75, 0.6266666666666667, 0.6039215686274509)\n",
      "(d, train_metric, valid_metric) = (80, 0.6375, 0.6294117647058823)\n",
      "(d, train_metric, valid_metric) = (85, 0.6823529411764706, 0.6137254901960785)\n",
      "(d, train_metric, valid_metric) = (90, 0.7, 0.6058823529411764)\n",
      "(d, train_metric, valid_metric) = (95, 0.7263157894736842, 0.6019607843137255)\n",
      "(d, train_metric, valid_metric) = (100, 0.75, 0.6058823529411764)\n",
      "(d, train_metric, valid_metric) = (105, 0.6952380952380952, 0.6058823529411764)\n",
      "(d, train_metric, valid_metric) = (110, 0.7090909090909091, 0.6)\n",
      "(d, train_metric, valid_metric) = (115, 0.6956521739130435, 0.6)\n",
      "(d, train_metric, valid_metric) = (120, 0.7083333333333334, 0.596078431372549)\n",
      "(d, train_metric, valid_metric) = (125, 0.72, 0.6019607843137255)\n",
      "(d, train_metric, valid_metric) = (130, 0.7384615384615385, 0.596078431372549)\n",
      "(d, train_metric, valid_metric) = (135, 0.7185185185185186, 0.5980392156862745)\n",
      "(d, train_metric, valid_metric) = (140, 0.7285714285714285, 0.5862745098039216)\n",
      "(d, train_metric, valid_metric) = (145, 0.7172413793103448, 0.5862745098039216)\n",
      "(d, train_metric, valid_metric) = (150, 0.7133333333333334, 0.5823529411764706)\n",
      "(d, train_metric, valid_metric) = (155, 0.7096774193548387, 0.5803921568627451)\n",
      "(d, train_metric, valid_metric) = (160, 0.68125, 0.5862745098039216)\n",
      "(d, train_metric, valid_metric) = (165, 0.6909090909090909, 0.5862745098039216)\n",
      "(d, train_metric, valid_metric) = (170, 0.6882352941176471, 0.592156862745098)\n",
      "(d, train_metric, valid_metric) = (175, 0.68, 0.6)\n",
      "(d, train_metric, valid_metric) = (180, 0.6777777777777778, 0.6)\n",
      "(d, train_metric, valid_metric) = (185, 0.6756756756756757, 0.6039215686274509)\n",
      "(d, train_metric, valid_metric) = (190, 0.6684210526315789, 0.6078431372549019)\n",
      "(d, train_metric, valid_metric) = (195, 0.6564102564102564, 0.6)\n",
      "(d, train_metric, valid_metric) = (200, 0.665, 0.611764705882353)\n",
      "(d, train_metric, valid_metric) = (205, 0.6926829268292682, 0.6254901960784314)\n",
      "(d, train_metric, valid_metric) = (210, 0.6952380952380952, 0.6196078431372549)\n",
      "(d, train_metric, valid_metric) = (215, 0.6976744186046512, 0.6274509803921569)\n",
      "(d, train_metric, valid_metric) = (220, 0.6818181818181818, 0.6254901960784314)\n",
      "(d, train_metric, valid_metric) = (225, 0.6888888888888889, 0.6254901960784314)\n",
      "(d, train_metric, valid_metric) = (230, 0.6869565217391305, 0.6294117647058823)\n",
      "(d, train_metric, valid_metric) = (235, 0.6851063829787234, 0.6274509803921569)\n",
      "(d, train_metric, valid_metric) = (240, 0.6833333333333333, 0.6254901960784314)\n",
      "(d, train_metric, valid_metric) = (245, 0.6816326530612244, 0.6313725490196078)\n",
      "(d, train_metric, valid_metric) = (250, 0.688, 0.6254901960784314)\n",
      "(d, train_metric, valid_metric) = (255, 0.6901960784313725, 0.6274509803921569)\n",
      "(d, train_metric, valid_metric) = (260, 0.6884615384615385, 0.6352941176470588)\n",
      "(d, train_metric, valid_metric) = (265, 0.6943396226415094, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (270, 0.6888888888888889, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (275, 0.6763636363636364, 0.6294117647058823)\n",
      "(d, train_metric, valid_metric) = (280, 0.6821428571428572, 0.6196078431372549)\n",
      "(d, train_metric, valid_metric) = (285, 0.6807017543859649, 0.615686274509804)\n",
      "(d, train_metric, valid_metric) = (290, 0.6862068965517242, 0.6215686274509804)\n",
      "(d, train_metric, valid_metric) = (295, 0.7050847457627119, 0.6176470588235294)\n",
      "(d, train_metric, valid_metric) = (300, 0.71, 0.615686274509804)\n",
      "(d, train_metric, valid_metric) = (305, 0.7147540983606557, 0.615686274509804)\n",
      "(d, train_metric, valid_metric) = (310, 0.7032258064516129, 0.615686274509804)\n",
      "(d, train_metric, valid_metric) = (315, 0.692063492063492, 0.6039215686274509)\n",
      "(d, train_metric, valid_metric) = (320, 0.6875, 0.6078431372549019)\n",
      "(d, train_metric, valid_metric) = (325, 0.6861538461538461, 0.6078431372549019)\n",
      "(d, train_metric, valid_metric) = (330, 0.6878787878787879, 0.611764705882353)\n",
      "(d, train_metric, valid_metric) = (335, 0.6835820895522388, 0.6098039215686275)\n",
      "(d, train_metric, valid_metric) = (340, 0.7058823529411765, 0.6078431372549019)\n",
      "(d, train_metric, valid_metric) = (345, 0.7072463768115942, 0.6098039215686275)\n",
      "(d, train_metric, valid_metric) = (350, 0.7057142857142857, 0.6078431372549019)\n",
      "(d, train_metric, valid_metric) = (355, 0.7014084507042253, 0.5980392156862745)\n",
      "(d, train_metric, valid_metric) = (360, 0.7083333333333334, 0.6019607843137255)\n",
      "(d, train_metric, valid_metric) = (365, 0.7095890410958904, 0.6019607843137255)\n",
      "(d, train_metric, valid_metric) = (370, 0.7162162162162162, 0.6039215686274509)\n",
      "(d, train_metric, valid_metric) = (375, 0.712, 0.6)\n",
      "(d, train_metric, valid_metric) = (380, 0.7157894736842105, 0.6039215686274509)\n",
      "(d, train_metric, valid_metric) = (385, 0.7194805194805195, 0.6058823529411764)\n",
      "(d, train_metric, valid_metric) = (390, 0.7153846153846154, 0.6098039215686275)\n",
      "(d, train_metric, valid_metric) = (395, 0.7139240506329114, 0.6098039215686275)\n",
      "(d, train_metric, valid_metric) = (400, 0.7, 0.6098039215686275)\n",
      "(d, train_metric, valid_metric) = (405, 0.7037037037037037, 0.5941176470588235)\n",
      "(d, train_metric, valid_metric) = (410, 0.7073170731707317, 0.592156862745098)\n",
      "(d, train_metric, valid_metric) = (415, 0.7108433734939759, 0.596078431372549)\n",
      "(d, train_metric, valid_metric) = (420, 0.7142857142857143, 0.5941176470588235)\n",
      "(d, train_metric, valid_metric) = (425, 0.7152941176470589, 0.5941176470588235)\n",
      "(d, train_metric, valid_metric) = (430, 0.7116279069767442, 0.5901960784313726)\n",
      "(d, train_metric, valid_metric) = (435, 0.7126436781609196, 0.5901960784313726)\n",
      "(d, train_metric, valid_metric) = (440, 0.7090909090909091, 0.5882352941176471)\n",
      "(d, train_metric, valid_metric) = (445, 0.7033707865168539, 0.5882352941176471)\n",
      "(d, train_metric, valid_metric) = (450, 0.7, 0.5901960784313726)\n",
      "(d, train_metric, valid_metric) = (455, 0.6945054945054945, 0.5862745098039216)\n",
      "(d, train_metric, valid_metric) = (460, 0.6978260869565217, 0.596078431372549)\n",
      "(d, train_metric, valid_metric) = (465, 0.6989247311827957, 0.596078431372549)\n",
      "(d, train_metric, valid_metric) = (470, 0.7021276595744681, 0.5941176470588235)\n",
      "(d, train_metric, valid_metric) = (475, 0.7242105263157895, 0.6)\n",
      "(d, train_metric, valid_metric) = (480, 0.7270833333333333, 0.6)\n",
      "(d, train_metric, valid_metric) = (485, 0.7278350515463917, 0.6039215686274509)\n",
      "(d, train_metric, valid_metric) = (490, 0.726530612244898, 0.6098039215686275)\n",
      "(d, train_metric, valid_metric) = (495, 0.7252525252525253, 0.6098039215686275)\n",
      "(d, train_metric, valid_metric) = (500, 0.726, 0.6098039215686275)\n",
      "(d, train_metric, valid_metric) = (505, 0.7247524752475247, 0.6078431372549019)\n",
      "(d, train_metric, valid_metric) = (510, 0.7235294117647059, 0.6039215686274509)\n",
      "(d, train_metric, valid_metric) = (515, 0.7262135922330097, 0.6039215686274509)\n",
      "(d, train_metric, valid_metric) = (520, 0.725, 0.6078431372549019)\n",
      "(d, train_metric, valid_metric) = (525, 0.7295238095238096, 0.6078431372549019)\n",
      "(d, train_metric, valid_metric) = (530, 0.7283018867924528, 0.6058823529411764)\n",
      "(d, train_metric, valid_metric) = (535, 0.7308411214953271, 0.6078431372549019)\n",
      "(d, train_metric, valid_metric) = (540, 0.7333333333333333, 0.6019607843137255)\n",
      "(d, train_metric, valid_metric) = (545, 0.7321100917431193, 0.6039215686274509)\n",
      "(d, train_metric, valid_metric) = (550, 0.7327272727272728, 0.6019607843137255)\n",
      "(d, train_metric, valid_metric) = (555, 0.7315315315315315, 0.6058823529411764)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (560, 0.7339285714285714, 0.611764705882353)\n",
      "(d, train_metric, valid_metric) = (565, 0.7292035398230089, 0.6098039215686275)\n",
      "(d, train_metric, valid_metric) = (570, 0.7298245614035088, 0.6058823529411764)\n",
      "(d, train_metric, valid_metric) = (575, 0.7234782608695652, 0.596078431372549)\n",
      "(d, train_metric, valid_metric) = (580, 0.7293103448275862, 0.5941176470588235)\n",
      "(d, train_metric, valid_metric) = (585, 0.7299145299145299, 0.5941176470588235)\n",
      "(d, train_metric, valid_metric) = (590, 0.7271186440677966, 0.592156862745098)\n",
      "(d, train_metric, valid_metric) = (595, 0.7277310924369748, 0.592156862745098)\n",
      "(d, train_metric, valid_metric) = (600, 0.7283333333333334, 0.5941176470588235)\n",
      "(d, train_metric, valid_metric) = (605, 0.7289256198347107, 0.5901960784313726)\n",
      "(d, train_metric, valid_metric) = (610, 0.7245901639344262, 0.5901960784313726)\n",
      "(d, train_metric, valid_metric) = (615, 0.7300813008130081, 0.592156862745098)\n",
      "(d, train_metric, valid_metric) = (620, 0.7306451612903225, 0.5941176470588235)\n",
      "(d, train_metric, valid_metric) = (625, 0.7296, 0.5941176470588235)\n",
      "(d, train_metric, valid_metric) = (630, 0.7301587301587301, 0.596078431372549)\n",
      "(d, train_metric, valid_metric) = (635, 0.7322834645669292, 0.6039215686274509)\n",
      "(d, train_metric, valid_metric) = (640, 0.734375, 0.6039215686274509)\n",
      "(d, train_metric, valid_metric) = (645, 0.7364341085271318, 0.6)\n",
      "(d, train_metric, valid_metric) = (650, 0.7369230769230769, 0.6019607843137255)\n",
      "(d, train_metric, valid_metric) = (655, 0.7450381679389313, 0.6098039215686275)\n",
      "(d, train_metric, valid_metric) = (660, 0.7454545454545455, 0.6137254901960785)\n",
      "(d, train_metric, valid_metric) = (665, 0.7458646616541353, 0.6176470588235294)\n",
      "(d, train_metric, valid_metric) = (670, 0.744776119402985, 0.6137254901960785)\n",
      "(d, train_metric, valid_metric) = (675, 0.7437037037037038, 0.615686274509804)\n",
      "(d, train_metric, valid_metric) = (680, 0.7397058823529412, 0.6215686274509804)\n",
      "(d, train_metric, valid_metric) = (685, 0.7386861313868613, 0.6215686274509804)\n",
      "(d, train_metric, valid_metric) = (690, 0.7434782608695653, 0.6254901960784314)\n",
      "(d, train_metric, valid_metric) = (695, 0.7438848920863309, 0.6274509803921569)\n",
      "(d, train_metric, valid_metric) = (700, 0.74, 0.6254901960784314)\n",
      "(d, train_metric, valid_metric) = (705, 0.7418439716312056, 0.6294117647058823)\n",
      "(d, train_metric, valid_metric) = (710, 0.7352112676056338, 0.6235294117647059)\n",
      "(d, train_metric, valid_metric) = (715, 0.737062937062937, 0.6274509803921569)\n",
      "(d, train_metric, valid_metric) = (720, 0.7388888888888889, 0.6274509803921569)\n",
      "(d, train_metric, valid_metric) = (725, 0.736551724137931, 0.6254901960784314)\n",
      "(d, train_metric, valid_metric) = (730, 0.7397260273972602, 0.6333333333333333)\n",
      "(d, train_metric, valid_metric) = (735, 0.7374149659863946, 0.6313725490196078)\n",
      "(d, train_metric, valid_metric) = (740, 0.7391891891891892, 0.6274509803921569)\n",
      "(d, train_metric, valid_metric) = (745, 0.7395973154362416, 0.6294117647058823)\n",
      "(d, train_metric, valid_metric) = (750, 0.74, 0.6372549019607843)\n",
      "(d, train_metric, valid_metric) = (755, 0.7417218543046358, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (760, 0.7421052631578947, 0.6372549019607843)\n",
      "(d, train_metric, valid_metric) = (765, 0.7411764705882353, 0.6352941176470588)\n",
      "(d, train_metric, valid_metric) = (770, 0.7415584415584415, 0.6372549019607843)\n",
      "(d, train_metric, valid_metric) = (775, 0.743225806451613, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (780, 0.7435897435897436, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (785, 0.7426751592356687, 0.6411764705882353)\n",
      "(d, train_metric, valid_metric) = (790, 0.7443037974683544, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (795, 0.7446540880503144, 0.6411764705882353)\n",
      "(d, train_metric, valid_metric) = (800, 0.74, 0.6294117647058823)\n",
      "(d, train_metric, valid_metric) = (805, 0.7403726708074534, 0.6333333333333333)\n",
      "(d, train_metric, valid_metric) = (810, 0.737037037037037, 0.6333333333333333)\n",
      "(d, train_metric, valid_metric) = (815, 0.7361963190184049, 0.6333333333333333)\n",
      "(d, train_metric, valid_metric) = (820, 0.7378048780487805, 0.6294117647058823)\n",
      "(d, train_metric, valid_metric) = (825, 0.7321212121212122, 0.6313725490196078)\n",
      "(d, train_metric, valid_metric) = (830, 0.7325301204819277, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (835, 0.7317365269461078, 0.6372549019607843)\n",
      "(d, train_metric, valid_metric) = (840, 0.7357142857142858, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (845, 0.7349112426035503, 0.6411764705882353)\n",
      "(d, train_metric, valid_metric) = (850, 0.7388235294117647, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (855, 0.7403508771929824, 0.6411764705882353)\n",
      "(d, train_metric, valid_metric) = (860, 0.7418604651162791, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (865, 0.7398843930635838, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (870, 0.7379310344827587, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (875, 0.7348571428571429, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (880, 0.7318181818181818, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (885, 0.7299435028248588, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (890, 0.7280898876404495, 0.6372549019607843)\n",
      "(d, train_metric, valid_metric) = (895, 0.7284916201117319, 0.6333333333333333)\n",
      "(d, train_metric, valid_metric) = (900, 0.7288888888888889, 0.6352941176470588)\n",
      "(d, train_metric, valid_metric) = (905, 0.7303867403314918, 0.6352941176470588)\n",
      "(d, train_metric, valid_metric) = (910, 0.7307692307692307, 0.6372549019607843)\n",
      "(d, train_metric, valid_metric) = (915, 0.7333333333333333, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (920, 0.7326086956521739, 0.6411764705882353)\n",
      "(d, train_metric, valid_metric) = (925, 0.7318918918918919, 0.6411764705882353)\n",
      "(d, train_metric, valid_metric) = (930, 0.7333333333333333, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (935, 0.7304812834224599, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (940, 0.7319148936170212, 0.6490196078431373)\n",
      "(d, train_metric, valid_metric) = (945, 0.7354497354497355, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (950, 0.7347368421052631, 0.6490196078431373)\n",
      "(d, train_metric, valid_metric) = (955, 0.7350785340314137, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (960, 0.7354166666666667, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (965, 0.7367875647668394, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (970, 0.7381443298969073, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (975, 0.7343589743589743, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (980, 0.7346938775510204, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (985, 0.7340101522842639, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (990, 0.7353535353535353, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (995, 0.7336683417085427, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1000, 0.733, 0.6588235294117647)\n",
      "(d, train_metric, valid_metric) = (1005, 0.7323383084577114, 0.6588235294117647)\n",
      "(d, train_metric, valid_metric) = (1010, 0.7297029702970297, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1015, 0.7300492610837438, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1020, 0.7274509803921568, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1025, 0.7297560975609756, 0.6588235294117647)\n",
      "(d, train_metric, valid_metric) = (1030, 0.7310679611650486, 0.6607843137254902)\n",
      "(d, train_metric, valid_metric) = (1035, 0.7314009661835749, 0.6607843137254902)\n",
      "(d, train_metric, valid_metric) = (1040, 0.7288461538461538, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1045, 0.7263157894736842, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1050, 0.7219047619047619, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (1055, 0.7251184834123223, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (1060, 0.7245283018867924, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (1065, 0.7248826291079812, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (1070, 0.7242990654205608, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (1075, 0.7246511627906976, 0.6450980392156863)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (1080, 0.7222222222222222, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (1085, 0.7225806451612903, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (1090, 0.7247706422018348, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (1095, 0.7251141552511415, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (1100, 0.7227272727272728, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (1105, 0.7248868778280543, 0.6352941176470588)\n",
      "(d, train_metric, valid_metric) = (1110, 0.7234234234234235, 0.6352941176470588)\n",
      "(d, train_metric, valid_metric) = (1115, 0.7255605381165919, 0.6372549019607843)\n",
      "(d, train_metric, valid_metric) = (1120, 0.7267857142857143, 0.6372549019607843)\n",
      "(d, train_metric, valid_metric) = (1125, 0.7288888888888889, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (1130, 0.7247787610619469, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (1135, 0.7251101321585903, 0.6411764705882353)\n",
      "(d, train_metric, valid_metric) = (1140, 0.724561403508772, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (1145, 0.725764192139738, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (1150, 0.7243478260869565, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (1155, 0.7264069264069264, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (1160, 0.7258620689655172, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (1165, 0.7244635193133048, 0.6372549019607843)\n",
      "(d, train_metric, valid_metric) = (1170, 0.7239316239316239, 0.6411764705882353)\n",
      "(d, train_metric, valid_metric) = (1175, 0.7276595744680852, 0.6372549019607843)\n",
      "(d, train_metric, valid_metric) = (1180, 0.726271186440678, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (1185, 0.7257383966244726, 0.6411764705882353)\n",
      "(d, train_metric, valid_metric) = (1190, 0.726890756302521, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (1195, 0.7288702928870293, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (1200, 0.73, 0.6411764705882353)\n",
      "(d, train_metric, valid_metric) = (1205, 0.7302904564315352, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (1210, 0.7289256198347107, 0.6372549019607843)\n",
      "(d, train_metric, valid_metric) = (1215, 0.7283950617283951, 0.6372549019607843)\n",
      "(d, train_metric, valid_metric) = (1220, 0.7295081967213115, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (1225, 0.7314285714285714, 0.6372549019607843)\n",
      "(d, train_metric, valid_metric) = (1230, 0.7317073170731707, 0.6372549019607843)\n",
      "(d, train_metric, valid_metric) = (1235, 0.7311740890688259, 0.6352941176470588)\n",
      "(d, train_metric, valid_metric) = (1240, 0.7338709677419355, 0.6313725490196078)\n",
      "(d, train_metric, valid_metric) = (1245, 0.7325301204819277, 0.6333333333333333)\n",
      "(d, train_metric, valid_metric) = (1250, 0.7344, 0.6352941176470588)\n",
      "(d, train_metric, valid_metric) = (1255, 0.7290836653386454, 0.6313725490196078)\n",
      "(d, train_metric, valid_metric) = (1260, 0.7277777777777777, 0.6333333333333333)\n",
      "(d, train_metric, valid_metric) = (1265, 0.7272727272727273, 0.6294117647058823)\n",
      "(d, train_metric, valid_metric) = (1270, 0.7283464566929134, 0.6313725490196078)\n",
      "(d, train_metric, valid_metric) = (1275, 0.7278431372549019, 0.6313725490196078)\n",
      "(d, train_metric, valid_metric) = (1280, 0.73046875, 0.6333333333333333)\n",
      "(d, train_metric, valid_metric) = (1285, 0.7291828793774319, 0.6313725490196078)\n",
      "(d, train_metric, valid_metric) = (1290, 0.7271317829457364, 0.6333333333333333)\n",
      "(d, train_metric, valid_metric) = (1295, 0.7266409266409266, 0.6352941176470588)\n",
      "(d, train_metric, valid_metric) = (1300, 0.7261538461538461, 0.6333333333333333)\n",
      "(d, train_metric, valid_metric) = (1305, 0.7256704980842912, 0.6333333333333333)\n",
      "(d, train_metric, valid_metric) = (1310, 0.7259541984732825, 0.6333333333333333)\n",
      "(d, train_metric, valid_metric) = (1315, 0.7277566539923954, 0.6333333333333333)\n",
      "(d, train_metric, valid_metric) = (1320, 0.7272727272727273, 0.6352941176470588)\n",
      "(d, train_metric, valid_metric) = (1325, 0.7267924528301887, 0.6372549019607843)\n",
      "(d, train_metric, valid_metric) = (1330, 0.7278195488721805, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (1335, 0.7288389513108614, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (1340, 0.7291044776119403, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1345, 0.7286245353159851, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (1350, 0.7281481481481481, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1355, 0.72619926199262, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1360, 0.725735294117647, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1365, 0.7245421245421245, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1370, 0.7248175182481752, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1375, 0.7250909090909091, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1380, 0.7268115942028985, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1385, 0.7277978339350181, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1390, 0.7258992805755395, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1395, 0.7290322580645161, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1400, 0.7278571428571429, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1405, 0.7274021352313167, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1410, 0.7269503546099291, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1415, 0.727208480565371, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1420, 0.7281690140845071, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1425, 0.7291228070175438, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1430, 0.7293706293706294, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1435, 0.7275261324041812, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1440, 0.7298611111111111, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1445, 0.7273356401384083, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1450, 0.7262068965517241, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1455, 0.7264604810996563, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1460, 0.726027397260274, 0.6490196078431373)\n",
      "(d, train_metric, valid_metric) = (1465, 0.7262798634812286, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1470, 0.7258503401360544, 0.6490196078431373)\n",
      "(d, train_metric, valid_metric) = (1475, 0.727457627118644, 0.6490196078431373)\n",
      "(d, train_metric, valid_metric) = (1480, 0.7277027027027027, 0.6490196078431373)\n",
      "(d, train_metric, valid_metric) = (1485, 0.7279461279461279, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1490, 0.7295302013422819, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1495, 0.7270903010033445, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1500, 0.7273333333333334, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1505, 0.7269102990033223, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (1510, 0.7258278145695364, 0.6392156862745098)\n",
      "(d, train_metric, valid_metric) = (1515, 0.7240924092409241, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (1520, 0.725, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (1525, 0.7245901639344262, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (1530, 0.7248366013071895, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (1535, 0.7257328990228012, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (1540, 0.7259740259740259, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (1545, 0.7281553398058253, 0.6431372549019608)\n",
      "(d, train_metric, valid_metric) = (1550, 0.727741935483871, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1555, 0.7279742765273312, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1560, 0.7282051282051282, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1565, 0.7246006389776358, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1570, 0.7248407643312101, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1575, 0.7244444444444444, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1580, 0.7240506329113924, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1585, 0.7242902208201892, 0.6509803921568628)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (1590, 0.7251572327044025, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1595, 0.7253918495297805, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1600, 0.72625, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1605, 0.7264797507788162, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1610, 0.7279503105590062, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1615, 0.7275541795665634, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1620, 0.7283950617283951, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1625, 0.7304615384615385, 0.6490196078431373)\n",
      "(d, train_metric, valid_metric) = (1630, 0.7306748466257669, 0.6490196078431373)\n",
      "(d, train_metric, valid_metric) = (1635, 0.7333333333333333, 0.6490196078431373)\n",
      "(d, train_metric, valid_metric) = (1640, 0.7317073170731707, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1645, 0.7319148936170212, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1650, 0.730909090909091, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1655, 0.7311178247734139, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (1660, 0.7295180722891567, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (1665, 0.7315315315315315, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1670, 0.7335329341317365, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (1675, 0.7307462686567164, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (1680, 0.7333333333333333, 0.6450980392156863)\n",
      "(d, train_metric, valid_metric) = (1685, 0.7359050445103857, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1690, 0.7372781065088757, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1695, 0.7374631268436578, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1700, 0.7376470588235294, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1705, 0.7366568914956012, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1710, 0.7368421052631579, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1715, 0.7370262390670554, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1720, 0.7377906976744186, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1725, 0.7373913043478261, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1730, 0.7375722543352601, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1735, 0.7371757925072047, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1740, 0.7373563218390805, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1745, 0.7369627507163323, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1750, 0.7382857142857143, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1755, 0.7378917378917379, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1760, 0.7386363636363636, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1765, 0.7359773371104816, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1770, 0.735593220338983, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1775, 0.7340845070422535, 0.6490196078431373)\n",
      "(d, train_metric, valid_metric) = (1780, 0.7308988764044944, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1785, 0.7316526610644257, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1790, 0.7329608938547486, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1795, 0.732033426183844, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1800, 0.7283333333333334, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1805, 0.7290858725761773, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1810, 0.7292817679558011, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1815, 0.7294765840220385, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1820, 0.7307692307692307, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1825, 0.7304109589041096, 0.6490196078431373)\n",
      "(d, train_metric, valid_metric) = (1830, 0.7306010928961748, 0.6490196078431373)\n",
      "(d, train_metric, valid_metric) = (1835, 0.7313351498637602, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1840, 0.7315217391304348, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1845, 0.7322493224932249, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1850, 0.7302702702702702, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1855, 0.7304582210242587, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1860, 0.7311827956989247, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1865, 0.7308310991957104, 0.6588235294117647)\n",
      "(d, train_metric, valid_metric) = (1870, 0.7310160427807486, 0.6588235294117647)\n",
      "(d, train_metric, valid_metric) = (1875, 0.7312, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1880, 0.7303191489361702, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1885, 0.7315649867374006, 0.6588235294117647)\n",
      "(d, train_metric, valid_metric) = (1890, 0.7253968253968254, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1895, 0.7271767810026385, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1900, 0.7278947368421053, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1905, 0.7275590551181103, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1910, 0.7298429319371728, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1915, 0.7300261096605745, 0.6490196078431373)\n",
      "(d, train_metric, valid_metric) = (1920, 0.73125, 0.6490196078431373)\n",
      "(d, train_metric, valid_metric) = (1925, 0.732987012987013, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1930, 0.7347150259067358, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1935, 0.7333333333333333, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1940, 0.7345360824742269, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1945, 0.7347043701799486, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1950, 0.735897435897436, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1955, 0.7350383631713555, 0.6549019607843137)\n",
      "(d, train_metric, valid_metric) = (1960, 0.7346938775510204, 0.6529411764705882)\n",
      "(d, train_metric, valid_metric) = (1965, 0.7353689567430025, 0.6509803921568628)\n",
      "(d, train_metric, valid_metric) = (1970, 0.7340101522842639, 0.6470588235294118)\n",
      "(d, train_metric, valid_metric) = (1975, 0.7341772151898734, 0.6568627450980392)\n",
      "(d, train_metric, valid_metric) = (1980, 0.7368686868686869, 0.6588235294117647)\n",
      "(d, train_metric, valid_metric) = (1985, 0.7375314861460958, 0.6588235294117647)\n",
      "(d, train_metric, valid_metric) = (1990, 0.7366834170854272, 0.6627450980392157)\n",
      "(d, train_metric, valid_metric) = (1995, 0.7368421052631579, 0.6627450980392157)\n",
      "(d, train_metric, valid_metric) = (2000, 0.7375, 0.6627450980392157)\n",
      "(d, train_metric, valid_metric) = (2005, 0.7351620947630922, 0.6647058823529411)\n",
      "(d, train_metric, valid_metric) = (2010, 0.7353233830845771, 0.6627450980392157)\n",
      "(d, train_metric, valid_metric) = (2015, 0.7334987593052109, 0.6627450980392157)\n",
      "(d, train_metric, valid_metric) = (2020, 0.7316831683168317, 0.6627450980392157)\n",
      "(d, train_metric, valid_metric) = (2025, 0.7303703703703703, 0.6647058823529411)\n",
      "(d, train_metric, valid_metric) = (2030, 0.7300492610837438, 0.6607843137254902)\n",
      "(d, train_metric, valid_metric) = (2035, 0.7316953316953317, 0.6647058823529411)\n",
      "(d, train_metric, valid_metric) = (2040, 0.7328431372549019, 0.6647058823529411)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10:5:2040, Any[0.6, 0.6666666666666666, 0.7, 0.68, 0.6666666666666666, 0.7142857142857143, 0.7, 0.7111111111111111, 0.64, 0.6909090909090909  …  0.7368421052631579, 0.7375, 0.7351620947630922, 0.7353233830845771, 0.7334987593052109, 0.7316831683168317, 0.7303703703703703, 0.7300492610837438, 0.7316953316953317, 0.7328431372549019], Any[0.5509803921568628, 0.5509803921568628, 0.5509803921568628, 0.5568627450980392, 0.5549019607843138, 0.5490196078431373, 0.5490196078431373, 0.5529411764705883, 0.5784313725490197, 0.5941176470588235  …  0.6627450980392157, 0.6627450980392157, 0.6647058823529411, 0.6627450980392157, 0.6627450980392157, 0.6627450980392157, 0.6647058823529411, 0.6607843137254902, 0.6647058823529411, 0.6647058823529411])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_schedule, training_losses, valid_losses = learn_curve(best.best_model, X[train,:], y[train], acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip240\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip240)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip241\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip240)\" d=\"\n",
       "M175.024 1486.45 L2352.76 1486.45 L2352.76 47.2441 L175.024 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip242\">\n",
       "    <rect x=\"175\" y=\"47\" width=\"2179\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  226.537,1486.45 226.537,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  732.563,1486.45 732.563,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1238.59,1486.45 1238.59,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1744.61,1486.45 1744.61,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2250.64,1486.45 2250.64,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.024,1439.09 2352.76,1439.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.024,1101.31 2352.76,1101.31 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.024,763.534 2352.76,763.534 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.024,425.755 2352.76,425.755 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.024,87.9763 2352.76,87.9763 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.024,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.024,1486.45 175.024,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  226.537,1486.45 226.537,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  732.563,1486.45 732.563,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1238.59,1486.45 1238.59,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1744.61,1486.45 1744.61,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2250.64,1486.45 2250.64,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.024,1439.09 201.157,1439.09 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.024,1101.31 201.157,1101.31 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.024,763.534 201.157,763.534 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.024,425.755 201.157,425.755 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.024,87.9763 201.157,87.9763 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip240)\" d=\"M 0 0 M226.537 1508.44 Q222.926 1508.44 221.098 1512 Q219.292 1515.55 219.292 1522.67 Q219.292 1529.78 221.098 1533.35 Q222.926 1536.89 226.537 1536.89 Q230.172 1536.89 231.977 1533.35 Q233.806 1529.78 233.806 1522.67 Q233.806 1515.55 231.977 1512 Q230.172 1508.44 226.537 1508.44 M226.537 1504.73 Q232.348 1504.73 235.403 1509.34 Q238.482 1513.92 238.482 1522.67 Q238.482 1531.4 235.403 1536.01 Q232.348 1540.59 226.537 1540.59 Q220.727 1540.59 217.649 1536.01 Q214.593 1531.4 214.593 1522.67 Q214.593 1513.92 217.649 1509.34 Q220.727 1504.73 226.537 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M695.827 1505.36 L714.184 1505.36 L714.184 1509.3 L700.11 1509.3 L700.11 1517.77 Q701.128 1517.42 702.147 1517.26 Q703.165 1517.07 704.184 1517.07 Q709.971 1517.07 713.35 1520.24 Q716.73 1523.42 716.73 1528.83 Q716.73 1534.41 713.258 1537.51 Q709.785 1540.59 703.466 1540.59 Q701.29 1540.59 699.022 1540.22 Q696.776 1539.85 694.369 1539.11 L694.369 1534.41 Q696.452 1535.54 698.674 1536.1 Q700.897 1536.66 703.373 1536.66 Q707.378 1536.66 709.716 1534.55 Q712.054 1532.44 712.054 1528.83 Q712.054 1525.22 709.716 1523.11 Q707.378 1521.01 703.373 1521.01 Q701.498 1521.01 699.623 1521.42 Q697.772 1521.84 695.827 1522.72 L695.827 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M731.799 1508.44 Q728.188 1508.44 726.359 1512 Q724.554 1515.55 724.554 1522.67 Q724.554 1529.78 726.359 1533.35 Q728.188 1536.89 731.799 1536.89 Q735.433 1536.89 737.239 1533.35 Q739.068 1529.78 739.068 1522.67 Q739.068 1515.55 737.239 1512 Q735.433 1508.44 731.799 1508.44 M731.799 1504.73 Q737.609 1504.73 740.665 1509.34 Q743.744 1513.92 743.744 1522.67 Q743.744 1531.4 740.665 1536.01 Q737.609 1540.59 731.799 1540.59 Q725.989 1540.59 722.91 1536.01 Q719.855 1531.4 719.855 1522.67 Q719.855 1513.92 722.91 1509.34 Q725.989 1504.73 731.799 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M758.813 1508.44 Q755.202 1508.44 753.373 1512 Q751.568 1515.55 751.568 1522.67 Q751.568 1529.78 753.373 1533.35 Q755.202 1536.89 758.813 1536.89 Q762.447 1536.89 764.253 1533.35 Q766.081 1529.78 766.081 1522.67 Q766.081 1515.55 764.253 1512 Q762.447 1508.44 758.813 1508.44 M758.813 1504.73 Q764.623 1504.73 767.679 1509.34 Q770.757 1513.92 770.757 1522.67 Q770.757 1531.4 767.679 1536.01 Q764.623 1540.59 758.813 1540.59 Q753.003 1540.59 749.924 1536.01 Q746.869 1531.4 746.869 1522.67 Q746.869 1513.92 749.924 1509.34 Q753.003 1504.73 758.813 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M1188.45 1535.98 L1196.09 1535.98 L1196.09 1509.62 L1187.78 1511.29 L1187.78 1507.03 L1196.04 1505.36 L1200.72 1505.36 L1200.72 1535.98 L1208.36 1535.98 L1208.36 1539.92 L1188.45 1539.92 L1188.45 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M1223.43 1508.44 Q1219.82 1508.44 1217.99 1512 Q1216.18 1515.55 1216.18 1522.67 Q1216.18 1529.78 1217.99 1533.35 Q1219.82 1536.89 1223.43 1536.89 Q1227.06 1536.89 1228.87 1533.35 Q1230.7 1529.78 1230.7 1522.67 Q1230.7 1515.55 1228.87 1512 Q1227.06 1508.44 1223.43 1508.44 M1223.43 1504.73 Q1229.24 1504.73 1232.29 1509.34 Q1235.37 1513.92 1235.37 1522.67 Q1235.37 1531.4 1232.29 1536.01 Q1229.24 1540.59 1223.43 1540.59 Q1217.62 1540.59 1214.54 1536.01 Q1211.48 1531.4 1211.48 1522.67 Q1211.48 1513.92 1214.54 1509.34 Q1217.62 1504.73 1223.43 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M1250.44 1508.44 Q1246.83 1508.44 1245 1512 Q1243.2 1515.55 1243.2 1522.67 Q1243.2 1529.78 1245 1533.35 Q1246.83 1536.89 1250.44 1536.89 Q1254.07 1536.89 1255.88 1533.35 Q1257.71 1529.78 1257.71 1522.67 Q1257.71 1515.55 1255.88 1512 Q1254.07 1508.44 1250.44 1508.44 M1250.44 1504.73 Q1256.25 1504.73 1259.31 1509.34 Q1262.38 1513.92 1262.38 1522.67 Q1262.38 1531.4 1259.31 1536.01 Q1256.25 1540.59 1250.44 1540.59 Q1244.63 1540.59 1241.55 1536.01 Q1238.5 1531.4 1238.5 1522.67 Q1238.5 1513.92 1241.55 1509.34 Q1244.63 1504.73 1250.44 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M1277.45 1508.44 Q1273.84 1508.44 1272.01 1512 Q1270.21 1515.55 1270.21 1522.67 Q1270.21 1529.78 1272.01 1533.35 Q1273.84 1536.89 1277.45 1536.89 Q1281.09 1536.89 1282.89 1533.35 Q1284.72 1529.78 1284.72 1522.67 Q1284.72 1515.55 1282.89 1512 Q1281.09 1508.44 1277.45 1508.44 M1277.45 1504.73 Q1283.26 1504.73 1286.32 1509.34 Q1289.4 1513.92 1289.4 1522.67 Q1289.4 1531.4 1286.32 1536.01 Q1283.26 1540.59 1277.45 1540.59 Q1271.64 1540.59 1268.57 1536.01 Q1265.51 1531.4 1265.51 1522.67 Q1265.51 1513.92 1268.57 1509.34 Q1271.64 1504.73 1277.45 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M1694.97 1535.98 L1702.61 1535.98 L1702.61 1509.62 L1694.3 1511.29 L1694.3 1507.03 L1702.57 1505.36 L1707.24 1505.36 L1707.24 1535.98 L1714.88 1535.98 L1714.88 1539.92 L1694.97 1539.92 L1694.97 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M1720 1505.36 L1738.35 1505.36 L1738.35 1509.3 L1724.28 1509.3 L1724.28 1517.77 Q1725.3 1517.42 1726.32 1517.26 Q1727.33 1517.07 1728.35 1517.07 Q1734.14 1517.07 1737.52 1520.24 Q1740.9 1523.42 1740.9 1528.83 Q1740.9 1534.41 1737.43 1537.51 Q1733.95 1540.59 1727.64 1540.59 Q1725.46 1540.59 1723.19 1540.22 Q1720.95 1539.85 1718.54 1539.11 L1718.54 1534.41 Q1720.62 1535.54 1722.84 1536.1 Q1725.07 1536.66 1727.54 1536.66 Q1731.55 1536.66 1733.89 1534.55 Q1736.22 1532.44 1736.22 1528.83 Q1736.22 1525.22 1733.89 1523.11 Q1731.55 1521.01 1727.54 1521.01 Q1725.67 1521.01 1723.79 1521.42 Q1721.94 1521.84 1720 1522.72 L1720 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M1755.97 1508.44 Q1752.36 1508.44 1750.53 1512 Q1748.72 1515.55 1748.72 1522.67 Q1748.72 1529.78 1750.53 1533.35 Q1752.36 1536.89 1755.97 1536.89 Q1759.6 1536.89 1761.41 1533.35 Q1763.24 1529.78 1763.24 1522.67 Q1763.24 1515.55 1761.41 1512 Q1759.6 1508.44 1755.97 1508.44 M1755.97 1504.73 Q1761.78 1504.73 1764.83 1509.34 Q1767.91 1513.92 1767.91 1522.67 Q1767.91 1531.4 1764.83 1536.01 Q1761.78 1540.59 1755.97 1540.59 Q1750.16 1540.59 1747.08 1536.01 Q1744.02 1531.4 1744.02 1522.67 Q1744.02 1513.92 1747.08 1509.34 Q1750.16 1504.73 1755.97 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M1782.98 1508.44 Q1779.37 1508.44 1777.54 1512 Q1775.74 1515.55 1775.74 1522.67 Q1775.74 1529.78 1777.54 1533.35 Q1779.37 1536.89 1782.98 1536.89 Q1786.62 1536.89 1788.42 1533.35 Q1790.25 1529.78 1790.25 1522.67 Q1790.25 1515.55 1788.42 1512 Q1786.62 1508.44 1782.98 1508.44 M1782.98 1504.73 Q1788.79 1504.73 1791.85 1509.34 Q1794.93 1513.92 1794.93 1522.67 Q1794.93 1531.4 1791.85 1536.01 Q1788.79 1540.59 1782.98 1540.59 Q1777.17 1540.59 1774.09 1536.01 Q1771.04 1531.4 1771.04 1522.67 Q1771.04 1513.92 1774.09 1509.34 Q1777.17 1504.73 1782.98 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2204.77 1535.98 L2221.09 1535.98 L2221.09 1539.92 L2199.15 1539.92 L2199.15 1535.98 Q2201.81 1533.23 2206.39 1528.6 Q2211 1523.95 2212.18 1522.61 Q2214.42 1520.08 2215.3 1518.35 Q2216.21 1516.59 2216.21 1514.9 Q2216.21 1512.14 2214.26 1510.41 Q2212.34 1508.67 2209.24 1508.67 Q2207.04 1508.67 2204.59 1509.43 Q2202.16 1510.2 2199.38 1511.75 L2199.38 1507.03 Q2202.2 1505.89 2204.66 1505.31 Q2207.11 1504.73 2209.15 1504.73 Q2214.52 1504.73 2217.71 1507.42 Q2220.91 1510.11 2220.91 1514.6 Q2220.91 1516.73 2220.1 1518.65 Q2219.31 1520.54 2217.2 1523.14 Q2216.62 1523.81 2213.52 1527.03 Q2210.42 1530.22 2204.77 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2236.16 1508.44 Q2232.55 1508.44 2230.72 1512 Q2228.92 1515.55 2228.92 1522.67 Q2228.92 1529.78 2230.72 1533.35 Q2232.55 1536.89 2236.16 1536.89 Q2239.8 1536.89 2241.6 1533.35 Q2243.43 1529.78 2243.43 1522.67 Q2243.43 1515.55 2241.6 1512 Q2239.8 1508.44 2236.16 1508.44 M2236.16 1504.73 Q2241.97 1504.73 2245.03 1509.34 Q2248.11 1513.92 2248.11 1522.67 Q2248.11 1531.4 2245.03 1536.01 Q2241.97 1540.59 2236.16 1540.59 Q2230.35 1540.59 2227.27 1536.01 Q2224.22 1531.4 2224.22 1522.67 Q2224.22 1513.92 2227.27 1509.34 Q2230.35 1504.73 2236.16 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2263.17 1508.44 Q2259.56 1508.44 2257.73 1512 Q2255.93 1515.55 2255.93 1522.67 Q2255.93 1529.78 2257.73 1533.35 Q2259.56 1536.89 2263.17 1536.89 Q2266.81 1536.89 2268.61 1533.35 Q2270.44 1529.78 2270.44 1522.67 Q2270.44 1515.55 2268.61 1512 Q2266.81 1508.44 2263.17 1508.44 M2263.17 1504.73 Q2268.98 1504.73 2272.04 1509.34 Q2275.12 1513.92 2275.12 1522.67 Q2275.12 1531.4 2272.04 1536.01 Q2268.98 1540.59 2263.17 1540.59 Q2257.36 1540.59 2254.29 1536.01 Q2251.23 1531.4 2251.23 1522.67 Q2251.23 1513.92 2254.29 1509.34 Q2257.36 1504.73 2263.17 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2290.19 1508.44 Q2286.58 1508.44 2284.75 1512 Q2282.94 1515.55 2282.94 1522.67 Q2282.94 1529.78 2284.75 1533.35 Q2286.58 1536.89 2290.19 1536.89 Q2293.82 1536.89 2295.63 1533.35 Q2297.46 1529.78 2297.46 1522.67 Q2297.46 1515.55 2295.63 1512 Q2293.82 1508.44 2290.19 1508.44 M2290.19 1504.73 Q2296 1504.73 2299.05 1509.34 Q2302.13 1513.92 2302.13 1522.67 Q2302.13 1531.4 2299.05 1536.01 Q2296 1540.59 2290.19 1540.59 Q2284.38 1540.59 2281.3 1536.01 Q2278.24 1531.4 2278.24 1522.67 Q2278.24 1513.92 2281.3 1509.34 Q2284.38 1504.73 2290.19 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M77.0893 1424.89 Q73.4782 1424.89 71.6495 1428.46 Q69.8439 1432 69.8439 1439.13 Q69.8439 1446.23 71.6495 1449.8 Q73.4782 1453.34 77.0893 1453.34 Q80.7235 1453.34 82.5291 1449.8 Q84.3578 1446.23 84.3578 1439.13 Q84.3578 1432 82.5291 1428.46 Q80.7235 1424.89 77.0893 1424.89 M77.0893 1421.19 Q82.8994 1421.19 85.955 1425.79 Q89.0337 1430.38 89.0337 1439.13 Q89.0337 1447.85 85.955 1452.46 Q82.8994 1457.04 77.0893 1457.04 Q71.2791 1457.04 68.2004 1452.46 Q65.1449 1447.85 65.1449 1439.13 Q65.1449 1430.38 68.2004 1425.79 Q71.2791 1421.19 77.0893 1421.19 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M94.1031 1450.49 L98.9873 1450.49 L98.9873 1456.37 L94.1031 1456.37 L94.1031 1450.49 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M104.103 1421.81 L122.459 1421.81 L122.459 1425.75 L108.385 1425.75 L108.385 1434.22 Q109.404 1433.87 110.422 1433.71 Q111.441 1433.53 112.459 1433.53 Q118.246 1433.53 121.626 1436.7 Q125.006 1439.87 125.006 1445.28 Q125.006 1450.86 121.533 1453.97 Q118.061 1457.04 111.742 1457.04 Q109.566 1457.04 107.297 1456.67 Q105.052 1456.3 102.645 1455.56 L102.645 1450.86 Q104.728 1452 106.95 1452.55 Q109.172 1453.11 111.649 1453.11 Q115.654 1453.11 117.992 1451 Q120.33 1448.9 120.33 1445.28 Q120.33 1441.67 117.992 1439.57 Q115.654 1437.46 111.649 1437.46 Q109.774 1437.46 107.899 1437.88 Q106.047 1438.29 104.103 1439.17 L104.103 1421.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M130.121 1421.81 L148.478 1421.81 L148.478 1425.75 L134.404 1425.75 L134.404 1434.22 Q135.422 1433.87 136.441 1433.71 Q137.459 1433.53 138.478 1433.53 Q144.265 1433.53 147.644 1436.7 Q151.024 1439.87 151.024 1445.28 Q151.024 1450.86 147.552 1453.97 Q144.08 1457.04 137.76 1457.04 Q135.584 1457.04 133.316 1456.67 Q131.07 1456.3 128.663 1455.56 L128.663 1450.86 Q130.746 1452 132.969 1452.55 Q135.191 1453.11 137.668 1453.11 Q141.672 1453.11 144.01 1451 Q146.348 1448.9 146.348 1445.28 Q146.348 1441.67 144.01 1439.57 Q141.672 1437.46 137.668 1437.46 Q135.793 1437.46 133.918 1437.88 Q132.066 1438.29 130.121 1439.17 L130.121 1421.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M74.9365 1087.11 Q71.3254 1087.11 69.4967 1090.68 Q67.6912 1094.22 67.6912 1101.35 Q67.6912 1108.45 69.4967 1112.02 Q71.3254 1115.56 74.9365 1115.56 Q78.5707 1115.56 80.3763 1112.02 Q82.205 1108.45 82.205 1101.35 Q82.205 1094.22 80.3763 1090.68 Q78.5707 1087.11 74.9365 1087.11 M74.9365 1083.41 Q80.7467 1083.41 83.8022 1088.01 Q86.8809 1092.6 86.8809 1101.35 Q86.8809 1110.08 83.8022 1114.68 Q80.7467 1119.26 74.9365 1119.26 Q69.1264 1119.26 66.0477 1114.68 Q62.9921 1110.08 62.9921 1101.35 Q62.9921 1092.6 66.0477 1088.01 Q69.1264 1083.41 74.9365 1083.41 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M91.9503 1112.71 L96.8345 1112.71 L96.8345 1118.59 L91.9503 1118.59 L91.9503 1112.71 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M112.483 1099.45 Q109.334 1099.45 107.483 1101.6 Q105.654 1103.76 105.654 1107.51 Q105.654 1111.23 107.483 1113.41 Q109.334 1115.56 112.483 1115.56 Q115.631 1115.56 117.459 1113.41 Q119.311 1111.23 119.311 1107.51 Q119.311 1103.76 117.459 1101.6 Q115.631 1099.45 112.483 1099.45 M121.765 1084.8 L121.765 1089.06 Q120.006 1088.22 118.2 1087.78 Q116.418 1087.34 114.659 1087.34 Q110.029 1087.34 107.575 1090.47 Q105.145 1093.59 104.797 1099.91 Q106.163 1097.9 108.223 1096.83 Q110.284 1095.75 112.76 1095.75 Q117.969 1095.75 120.978 1098.92 Q124.01 1102.07 124.01 1107.51 Q124.01 1112.83 120.862 1116.05 Q117.714 1119.26 112.483 1119.26 Q106.487 1119.26 103.316 1114.68 Q100.145 1110.08 100.145 1101.35 Q100.145 1093.15 104.034 1088.29 Q107.922 1083.41 114.473 1083.41 Q116.233 1083.41 118.015 1083.76 Q119.821 1084.1 121.765 1084.8 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M139.08 1087.11 Q135.469 1087.11 133.64 1090.68 Q131.834 1094.22 131.834 1101.35 Q131.834 1108.45 133.64 1112.02 Q135.469 1115.56 139.08 1115.56 Q142.714 1115.56 144.519 1112.02 Q146.348 1108.45 146.348 1101.35 Q146.348 1094.22 144.519 1090.68 Q142.714 1087.11 139.08 1087.11 M139.08 1083.41 Q144.89 1083.41 147.945 1088.01 Q151.024 1092.6 151.024 1101.35 Q151.024 1110.08 147.945 1114.68 Q144.89 1119.26 139.08 1119.26 Q133.27 1119.26 130.191 1114.68 Q127.135 1110.08 127.135 1101.35 Q127.135 1092.6 130.191 1088.01 Q133.27 1083.41 139.08 1083.41 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M75.9319 749.333 Q72.3208 749.333 70.4921 752.898 Q68.6865 756.44 68.6865 763.569 Q68.6865 770.676 70.4921 774.24 Q72.3208 777.782 75.9319 777.782 Q79.5661 777.782 81.3717 774.24 Q83.2004 770.676 83.2004 763.569 Q83.2004 756.44 81.3717 752.898 Q79.5661 749.333 75.9319 749.333 M75.9319 745.629 Q81.742 745.629 84.7976 750.236 Q87.8763 754.819 87.8763 763.569 Q87.8763 772.296 84.7976 776.902 Q81.742 781.486 75.9319 781.486 Q70.1217 781.486 67.043 776.902 Q63.9875 772.296 63.9875 763.569 Q63.9875 754.819 67.043 750.236 Q70.1217 745.629 75.9319 745.629 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M92.9457 774.935 L97.8299 774.935 L97.8299 780.814 L92.9457 780.814 L92.9457 774.935 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M113.478 761.671 Q110.33 761.671 108.478 763.824 Q106.649 765.977 106.649 769.727 Q106.649 773.453 108.478 775.629 Q110.33 777.782 113.478 777.782 Q116.626 777.782 118.455 775.629 Q120.307 773.453 120.307 769.727 Q120.307 765.977 118.455 763.824 Q116.626 761.671 113.478 761.671 M122.76 747.018 L122.76 751.278 Q121.001 750.444 119.196 750.004 Q117.413 749.565 115.654 749.565 Q111.024 749.565 108.571 752.69 Q106.14 755.815 105.793 762.134 Q107.159 760.12 109.219 759.055 Q111.279 757.967 113.756 757.967 Q118.964 757.967 121.973 761.139 Q125.006 764.287 125.006 769.727 Q125.006 775.051 121.858 778.268 Q118.709 781.486 113.478 781.486 Q107.483 781.486 104.311 776.902 Q101.14 772.296 101.14 763.569 Q101.14 755.375 105.029 750.514 Q108.918 745.629 115.469 745.629 Q117.228 745.629 119.01 745.977 Q120.816 746.324 122.76 747.018 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M130.121 746.254 L148.478 746.254 L148.478 750.19 L134.404 750.19 L134.404 758.662 Q135.422 758.315 136.441 758.153 Q137.459 757.967 138.478 757.967 Q144.265 757.967 147.644 761.139 Q151.024 764.31 151.024 769.727 Q151.024 775.305 147.552 778.407 Q144.08 781.486 137.76 781.486 Q135.584 781.486 133.316 781.115 Q131.07 780.745 128.663 780.004 L128.663 775.305 Q130.746 776.439 132.969 776.995 Q135.191 777.551 137.668 777.551 Q141.672 777.551 144.01 775.444 Q146.348 773.338 146.348 769.727 Q146.348 766.115 144.01 764.009 Q141.672 761.902 137.668 761.902 Q135.793 761.902 133.918 762.319 Q132.066 762.736 130.121 763.615 L130.121 746.254 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M76.0013 411.554 Q72.3902 411.554 70.5615 415.119 Q68.756 418.66 68.756 425.79 Q68.756 432.897 70.5615 436.461 Q72.3902 440.003 76.0013 440.003 Q79.6356 440.003 81.4411 436.461 Q83.2698 432.897 83.2698 425.79 Q83.2698 418.66 81.4411 415.119 Q79.6356 411.554 76.0013 411.554 M76.0013 407.85 Q81.8115 407.85 84.867 412.457 Q87.9457 417.04 87.9457 425.79 Q87.9457 434.517 84.867 439.123 Q81.8115 443.707 76.0013 443.707 Q70.1912 443.707 67.1125 439.123 Q64.0569 434.517 64.0569 425.79 Q64.0569 417.04 67.1125 412.457 Q70.1912 407.85 76.0013 407.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M93.0151 437.156 L97.8993 437.156 L97.8993 443.035 L93.0151 443.035 L93.0151 437.156 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M101.788 408.475 L124.01 408.475 L124.01 410.466 L111.464 443.035 L106.58 443.035 L118.385 412.411 L101.788 412.411 L101.788 408.475 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M139.08 411.554 Q135.469 411.554 133.64 415.119 Q131.834 418.66 131.834 425.79 Q131.834 432.897 133.64 436.461 Q135.469 440.003 139.08 440.003 Q142.714 440.003 144.519 436.461 Q146.348 432.897 146.348 425.79 Q146.348 418.66 144.519 415.119 Q142.714 411.554 139.08 411.554 M139.08 407.85 Q144.89 407.85 147.945 412.457 Q151.024 417.04 151.024 425.79 Q151.024 434.517 147.945 439.123 Q144.89 443.707 139.08 443.707 Q133.27 443.707 130.191 439.123 Q127.135 434.517 127.135 425.79 Q127.135 417.04 130.191 412.457 Q133.27 407.85 139.08 407.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M76.9967 73.775 Q73.3856 73.775 71.5569 77.3398 Q69.7513 80.8814 69.7513 88.011 Q69.7513 95.1174 71.5569 98.6822 Q73.3856 102.224 76.9967 102.224 Q80.6309 102.224 82.4365 98.6822 Q84.2652 95.1174 84.2652 88.011 Q84.2652 80.8814 82.4365 77.3398 Q80.6309 73.775 76.9967 73.775 M76.9967 70.0713 Q82.8068 70.0713 85.8624 74.6777 Q88.9411 79.261 88.9411 88.011 Q88.9411 96.7378 85.8624 101.344 Q82.8068 105.928 76.9967 105.928 Q71.1865 105.928 68.1078 101.344 Q65.0523 96.7378 65.0523 88.011 Q65.0523 79.261 68.1078 74.6777 Q71.1865 70.0713 76.9967 70.0713 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M94.0105 99.3767 L98.8947 99.3767 L98.8947 105.256 L94.0105 105.256 L94.0105 99.3767 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M102.784 70.6963 L125.006 70.6963 L125.006 72.687 L112.459 105.256 L107.575 105.256 L119.381 74.6314 L102.784 74.6314 L102.784 70.6963 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M130.121 70.6963 L148.478 70.6963 L148.478 74.6314 L134.404 74.6314 L134.404 83.1036 Q135.422 82.7564 136.441 82.5944 Q137.459 82.4092 138.478 82.4092 Q144.265 82.4092 147.644 85.5805 Q151.024 88.7517 151.024 94.1684 Q151.024 99.747 147.552 102.849 Q144.08 105.928 137.76 105.928 Q135.584 105.928 133.316 105.557 Q131.07 105.187 128.663 104.446 L128.663 99.747 Q130.746 100.881 132.969 101.437 Q135.191 101.992 137.668 101.992 Q141.672 101.992 144.01 99.8859 Q146.348 97.7795 146.348 94.1684 Q146.348 90.5573 144.01 88.4508 Q141.672 86.3443 137.668 86.3443 Q135.793 86.3443 133.918 86.761 Q132.066 87.1777 130.121 88.0573 L130.121 70.6963 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip242)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  236.658,1101.31 241.718,650.941 246.778,425.755 251.839,560.867 256.899,650.941 261.959,329.247 267.02,425.755 272.08,350.693 277.14,831.09 282.2,487.17 \n",
       "  287.261,650.941 292.321,685.585 297.381,908.297 302.441,921.165 307.502,847.979 312.562,544.971 317.622,425.755 322.682,247.977 327.743,87.9763 332.803,457.925 \n",
       "  337.863,364.341 342.923,455.127 347.984,369.459 353.044,290.644 358.104,165.925 363.164,300.652 368.225,232.739 373.285,309.28 378.345,335.681 383.405,360.379 \n",
       "  388.466,552.423 393.526,487.17 398.586,505.233 403.646,560.867 408.707,575.879 413.767,590.08 418.827,639.09 423.887,720.229 428.948,662.201 434.008,475.186 \n",
       "  439.068,457.925 444.128,441.466 449.189,548.584 454.249,500.817 459.309,513.872 464.37,526.37 469.43,538.348 474.49,549.837 479.55,506.822 484.611,491.987 \n",
       "  489.671,503.704 494.731,463.994 499.791,500.817 504.852,585.433 509.912,546.391 514.972,556.126 520.032,518.936 525.093,391.405 530.153,358.2 535.213,326.083 \n",
       "  540.273,403.963 545.334,479.371 550.394,510.2 555.454,519.294 560.514,507.641 565.575,536.668 570.635,386.017 575.695,376.802 580.755,387.152 585.816,416.24 \n",
       "  590.876,369.459 595.936,360.976 600.996,316.205 606.057,344.688 611.117,319.088 616.177,294.153 621.237,321.823 626.298,331.69 631.358,425.755 636.418,400.735 \n",
       "  641.478,376.324 646.539,352.502 651.599,329.247 656.659,322.435 661.719,347.202 666.78,340.34 671.84,364.341 676.9,402.984 681.961,425.755 687.021,462.874 \n",
       "  692.081,440.441 697.141,433.019 702.202,411.382 707.262,262.199 712.322,242.792 717.382,237.713 722.443,246.526 727.503,255.16 732.563,250.11 737.623,258.538 \n",
       "  742.684,266.8 747.744,248.667 752.804,256.866 757.864,226.305 762.925,234.56 767.985,217.406 773.045,200.569 778.105,208.833 783.166,204.664 788.226,212.742 \n",
       "  793.286,196.548 798.346,228.468 803.407,224.273 808.467,267.146 813.527,227.747 818.587,223.665 823.648,242.553 828.708,238.416 833.768,234.347 838.828,230.346 \n",
       "  843.889,259.634 848.949,222.539 854.009,218.729 859.069,225.79 864.13,222.016 869.19,207.662 874.25,193.532 879.311,179.622 884.371,176.318 889.431,121.496 \n",
       "  894.491,118.683 899.552,115.913 904.612,123.267 909.672,130.511 914.732,157.519 919.793,164.408 924.853,132.034 929.913,129.287 934.973,155.532 940.034,143.075 \n",
       "  945.094,187.883 950.154,175.374 955.214,163.038 960.275,178.827 965.335,157.383 970.395,172.995 975.455,161.01 980.516,158.252 985.576,155.532 990.636,143.9 \n",
       "  995.696,141.31 1000.76,147.584 1005.82,145.004 1010.88,133.74 1015.94,131.281 1021,137.46 1026.06,126.457 1031.12,124.091 1036.18,155.532 1041.24,153.014 \n",
       "  1046.3,175.549 1051.36,181.228 1056.42,170.361 1061.48,208.758 1066.54,205.995 1071.6,211.357 1076.66,184.485 1081.72,189.91 1086.78,163.48 1091.84,153.162 \n",
       "  1096.9,142.964 1101.96,156.313 1107.02,169.509 1112.08,190.275 1117.14,210.805 1122.2,223.47 1127.26,235.992 1132.32,233.278 1137.38,230.594 1142.44,220.475 \n",
       "  1147.5,217.891 1152.56,200.569 1157.62,205.465 1162.68,210.307 1167.75,200.569 1172.81,219.837 1177.87,210.152 1182.93,186.272 1187.99,191.088 1193.05,188.779 \n",
       "  1198.11,186.495 1203.17,177.234 1208.23,168.068 1213.29,193.641 1218.35,191.378 1223.41,195.997 1228.47,186.922 1233.53,198.306 1238.59,202.821 1243.65,207.291 \n",
       "  1248.71,225.095 1253.77,222.755 1258.83,240.308 1263.89,224.736 1268.95,215.873 1274.01,213.624 1279.07,230.883 1284.13,247.977 1289.19,277.776 1294.25,256.065 \n",
       "  1299.31,260.052 1304.37,257.659 1309.43,261.601 1314.49,259.222 1319.55,275.631 1324.61,273.21 1329.67,258.415 1334.73,256.095 1339.79,272.219 1344.85,257.63 \n",
       "  1349.91,267.517 1354.97,253.079 1360.03,244.802 1365.1,230.594 1370.16,258.36 1375.22,256.122 1380.28,259.829 1385.34,251.703 1390.4,261.272 1395.46,247.361 \n",
       "  1400.52,251.042 1405.58,260.49 1410.64,264.083 1415.7,238.899 1420.76,248.278 1425.82,251.878 1430.88,244.093 1435.94,230.72 1441,223.088 1446.06,221.126 \n",
       "  1451.12,230.346 1456.18,233.93 1461.24,226.41 1466.3,213.437 1471.36,211.554 1476.42,215.156 1481.48,196.937 1486.54,205.995 1491.6,193.363 1496.66,229.278 \n",
       "  1501.72,238.1 1506.78,241.512 1511.84,234.259 1516.9,237.659 1521.96,219.921 1527.02,228.608 1532.08,242.464 1537.14,245.78 1542.2,249.071 1547.26,252.336 \n",
       "  1552.32,250.42 1557.38,238.243 1562.45,241.512 1567.51,244.757 1572.57,237.818 1577.63,230.931 1582.69,229.138 1587.75,232.38 1592.81,235.598 1597.87,248.764 \n",
       "  1602.93,251.898 1607.99,259.959 1613.05,258.099 1618.11,256.252 1623.17,244.627 1628.23,237.965 1633.29,250.791 1638.35,229.626 1643.41,237.564 1648.47,240.638 \n",
       "  1653.53,243.69 1658.59,241.946 1663.65,235.457 1668.71,229.014 1673.77,227.34 1678.83,239.8 1683.89,224.026 1688.95,241.087 1694.01,248.713 1699.07,246.999 \n",
       "  1704.13,249.925 1709.19,248.22 1714.25,251.121 1719.31,240.263 1724.37,238.607 1729.43,236.963 1734.49,226.262 1739.55,242.745 1744.61,241.103 1749.67,243.961 \n",
       "  1754.73,251.273 1759.8,262.997 1764.86,256.866 1769.92,259.634 1774.98,257.97 1780.04,251.915 1785.1,250.286 1790.16,235.55 1795.22,238.342 1800.28,236.773 \n",
       "  1805.34,235.213 1810.4,259.564 1815.46,257.942 1820.52,260.619 1825.58,263.279 1830.64,261.661 1835.7,255.804 1840.76,254.219 1845.82,248.421 1850.88,246.869 \n",
       "  1855.94,236.935 1861,239.611 1866.06,233.93 1871.12,219.97 1876.18,218.529 1881.24,200.569 1886.3,211.554 1891.36,210.152 1896.42,216.946 1901.48,215.536 \n",
       "  1906.54,226.344 1911.6,212.742 1916.66,199.221 1921.72,218.046 1926.78,200.569 1931.84,183.196 1936.9,173.92 1941.96,172.67 1947.02,171.428 1952.08,178.117 \n",
       "  1957.15,176.866 1962.21,175.622 1967.27,170.457 1972.33,173.155 1977.39,171.933 1982.45,174.611 1987.51,173.392 1992.57,176.05 1997.63,167.113 2002.69,169.775 \n",
       "  2007.75,164.744 2012.81,182.708 2017.87,185.302 2022.93,195.495 2027.99,217.015 2033.05,211.923 2038.11,203.085 2043.17,209.351 2048.23,234.347 2053.29,229.263 \n",
       "  2058.35,227.94 2063.41,226.624 2068.47,217.891 2073.53,220.312 2078.59,219.027 2083.65,214.068 2088.71,212.808 2093.77,207.892 2098.83,221.262 2103.89,219.992 \n",
       "  2108.95,215.097 2114.01,217.473 2119.07,216.224 2124.13,214.981 2129.19,220.932 2134.25,212.516 2139.31,254.185 2144.37,242.16 2149.43,237.31 2154.5,239.578 \n",
       "  2159.56,224.149 2164.62,222.912 2169.68,214.643 2174.74,202.909 2179.8,191.235 2184.86,200.569 2189.92,192.444 2194.98,191.307 2200.04,183.247 2205.1,189.051 \n",
       "  2210.16,191.378 2215.22,186.817 2220.28,195.997 2225.34,194.868 2230.4,176.686 2235.46,172.208 2240.52,177.938 2245.58,176.866 2250.64,172.421 2255.7,188.215 \n",
       "  2260.76,187.125 2265.82,199.452 2270.88,211.717 2275.94,220.586 2281,222.755 2286.06,211.635 2291.12,203.881 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip242)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  236.658,1432.47 241.718,1432.47 246.778,1432.47 251.839,1392.73 256.899,1405.98 261.959,1445.72 267.02,1445.72 272.08,1419.22 277.14,1247.02 282.2,1141.05 \n",
       "  287.261,1035.08 292.321,1061.57 297.381,1114.56 302.441,1074.82 307.502,902.62 312.562,1008.59 317.622,1061.57 322.682,1088.07 327.743,1061.57 332.803,1061.57 \n",
       "  337.863,1101.31 342.923,1101.31 347.984,1127.81 353.044,1088.07 358.104,1127.81 363.164,1114.56 368.225,1194.04 373.285,1194.04 378.345,1220.53 383.405,1233.78 \n",
       "  388.466,1194.04 393.526,1194.04 398.586,1154.3 403.646,1101.31 408.707,1101.31 413.767,1074.82 418.827,1048.33 423.887,1101.31 428.948,1021.84 434.008,929.112 \n",
       "  439.068,968.851 444.128,915.866 449.189,929.112 454.249,929.112 459.309,902.62 464.37,915.866 469.43,929.112 474.49,889.374 479.55,929.112 484.611,915.866 \n",
       "  489.671,862.881 494.731,809.896 499.791,809.896 504.852,902.62 509.912,968.851 514.972,995.344 520.032,955.605 525.093,982.097 530.153,995.344 535.213,995.344 \n",
       "  540.273,995.344 545.334,1074.82 550.394,1048.33 555.454,1048.33 560.514,1021.84 565.575,1035.08 570.635,1048.33 575.695,1035.08 580.755,1048.33 585.816,1114.56 \n",
       "  590.876,1088.07 595.936,1088.07 600.996,1074.82 606.057,1101.31 611.117,1074.82 616.177,1061.57 621.237,1035.08 626.298,1035.08 631.358,1035.08 636.418,1141.05 \n",
       "  641.478,1154.3 646.539,1127.81 651.599,1141.05 656.659,1141.05 661.719,1167.54 666.78,1167.54 671.84,1180.79 676.9,1180.79 681.961,1167.54 687.021,1194.04 \n",
       "  692.081,1127.81 697.141,1127.81 702.202,1141.05 707.262,1101.31 712.322,1101.31 717.382,1074.82 722.443,1035.08 727.503,1035.08 732.563,1035.08 737.623,1048.33 \n",
       "  742.684,1074.82 747.744,1074.82 752.804,1048.33 757.864,1048.33 762.925,1061.57 767.985,1048.33 773.045,1088.07 778.105,1074.82 783.166,1088.07 788.226,1061.57 \n",
       "  793.286,1021.84 798.346,1035.08 803.407,1061.57 808.467,1127.81 813.527,1141.05 818.587,1141.05 823.648,1154.3 828.708,1154.3 833.768,1141.05 838.828,1167.54 \n",
       "  843.889,1167.54 848.949,1154.3 854.009,1141.05 859.069,1141.05 864.13,1127.81 869.19,1074.82 874.25,1074.82 879.311,1101.31 884.371,1088.07 889.431,1035.08 \n",
       "  894.491,1008.59 899.552,982.097 904.612,1008.59 909.672,995.344 914.732,955.605 919.793,955.605 924.853,929.112 929.913,915.866 934.973,929.112 940.034,902.62 \n",
       "  945.094,942.359 950.154,915.866 955.214,915.866 960.275,929.112 965.335,876.127 970.395,889.374 975.455,915.866 980.516,902.62 985.576,849.635 990.636,836.389 \n",
       "  995.696,849.635 1000.76,862.881 1005.82,849.635 1010.88,836.389 1015.94,836.389 1021,823.142 1026.06,809.896 1031.12,823.142 1036.18,902.62 1041.24,876.127 \n",
       "  1046.3,876.127 1051.36,876.127 1056.42,902.62 1061.48,889.374 1066.54,836.389 1071.6,849.635 1076.66,836.389 1081.72,823.142 1086.78,809.896 1091.84,823.142 \n",
       "  1096.9,809.896 1101.96,809.896 1107.02,796.65 1112.08,796.65 1117.14,809.896 1122.2,809.896 1127.26,849.635 1132.32,876.127 1137.38,862.881 1142.44,862.881 \n",
       "  1147.5,849.635 1152.56,836.389 1157.62,823.142 1162.68,823.142 1167.75,796.65 1172.81,809.896 1177.87,770.158 1182.93,756.911 1187.99,770.158 1193.05,783.404 \n",
       "  1198.11,783.404 1203.17,796.65 1208.23,783.404 1213.29,783.404 1218.35,756.911 1223.41,756.911 1228.47,743.665 1233.53,717.173 1238.59,703.926 1243.65,703.926 \n",
       "  1248.71,717.173 1253.77,717.173 1258.83,717.173 1263.89,703.926 1268.95,690.68 1274.01,690.68 1279.07,743.665 1284.13,756.911 1289.19,796.65 1294.25,796.65 \n",
       "  1299.31,809.896 1304.37,809.896 1309.43,796.65 1314.49,796.65 1319.55,809.896 1324.61,809.896 1329.67,809.896 1334.73,809.896 1339.79,836.389 1344.85,862.881 \n",
       "  1349.91,862.881 1354.97,849.635 1360.03,849.635 1365.1,836.389 1370.16,836.389 1375.22,823.142 1380.28,809.896 1385.34,836.389 1390.4,836.389 1395.46,836.389 \n",
       "  1400.52,836.389 1405.58,849.635 1410.64,823.142 1415.7,849.635 1420.76,836.389 1425.82,823.142 1430.88,809.896 1435.94,836.389 1441,823.142 1446.06,836.389 \n",
       "  1451.12,849.635 1456.18,849.635 1461.24,836.389 1466.3,849.635 1471.36,849.635 1476.42,862.881 1481.48,889.374 1486.54,876.127 1491.6,862.881 1496.66,889.374 \n",
       "  1501.72,876.127 1506.78,902.62 1511.84,889.374 1516.9,889.374 1521.96,876.127 1527.02,889.374 1532.08,876.127 1537.14,862.881 1542.2,876.127 1547.26,876.127 \n",
       "  1552.32,876.127 1557.38,876.127 1562.45,862.881 1567.51,849.635 1572.57,836.389 1577.63,836.389 1582.69,783.404 1587.75,809.896 1592.81,783.404 1597.87,783.404 \n",
       "  1602.93,756.911 1607.99,743.665 1613.05,743.665 1618.11,743.665 1623.17,743.665 1628.23,743.665 1633.29,717.173 1638.35,717.173 1643.41,717.173 1648.47,717.173 \n",
       "  1653.53,730.419 1658.59,743.665 1663.65,730.419 1668.71,730.419 1673.77,730.419 1678.83,743.665 1683.89,756.911 1688.95,756.911 1694.01,756.911 1699.07,743.665 \n",
       "  1704.13,770.158 1709.19,756.911 1714.25,770.158 1719.31,770.158 1724.37,770.158 1729.43,756.911 1734.49,756.911 1739.55,783.404 1744.61,783.404 1749.67,796.65 \n",
       "  1754.73,836.389 1759.8,809.896 1764.86,796.65 1769.92,796.65 1774.98,796.65 1780.04,796.65 1785.1,809.896 1790.16,809.896 1795.22,783.404 1800.28,783.404 \n",
       "  1805.34,756.911 1810.4,756.911 1815.46,756.911 1820.52,756.911 1825.58,756.911 1830.64,756.911 1835.7,743.665 1840.76,743.665 1845.82,730.419 1850.88,717.173 \n",
       "  1855.94,717.173 1861,717.173 1866.06,730.419 1871.12,770.158 1876.18,770.158 1881.24,770.158 1886.3,783.404 1891.36,783.404 1896.42,783.404 1901.48,796.65 \n",
       "  1906.54,796.65 1911.6,783.404 1916.66,796.65 1921.72,796.65 1926.78,796.65 1931.84,783.404 1936.9,783.404 1941.96,783.404 1947.02,756.911 1952.08,756.911 \n",
       "  1957.15,756.911 1962.21,730.419 1967.27,730.419 1972.33,730.419 1977.39,717.173 1982.45,717.173 1987.51,717.173 1992.57,717.173 1997.63,730.419 2002.69,743.665 \n",
       "  2007.75,743.665 2012.81,756.911 2017.87,743.665 2022.93,770.158 2027.99,756.911 2033.05,756.911 2038.11,756.911 2043.17,756.911 2048.23,756.911 2053.29,743.665 \n",
       "  2058.35,783.404 2063.41,783.404 2068.47,783.404 2073.53,770.158 2078.59,770.158 2083.65,756.911 2088.71,756.911 2093.77,743.665 2098.83,730.419 2103.89,730.419 \n",
       "  2108.95,717.173 2114.01,703.926 2119.07,703.926 2124.13,717.173 2129.19,717.173 2134.25,703.926 2139.31,717.173 2144.37,730.419 2149.43,730.419 2154.5,743.665 \n",
       "  2159.56,743.665 2164.62,770.158 2169.68,770.158 2174.74,730.419 2179.8,730.419 2184.86,730.419 2189.92,730.419 2194.98,717.173 2200.04,717.173 2205.1,730.419 \n",
       "  2210.16,743.665 2215.22,756.911 2220.28,783.404 2225.34,717.173 2230.4,703.926 2235.46,703.926 2240.52,677.434 2245.58,677.434 2250.64,677.434 2255.7,664.188 \n",
       "  2260.76,677.434 2265.82,677.434 2270.88,677.434 2275.94,664.188 2281,690.68 2286.06,664.188 2291.12,664.188 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip240)\" d=\"\n",
       "M1838.87 276.658 L2280.16 276.658 L2280.16 95.2176 L1838.87 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1838.87,276.658 2280.16,276.658 2280.16,95.2176 1838.87,95.2176 1838.87,276.658 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip240)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1863.06,155.698 2008.25,155.698 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip240)\" d=\"M 0 0 M2032.44 138.418 L2061.68 138.418 L2061.68 142.353 L2049.41 142.353 L2049.41 172.978 L2044.71 172.978 L2044.71 142.353 L2032.44 142.353 L2032.44 138.418 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2074.2 151.033 Q2073.48 150.617 2072.63 150.431 Q2071.79 150.223 2070.78 150.223 Q2067.17 150.223 2065.22 152.584 Q2063.3 154.922 2063.3 159.32 L2063.3 172.978 L2059.02 172.978 L2059.02 147.052 L2063.3 147.052 L2063.3 151.08 Q2064.64 148.718 2066.79 147.584 Q2068.95 146.427 2072.03 146.427 Q2072.47 146.427 2073 146.496 Q2073.53 146.543 2074.18 146.658 L2074.2 151.033 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2090.45 159.945 Q2085.29 159.945 2083.3 161.126 Q2081.31 162.306 2081.31 165.154 Q2081.31 167.422 2082.79 168.765 Q2084.29 170.084 2086.86 170.084 Q2090.41 170.084 2092.54 167.584 Q2094.69 165.061 2094.69 160.894 L2094.69 159.945 L2090.45 159.945 M2098.95 158.186 L2098.95 172.978 L2094.69 172.978 L2094.69 169.042 Q2093.23 171.403 2091.05 172.538 Q2088.88 173.649 2085.73 173.649 Q2081.75 173.649 2079.39 171.427 Q2077.05 169.181 2077.05 165.431 Q2077.05 161.056 2079.97 158.834 Q2082.91 156.612 2088.72 156.612 L2094.69 156.612 L2094.69 156.195 Q2094.69 153.255 2092.74 151.658 Q2090.82 150.038 2087.33 150.038 Q2085.1 150.038 2083 150.57 Q2080.89 151.103 2078.95 152.167 L2078.95 148.232 Q2081.29 147.33 2083.48 146.89 Q2085.68 146.427 2087.77 146.427 Q2093.39 146.427 2096.17 149.343 Q2098.95 152.26 2098.95 158.186 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2103.42 147.052 L2107.67 147.052 L2107.67 172.978 L2103.42 172.978 L2103.42 147.052 M2103.42 136.959 L2107.67 136.959 L2107.67 142.353 L2103.42 142.353 L2103.42 136.959 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2133.69 157.329 L2133.69 172.978 L2129.43 172.978 L2129.43 157.468 Q2129.43 153.788 2128 151.959 Q2126.56 150.13 2123.69 150.13 Q2120.24 150.13 2118.25 152.33 Q2116.26 154.529 2116.26 158.325 L2116.26 172.978 L2111.98 172.978 L2111.98 147.052 L2116.26 147.052 L2116.26 151.08 Q2117.79 148.742 2119.85 147.584 Q2121.93 146.427 2124.64 146.427 Q2129.11 146.427 2131.4 149.205 Q2133.69 151.959 2133.69 157.329 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2138.16 147.052 L2142.42 147.052 L2142.42 172.978 L2138.16 172.978 L2138.16 147.052 M2138.16 136.959 L2142.42 136.959 L2142.42 142.353 L2138.16 142.353 L2138.16 136.959 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2168.44 157.329 L2168.44 172.978 L2164.18 172.978 L2164.18 157.468 Q2164.18 153.788 2162.74 151.959 Q2161.31 150.13 2158.44 150.13 Q2154.99 150.13 2153 152.33 Q2151.01 154.529 2151.01 158.325 L2151.01 172.978 L2146.72 172.978 L2146.72 147.052 L2151.01 147.052 L2151.01 151.08 Q2152.54 148.742 2154.6 147.584 Q2156.68 146.427 2159.39 146.427 Q2163.85 146.427 2166.15 149.205 Q2168.44 151.959 2168.44 157.329 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2189.97 159.714 Q2189.97 155.084 2188.04 152.538 Q2186.15 149.992 2182.7 149.992 Q2179.27 149.992 2177.35 152.538 Q2175.45 155.084 2175.45 159.714 Q2175.45 164.32 2177.35 166.866 Q2179.27 169.413 2182.7 169.413 Q2186.15 169.413 2188.04 166.866 Q2189.97 164.32 2189.97 159.714 M2194.22 169.76 Q2194.22 176.38 2191.28 179.598 Q2188.35 182.839 2182.28 182.839 Q2180.03 182.839 2178.04 182.491 Q2176.05 182.167 2174.18 181.473 L2174.18 177.329 Q2176.05 178.348 2177.88 178.834 Q2179.71 179.32 2181.61 179.32 Q2185.8 179.32 2187.88 177.121 Q2189.97 174.945 2189.97 170.524 L2189.97 168.417 Q2188.65 170.709 2186.59 171.843 Q2184.53 172.978 2181.66 172.978 Q2176.89 172.978 2173.97 169.343 Q2171.05 165.709 2171.05 159.714 Q2171.05 153.695 2173.97 150.061 Q2176.89 146.427 2181.66 146.427 Q2184.53 146.427 2186.59 147.561 Q2188.65 148.695 2189.97 150.987 L2189.97 147.052 L2194.22 147.052 L2194.22 169.76 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip240)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1863.06,216.178 2008.25,216.178 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip240)\" d=\"M 0 0 M2045.64 233.458 L2032.44 198.898 L2037.33 198.898 L2048.28 227.995 L2059.25 198.898 L2064.11 198.898 L2050.94 233.458 L2045.64 233.458 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2076.68 220.425 Q2071.52 220.425 2069.53 221.606 Q2067.54 222.786 2067.54 225.634 Q2067.54 227.902 2069.02 229.245 Q2070.52 230.564 2073.09 230.564 Q2076.63 230.564 2078.76 228.064 Q2080.92 225.541 2080.92 221.374 L2080.92 220.425 L2076.68 220.425 M2085.17 218.666 L2085.17 233.458 L2080.92 233.458 L2080.92 229.522 Q2079.46 231.883 2077.28 233.018 Q2075.1 234.129 2071.96 234.129 Q2067.98 234.129 2065.61 231.907 Q2063.28 229.661 2063.28 225.911 Q2063.28 221.536 2066.19 219.314 Q2069.13 217.092 2074.94 217.092 L2080.92 217.092 L2080.92 216.675 Q2080.92 213.735 2078.97 212.138 Q2077.05 210.518 2073.55 210.518 Q2071.33 210.518 2069.23 211.05 Q2067.12 211.583 2065.17 212.647 L2065.17 208.712 Q2067.51 207.81 2069.71 207.37 Q2071.91 206.907 2073.99 206.907 Q2079.62 206.907 2082.4 209.823 Q2085.17 212.74 2085.17 218.666 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2089.64 197.439 L2093.9 197.439 L2093.9 233.458 L2089.64 233.458 L2089.64 197.439 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2098.37 207.532 L2102.63 207.532 L2102.63 233.458 L2098.37 233.458 L2098.37 207.532 M2098.37 197.439 L2102.63 197.439 L2102.63 202.833 L2098.37 202.833 L2098.37 197.439 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2124.16 211.467 L2124.16 197.439 L2128.41 197.439 L2128.41 233.458 L2124.16 233.458 L2124.16 229.569 Q2122.81 231.883 2120.75 233.018 Q2118.72 234.129 2115.85 234.129 Q2111.15 234.129 2108.18 230.379 Q2105.24 226.629 2105.24 220.518 Q2105.24 214.407 2108.18 210.657 Q2111.15 206.907 2115.85 206.907 Q2118.72 206.907 2120.75 208.041 Q2122.81 209.152 2124.16 211.467 M2109.64 220.518 Q2109.64 225.217 2111.56 227.902 Q2113.51 230.564 2116.89 230.564 Q2120.27 230.564 2122.21 227.902 Q2124.16 225.217 2124.16 220.518 Q2124.16 215.819 2122.21 213.157 Q2120.27 210.472 2116.89 210.472 Q2113.51 210.472 2111.56 213.157 Q2109.64 215.819 2109.64 220.518 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2144.66 220.425 Q2139.5 220.425 2137.51 221.606 Q2135.52 222.786 2135.52 225.634 Q2135.52 227.902 2137 229.245 Q2138.51 230.564 2141.08 230.564 Q2144.62 230.564 2146.75 228.064 Q2148.9 225.541 2148.9 221.374 L2148.9 220.425 L2144.66 220.425 M2153.16 218.666 L2153.16 233.458 L2148.9 233.458 L2148.9 229.522 Q2147.44 231.883 2145.27 233.018 Q2143.09 234.129 2139.94 234.129 Q2135.96 234.129 2133.6 231.907 Q2131.26 229.661 2131.26 225.911 Q2131.26 221.536 2134.18 219.314 Q2137.12 217.092 2142.93 217.092 L2148.9 217.092 L2148.9 216.675 Q2148.9 213.735 2146.96 212.138 Q2145.04 210.518 2141.54 210.518 Q2139.32 210.518 2137.21 211.05 Q2135.1 211.583 2133.16 212.647 L2133.16 208.712 Q2135.5 207.81 2137.7 207.37 Q2139.9 206.907 2141.98 206.907 Q2147.6 206.907 2150.38 209.823 Q2153.16 212.74 2153.16 218.666 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2161.84 200.171 L2161.84 207.532 L2170.61 207.532 L2170.61 210.842 L2161.84 210.842 L2161.84 224.916 Q2161.84 228.087 2162.7 228.99 Q2163.58 229.893 2166.24 229.893 L2170.61 229.893 L2170.61 233.458 L2166.24 233.458 Q2161.31 233.458 2159.43 231.629 Q2157.56 229.777 2157.56 224.916 L2157.56 210.842 L2154.43 210.842 L2154.43 207.532 L2157.56 207.532 L2157.56 200.171 L2161.84 200.171 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2175.08 207.532 L2179.34 207.532 L2179.34 233.458 L2175.08 233.458 L2175.08 207.532 M2175.08 197.439 L2179.34 197.439 L2179.34 202.833 L2175.08 202.833 L2175.08 197.439 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2193.85 210.518 Q2190.43 210.518 2188.44 213.203 Q2186.45 215.865 2186.45 220.518 Q2186.45 225.171 2188.41 227.856 Q2190.41 230.518 2193.85 230.518 Q2197.26 230.518 2199.25 227.833 Q2201.24 225.147 2201.24 220.518 Q2201.24 215.911 2199.25 213.226 Q2197.26 210.518 2193.85 210.518 M2193.85 206.907 Q2199.41 206.907 2202.58 210.518 Q2205.75 214.129 2205.75 220.518 Q2205.75 226.884 2202.58 230.518 Q2199.41 234.129 2193.85 234.129 Q2188.28 234.129 2185.1 230.518 Q2181.96 226.884 2181.96 220.518 Q2181.96 214.129 2185.1 210.518 Q2188.28 206.907 2193.85 206.907 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip240)\" d=\"M 0 0 M2231.77 217.809 L2231.77 233.458 L2227.51 233.458 L2227.51 217.948 Q2227.51 214.268 2226.08 212.439 Q2224.64 210.61 2221.77 210.61 Q2218.32 210.61 2216.33 212.81 Q2214.34 215.009 2214.34 218.805 L2214.34 233.458 L2210.06 233.458 L2210.06 207.532 L2214.34 207.532 L2214.34 211.56 Q2215.87 209.222 2217.93 208.064 Q2220.01 206.907 2222.72 206.907 Q2227.19 206.907 2229.48 209.685 Q2231.77 212.439 2231.77 217.809 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(data_schedule, training_losses, label=\"Training\")\n",
    "plot!(data_schedule, valid_losses, label=\"Validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 10,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = :uniform)\u001b[34m @757\u001b[39m"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_final = KNNClassifier(K=best.best_model.K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{KNNClassifier} @364\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @319\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @853\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_Final = machine(knn_final, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{KNNClassifier} @998\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{KNNClassifier} @998\u001b[39m trained 8 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @849\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @114\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(KNN, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = MLJ.predict(KNN, X_stand[test,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.1576761388633425"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(ŷ, y[test]) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6426332288401254"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(ŷ, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The classes are un-ordered,\n",
      "│ using order: [\"EI\", \"IE\", \"N\"].\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/measures/confusion_matrix.jl:87\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "              ┌─────────────────────────────────────────┐\n",
       "              │              Ground Truth               │\n",
       "┌─────────────┼─────────────┬─────────────┬─────────────┤\n",
       "│  Predicted  │     EI      │     IE      │      N      │\n",
       "├─────────────┼─────────────┼─────────────┼─────────────┤\n",
       "│     EI      │     129     │     40      │     100     │\n",
       "├─────────────┼─────────────┼─────────────┼─────────────┤\n",
       "│     IE      │      5      │     68      │     18      │\n",
       "├─────────────┼─────────────┼─────────────┼─────────────┤\n",
       "│      N      │     19      │     46      │     213     │\n",
       "└─────────────┴─────────────┴─────────────┴─────────────┘\n"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(mode.(ŷ), y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
