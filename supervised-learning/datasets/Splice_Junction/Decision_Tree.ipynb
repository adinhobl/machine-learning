{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using CSV\n",
    "using MLJ\n",
    "using DecisionTree: print_tree\n",
    "using Plots\n",
    "using StatsBase\n",
    "\n",
    "include(\"../../lib.jl\")\n",
    "\n",
    "ENV[\"LINES\"]=30;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Class</th><th> Instance</th><th>                       Nucleotide_Sequence</th></tr><tr><th></th><th>String</th><th>String</th><th>String</th></tr></thead><tbody><p>3,190 rows × 3 columns</p><tr><th>1</th><td>EI</td><td>    ATRINS-DONOR-521</td><td>               CCAGCTGCATCACAGGAGGCCAGCGAGCAGGTCTGTTCCAAGGGCCTTCGAGCCAGTCTG</td></tr><tr><th>2</th><td>EI</td><td>    ATRINS-DONOR-905</td><td>               AGACCCGCCGGGAGGCGGAGGACCTGCAGGGTGAGCCCCACCGCCCCTCCGTGCCCCCGC</td></tr><tr><th>3</th><td>EI</td><td>    BABAPOE-DONOR-30</td><td>               GAGGTGAAGGACGTCCTTCCCCAGGAGCCGGTGAGAAGCGCAGTCGGGGGCACGGGGATG</td></tr><tr><th>4</th><td>EI</td><td>    BABAPOE-DONOR-867</td><td>              GGGCTGCGTTGCTGGTCACATTCCTGGCAGGTATGGGGCGGGGCTTGCTCGGTTTTCCCC</td></tr><tr><th>5</th><td>EI</td><td>    BABAPOE-DONOR-2817</td><td>             GCTCAGCCCCCAGGTCACCCAGGAACTGACGTGAGTGTCCCCATCCCGGCCCTTGACCCT</td></tr><tr><th>6</th><td>EI</td><td>    CHPIGECA-DONOR-378</td><td>             CAGACTGGGTGGACAACAAAACCTTCAGCGGTAAGAGAGGGCCAAGCTCAGAGACCACAG</td></tr><tr><th>7</th><td>EI</td><td>    CHPIGECA-DONOR-903</td><td>             CCTTTGAGGACAGCACCAAGAAGTGTGCAGGTACGTTCCCACCTGCCCTGGTGGCCGCCA</td></tr><tr><th>8</th><td>EI</td><td>    CHPIGECA-DONOR-1313</td><td>            CCCTCGTGCGGTCCACGACCAAGACCAGCGGTGAGCCACGGGCAGGCCGGGGTCGTGGGG</td></tr><tr><th>9</th><td>EI</td><td>    GCRHBBA1-DONOR-1260</td><td>            TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTCCTGTCCCTGCTCCAGTCC</td></tr><tr><th>10</th><td>EI</td><td>    GCRHBBA1-DONOR-1590</td><td>            AAGCTGACAGTGGACCCGGTCAACTTCAAGGTGAGCCAGGAGTCGGGTGGGAGGGTGAGA</td></tr><tr><th>11</th><td>EI</td><td>    GCRHBBA6-DONOR-461</td><td>             TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTGGTATCCCTGCTGCCAGTC</td></tr><tr><th>12</th><td>EI</td><td>    GCRHBBA6-DONOR-795</td><td>             AAGCTGAGAGTGGACCCTGTCAACTTCAAGGTGAGCCACCAGTCGGGTGGGGAGGGTGAG</td></tr><tr><th>13</th><td>EI</td><td>    GIBHBGGL-DONOR-2278</td><td>            GGAAGATGCTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG</td></tr><tr><th>14</th><td>EI</td><td>    GIBHBGGL-DONOR-2624</td><td>            AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG</td></tr><tr><th>15</th><td>EI</td><td>    GIBHBGGL-DONOR-7198</td><td>            GGAAGATGTTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG</td></tr><tr><th>16</th><td>EI</td><td>    GIBHBGGL-DONOR-7544</td><td>            AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG</td></tr><tr><th>17</th><td>EI</td><td>    HUMA1ATP-DONOR-1972</td><td>            GGCACCACCACTGACCTGGGACAGTGAATCGTAAGTATGCCTTTCACTGCGAGGGGTTCT</td></tr><tr><th>18</th><td>EI</td><td>    HUMA1ATP-DONOR-7932</td><td>            TTGCTCTGGTGAATTACATCTTCTTTAAAGGTAAGGTTGCTCAACCAGCCTGAGCTGTTT</td></tr><tr><th>19</th><td>EI</td><td>    HUMA1ATP-DONOR-9653</td><td>            CACCAAGTTCCTGGAAAATGAAGACAGAAGGTGATTCCCCAACCTGAGGGTGACCAAGAA</td></tr><tr><th>20</th><td>EI</td><td>    HUMA1ATP-DONOR-11057</td><td>           ACAGAGGAGGCACCCCTGAAGCTCTCCAAGGTGAGATCACCCTGACGACCTTGTTGCACC</td></tr><tr><th>21</th><td>EI</td><td>    HUMA1GLY2-DONOR-1693</td><td>           GTGCCCATCACCAACGCCACCCTGGACCGGGTGAGTGCCTGGGCTAGCCCTGTCCTGAGC</td></tr><tr><th>22</th><td>EI</td><td>    HUMA1GLY2-DONOR-2251</td><td>           CACGATCTTTCTCAGAGAGTACCAGACCCGGTGAGAGCCCCCATTCCAATGCACCCCCGA</td></tr><tr><th>23</th><td>EI</td><td>    HUMA1GLY2-DONOR-2540</td><td>           AGCGGGAGAATGGGACCGTCTCCAGATACGGTGAGGGCCAGCCCTCAGGCAGGAGGGTTC</td></tr><tr><th>24</th><td>EI</td><td>    HUMA1GLY2-DONOR-3352</td><td>           ATGAGAAGAACTGGGGGCTGTCTTTCTATGGTAGGCATGCTTAGCAGCCCCAAACTCATG</td></tr><tr><th>25</th><td>EI</td><td>    HUMA1GLY2-DONOR-3606</td><td>           TCAGATGTCATGTACACCGACTGGAAAAAGGTAAACGCAAGGGATTGGACATTGCCCACC</td></tr><tr><th>26</th><td>EI</td><td>    HUMACCYBA-DONOR-289</td><td>            GATCCGCCGCCCGTCCACACCCGCCGCCAGGTAAGCCCGGCCAGCCGACCGGGGCATGCG</td></tr><tr><th>27</th><td>EI</td><td>    HUMACCYBA-DONOR-1250</td><td>           CCCTCCATCGTGGGGCGCCCCAGGCACCAGGTAGGGGAGCTGGCTGGGTGGGGCAGCCCC</td></tr><tr><th>28</th><td>EI</td><td>    HUMACCYBA-DONOR-1624</td><td>           CCCAAGGCCAACCGCGAGAAGATGACCCAGGTGAGTGGCCCGCTACCTCTTCTGGTGGCC</td></tr><tr><th>29</th><td>EI</td><td>    HUMACCYBA-DONOR-2504</td><td>           CTGAGGCACTCTTCCAGCCTTCCTTCCTGGGTGAGTGGAGACTGTCTCCCGGCTCTGCCT</td></tr><tr><th>30</th><td>EI</td><td>    HUMACCYBA-DONOR-2781</td><td>           GCCCTGGCACCCAGCACAATGAAGATCAAGGTGGGTGTCTTTCCTGCCTGAGCTGACCTG</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t& Class &  Instance &                        Nucleotide\\_Sequence\\\\\n",
       "\t\\hline\n",
       "\t& String & String & String\\\\\n",
       "\t\\hline\n",
       "\t1 & EI &     ATRINS-DONOR-521 &                CCAGCTGCATCACAGGAGGCCAGCGAGCAGGTCTGTTCCAAGGGCCTTCGAGCCAGTCTG \\\\\n",
       "\t2 & EI &     ATRINS-DONOR-905 &                AGACCCGCCGGGAGGCGGAGGACCTGCAGGGTGAGCCCCACCGCCCCTCCGTGCCCCCGC \\\\\n",
       "\t3 & EI &     BABAPOE-DONOR-30 &                GAGGTGAAGGACGTCCTTCCCCAGGAGCCGGTGAGAAGCGCAGTCGGGGGCACGGGGATG \\\\\n",
       "\t4 & EI &     BABAPOE-DONOR-867 &               GGGCTGCGTTGCTGGTCACATTCCTGGCAGGTATGGGGCGGGGCTTGCTCGGTTTTCCCC \\\\\n",
       "\t5 & EI &     BABAPOE-DONOR-2817 &              GCTCAGCCCCCAGGTCACCCAGGAACTGACGTGAGTGTCCCCATCCCGGCCCTTGACCCT \\\\\n",
       "\t6 & EI &     CHPIGECA-DONOR-378 &              CAGACTGGGTGGACAACAAAACCTTCAGCGGTAAGAGAGGGCCAAGCTCAGAGACCACAG \\\\\n",
       "\t7 & EI &     CHPIGECA-DONOR-903 &              CCTTTGAGGACAGCACCAAGAAGTGTGCAGGTACGTTCCCACCTGCCCTGGTGGCCGCCA \\\\\n",
       "\t8 & EI &     CHPIGECA-DONOR-1313 &             CCCTCGTGCGGTCCACGACCAAGACCAGCGGTGAGCCACGGGCAGGCCGGGGTCGTGGGG \\\\\n",
       "\t9 & EI &     GCRHBBA1-DONOR-1260 &             TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTCCTGTCCCTGCTCCAGTCC \\\\\n",
       "\t10 & EI &     GCRHBBA1-DONOR-1590 &             AAGCTGACAGTGGACCCGGTCAACTTCAAGGTGAGCCAGGAGTCGGGTGGGAGGGTGAGA \\\\\n",
       "\t11 & EI &     GCRHBBA6-DONOR-461 &              TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTGGTATCCCTGCTGCCAGTC \\\\\n",
       "\t12 & EI &     GCRHBBA6-DONOR-795 &              AAGCTGAGAGTGGACCCTGTCAACTTCAAGGTGAGCCACCAGTCGGGTGGGGAGGGTGAG \\\\\n",
       "\t13 & EI &     GIBHBGGL-DONOR-2278 &             GGAAGATGCTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG \\\\\n",
       "\t14 & EI &     GIBHBGGL-DONOR-2624 &             AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG \\\\\n",
       "\t15 & EI &     GIBHBGGL-DONOR-7198 &             GGAAGATGTTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG \\\\\n",
       "\t16 & EI &     GIBHBGGL-DONOR-7544 &             AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG \\\\\n",
       "\t17 & EI &     HUMA1ATP-DONOR-1972 &             GGCACCACCACTGACCTGGGACAGTGAATCGTAAGTATGCCTTTCACTGCGAGGGGTTCT \\\\\n",
       "\t18 & EI &     HUMA1ATP-DONOR-7932 &             TTGCTCTGGTGAATTACATCTTCTTTAAAGGTAAGGTTGCTCAACCAGCCTGAGCTGTTT \\\\\n",
       "\t19 & EI &     HUMA1ATP-DONOR-9653 &             CACCAAGTTCCTGGAAAATGAAGACAGAAGGTGATTCCCCAACCTGAGGGTGACCAAGAA \\\\\n",
       "\t20 & EI &     HUMA1ATP-DONOR-11057 &            ACAGAGGAGGCACCCCTGAAGCTCTCCAAGGTGAGATCACCCTGACGACCTTGTTGCACC \\\\\n",
       "\t21 & EI &     HUMA1GLY2-DONOR-1693 &            GTGCCCATCACCAACGCCACCCTGGACCGGGTGAGTGCCTGGGCTAGCCCTGTCCTGAGC \\\\\n",
       "\t22 & EI &     HUMA1GLY2-DONOR-2251 &            CACGATCTTTCTCAGAGAGTACCAGACCCGGTGAGAGCCCCCATTCCAATGCACCCCCGA \\\\\n",
       "\t23 & EI &     HUMA1GLY2-DONOR-2540 &            AGCGGGAGAATGGGACCGTCTCCAGATACGGTGAGGGCCAGCCCTCAGGCAGGAGGGTTC \\\\\n",
       "\t24 & EI &     HUMA1GLY2-DONOR-3352 &            ATGAGAAGAACTGGGGGCTGTCTTTCTATGGTAGGCATGCTTAGCAGCCCCAAACTCATG \\\\\n",
       "\t25 & EI &     HUMA1GLY2-DONOR-3606 &            TCAGATGTCATGTACACCGACTGGAAAAAGGTAAACGCAAGGGATTGGACATTGCCCACC \\\\\n",
       "\t26 & EI &     HUMACCYBA-DONOR-289 &             GATCCGCCGCCCGTCCACACCCGCCGCCAGGTAAGCCCGGCCAGCCGACCGGGGCATGCG \\\\\n",
       "\t27 & EI &     HUMACCYBA-DONOR-1250 &            CCCTCCATCGTGGGGCGCCCCAGGCACCAGGTAGGGGAGCTGGCTGGGTGGGGCAGCCCC \\\\\n",
       "\t28 & EI &     HUMACCYBA-DONOR-1624 &            CCCAAGGCCAACCGCGAGAAGATGACCCAGGTGAGTGGCCCGCTACCTCTTCTGGTGGCC \\\\\n",
       "\t29 & EI &     HUMACCYBA-DONOR-2504 &            CTGAGGCACTCTTCCAGCCTTCCTTCCTGGGTGAGTGGAGACTGTCTCCCGGCTCTGCCT \\\\\n",
       "\t30 & EI &     HUMACCYBA-DONOR-2781 &            GCCCTGGCACCCAGCACAATGAAGATCAAGGTGGGTGTCTTTCCTGCCTGAGCTGACCTG \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "3190×3 DataFrame. Omitted printing of 1 columns\n",
       "│ Row  │ Class  │  Instance               │\n",
       "│      │ \u001b[90mString\u001b[39m │ \u001b[90mString\u001b[39m                  │\n",
       "├──────┼────────┼─────────────────────────┤\n",
       "│ 1    │ EI     │     ATRINS-DONOR-521    │\n",
       "│ 2    │ EI     │     ATRINS-DONOR-905    │\n",
       "│ 3    │ EI     │     BABAPOE-DONOR-30    │\n",
       "│ 4    │ EI     │     BABAPOE-DONOR-867   │\n",
       "│ 5    │ EI     │     BABAPOE-DONOR-2817  │\n",
       "│ 6    │ EI     │     CHPIGECA-DONOR-378  │\n",
       "│ 7    │ EI     │     CHPIGECA-DONOR-903  │\n",
       "│ 8    │ EI     │     CHPIGECA-DONOR-1313 │\n",
       "│ 9    │ EI     │     GCRHBBA1-DONOR-1260 │\n",
       "│ 10   │ EI     │     GCRHBBA1-DONOR-1590 │\n",
       "⋮\n",
       "│ 3180 │ N      │      MNKHBPSBD-NEG-961  │\n",
       "│ 3181 │ N      │      ORAHBA01-NEG-121   │\n",
       "│ 3182 │ N      │      ORAHBBE-NEG-2581   │\n",
       "│ 3183 │ N      │      ORAHBBPSE-NEG-2101 │\n",
       "│ 3184 │ N      │      ORAHBBPSE-NEG-6661 │\n",
       "│ 3185 │ N      │      ORAHBG2F-NEG-181   │\n",
       "│ 3186 │ N      │      ORAHBPSBD-NEG-2881 │\n",
       "│ 3187 │ N      │      ORAINVOL-NEG-2161  │\n",
       "│ 3188 │ N      │      ORARGIT-NEG-241    │\n",
       "│ 3189 │ N      │      TARHBB-NEG-541     │\n",
       "│ 3190 │ N      │      TARHBD-NEG-1981    │"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CSV.read(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "separate_bases (generic function with 1 method)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function separate_bases(df)\n",
    "    d = map(s->[i for i in strip(s)], data[:,3])\n",
    "    a = zeros(size(d)[1], size(d[1])[1])\n",
    "    \n",
    "    for i in 1:size(a)[1]\n",
    "        a[i,:] = d[i]\n",
    "    end\n",
    "    \n",
    "    return DataFrame(a)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>x1</th><th>x2</th><th>x3</th><th>x4</th><th>x5</th><th>x6</th><th>x7</th><th>x8</th><th>x9</th></tr><tr><th></th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>3,190 rows × 60 columns (omitted printing of 51 columns)</p><tr><th>1</th><td>67.0</td><td>67.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>65.0</td></tr><tr><th>2</th><td>65.0</td><td>71.0</td><td>65.0</td><td>67.0</td><td>67.0</td><td>67.0</td><td>71.0</td><td>67.0</td><td>67.0</td></tr><tr><th>3</th><td>71.0</td><td>65.0</td><td>71.0</td><td>71.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>65.0</td><td>71.0</td></tr><tr><th>4</th><td>71.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>71.0</td><td>84.0</td></tr><tr><th>5</th><td>71.0</td><td>67.0</td><td>84.0</td><td>67.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>67.0</td><td>67.0</td></tr><tr><th>6</th><td>67.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>71.0</td><td>71.0</td></tr><tr><th>7</th><td>67.0</td><td>67.0</td><td>84.0</td><td>84.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>71.0</td><td>71.0</td></tr><tr><th>8</th><td>67.0</td><td>67.0</td><td>67.0</td><td>84.0</td><td>67.0</td><td>71.0</td><td>84.0</td><td>71.0</td><td>67.0</td></tr><tr><th>9</th><td>84.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>71.0</td><td>65.0</td><td>67.0</td><td>84.0</td><td>65.0</td></tr><tr><th>10</th><td>65.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>67.0</td><td>65.0</td></tr><tr><th>11</th><td>84.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>71.0</td><td>65.0</td><td>67.0</td><td>84.0</td><td>65.0</td></tr><tr><th>12</th><td>65.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>71.0</td><td>65.0</td></tr><tr><th>13</th><td>71.0</td><td>71.0</td><td>65.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>84.0</td><td>71.0</td><td>67.0</td></tr><tr><th>14</th><td>65.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>65.0</td><td>84.0</td></tr><tr><th>15</th><td>71.0</td><td>71.0</td><td>65.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>84.0</td><td>71.0</td><td>84.0</td></tr><tr><th>16</th><td>65.0</td><td>65.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>65.0</td><td>84.0</td></tr><tr><th>17</th><td>71.0</td><td>71.0</td><td>67.0</td><td>65.0</td><td>67.0</td><td>67.0</td><td>65.0</td><td>67.0</td><td>67.0</td></tr><tr><th>18</th><td>84.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>84.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>71.0</td></tr><tr><th>19</th><td>67.0</td><td>65.0</td><td>67.0</td><td>67.0</td><td>65.0</td><td>65.0</td><td>71.0</td><td>84.0</td><td>84.0</td></tr><tr><th>20</th><td>65.0</td><td>67.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>71.0</td><td>71.0</td><td>65.0</td><td>71.0</td></tr><tr><th>21</th><td>71.0</td><td>84.0</td><td>71.0</td><td>67.0</td><td>67.0</td><td>67.0</td><td>65.0</td><td>84.0</td><td>67.0</td></tr><tr><th>22</th><td>67.0</td><td>65.0</td><td>67.0</td><td>71.0</td><td>65.0</td><td>84.0</td><td>67.0</td><td>84.0</td><td>84.0</td></tr><tr><th>23</th><td>65.0</td><td>71.0</td><td>67.0</td><td>71.0</td><td>71.0</td><td>71.0</td><td>65.0</td><td>71.0</td><td>65.0</td></tr><tr><th>24</th><td>65.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>65.0</td><td>71.0</td><td>65.0</td></tr><tr><th>25</th><td>84.0</td><td>67.0</td><td>65.0</td><td>71.0</td><td>65.0</td><td>84.0</td><td>71.0</td><td>84.0</td><td>67.0</td></tr><tr><th>26</th><td>71.0</td><td>65.0</td><td>84.0</td><td>67.0</td><td>67.0</td><td>71.0</td><td>67.0</td><td>67.0</td><td>71.0</td></tr><tr><th>27</th><td>67.0</td><td>67.0</td><td>67.0</td><td>84.0</td><td>67.0</td><td>67.0</td><td>65.0</td><td>84.0</td><td>67.0</td></tr><tr><th>28</th><td>67.0</td><td>67.0</td><td>67.0</td><td>65.0</td><td>65.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>67.0</td></tr><tr><th>29</th><td>67.0</td><td>84.0</td><td>71.0</td><td>65.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>65.0</td><td>67.0</td></tr><tr><th>30</th><td>71.0</td><td>67.0</td><td>67.0</td><td>67.0</td><td>84.0</td><td>71.0</td><td>71.0</td><td>67.0</td><td>65.0</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccccccc}\n",
       "\t& x1 & x2 & x3 & x4 & x5 & x6 & x7 & x8 & x9 & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 67.0 & 67.0 & 65.0 & 71.0 & 67.0 & 84.0 & 71.0 & 67.0 & 65.0 & $\\dots$ \\\\\n",
       "\t2 & 65.0 & 71.0 & 65.0 & 67.0 & 67.0 & 67.0 & 71.0 & 67.0 & 67.0 & $\\dots$ \\\\\n",
       "\t3 & 71.0 & 65.0 & 71.0 & 71.0 & 84.0 & 71.0 & 65.0 & 65.0 & 71.0 & $\\dots$ \\\\\n",
       "\t4 & 71.0 & 71.0 & 71.0 & 67.0 & 84.0 & 71.0 & 67.0 & 71.0 & 84.0 & $\\dots$ \\\\\n",
       "\t5 & 71.0 & 67.0 & 84.0 & 67.0 & 65.0 & 71.0 & 67.0 & 67.0 & 67.0 & $\\dots$ \\\\\n",
       "\t6 & 67.0 & 65.0 & 71.0 & 65.0 & 67.0 & 84.0 & 71.0 & 71.0 & 71.0 & $\\dots$ \\\\\n",
       "\t7 & 67.0 & 67.0 & 84.0 & 84.0 & 84.0 & 71.0 & 65.0 & 71.0 & 71.0 & $\\dots$ \\\\\n",
       "\t8 & 67.0 & 67.0 & 67.0 & 84.0 & 67.0 & 71.0 & 84.0 & 71.0 & 67.0 & $\\dots$ \\\\\n",
       "\t9 & 84.0 & 71.0 & 71.0 & 67.0 & 71.0 & 65.0 & 67.0 & 84.0 & 65.0 & $\\dots$ \\\\\n",
       "\t10 & 65.0 & 65.0 & 71.0 & 67.0 & 84.0 & 71.0 & 65.0 & 67.0 & 65.0 & $\\dots$ \\\\\n",
       "\t11 & 84.0 & 71.0 & 71.0 & 67.0 & 71.0 & 65.0 & 67.0 & 84.0 & 65.0 & $\\dots$ \\\\\n",
       "\t12 & 65.0 & 65.0 & 71.0 & 67.0 & 84.0 & 71.0 & 65.0 & 71.0 & 65.0 & $\\dots$ \\\\\n",
       "\t13 & 71.0 & 71.0 & 65.0 & 65.0 & 71.0 & 65.0 & 84.0 & 71.0 & 67.0 & $\\dots$ \\\\\n",
       "\t14 & 65.0 & 65.0 & 71.0 & 67.0 & 84.0 & 71.0 & 67.0 & 65.0 & 84.0 & $\\dots$ \\\\\n",
       "\t15 & 71.0 & 71.0 & 65.0 & 65.0 & 71.0 & 65.0 & 84.0 & 71.0 & 84.0 & $\\dots$ \\\\\n",
       "\t16 & 65.0 & 65.0 & 71.0 & 67.0 & 84.0 & 71.0 & 67.0 & 65.0 & 84.0 & $\\dots$ \\\\\n",
       "\t17 & 71.0 & 71.0 & 67.0 & 65.0 & 67.0 & 67.0 & 65.0 & 67.0 & 67.0 & $\\dots$ \\\\\n",
       "\t18 & 84.0 & 84.0 & 71.0 & 67.0 & 84.0 & 67.0 & 84.0 & 71.0 & 71.0 & $\\dots$ \\\\\n",
       "\t19 & 67.0 & 65.0 & 67.0 & 67.0 & 65.0 & 65.0 & 71.0 & 84.0 & 84.0 & $\\dots$ \\\\\n",
       "\t20 & 65.0 & 67.0 & 65.0 & 71.0 & 65.0 & 71.0 & 71.0 & 65.0 & 71.0 & $\\dots$ \\\\\n",
       "\t21 & 71.0 & 84.0 & 71.0 & 67.0 & 67.0 & 67.0 & 65.0 & 84.0 & 67.0 & $\\dots$ \\\\\n",
       "\t22 & 67.0 & 65.0 & 67.0 & 71.0 & 65.0 & 84.0 & 67.0 & 84.0 & 84.0 & $\\dots$ \\\\\n",
       "\t23 & 65.0 & 71.0 & 67.0 & 71.0 & 71.0 & 71.0 & 65.0 & 71.0 & 65.0 & $\\dots$ \\\\\n",
       "\t24 & 65.0 & 84.0 & 71.0 & 65.0 & 71.0 & 65.0 & 65.0 & 71.0 & 65.0 & $\\dots$ \\\\\n",
       "\t25 & 84.0 & 67.0 & 65.0 & 71.0 & 65.0 & 84.0 & 71.0 & 84.0 & 67.0 & $\\dots$ \\\\\n",
       "\t26 & 71.0 & 65.0 & 84.0 & 67.0 & 67.0 & 71.0 & 67.0 & 67.0 & 71.0 & $\\dots$ \\\\\n",
       "\t27 & 67.0 & 67.0 & 67.0 & 84.0 & 67.0 & 67.0 & 65.0 & 84.0 & 67.0 & $\\dots$ \\\\\n",
       "\t28 & 67.0 & 67.0 & 67.0 & 65.0 & 65.0 & 71.0 & 71.0 & 67.0 & 67.0 & $\\dots$ \\\\\n",
       "\t29 & 67.0 & 84.0 & 71.0 & 65.0 & 71.0 & 71.0 & 67.0 & 65.0 & 67.0 & $\\dots$ \\\\\n",
       "\t30 & 71.0 & 67.0 & 67.0 & 67.0 & 84.0 & 71.0 & 71.0 & 67.0 & 65.0 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "3190×60 DataFrame. Omitted printing of 53 columns\n",
       "│ Row  │ x1      │ x2      │ x3      │ x4      │ x5      │ x6      │ x7      │\n",
       "│      │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │ \u001b[90mFloat64\u001b[39m │\n",
       "├──────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┼─────────┤\n",
       "│ 1    │ 67.0    │ 67.0    │ 65.0    │ 71.0    │ 67.0    │ 84.0    │ 71.0    │\n",
       "│ 2    │ 65.0    │ 71.0    │ 65.0    │ 67.0    │ 67.0    │ 67.0    │ 71.0    │\n",
       "│ 3    │ 71.0    │ 65.0    │ 71.0    │ 71.0    │ 84.0    │ 71.0    │ 65.0    │\n",
       "│ 4    │ 71.0    │ 71.0    │ 71.0    │ 67.0    │ 84.0    │ 71.0    │ 67.0    │\n",
       "│ 5    │ 71.0    │ 67.0    │ 84.0    │ 67.0    │ 65.0    │ 71.0    │ 67.0    │\n",
       "│ 6    │ 67.0    │ 65.0    │ 71.0    │ 65.0    │ 67.0    │ 84.0    │ 71.0    │\n",
       "│ 7    │ 67.0    │ 67.0    │ 84.0    │ 84.0    │ 84.0    │ 71.0    │ 65.0    │\n",
       "│ 8    │ 67.0    │ 67.0    │ 67.0    │ 84.0    │ 67.0    │ 71.0    │ 84.0    │\n",
       "│ 9    │ 84.0    │ 71.0    │ 71.0    │ 67.0    │ 71.0    │ 65.0    │ 67.0    │\n",
       "│ 10   │ 65.0    │ 65.0    │ 71.0    │ 67.0    │ 84.0    │ 71.0    │ 65.0    │\n",
       "⋮\n",
       "│ 3180 │ 67.0    │ 67.0    │ 84.0    │ 67.0    │ 65.0    │ 71.0    │ 84.0    │\n",
       "│ 3181 │ 67.0    │ 67.0    │ 84.0    │ 71.0    │ 67.0    │ 67.0    │ 71.0    │\n",
       "│ 3182 │ 67.0    │ 84.0    │ 71.0    │ 71.0    │ 65.0    │ 65.0    │ 71.0    │\n",
       "│ 3183 │ 84.0    │ 71.0    │ 84.0    │ 84.0    │ 84.0    │ 67.0    │ 84.0    │\n",
       "│ 3184 │ 84.0    │ 65.0    │ 65.0    │ 65.0    │ 65.0    │ 65.0    │ 65.0    │\n",
       "│ 3185 │ 65.0    │ 84.0    │ 67.0    │ 65.0    │ 65.0    │ 84.0    │ 65.0    │\n",
       "│ 3186 │ 84.0    │ 67.0    │ 84.0    │ 67.0    │ 84.0    │ 84.0    │ 67.0    │\n",
       "│ 3187 │ 71.0    │ 65.0    │ 71.0    │ 67.0    │ 84.0    │ 67.0    │ 67.0    │\n",
       "│ 3188 │ 84.0    │ 67.0    │ 84.0    │ 67.0    │ 71.0    │ 71.0    │ 71.0    │\n",
       "│ 3189 │ 65.0    │ 84.0    │ 84.0    │ 67.0    │ 84.0    │ 65.0    │ 67.0    │\n",
       "│ 3190 │ 65.0    │ 71.0    │ 71.0    │ 67.0    │ 84.0    │ 71.0    │ 67.0    │"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "separate_bases(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "DataFrame <: AbstractDataFrame\n",
       "\\end{verbatim}\n",
       "An AbstractDataFrame that stores a set of named columns\n",
       "\n",
       "The columns are normally AbstractVectors stored in memory, particularly a Vector or CategoricalVector.\n",
       "\n",
       "\\section{Constructors}\n",
       "\\begin{verbatim}\n",
       "DataFrame(columns::AbstractVector, names::AbstractVector{Symbol};\n",
       "          makeunique::Bool=false, copycols::Bool=true)\n",
       "DataFrame(columns::AbstractVector, names::AbstractVector{<:AbstractString};\n",
       "          makeunique::Bool=false, copycols::Bool=true)\n",
       "DataFrame(columns::NTuple{N,AbstractVector}, names::NTuple{N,Symbol};\n",
       "          makeunique::Bool=false, copycols::Bool=true)\n",
       "DataFrame(columns::NTuple{N,AbstractVector}, names::NTuple{N,<:AbstractString};\n",
       "          makeunique::Bool=false, copycols::Bool=true)\n",
       "DataFrame(columns::Matrix, names::AbstractVector{Symbol}; makeunique::Bool=false)\n",
       "DataFrame(columns::Matrix, names::AbstractVector{<:AbstractString};\n",
       "          makeunique::Bool=false)\n",
       "DataFrame(kwargs...)\n",
       "DataFrame(pairs::Pair{Symbol,<:Any}...; makeunique::Bool=false, copycols::Bool=true)\n",
       "DataFrame(pairs::Pair{<:AbstractString,<:Any}...; makeunique::Bool=false,\n",
       "          copycols::Bool=true)\n",
       "DataFrame() # an empty DataFrame\n",
       "DataFrame(column_eltypes::AbstractVector, names::AbstractVector{Symbol},\n",
       "          nrows::Integer=0; makeunique::Bool=false)\n",
       "DataFrame(column_eltypes::AbstractVector, names::AbstractVector{<:AbstractString},\n",
       "          nrows::Integer=0; makeunique::Bool=false)\n",
       "DataFrame(ds::AbstractDict; copycols::Bool=true)\n",
       "DataFrame(table; makeunique::Bool=false, copycols::Bool=true)\n",
       "DataFrame(::Union{DataFrame, SubDataFrame}; copycols::Bool=true)\n",
       "DataFrame(::GroupedDataFrame; keepkeys::Bool=true)\n",
       "\\end{verbatim}\n",
       "\\section{Arguments}\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{columns} : a Vector with each column as contents or a Matrix\n",
       "\n",
       "\n",
       "\\item \\texttt{names} : the column names\n",
       "\n",
       "\n",
       "\\item \\texttt{makeunique} : if \\texttt{false} (the default), an error will be raised if duplicates in \\texttt{names} are found; if \\texttt{true}, duplicate names will be suffixed with \\texttt{\\_i} (\\texttt{i} starting at 1 for the first duplicate).\n",
       "\n",
       "\n",
       "\\item \\texttt{kwargs} : the key gives the column names, and the value is the column contents; note that the \\texttt{copycols} keyword argument indicates if if vectors passed as columns should be copied so it is not possible to create a column whose name is \\texttt{:copycols} using this constructor\n",
       "\n",
       "\n",
       "\\item \\texttt{t} : elemental type of all columns\n",
       "\n",
       "\n",
       "\\item \\texttt{nrows}, \\texttt{ncols} : number of rows and columns\n",
       "\n",
       "\n",
       "\\item \\texttt{column\\_eltypes} : element type of each column\n",
       "\n",
       "\n",
       "\\item \\texttt{categorical} : a vector of \\texttt{Bool} indicating which columns should be converted to \\texttt{CategoricalVector}\n",
       "\n",
       "\n",
       "\\item \\texttt{ds} : \\texttt{AbstractDict} of columns\n",
       "\n",
       "\n",
       "\\item \\texttt{table} : any type that implements the \\href{https://github.com/JuliaData/Tables.jl}{Tables.jl} interface; in particular a tuple or vector of \\texttt{Pair\\{Symbol, <:AbstractVector\\}\\}} objects is a table.\n",
       "\n",
       "\n",
       "\\item \\texttt{copycols} : whether vectors passed as columns should be copied; if set to \\texttt{false} then the constructor will still copy the passed columns if it is not possible to construct a \\texttt{DataFrame} without materializing new columns.\n",
       "\n",
       "\\end{itemize}\n",
       "All columns in \\texttt{columns} must be \\texttt{AbstractVector}s and have the same length. An exception are \\texttt{DataFrame(kwargs...)} and \\texttt{DataFrame(pairs::Pair...)} form constructors which additionally allow a column to be of any other type that is not an \\texttt{AbstractArray}, in which case the passed value is automatically repeated to fill a new vector of the appropriate length. As a particular rule values stored in a \\texttt{Ref} or a \\texttt{0}-dimensional \\texttt{AbstractArray} are unwrapped and treated in the same way.\n",
       "\n",
       "Additionally \\texttt{DataFrame} can be used to collect a \\href{@ref}{\\texttt{GroupedDataFrame}} into a \\texttt{DataFrame}. In this case the order of rows in the result follows the order of groups in the \\texttt{GroupedDataFrame} passed.\n",
       "\n",
       "\\section{Notes}\n",
       "The \\texttt{DataFrame} constructor by default copies all columns vectors passed to it. Pass \\texttt{copycols=false} to reuse vectors without copying them\n",
       "\n",
       "If a column is passed to a \\texttt{DataFrame} constructor or is assigned as a whole using \\texttt{setindex!} then its reference is stored in the \\texttt{DataFrame}. An exception to this rule is assignment of an \\texttt{AbstractRange} as a column, in which case the range is collected to a \\texttt{Vector}.\n",
       "\n",
       "Because column types can vary, a \\texttt{DataFrame} is not type stable. For performance-critical code, do not index into a \\texttt{DataFrame} inside of loops.\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "df = DataFrame()\n",
       "v = [\"x\",\"y\",\"z\"][rand(1:3, 10)]\n",
       "df1 = DataFrame(Any[collect(1:10), v, rand(10)], [:A, :B, :C])\n",
       "df2 = DataFrame(A = 1:10, B = v, C = rand(10))\n",
       "summary(df1)\n",
       "describe(df2)\n",
       "first(df1, 10)\n",
       "df1.B\n",
       "df2[!, :C]\n",
       "df1[:, :A]\n",
       "df1[1:4, 1:2]\n",
       "df1[Not(1:4), Not(1:2)]\n",
       "df1[1:2, [:A,:C]]\n",
       "df1[1:2, r\"[AC]\"]\n",
       "df1[:, [:A,:C]]\n",
       "df1[:, [1,3]]\n",
       "df1[1:4, :]\n",
       "df1[1:4, :C]\n",
       "df1[1:4, :C] = 40. * df1[1:4, :C]\n",
       "[df1; df2]  # vcat\n",
       "[df1 df2]  # hcat\n",
       "size(df1)\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "DataFrame <: AbstractDataFrame\n",
       "```\n",
       "\n",
       "An AbstractDataFrame that stores a set of named columns\n",
       "\n",
       "The columns are normally AbstractVectors stored in memory, particularly a Vector or CategoricalVector.\n",
       "\n",
       "# Constructors\n",
       "\n",
       "```julia\n",
       "DataFrame(columns::AbstractVector, names::AbstractVector{Symbol};\n",
       "          makeunique::Bool=false, copycols::Bool=true)\n",
       "DataFrame(columns::AbstractVector, names::AbstractVector{<:AbstractString};\n",
       "          makeunique::Bool=false, copycols::Bool=true)\n",
       "DataFrame(columns::NTuple{N,AbstractVector}, names::NTuple{N,Symbol};\n",
       "          makeunique::Bool=false, copycols::Bool=true)\n",
       "DataFrame(columns::NTuple{N,AbstractVector}, names::NTuple{N,<:AbstractString};\n",
       "          makeunique::Bool=false, copycols::Bool=true)\n",
       "DataFrame(columns::Matrix, names::AbstractVector{Symbol}; makeunique::Bool=false)\n",
       "DataFrame(columns::Matrix, names::AbstractVector{<:AbstractString};\n",
       "          makeunique::Bool=false)\n",
       "DataFrame(kwargs...)\n",
       "DataFrame(pairs::Pair{Symbol,<:Any}...; makeunique::Bool=false, copycols::Bool=true)\n",
       "DataFrame(pairs::Pair{<:AbstractString,<:Any}...; makeunique::Bool=false,\n",
       "          copycols::Bool=true)\n",
       "DataFrame() # an empty DataFrame\n",
       "DataFrame(column_eltypes::AbstractVector, names::AbstractVector{Symbol},\n",
       "          nrows::Integer=0; makeunique::Bool=false)\n",
       "DataFrame(column_eltypes::AbstractVector, names::AbstractVector{<:AbstractString},\n",
       "          nrows::Integer=0; makeunique::Bool=false)\n",
       "DataFrame(ds::AbstractDict; copycols::Bool=true)\n",
       "DataFrame(table; makeunique::Bool=false, copycols::Bool=true)\n",
       "DataFrame(::Union{DataFrame, SubDataFrame}; copycols::Bool=true)\n",
       "DataFrame(::GroupedDataFrame; keepkeys::Bool=true)\n",
       "```\n",
       "\n",
       "# Arguments\n",
       "\n",
       "  * `columns` : a Vector with each column as contents or a Matrix\n",
       "  * `names` : the column names\n",
       "  * `makeunique` : if `false` (the default), an error will be raised if duplicates in `names` are found; if `true`, duplicate names will be suffixed with `_i` (`i` starting at 1 for the first duplicate).\n",
       "  * `kwargs` : the key gives the column names, and the value is the column contents; note that the `copycols` keyword argument indicates if if vectors passed as columns should be copied so it is not possible to create a column whose name is `:copycols` using this constructor\n",
       "  * `t` : elemental type of all columns\n",
       "  * `nrows`, `ncols` : number of rows and columns\n",
       "  * `column_eltypes` : element type of each column\n",
       "  * `categorical` : a vector of `Bool` indicating which columns should be converted to `CategoricalVector`\n",
       "  * `ds` : `AbstractDict` of columns\n",
       "  * `table` : any type that implements the [Tables.jl](https://github.com/JuliaData/Tables.jl) interface; in particular a tuple or vector of `Pair{Symbol, <:AbstractVector}}` objects is a table.\n",
       "  * `copycols` : whether vectors passed as columns should be copied; if set to `false` then the constructor will still copy the passed columns if it is not possible to construct a `DataFrame` without materializing new columns.\n",
       "\n",
       "All columns in `columns` must be `AbstractVector`s and have the same length. An exception are `DataFrame(kwargs...)` and `DataFrame(pairs::Pair...)` form constructors which additionally allow a column to be of any other type that is not an `AbstractArray`, in which case the passed value is automatically repeated to fill a new vector of the appropriate length. As a particular rule values stored in a `Ref` or a `0`-dimensional `AbstractArray` are unwrapped and treated in the same way.\n",
       "\n",
       "Additionally `DataFrame` can be used to collect a [`GroupedDataFrame`](@ref) into a `DataFrame`. In this case the order of rows in the result follows the order of groups in the `GroupedDataFrame` passed.\n",
       "\n",
       "# Notes\n",
       "\n",
       "The `DataFrame` constructor by default copies all columns vectors passed to it. Pass `copycols=false` to reuse vectors without copying them\n",
       "\n",
       "If a column is passed to a `DataFrame` constructor or is assigned as a whole using `setindex!` then its reference is stored in the `DataFrame`. An exception to this rule is assignment of an `AbstractRange` as a column, in which case the range is collected to a `Vector`.\n",
       "\n",
       "Because column types can vary, a `DataFrame` is not type stable. For performance-critical code, do not index into a `DataFrame` inside of loops.\n",
       "\n",
       "# Examples\n",
       "\n",
       "```julia\n",
       "df = DataFrame()\n",
       "v = [\"x\",\"y\",\"z\"][rand(1:3, 10)]\n",
       "df1 = DataFrame(Any[collect(1:10), v, rand(10)], [:A, :B, :C])\n",
       "df2 = DataFrame(A = 1:10, B = v, C = rand(10))\n",
       "summary(df1)\n",
       "describe(df2)\n",
       "first(df1, 10)\n",
       "df1.B\n",
       "df2[!, :C]\n",
       "df1[:, :A]\n",
       "df1[1:4, 1:2]\n",
       "df1[Not(1:4), Not(1:2)]\n",
       "df1[1:2, [:A,:C]]\n",
       "df1[1:2, r\"[AC]\"]\n",
       "df1[:, [:A,:C]]\n",
       "df1[:, [1,3]]\n",
       "df1[1:4, :]\n",
       "df1[1:4, :C]\n",
       "df1[1:4, :C] = 40. * df1[1:4, :C]\n",
       "[df1; df2]  # vcat\n",
       "[df1 df2]  # hcat\n",
       "size(df1)\n",
       "```\n"
      ],
      "text/plain": [
       "  \u001b[39mDataFrame\u001b[0m \u001b[0m\u001b[38;2;249;38;114m<:\u001b[0m \u001b[0m\u001b[39mAbstractDataFrame\u001b[0m\u001b[39m\u001b[0m\n",
       "\n",
       "\n",
       "  An AbstractDataFrame that stores a set of named columns\n",
       "\n",
       "  The columns are normally AbstractVectors stored in memory, particularly a\n",
       "  Vector or CategoricalVector.\n",
       "\n",
       "\u001b[1m  Constructors\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39mcolumns\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mAbstractVector\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mnames\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mAbstractVector\u001b[0m\u001b[39m{\u001b[0m\u001b[39mSymbol\u001b[0m\u001b[39m}\u001b[0m\u001b[39m;\u001b[0m\n",
       "            \u001b[0m\u001b[39mmakeunique\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mfalse\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mcopycols\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mtrue\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39mcolumns\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mAbstractVector\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mnames\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mAbstractVector\u001b[0m\u001b[39m{\u001b[0m\u001b[38;2;249;38;114m<:\u001b[0m\u001b[39mAbstractString\u001b[0m\u001b[39m}\u001b[0m\u001b[39m;\u001b[0m\n",
       "            \u001b[0m\u001b[39mmakeunique\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mfalse\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mcopycols\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mtrue\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39mcolumns\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mNTuple\u001b[0m\u001b[39m{\u001b[0m\u001b[39mN\u001b[0m\u001b[39m,\u001b[0m\u001b[39mAbstractVector\u001b[0m\u001b[39m}\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mnames\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mNTuple\u001b[0m\u001b[39m{\u001b[0m\u001b[39mN\u001b[0m\u001b[39m,\u001b[0m\u001b[39mSymbol\u001b[0m\u001b[39m}\u001b[0m\u001b[39m;\u001b[0m\n",
       "            \u001b[0m\u001b[39mmakeunique\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mfalse\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mcopycols\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mtrue\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39mcolumns\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mNTuple\u001b[0m\u001b[39m{\u001b[0m\u001b[39mN\u001b[0m\u001b[39m,\u001b[0m\u001b[39mAbstractVector\u001b[0m\u001b[39m}\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mnames\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mNTuple\u001b[0m\u001b[39m{\u001b[0m\u001b[39mN\u001b[0m\u001b[39m,\u001b[0m\u001b[38;2;249;38;114m<:\u001b[0m\u001b[39mAbstractString\u001b[0m\u001b[39m}\u001b[0m\u001b[39m;\u001b[0m\n",
       "            \u001b[0m\u001b[39mmakeunique\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mfalse\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mcopycols\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mtrue\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39mcolumns\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mMatrix\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mnames\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mAbstractVector\u001b[0m\u001b[39m{\u001b[0m\u001b[39mSymbol\u001b[0m\u001b[39m}\u001b[0m\u001b[39m;\u001b[0m \u001b[0m\u001b[39mmakeunique\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mfalse\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39mcolumns\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mMatrix\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mnames\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mAbstractVector\u001b[0m\u001b[39m{\u001b[0m\u001b[38;2;249;38;114m<:\u001b[0m\u001b[39mAbstractString\u001b[0m\u001b[39m}\u001b[0m\u001b[39m;\u001b[0m\n",
       "            \u001b[0m\u001b[39mmakeunique\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mfalse\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39mkwargs\u001b[0m\u001b[38;2;249;38;114m...\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39mpairs\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mPair\u001b[0m\u001b[39m{\u001b[0m\u001b[39mSymbol\u001b[0m\u001b[39m,\u001b[0m\u001b[38;2;249;38;114m<:\u001b[0m\u001b[39mAny\u001b[0m\u001b[39m}\u001b[0m\u001b[38;2;249;38;114m...\u001b[0m\u001b[39m;\u001b[0m \u001b[0m\u001b[39mmakeunique\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mfalse\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mcopycols\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mtrue\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39mpairs\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mPair\u001b[0m\u001b[39m{\u001b[0m\u001b[38;2;249;38;114m<:\u001b[0m\u001b[39mAbstractString\u001b[0m\u001b[39m,\u001b[0m\u001b[38;2;249;38;114m<:\u001b[0m\u001b[39mAny\u001b[0m\u001b[39m}\u001b[0m\u001b[38;2;249;38;114m...\u001b[0m\u001b[39m;\u001b[0m \u001b[0m\u001b[39mmakeunique\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mfalse\u001b[0m\u001b[39m,\u001b[0m\n",
       "            \u001b[0m\u001b[39mcopycols\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mtrue\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39m)\u001b[0m \u001b[0m\u001b[38;2;89;89;89m# an empty DataFrame\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39mcolumn_eltypes\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mAbstractVector\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mnames\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mAbstractVector\u001b[0m\u001b[39m{\u001b[0m\u001b[39mSymbol\u001b[0m\u001b[39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "            \u001b[0m\u001b[39mnrows\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mInteger\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;174;129;255m0\u001b[0m\u001b[39m;\u001b[0m \u001b[0m\u001b[39mmakeunique\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mfalse\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39mcolumn_eltypes\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mAbstractVector\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mnames\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mAbstractVector\u001b[0m\u001b[39m{\u001b[0m\u001b[38;2;249;38;114m<:\u001b[0m\u001b[39mAbstractString\u001b[0m\u001b[39m}\u001b[0m\u001b[39m,\u001b[0m\n",
       "            \u001b[0m\u001b[39mnrows\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mInteger\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;174;129;255m0\u001b[0m\u001b[39m;\u001b[0m \u001b[0m\u001b[39mmakeunique\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mfalse\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39mds\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mAbstractDict\u001b[0m\u001b[39m;\u001b[0m \u001b[0m\u001b[39mcopycols\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mtrue\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39mtable\u001b[0m\u001b[39m;\u001b[0m \u001b[0m\u001b[39mmakeunique\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mfalse\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mcopycols\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mtrue\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mUnion\u001b[0m\u001b[39m{\u001b[0m\u001b[39mDataFrame\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mSubDataFrame\u001b[0m\u001b[39m}\u001b[0m\u001b[39m;\u001b[0m \u001b[0m\u001b[39mcopycols\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mtrue\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mGroupedDataFrame\u001b[0m\u001b[39m;\u001b[0m \u001b[0m\u001b[39mkeepkeys\u001b[0m\u001b[38;2;102;217;239m::\u001b[0m\u001b[38;2;102;217;239mBool\u001b[0m\u001b[38;2;249;38;114m=\u001b[0m\u001b[38;2;249;38;114mtrue\u001b[0m\u001b[39m)\u001b[0m\u001b[39m\u001b[0m\n",
       "\n",
       "\n",
       "\u001b[1m  Arguments\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "    •    \u001b[36mcolumns\u001b[39m : a Vector with each column as contents or a Matrix\n",
       "\n",
       "    •    \u001b[36mnames\u001b[39m : the column names\n",
       "\n",
       "    •    \u001b[36mmakeunique\u001b[39m : if \u001b[36mfalse\u001b[39m (the default), an error will be raised if\n",
       "        duplicates in \u001b[36mnames\u001b[39m are found; if \u001b[36mtrue\u001b[39m, duplicate names will be\n",
       "        suffixed with \u001b[36m_i\u001b[39m (\u001b[36mi\u001b[39m starting at 1 for the first duplicate).\n",
       "\n",
       "    •    \u001b[36mkwargs\u001b[39m : the key gives the column names, and the value is the\n",
       "        column contents; note that the \u001b[36mcopycols\u001b[39m keyword argument indicates\n",
       "        if if vectors passed as columns should be copied so it is not\n",
       "        possible to create a column whose name is \u001b[36m:copycols\u001b[39m using this\n",
       "        constructor\n",
       "\n",
       "    •    \u001b[36mt\u001b[39m : elemental type of all columns\n",
       "\n",
       "    •    \u001b[36mnrows\u001b[39m, \u001b[36mncols\u001b[39m : number of rows and columns\n",
       "\n",
       "    •    \u001b[36mcolumn_eltypes\u001b[39m : element type of each column\n",
       "\n",
       "    •    \u001b[36mcategorical\u001b[39m : a vector of \u001b[36mBool\u001b[39m indicating which columns should be\n",
       "        converted to \u001b[36mCategoricalVector\u001b[39m\n",
       "\n",
       "    •    \u001b[36mds\u001b[39m : \u001b[36mAbstractDict\u001b[39m of columns\n",
       "\n",
       "    •    \u001b[36mtable\u001b[39m : any type that implements the Tables.jl\n",
       "        (https://github.com/JuliaData/Tables.jl) interface; in particular\n",
       "        a tuple or vector of \u001b[36mPair{Symbol, <:AbstractVector}}\u001b[39m objects is a\n",
       "        table.\n",
       "\n",
       "    •    \u001b[36mcopycols\u001b[39m : whether vectors passed as columns should be copied; if\n",
       "        set to \u001b[36mfalse\u001b[39m then the constructor will still copy the passed\n",
       "        columns if it is not possible to construct a \u001b[36mDataFrame\u001b[39m without\n",
       "        materializing new columns.\n",
       "\n",
       "  All columns in \u001b[36mcolumns\u001b[39m must be \u001b[36mAbstractVector\u001b[39ms and have the same length. An\n",
       "  exception are \u001b[36mDataFrame(kwargs...)\u001b[39m and \u001b[36mDataFrame(pairs::Pair...)\u001b[39m form\n",
       "  constructors which additionally allow a column to be of any other type that\n",
       "  is not an \u001b[36mAbstractArray\u001b[39m, in which case the passed value is automatically\n",
       "  repeated to fill a new vector of the appropriate length. As a particular\n",
       "  rule values stored in a \u001b[36mRef\u001b[39m or a \u001b[36m0\u001b[39m-dimensional \u001b[36mAbstractArray\u001b[39m are unwrapped\n",
       "  and treated in the same way.\n",
       "\n",
       "  Additionally \u001b[36mDataFrame\u001b[39m can be used to collect a \u001b[36mGroupedDataFrame\u001b[39m into a\n",
       "  \u001b[36mDataFrame\u001b[39m. In this case the order of rows in the result follows the order of\n",
       "  groups in the \u001b[36mGroupedDataFrame\u001b[39m passed.\n",
       "\n",
       "\u001b[1m  Notes\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  The \u001b[36mDataFrame\u001b[39m constructor by default copies all columns vectors passed to\n",
       "  it. Pass \u001b[36mcopycols=false\u001b[39m to reuse vectors without copying them\n",
       "\n",
       "  If a column is passed to a \u001b[36mDataFrame\u001b[39m constructor or is assigned as a whole\n",
       "  using \u001b[36msetindex!\u001b[39m then its reference is stored in the \u001b[36mDataFrame\u001b[39m. An exception\n",
       "  to this rule is assignment of an \u001b[36mAbstractRange\u001b[39m as a column, in which case\n",
       "  the range is collected to a \u001b[36mVector\u001b[39m.\n",
       "\n",
       "  Because column types can vary, a \u001b[36mDataFrame\u001b[39m is not type stable. For\n",
       "  performance-critical code, do not index into a \u001b[36mDataFrame\u001b[39m inside of loops.\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "  \u001b[39mdf\u001b[0m \u001b[0m\u001b[38;2;249;38;114m=\u001b[0m \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[39mv\u001b[0m \u001b[0m\u001b[38;2;249;38;114m=\u001b[0m \u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;253;151;31m\"x\"\u001b[0m\u001b[39m,\u001b[0m\u001b[38;2;253;151;31m\"y\"\u001b[0m\u001b[39m,\u001b[0m\u001b[38;2;253;151;31m\"z\"\u001b[0m\u001b[39m]\u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;102;217;239mrand\u001b[0m(\u001b[0m\u001b[38;2;174;129;255m1\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[38;2;174;129;255m3\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[38;2;174;129;255m10\u001b[0m\u001b[39m)\u001b[0m\u001b[39m]\u001b[0m\n",
       "  \u001b[0m\u001b[39mdf1\u001b[0m \u001b[0m\u001b[38;2;249;38;114m=\u001b[0m \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39mAny\u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;102;217;239mcollect\u001b[0m(\u001b[0m\u001b[38;2;174;129;255m1\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[38;2;174;129;255m10\u001b[0m\u001b[39m)\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mv\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[38;2;102;217;239mrand\u001b[0m(\u001b[0m\u001b[38;2;174;129;255m10\u001b[0m\u001b[39m)\u001b[0m\u001b[39m]\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;174;129;255m:\u001b[0m\u001b[38;2;174;129;255mA\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[38;2;174;129;255m:\u001b[0m\u001b[38;2;174;129;255mB\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[38;2;174;129;255m:\u001b[0m\u001b[38;2;174;129;255mC\u001b[0m\u001b[39m]\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[39mdf2\u001b[0m \u001b[0m\u001b[38;2;249;38;114m=\u001b[0m \u001b[0m\u001b[38;2;102;217;239mDataFrame\u001b[0m(\u001b[0m\u001b[39mA\u001b[0m \u001b[0m\u001b[38;2;249;38;114m=\u001b[0m \u001b[0m\u001b[38;2;174;129;255m1\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[38;2;174;129;255m10\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mB\u001b[0m \u001b[0m\u001b[38;2;249;38;114m=\u001b[0m \u001b[0m\u001b[39mv\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mC\u001b[0m \u001b[0m\u001b[38;2;249;38;114m=\u001b[0m \u001b[0m\u001b[38;2;102;217;239mrand\u001b[0m(\u001b[0m\u001b[38;2;174;129;255m10\u001b[0m\u001b[39m)\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239msummary\u001b[0m(\u001b[0m\u001b[39mdf1\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mdescribe\u001b[0m(\u001b[0m\u001b[39mdf2\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239mfirst\u001b[0m(\u001b[0m\u001b[39mdf1\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[38;2;174;129;255m10\u001b[0m\u001b[39m)\u001b[0m\n",
       "  \u001b[0m\u001b[39mdf1\u001b[0m\u001b[38;2;249;38;114m.\u001b[0m\u001b[39mB\u001b[0m\n",
       "  \u001b[0m\u001b[39mdf2\u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;249;38;114m!\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[38;2;174;129;255m:\u001b[0m\u001b[38;2;174;129;255mC\u001b[0m\u001b[39m]\u001b[0m\n",
       "  \u001b[0m\u001b[39mdf1\u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[38;2;174;129;255m:\u001b[0m\u001b[38;2;174;129;255mA\u001b[0m\u001b[39m]\u001b[0m\n",
       "  \u001b[0m\u001b[39mdf1\u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;174;129;255m1\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[38;2;174;129;255m4\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[38;2;174;129;255m1\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[38;2;174;129;255m2\u001b[0m\u001b[39m]\u001b[0m\n",
       "  \u001b[0m\u001b[39mdf1\u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;102;217;239mNot\u001b[0m(\u001b[0m\u001b[38;2;174;129;255m1\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[38;2;174;129;255m4\u001b[0m\u001b[39m)\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[38;2;102;217;239mNot\u001b[0m(\u001b[0m\u001b[38;2;174;129;255m1\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[38;2;174;129;255m2\u001b[0m\u001b[39m)\u001b[0m\u001b[39m]\u001b[0m\n",
       "  \u001b[0m\u001b[39mdf1\u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;174;129;255m1\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[38;2;174;129;255m2\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;174;129;255m:\u001b[0m\u001b[38;2;174;129;255mA\u001b[0m\u001b[39m,\u001b[0m\u001b[38;2;174;129;255m:\u001b[0m\u001b[38;2;174;129;255mC\u001b[0m\u001b[39m]\u001b[0m\u001b[39m]\u001b[0m\n",
       "  \u001b[0m\u001b[39mdf1\u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;174;129;255m1\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[38;2;174;129;255m2\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39mr\u001b[0m\u001b[38;2;253;151;31m\"[AC]\"\u001b[0m\u001b[39m]\u001b[0m\n",
       "  \u001b[0m\u001b[39mdf1\u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;174;129;255m:\u001b[0m\u001b[38;2;174;129;255mA\u001b[0m\u001b[39m,\u001b[0m\u001b[38;2;174;129;255m:\u001b[0m\u001b[38;2;174;129;255mC\u001b[0m\u001b[39m]\u001b[0m\u001b[39m]\u001b[0m\n",
       "  \u001b[0m\u001b[39mdf1\u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;174;129;255m1\u001b[0m\u001b[39m,\u001b[0m\u001b[38;2;174;129;255m3\u001b[0m\u001b[39m]\u001b[0m\u001b[39m]\u001b[0m\n",
       "  \u001b[0m\u001b[39mdf1\u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;174;129;255m1\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[38;2;174;129;255m4\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[39m]\u001b[0m\n",
       "  \u001b[0m\u001b[39mdf1\u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;174;129;255m1\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[38;2;174;129;255m4\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[38;2;174;129;255m:\u001b[0m\u001b[38;2;174;129;255mC\u001b[0m\u001b[39m]\u001b[0m\n",
       "  \u001b[0m\u001b[39mdf1\u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;174;129;255m1\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[38;2;174;129;255m4\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[38;2;174;129;255m:\u001b[0m\u001b[38;2;174;129;255mC\u001b[0m\u001b[39m]\u001b[0m \u001b[0m\u001b[38;2;249;38;114m=\u001b[0m \u001b[0m\u001b[38;2;174;129;255m40.\u001b[0m \u001b[0m\u001b[38;2;249;38;114m*\u001b[0m \u001b[0m\u001b[39mdf1\u001b[0m\u001b[39m[\u001b[0m\u001b[38;2;174;129;255m1\u001b[0m\u001b[38;2;249;38;114m:\u001b[0m\u001b[38;2;174;129;255m4\u001b[0m\u001b[39m,\u001b[0m \u001b[0m\u001b[38;2;174;129;255m:\u001b[0m\u001b[38;2;174;129;255mC\u001b[0m\u001b[39m]\u001b[0m\n",
       "  \u001b[0m\u001b[39m[\u001b[0m\u001b[39mdf1\u001b[0m\u001b[39m;\u001b[0m \u001b[0m\u001b[39mdf2\u001b[0m\u001b[39m]\u001b[0m  \u001b[0m\u001b[38;2;89;89;89m# vcat\u001b[0m\n",
       "  \u001b[0m\u001b[39m[\u001b[0m\u001b[39mdf1\u001b[0m \u001b[0m\u001b[39mdf2\u001b[0m\u001b[39m]\u001b[0m  \u001b[0m\u001b[38;2;89;89;89m# hcat\u001b[0m\n",
       "  \u001b[0m\u001b[38;2;102;217;239msize\u001b[0m(\u001b[0m\u001b[39mdf1\u001b[0m\u001b[39m)\u001b[0m\u001b[39m\u001b[0m\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>Class</th><th>                       Nucleotide_Sequence</th></tr><tr><th></th><th>String</th><th>String</th></tr></thead><tbody><p>3,190 rows × 2 columns</p><tr><th>1</th><td>EI</td><td>               CCAGCTGCATCACAGGAGGCCAGCGAGCAGGTCTGTTCCAAGGGCCTTCGAGCCAGTCTG</td></tr><tr><th>2</th><td>EI</td><td>               AGACCCGCCGGGAGGCGGAGGACCTGCAGGGTGAGCCCCACCGCCCCTCCGTGCCCCCGC</td></tr><tr><th>3</th><td>EI</td><td>               GAGGTGAAGGACGTCCTTCCCCAGGAGCCGGTGAGAAGCGCAGTCGGGGGCACGGGGATG</td></tr><tr><th>4</th><td>EI</td><td>              GGGCTGCGTTGCTGGTCACATTCCTGGCAGGTATGGGGCGGGGCTTGCTCGGTTTTCCCC</td></tr><tr><th>5</th><td>EI</td><td>             GCTCAGCCCCCAGGTCACCCAGGAACTGACGTGAGTGTCCCCATCCCGGCCCTTGACCCT</td></tr><tr><th>6</th><td>EI</td><td>             CAGACTGGGTGGACAACAAAACCTTCAGCGGTAAGAGAGGGCCAAGCTCAGAGACCACAG</td></tr><tr><th>7</th><td>EI</td><td>             CCTTTGAGGACAGCACCAAGAAGTGTGCAGGTACGTTCCCACCTGCCCTGGTGGCCGCCA</td></tr><tr><th>8</th><td>EI</td><td>            CCCTCGTGCGGTCCACGACCAAGACCAGCGGTGAGCCACGGGCAGGCCGGGGTCGTGGGG</td></tr><tr><th>9</th><td>EI</td><td>            TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTCCTGTCCCTGCTCCAGTCC</td></tr><tr><th>10</th><td>EI</td><td>            AAGCTGACAGTGGACCCGGTCAACTTCAAGGTGAGCCAGGAGTCGGGTGGGAGGGTGAGA</td></tr><tr><th>11</th><td>EI</td><td>             TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTGGTATCCCTGCTGCCAGTC</td></tr><tr><th>12</th><td>EI</td><td>             AAGCTGAGAGTGGACCCTGTCAACTTCAAGGTGAGCCACCAGTCGGGTGGGGAGGGTGAG</td></tr><tr><th>13</th><td>EI</td><td>            GGAAGATGCTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG</td></tr><tr><th>14</th><td>EI</td><td>            AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG</td></tr><tr><th>15</th><td>EI</td><td>            GGAAGATGTTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG</td></tr><tr><th>16</th><td>EI</td><td>            AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG</td></tr><tr><th>17</th><td>EI</td><td>            GGCACCACCACTGACCTGGGACAGTGAATCGTAAGTATGCCTTTCACTGCGAGGGGTTCT</td></tr><tr><th>18</th><td>EI</td><td>            TTGCTCTGGTGAATTACATCTTCTTTAAAGGTAAGGTTGCTCAACCAGCCTGAGCTGTTT</td></tr><tr><th>19</th><td>EI</td><td>            CACCAAGTTCCTGGAAAATGAAGACAGAAGGTGATTCCCCAACCTGAGGGTGACCAAGAA</td></tr><tr><th>20</th><td>EI</td><td>           ACAGAGGAGGCACCCCTGAAGCTCTCCAAGGTGAGATCACCCTGACGACCTTGTTGCACC</td></tr><tr><th>21</th><td>EI</td><td>           GTGCCCATCACCAACGCCACCCTGGACCGGGTGAGTGCCTGGGCTAGCCCTGTCCTGAGC</td></tr><tr><th>22</th><td>EI</td><td>           CACGATCTTTCTCAGAGAGTACCAGACCCGGTGAGAGCCCCCATTCCAATGCACCCCCGA</td></tr><tr><th>23</th><td>EI</td><td>           AGCGGGAGAATGGGACCGTCTCCAGATACGGTGAGGGCCAGCCCTCAGGCAGGAGGGTTC</td></tr><tr><th>24</th><td>EI</td><td>           ATGAGAAGAACTGGGGGCTGTCTTTCTATGGTAGGCATGCTTAGCAGCCCCAAACTCATG</td></tr><tr><th>25</th><td>EI</td><td>           TCAGATGTCATGTACACCGACTGGAAAAAGGTAAACGCAAGGGATTGGACATTGCCCACC</td></tr><tr><th>26</th><td>EI</td><td>            GATCCGCCGCCCGTCCACACCCGCCGCCAGGTAAGCCCGGCCAGCCGACCGGGGCATGCG</td></tr><tr><th>27</th><td>EI</td><td>           CCCTCCATCGTGGGGCGCCCCAGGCACCAGGTAGGGGAGCTGGCTGGGTGGGGCAGCCCC</td></tr><tr><th>28</th><td>EI</td><td>           CCCAAGGCCAACCGCGAGAAGATGACCCAGGTGAGTGGCCCGCTACCTCTTCTGGTGGCC</td></tr><tr><th>29</th><td>EI</td><td>           CTGAGGCACTCTTCCAGCCTTCCTTCCTGGGTGAGTGGAGACTGTCTCCCGGCTCTGCCT</td></tr><tr><th>30</th><td>EI</td><td>           GCCCTGGCACCCAGCACAATGAAGATCAAGGTGGGTGTCTTTCCTGCCTGAGCTGACCTG</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cc}\n",
       "\t& Class &                        Nucleotide\\_Sequence\\\\\n",
       "\t\\hline\n",
       "\t& String & String\\\\\n",
       "\t\\hline\n",
       "\t1 & EI &                CCAGCTGCATCACAGGAGGCCAGCGAGCAGGTCTGTTCCAAGGGCCTTCGAGCCAGTCTG \\\\\n",
       "\t2 & EI &                AGACCCGCCGGGAGGCGGAGGACCTGCAGGGTGAGCCCCACCGCCCCTCCGTGCCCCCGC \\\\\n",
       "\t3 & EI &                GAGGTGAAGGACGTCCTTCCCCAGGAGCCGGTGAGAAGCGCAGTCGGGGGCACGGGGATG \\\\\n",
       "\t4 & EI &               GGGCTGCGTTGCTGGTCACATTCCTGGCAGGTATGGGGCGGGGCTTGCTCGGTTTTCCCC \\\\\n",
       "\t5 & EI &              GCTCAGCCCCCAGGTCACCCAGGAACTGACGTGAGTGTCCCCATCCCGGCCCTTGACCCT \\\\\n",
       "\t6 & EI &              CAGACTGGGTGGACAACAAAACCTTCAGCGGTAAGAGAGGGCCAAGCTCAGAGACCACAG \\\\\n",
       "\t7 & EI &              CCTTTGAGGACAGCACCAAGAAGTGTGCAGGTACGTTCCCACCTGCCCTGGTGGCCGCCA \\\\\n",
       "\t8 & EI &             CCCTCGTGCGGTCCACGACCAAGACCAGCGGTGAGCCACGGGCAGGCCGGGGTCGTGGGG \\\\\n",
       "\t9 & EI &             TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTCCTGTCCCTGCTCCAGTCC \\\\\n",
       "\t10 & EI &             AAGCTGACAGTGGACCCGGTCAACTTCAAGGTGAGCCAGGAGTCGGGTGGGAGGGTGAGA \\\\\n",
       "\t11 & EI &              TGGCGACTACGGCGCGGAGGCCCTGGAGAGGTGAGGACCCTGGTATCCCTGCTGCCAGTC \\\\\n",
       "\t12 & EI &              AAGCTGAGAGTGGACCCTGTCAACTTCAAGGTGAGCCACCAGTCGGGTGGGGAGGGTGAG \\\\\n",
       "\t13 & EI &             GGAAGATGCTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG \\\\\n",
       "\t14 & EI &             AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG \\\\\n",
       "\t15 & EI &             GGAAGATGTTGGAGGAGAAACCCTGGGAAGGTAGGCTCTGGTGACCAGGACAAGGGAGGG \\\\\n",
       "\t16 & EI &             AAGCTGCATGTGGATCCTGAGAACTTCAGGGTGAGTACAGGAGATGTTTCAGCCCTGTTG \\\\\n",
       "\t17 & EI &             GGCACCACCACTGACCTGGGACAGTGAATCGTAAGTATGCCTTTCACTGCGAGGGGTTCT \\\\\n",
       "\t18 & EI &             TTGCTCTGGTGAATTACATCTTCTTTAAAGGTAAGGTTGCTCAACCAGCCTGAGCTGTTT \\\\\n",
       "\t19 & EI &             CACCAAGTTCCTGGAAAATGAAGACAGAAGGTGATTCCCCAACCTGAGGGTGACCAAGAA \\\\\n",
       "\t20 & EI &            ACAGAGGAGGCACCCCTGAAGCTCTCCAAGGTGAGATCACCCTGACGACCTTGTTGCACC \\\\\n",
       "\t21 & EI &            GTGCCCATCACCAACGCCACCCTGGACCGGGTGAGTGCCTGGGCTAGCCCTGTCCTGAGC \\\\\n",
       "\t22 & EI &            CACGATCTTTCTCAGAGAGTACCAGACCCGGTGAGAGCCCCCATTCCAATGCACCCCCGA \\\\\n",
       "\t23 & EI &            AGCGGGAGAATGGGACCGTCTCCAGATACGGTGAGGGCCAGCCCTCAGGCAGGAGGGTTC \\\\\n",
       "\t24 & EI &            ATGAGAAGAACTGGGGGCTGTCTTTCTATGGTAGGCATGCTTAGCAGCCCCAAACTCATG \\\\\n",
       "\t25 & EI &            TCAGATGTCATGTACACCGACTGGAAAAAGGTAAACGCAAGGGATTGGACATTGCCCACC \\\\\n",
       "\t26 & EI &             GATCCGCCGCCCGTCCACACCCGCCGCCAGGTAAGCCCGGCCAGCCGACCGGGGCATGCG \\\\\n",
       "\t27 & EI &            CCCTCCATCGTGGGGCGCCCCAGGCACCAGGTAGGGGAGCTGGCTGGGTGGGGCAGCCCC \\\\\n",
       "\t28 & EI &            CCCAAGGCCAACCGCGAGAAGATGACCCAGGTGAGTGGCCCGCTACCTCTTCTGGTGGCC \\\\\n",
       "\t29 & EI &            CTGAGGCACTCTTCCAGCCTTCCTTCCTGGGTGAGTGGAGACTGTCTCCCGGCTCTGCCT \\\\\n",
       "\t30 & EI &            GCCCTGGCACCCAGCACAATGAAGATCAAGGTGGGTGTCTTTCCTGCCTGAGCTGACCTG \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "3190×2 DataFrame. Omitted printing of 1 columns\n",
       "│ Row  │ Class  │\n",
       "│      │ \u001b[90mString\u001b[39m │\n",
       "├──────┼────────┤\n",
       "│ 1    │ EI     │\n",
       "│ 2    │ EI     │\n",
       "│ 3    │ EI     │\n",
       "│ 4    │ EI     │\n",
       "│ 5    │ EI     │\n",
       "│ 6    │ EI     │\n",
       "│ 7    │ EI     │\n",
       "│ 8    │ EI     │\n",
       "│ 9    │ EI     │\n",
       "│ 10   │ EI     │\n",
       "⋮\n",
       "│ 3180 │ N      │\n",
       "│ 3181 │ N      │\n",
       "│ 3182 │ N      │\n",
       "│ 3183 │ N      │\n",
       "│ 3184 │ N      │\n",
       "│ 3185 │ N      │\n",
       "│ 3186 │ N      │\n",
       "│ 3187 │ N      │\n",
       "│ 3188 │ N      │\n",
       "│ 3189 │ N      │\n",
       "│ 3190 │ N      │"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[:,Not([2])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at class labels to see if dataset is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{String,Int64} with 3 entries:\n",
       "  \"IE\" => 768\n",
       "  \"EI\" => 767\n",
       "  \"N\"  => 1655"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = countmap(data[:(Class)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.0784\n",
       " 0.4608\n",
       " 0.4608"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(label_counts[i] / size(data)[1] for i in keys(label_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m────────────────\u001b[0m┬\u001b[0m─────────────────────────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.names        \u001b[0m│\u001b[0m\u001b[22m _.types                         \u001b[0m│\u001b[0m\u001b[22m _.scitypes    \u001b[0m│\u001b[0m\n",
       "├\u001b[0m────────────────\u001b[0m┼\u001b[0m─────────────────────────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m Class_Name     \u001b[0m│\u001b[0m CategoricalValue{String,UInt32} \u001b[0m│\u001b[0m Multiclass{3} \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Left_Weight    \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Left_Distance  \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Right_Weight   \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m Right_Distance \u001b[0m│\u001b[0m Float64                         \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "└\u001b[0m────────────────\u001b[0m┴\u001b[0m─────────────────────────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┘\u001b[0m\n",
       "_.nrows = 625\n"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coerce!(data, :Class_Name=>Multiclass,\n",
    "              :Left_Weight=>Continuous,\n",
    "              :Right_Weight=>Continuous,\n",
    "              :Left_Distance=>Continuous,\n",
    "              :Right_Distance=>Continuous)\n",
    "schema(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CategoricalValue{String,UInt32}[\"B\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\", \"R\"  …  \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"L\", \"B\"], 625×4 DataFrame\n",
       "│ Row │ Left_Weight │ Left_Distance │ Right_Weight │ Right_Distance │\n",
       "│     │ \u001b[90mFloat64\u001b[39m     │ \u001b[90mFloat64\u001b[39m       │ \u001b[90mFloat64\u001b[39m      │ \u001b[90mFloat64\u001b[39m        │\n",
       "├─────┼─────────────┼───────────────┼──────────────┼────────────────┤\n",
       "│ 1   │ 1.0         │ 1.0           │ 1.0          │ 1.0            │\n",
       "│ 2   │ 1.0         │ 1.0           │ 1.0          │ 2.0            │\n",
       "│ 3   │ 1.0         │ 1.0           │ 1.0          │ 3.0            │\n",
       "│ 4   │ 1.0         │ 1.0           │ 1.0          │ 4.0            │\n",
       "│ 5   │ 1.0         │ 1.0           │ 1.0          │ 5.0            │\n",
       "│ 6   │ 1.0         │ 1.0           │ 2.0          │ 1.0            │\n",
       "│ 7   │ 1.0         │ 1.0           │ 2.0          │ 2.0            │\n",
       "│ 8   │ 1.0         │ 1.0           │ 2.0          │ 3.0            │\n",
       "│ 9   │ 1.0         │ 1.0           │ 2.0          │ 4.0            │\n",
       "│ 10  │ 1.0         │ 1.0           │ 2.0          │ 5.0            │\n",
       "⋮\n",
       "│ 615 │ 5.0         │ 5.0           │ 3.0          │ 5.0            │\n",
       "│ 616 │ 5.0         │ 5.0           │ 4.0          │ 1.0            │\n",
       "│ 617 │ 5.0         │ 5.0           │ 4.0          │ 2.0            │\n",
       "│ 618 │ 5.0         │ 5.0           │ 4.0          │ 3.0            │\n",
       "│ 619 │ 5.0         │ 5.0           │ 4.0          │ 4.0            │\n",
       "│ 620 │ 5.0         │ 5.0           │ 4.0          │ 5.0            │\n",
       "│ 621 │ 5.0         │ 5.0           │ 5.0          │ 1.0            │\n",
       "│ 622 │ 5.0         │ 5.0           │ 5.0          │ 2.0            │\n",
       "│ 623 │ 5.0         │ 5.0           │ 5.0          │ 3.0            │\n",
       "│ 624 │ 5.0         │ 5.0           │ 5.0          │ 4.0            │\n",
       "│ 625 │ 5.0         │ 5.0           │ 5.0          │ 5.0            │)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, X = unpack(data, ==(:Class_Name), colname->true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition train and test data accoring to class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([595, 102, 55, 568, 425, 389, 146, 63, 372, 250  …  195, 500, 571, 533, 112, 396, 297, 106, 303, 261], [444, 144, 546, 43, 19, 173, 365, 423, 27, 218  …  293, 614, 90, 471, 13, 134, 296, 79, 395, 415])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data to use when trying to fit a single validation set\n",
    "train, test = partition(eachindex(y), 0.7, shuffle=true, rng=123, stratify=values(data[:Class_Name])) # gives 70:30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.0776255707762557\n",
       " 0.4611872146118721\n",
       " 0.4611872146118721"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts = countmap(data[train,:Class_Name])\n",
    "collect(train_counts[i] / size(train)[1] for i in keys(train_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Float64,1}:\n",
       " 0.08021390374331551\n",
       " 0.45989304812834225\n",
       " 0.45989304812834225"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_counts = countmap(data[test,:Class_Name])\n",
    "collect(test_counts[i] / size(test)[1] for i in keys(test_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five Learning Algorithms\n",
    "\n",
    "* Decision trees with some form of pruning\n",
    "* Neural networks\n",
    "* Boosting\n",
    "* Support Vector Machines\n",
    "* k-nearest neighbors\n",
    "\n",
    "\n",
    "##### Testing\n",
    "* Implement the algorithms\n",
    "* Design two *interesting* classification problems. For the purposes of this assignment, a classification problem is just a set of training examples and a set of test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42-element Array{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype),T} where T<:Tuple,1}:\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianLDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianQDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = ScikitLearn, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " (name = ExtraTreesClassifier, package_name = ScikitLearn, ... )\n",
       " ⋮\n",
       " (name = ProbabilisticSGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVC, package_name = LIBSVM, ... )\n",
       " (name = SVMClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(matching(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import MLJModels ✔\n",
      "import DecisionTree ✔\n",
      "import MLJModels.DecisionTree_ ✔\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Loading into module \"Main\": \n",
      "└ @ MLJModels /home/andrew/.julia/packages/MLJModels/mUBFt/src/loading.jl:70\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = false,\n",
       "    merge_purity_threshold = 1.0,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 5)\u001b[34m @083\u001b[39m"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load DecisionTreeClassifier verbosity=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision trees\n",
    "* Be sure to use some form of pruning. \n",
    "* You are not required to use information gain (for example, there is something called the GINI index that is sometimes used) to split attributes, but you should describe whatever it is that you do use.\n",
    "\n",
    "1. https://alan-turing-institute.github.io/MLJ.jl/dev/transformers/#MLJModels.UnivariateDiscretizer\n",
    "1. https://alan-turing-institute.github.io/MLJ.jl/dev/getting_started/#Getting-Started-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No post-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = false,\n",
       "    merge_purity_threshold = 1.0,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 8)\u001b[34m @963\u001b[39m"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(post_prune=false, display_depth=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @227\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @176\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @527\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tree = machine(dt, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{DecisionTreeClassifier} @227\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 4, Threshold 2.5\n",
      "L-> Feature 2, Threshold 2.5\n",
      "    L-> Feature 1, Threshold 1.5\n",
      "        L-> Feature 3, Threshold 2.5\n",
      "            L-> Feature 4, Threshold 1.5\n",
      "                L-> Feature 3, Threshold 1.5\n",
      "                    L-> 1 : 1/1\n",
      "                    R-> Feature 2, Threshold 1.5\n",
      "                        L-> 3 : 1/1\n",
      "                        R-> 1 : 1/1\n",
      "                R-> 3 : 3/3\n",
      "            R-> 3 : 11/11\n",
      "        R-> Feature 3, Threshold 2.5\n",
      "            L-> Feature 1, Threshold 2.5\n",
      "                L-> Feature 4, Threshold 1.5\n",
      "                    L-> Feature 3, Threshold 1.5\n",
      "                        L-> 2 : 2/2\n",
      "                        R-> Feature 2, Threshold 1.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> 2 : 1/1\n",
      "                    R-> 1 : 2/2\n",
      "                R-> Feature 2, Threshold 1.5\n",
      "                    L-> Feature 3, Threshold 1.5\n",
      "                        L-> 2 : 4/4\n",
      "                        R-> Feature 1, Threshold 4.5\n",
      "                            L-> \n",
      "                            R-> 2 : 1/1\n",
      "                    R-> 2 : 9/9\n",
      "            R-> Feature 2, Threshold 1.5\n",
      "                L-> Feature 4, Threshold 1.5\n",
      "                    L-> Feature 1, Threshold 3.5\n",
      "                        L-> Feature 3, Threshold 3.5\n",
      "                            L-> \n",
      "                            R-> 3 : 4/4\n",
      "                        R-> Feature 3, Threshold 4.5\n",
      "                            L-> 2 : 3/3\n",
      "                            R-> 3 : 1/1\n",
      "                    R-> 3 : 9/9\n",
      "                R-> Feature 3, Threshold 4.5\n",
      "                    L-> Feature 1, Threshold 4.5\n",
      "                        L-> Feature 4, Threshold 1.5\n",
      "                            L-> \n",
      "                            R-> \n",
      "                        R-> 2 : 3/3\n",
      "                    R-> Feature 1, Threshold 4.0\n",
      "                        L-> 3 : 3/3\n",
      "                        R-> 1 : 1/1\n",
      "    R-> Feature 1, Threshold 2.5\n",
      "        L-> Feature 3, Threshold 2.5\n",
      "            L-> Feature 3, Threshold 1.5\n",
      "                L-> 2 : 11/11\n",
      "                R-> Feature 1, Threshold 1.5\n",
      "                    L-> Feature 2, Threshold 4.5\n",
      "                        L-> Feature 2, Threshold 3.5\n",
      "                            L-> 2 : 1/1\n",
      "                            R-> 1 : 1/1\n",
      "                        R-> 2 : 2/2\n",
      "                    R-> 2 : 5/5\n",
      "            R-> Feature 1, Threshold 1.5\n",
      "                L-> Feature 4, Threshold 1.5\n",
      "                    L-> Feature 2, Threshold 4.5\n",
      "                        L-> Feature 3, Threshold 3.5\n",
      "                            L-> 2 : 1/1\n",
      "                            R-> 3 : 3/3\n",
      "                        R-> Feature 3, Threshold 4.5\n",
      "                            L-> 2 : 2/2\n",
      "                            R-> 1 : 1/1\n",
      "                    R-> 3 : 6/6\n",
      "                R-> Feature 4, Threshold 1.5\n",
      "                    L-> 2 : 8/8\n",
      "                    R-> Feature 2, Threshold 3.5\n",
      "                        L-> Feature 3, Threshold 3.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> 3 : 2/2\n",
      "                        R-> Feature 3, Threshold 4.5\n",
      "                            L-> 2 : 3/3\n",
      "                            R-> 1 : 1/1\n",
      "        R-> 2 : 66/66\n",
      "R-> Feature 1, Threshold 2.5\n",
      "    L-> Feature 3, Threshold 2.5\n",
      "        L-> Feature 2, Threshold 3.5\n",
      "            L-> Feature 1, Threshold 1.5\n",
      "                L-> 3 : 13/13\n",
      "                R-> Feature 2, Threshold 1.5\n",
      "                    L-> 3 : 5/5\n",
      "                    R-> Feature 4, Threshold 4.5\n",
      "                        L-> Feature 3, Threshold 1.5\n",
      "                            L-> \n",
      "                            R-> \n",
      "                        R-> 3 : 2/2\n",
      "            R-> Feature 1, Threshold 1.5\n",
      "                L-> Feature 3, Threshold 1.5\n",
      "                    L-> Feature 4, Threshold 4.5\n",
      "                        L-> 2 : 2/2\n",
      "                        R-> 3 : 1/1\n",
      "                    R-> 3 : 4/4\n",
      "                R-> Feature 2, Threshold 4.5\n",
      "                    L-> Feature 3, Threshold 1.5\n",
      "                        L-> 2 : 1/1\n",
      "                        R-> Feature 4, Threshold 4.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> 3 : 1/1\n",
      "                    R-> 2 : 4/4\n",
      "        R-> Feature 2, Threshold 4.5\n",
      "            L-> 3 : 47/47\n",
      "            R-> Feature 4, Threshold 3.5\n",
      "                L-> Feature 3, Threshold 3.5\n",
      "                    L-> Feature 1, Threshold 1.5\n",
      "                        L-> 3 : 1/1\n",
      "                        R-> 2 : 1/1\n",
      "                    R-> 3 : 3/3\n",
      "                R-> 3 : 8/8\n",
      "    R-> Feature 3, Threshold 2.5\n",
      "        L-> Feature 2, Threshold 1.5\n",
      "            L-> Feature 3, Threshold 1.5\n",
      "                L-> Feature 1, Threshold 3.5\n",
      "                    L-> Feature 4, Threshold 3.5\n",
      "                        L-> 1 : 1/1\n",
      "                        R-> 3 : 2/2\n",
      "                    R-> Feature 4, Threshold 3.5\n",
      "                        L-> 2 : 1/1\n",
      "                        R-> Feature 1, Threshold 4.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> \n",
      "                R-> 3 : 7/7\n",
      "            R-> Feature 3, Threshold 1.5\n",
      "                L-> 2 : 25/25\n",
      "                R-> Feature 2, Threshold 2.5\n",
      "                    L-> Feature 4, Threshold 3.5\n",
      "                        L-> 2 : 2/2\n",
      "                        R-> Feature 4, Threshold 4.5\n",
      "                            L-> \n",
      "                            R-> 3 : 1/1\n",
      "                    R-> Feature 4, Threshold 4.5\n",
      "                        L-> 2 : 10/10\n",
      "                        R-> Feature 1, Threshold 3.5\n",
      "                            L-> 3 : 1/1\n",
      "                            R-> 2 : 4/4\n",
      "        R-> Feature 2, Threshold 2.5\n",
      "            L-> Feature 4, Threshold 3.5\n",
      "                L-> Feature 1, Threshold 4.5\n",
      "                    L-> 3 : 7/7\n",
      "                    R-> Feature 3, Threshold 3.5\n",
      "                        L-> 2 : 1/1\n",
      "                        R-> 3 : 2/2\n",
      "                R-> 3 : 26/26\n",
      "            R-> Feature 4, Threshold 3.5\n",
      "                L-> Feature 3, Threshold 4.5\n",
      "                    L-> Feature 1, Threshold 3.5\n",
      "                        L-> Feature 2, Threshold 4.5\n",
      "                            L-> \n",
      "                            R-> 2 : 2/2\n",
      "                        R-> 2 : 9/9\n",
      "                    R-> Feature 1, Threshold 3.5\n",
      "                        L-> Feature 2, Threshold 4.5\n",
      "                            L-> 3 : 2/2\n",
      "                            R-> 1 : 1/1\n",
      "                        R-> Feature 2, Threshold 3.5\n",
      "                            L-> \n",
      "                            R-> 2 : 2/2\n",
      "                R-> Feature 1, Threshold 3.5\n",
      "                    L-> Feature 3, Threshold 3.5\n",
      "                        L-> Feature 4, Threshold 4.5\n",
      "                            L-> 1 : 1/1\n",
      "                            R-> 3 : 1/1\n",
      "                        R-> 3 : 11/11\n",
      "                    R-> Feature 3, Threshold 3.5\n",
      "                        L-> Feature 2, Threshold 3.5\n",
      "                            L-> \n",
      "                            R-> 2 : 4/4\n",
      "                        R-> Feature 2, Threshold 3.5\n",
      "                            L-> 3 : 6/6\n",
      "                            R-> \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @227\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @176\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @527\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(Tree, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:04\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m───────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┬\u001b[0m─────────────────────────────────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.measure     \u001b[0m│\u001b[0m\u001b[22m _.measurement \u001b[0m│\u001b[0m\u001b[22m _.per_fold                              \u001b[0m│\u001b[0m\n",
       "├\u001b[0m───────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┼\u001b[0m─────────────────────────────────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m cross_entropy \u001b[0m│\u001b[0m 7.84          \u001b[0m│\u001b[0m [8.24, 9.01, 5.89, 9.01, 5.55, 9.36]    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m acc           \u001b[0m│\u001b[0m 0.782         \u001b[0m│\u001b[0m [0.771, 0.75, 0.837, 0.75, 0.846, 0.74] \u001b[0m│\u001b[0m\n",
       "└\u001b[0m───────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┴\u001b[0m─────────────────────────────────────────\u001b[0m┘\u001b[0m\n",
       "_.per_observation = [[[2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16]], missing]\n",
       "_.fitted_params_per_fold = [ … ]\n",
       "_.report_per_fold = [ … ]\n"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_acc = evaluate!(Tree, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tree = Decision Tree\n",
       "Leaves: 133\n",
       "Depth:  11,\n",
       " encoding = Dict{CategoricalValue{String,UInt32},UInt32}(\"B\" => 0x00000001,\"L\" => 0x00000002,\"R\" => 0x00000003),)"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_params(Tree) \n",
    "# print_tree(Tree.fitresult[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(classes_seen = CategoricalValue{String,UInt32}[\"B\", \"L\", \"R\"],\n",
       " print_tree = TreePrinter object (call with display depth),)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(Tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Post-pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = true,\n",
       "    merge_purity_threshold = 1.0,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 5)\u001b[34m @598\u001b[39m"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt2 = DecisionTreeClassifier(post_prune=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @818\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @139\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @098\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tree2 = machine(dt2, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 4, Threshold 2.5\n",
      "L-> Feature 2, Threshold 2.5\n",
      "    L-> Feature 1, Threshold 1.5\n",
      "        L-> Feature 3, Threshold 2.5\n",
      "            L-> Feature 4, Threshold 1.5\n",
      "                L-> \n",
      "                R-> 3 : 3/3\n",
      "            R-> 3 : 11/11\n",
      "        R-> Feature 3, Threshold 2.5\n",
      "            L-> Feature 1, Threshold 2.5\n",
      "                L-> \n",
      "                R-> \n",
      "            R-> Feature 2, Threshold 1.5\n",
      "                L-> \n",
      "                R-> \n",
      "    R-> Feature 1, Threshold 2.5\n",
      "        L-> Feature 3, Threshold 2.5\n",
      "            L-> Feature 3, Threshold 1.5\n",
      "                L-> 2 : 11/11\n",
      "                R-> \n",
      "            R-> Feature 1, Threshold 1.5\n",
      "                L-> \n",
      "                R-> \n",
      "        R-> 2 : 66/66\n",
      "R-> Feature 1, Threshold 2.5\n",
      "    L-> Feature 3, Threshold 2.5\n",
      "        L-> Feature 2, Threshold 3.5\n",
      "            L-> Feature 1, Threshold 1.5\n",
      "                L-> 3 : 13/13\n",
      "                R-> \n",
      "            R-> Feature 1, Threshold 1.5\n",
      "                L-> \n",
      "                R-> \n",
      "        R-> Feature 2, Threshold 4.5\n",
      "            L-> 3 : 47/47\n",
      "            R-> Feature 3, Threshold 3.5\n",
      "                L-> \n",
      "                R-> 3 : 8/8\n",
      "    R-> Feature 3, Threshold 2.5\n",
      "        L-> Feature 2, Threshold 1.5\n",
      "            L-> Feature 3, Threshold 1.5\n",
      "                L-> \n",
      "                R-> 3 : 7/7\n",
      "            R-> Feature 3, Threshold 1.5\n",
      "                L-> 2 : 25/25\n",
      "                R-> \n",
      "        R-> Feature 2, Threshold 2.5\n",
      "            L-> Feature 1, Threshold 4.5\n",
      "                L-> 3 : 26/26\n",
      "                R-> \n",
      "            R-> Feature 4, Threshold 3.5\n",
      "                L-> \n",
      "                R-> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{DecisionTreeClassifier} @818\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @818\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @139\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @098\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(Tree2, rows=train, verbosity=2, force=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m───────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┬\u001b[0m───────────────────────────────────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.measure     \u001b[0m│\u001b[0m\u001b[22m _.measurement \u001b[0m│\u001b[0m\u001b[22m _.per_fold                                \u001b[0m│\u001b[0m\n",
       "├\u001b[0m───────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┼\u001b[0m───────────────────────────────────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m cross_entropy \u001b[0m│\u001b[0m 8.07          \u001b[0m│\u001b[0m [7.55, 10.7, 7.28, 7.97, 7.62, 7.28]      \u001b[0m│\u001b[0m\n",
       "│\u001b[0m acc           \u001b[0m│\u001b[0m 0.776         \u001b[0m│\u001b[0m [0.79, 0.702, 0.798, 0.779, 0.788, 0.798] \u001b[0m│\u001b[0m\n",
       "└\u001b[0m───────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┴\u001b[0m───────────────────────────────────────────\u001b[0m┘\u001b[0m\n",
       "_.per_observation = [[[2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [36.0, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 36.0, ..., 2.22e-16]], missing]\n",
       "_.fitted_params_per_fold = [ … ]\n",
       "_.report_per_fold = [ … ]\n"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_acc = evaluate!(Tree2, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate!(Tree2, resampling=CV(shuffle=true), measure=[tnr,tpr,fnr,fpr], verbosity=1, operation=predict_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tree = Decision Tree\n",
       "Leaves: 138\n",
       "Depth:  12,\n",
       " encoding = Dict{CategoricalValue{String,UInt32},UInt32}(\"B\" => 0x00000001,\"L\" => 0x00000002,\"R\" => 0x00000003),)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_params(Tree2) \n",
    "# print_tree(Tree.fitresult[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(classes_seen = CategoricalValue{String,UInt32}[\"B\", \"L\", \"R\"],\n",
       " print_tree = TreePrinter object (call with display depth),)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report(Tree2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch / RandomSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @017\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      "┌ Info: Attempting to evaluate 101 models.\n",
      "└ @ MLJTuning /home/andrew/.julia/packages/MLJTuning/Bbgvk/src/tuned_models.jl:494\n",
      "\u001b[33mEvaluating over 101 metamodels: 100%[=========================] Time: 0:00:02\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(parameter_name = \"merge_purity_threshold\",\n",
       " parameter_scale = :none,\n",
       " parameter_values = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09  …  0.91, 0.92, 0.93, 0.94, 0.95, 0.96, 0.97, 0.98, 0.99, 1.0],\n",
       " measurements = [0.9594991876228448, 0.9594991876228448, 0.9594991876228448, 0.9594991876228448, 0.9594991876228448, 0.9594991876228448, 0.9594991876228448, 0.9594991876228448, 0.9594991876228448, 0.9594991876228448  …  7.091005680490233, 7.091005680490233, 7.091005680490233, 7.091005680490233, 7.091005680490233, 7.091005680490233, 7.091005680490233, 7.091005680490233, 7.091005680490233, 7.091005680490233],)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vals = collect(0:.01:1)\n",
    "r = range(dt2, :merge_purity_threshold, values=vals)\n",
    "# r = range(nn2, :epochs, lower=0, upper=max_epochs)\n",
    "curve = learning_curve(Tree2, \n",
    "                        range=r, \n",
    "#                         resampling=Holdout(fraction_train=0.7), \n",
    "                        resampling=CV(), \n",
    "                        measure=cross_entropy, \n",
    "                        acceleration=CPUThreads())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip040\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip040)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip041\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip040)\" d=\"\n",
       "M175.445 1423.18 L2352.76 1423.18 L2352.76 47.2441 L175.445 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip042\">\n",
       "    <rect x=\"175\" y=\"47\" width=\"2178\" height=\"1377\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip042)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  237.067,1423.18 237.067,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip042)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  750.584,1423.18 750.584,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip042)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1264.1,1423.18 1264.1,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip042)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1777.62,1423.18 1777.62,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip042)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2291.13,1423.18 2291.13,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip042)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.445,1375.66 2352.76,1375.66 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip042)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.445,1163.98 2352.76,1163.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip042)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.445,952.287 2352.76,952.287 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip042)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.445,740.598 2352.76,740.598 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip042)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.445,528.909 2352.76,528.909 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip042)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.445,317.22 2352.76,317.22 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip042)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.445,105.531 2352.76,105.531 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.445,1423.18 2352.76,1423.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.445,1423.18 175.445,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  237.067,1423.18 237.067,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  750.584,1423.18 750.584,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1264.1,1423.18 1264.1,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1777.62,1423.18 1777.62,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2291.13,1423.18 2291.13,1406.67 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.445,1375.66 201.573,1375.66 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.445,1163.98 201.573,1163.98 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.445,952.287 201.573,952.287 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.445,740.598 201.573,740.598 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.445,528.909 201.573,528.909 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.445,317.22 201.573,317.22 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.445,105.531 201.573,105.531 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip040)\" d=\"M 0 0 M205.077 1445.17 Q201.466 1445.17 199.637 1448.74 Q197.831 1452.28 197.831 1459.41 Q197.831 1466.51 199.637 1470.08 Q201.466 1473.62 205.077 1473.62 Q208.711 1473.62 210.516 1470.08 Q212.345 1466.51 212.345 1459.41 Q212.345 1452.28 210.516 1448.74 Q208.711 1445.17 205.077 1445.17 M205.077 1441.47 Q210.887 1441.47 213.942 1446.07 Q217.021 1450.66 217.021 1459.41 Q217.021 1468.13 213.942 1472.74 Q210.887 1477.32 205.077 1477.32 Q199.266 1477.32 196.188 1472.74 Q193.132 1468.13 193.132 1459.41 Q193.132 1450.66 196.188 1446.07 Q199.266 1441.47 205.077 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M222.09 1470.77 L226.975 1470.77 L226.975 1476.65 L222.09 1476.65 L222.09 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M242.044 1445.17 Q238.433 1445.17 236.604 1448.74 Q234.799 1452.28 234.799 1459.41 Q234.799 1466.51 236.604 1470.08 Q238.433 1473.62 242.044 1473.62 Q245.678 1473.62 247.484 1470.08 Q249.313 1466.51 249.313 1459.41 Q249.313 1452.28 247.484 1448.74 Q245.678 1445.17 242.044 1445.17 M242.044 1441.47 Q247.854 1441.47 250.91 1446.07 Q253.988 1450.66 253.988 1459.41 Q253.988 1468.13 250.91 1472.74 Q247.854 1477.32 242.044 1477.32 Q236.234 1477.32 233.155 1472.74 Q230.1 1468.13 230.1 1459.41 Q230.1 1450.66 233.155 1446.07 Q236.234 1441.47 242.044 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M269.058 1445.17 Q265.447 1445.17 263.618 1448.74 Q261.812 1452.28 261.812 1459.41 Q261.812 1466.51 263.618 1470.08 Q265.447 1473.62 269.058 1473.62 Q272.692 1473.62 274.498 1470.08 Q276.326 1466.51 276.326 1459.41 Q276.326 1452.28 274.498 1448.74 Q272.692 1445.17 269.058 1445.17 M269.058 1441.47 Q274.868 1441.47 277.923 1446.07 Q281.002 1450.66 281.002 1459.41 Q281.002 1468.13 277.923 1472.74 Q274.868 1477.32 269.058 1477.32 Q263.248 1477.32 260.169 1472.74 Q257.113 1468.13 257.113 1459.41 Q257.113 1450.66 260.169 1446.07 Q263.248 1441.47 269.058 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M719.89 1445.17 Q716.279 1445.17 714.45 1448.74 Q712.644 1452.28 712.644 1459.41 Q712.644 1466.51 714.45 1470.08 Q716.279 1473.62 719.89 1473.62 Q723.524 1473.62 725.329 1470.08 Q727.158 1466.51 727.158 1459.41 Q727.158 1452.28 725.329 1448.74 Q723.524 1445.17 719.89 1445.17 M719.89 1441.47 Q725.7 1441.47 728.755 1446.07 Q731.834 1450.66 731.834 1459.41 Q731.834 1468.13 728.755 1472.74 Q725.7 1477.32 719.89 1477.32 Q714.079 1477.32 711.001 1472.74 Q707.945 1468.13 707.945 1459.41 Q707.945 1450.66 711.001 1446.07 Q714.079 1441.47 719.89 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M736.903 1470.77 L741.788 1470.77 L741.788 1476.65 L736.903 1476.65 L736.903 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M750.885 1472.72 L767.204 1472.72 L767.204 1476.65 L745.26 1476.65 L745.26 1472.72 Q747.922 1469.96 752.505 1465.33 Q757.112 1460.68 758.292 1459.34 Q760.538 1456.81 761.417 1455.08 Q762.32 1453.32 762.32 1451.63 Q762.32 1448.87 760.375 1447.14 Q758.454 1445.4 755.352 1445.4 Q753.153 1445.4 750.7 1446.17 Q748.269 1446.93 745.491 1448.48 L745.491 1443.76 Q748.315 1442.62 750.769 1442.05 Q753.223 1441.47 755.26 1441.47 Q760.63 1441.47 763.825 1444.15 Q767.019 1446.84 767.019 1451.33 Q767.019 1453.46 766.209 1455.38 Q765.422 1457.28 763.315 1459.87 Q762.737 1460.54 759.635 1463.76 Q756.533 1466.95 750.885 1472.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M772.32 1442.09 L790.676 1442.09 L790.676 1446.03 L776.602 1446.03 L776.602 1454.5 Q777.621 1454.15 778.639 1453.99 Q779.658 1453.8 780.676 1453.8 Q786.463 1453.8 789.843 1456.98 Q793.223 1460.15 793.223 1465.56 Q793.223 1471.14 789.75 1474.24 Q786.278 1477.32 779.959 1477.32 Q777.783 1477.32 775.514 1476.95 Q773.269 1476.58 770.862 1475.84 L770.862 1471.14 Q772.945 1472.28 775.167 1472.83 Q777.389 1473.39 779.866 1473.39 Q783.871 1473.39 786.209 1471.28 Q788.547 1469.18 788.547 1465.56 Q788.547 1461.95 786.209 1459.85 Q783.871 1457.74 779.866 1457.74 Q777.991 1457.74 776.116 1458.16 Q774.264 1458.57 772.32 1459.45 L772.32 1442.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1232.61 1445.17 Q1229 1445.17 1227.17 1448.74 Q1225.36 1452.28 1225.36 1459.41 Q1225.36 1466.51 1227.17 1470.08 Q1229 1473.62 1232.61 1473.62 Q1236.24 1473.62 1238.05 1470.08 Q1239.88 1466.51 1239.88 1459.41 Q1239.88 1452.28 1238.05 1448.74 Q1236.24 1445.17 1232.61 1445.17 M1232.61 1441.47 Q1238.42 1441.47 1241.47 1446.07 Q1244.55 1450.66 1244.55 1459.41 Q1244.55 1468.13 1241.47 1472.74 Q1238.42 1477.32 1232.61 1477.32 Q1226.8 1477.32 1223.72 1472.74 Q1220.66 1468.13 1220.66 1459.41 Q1220.66 1450.66 1223.72 1446.07 Q1226.8 1441.47 1232.61 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1249.62 1470.77 L1254.51 1470.77 L1254.51 1476.65 L1249.62 1476.65 L1249.62 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1259.62 1442.09 L1277.98 1442.09 L1277.98 1446.03 L1263.9 1446.03 L1263.9 1454.5 Q1264.92 1454.15 1265.94 1453.99 Q1266.96 1453.8 1267.98 1453.8 Q1273.76 1453.8 1277.14 1456.98 Q1280.52 1460.15 1280.52 1465.56 Q1280.52 1471.14 1277.05 1474.24 Q1273.58 1477.32 1267.26 1477.32 Q1265.08 1477.32 1262.82 1476.95 Q1260.57 1476.58 1258.16 1475.84 L1258.16 1471.14 Q1260.25 1472.28 1262.47 1472.83 Q1264.69 1473.39 1267.17 1473.39 Q1271.17 1473.39 1273.51 1471.28 Q1275.85 1469.18 1275.85 1465.56 Q1275.85 1461.95 1273.51 1459.85 Q1271.17 1457.74 1267.17 1457.74 Q1265.29 1457.74 1263.42 1458.16 Q1261.57 1458.57 1259.62 1459.45 L1259.62 1442.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1295.59 1445.17 Q1291.98 1445.17 1290.15 1448.74 Q1288.35 1452.28 1288.35 1459.41 Q1288.35 1466.51 1290.15 1470.08 Q1291.98 1473.62 1295.59 1473.62 Q1299.23 1473.62 1301.03 1470.08 Q1302.86 1466.51 1302.86 1459.41 Q1302.86 1452.28 1301.03 1448.74 Q1299.23 1445.17 1295.59 1445.17 M1295.59 1441.47 Q1301.4 1441.47 1304.46 1446.07 Q1307.54 1450.66 1307.54 1459.41 Q1307.54 1468.13 1304.46 1472.74 Q1301.4 1477.32 1295.59 1477.32 Q1289.78 1477.32 1286.7 1472.74 Q1283.65 1468.13 1283.65 1459.41 Q1283.65 1450.66 1286.7 1446.07 Q1289.78 1441.47 1295.59 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1746.58 1445.17 Q1742.96 1445.17 1741.14 1448.74 Q1739.33 1452.28 1739.33 1459.41 Q1739.33 1466.51 1741.14 1470.08 Q1742.96 1473.62 1746.58 1473.62 Q1750.21 1473.62 1752.02 1470.08 Q1753.84 1466.51 1753.84 1459.41 Q1753.84 1452.28 1752.02 1448.74 Q1750.21 1445.17 1746.58 1445.17 M1746.58 1441.47 Q1752.39 1441.47 1755.44 1446.07 Q1758.52 1450.66 1758.52 1459.41 Q1758.52 1468.13 1755.44 1472.74 Q1752.39 1477.32 1746.58 1477.32 Q1740.77 1477.32 1737.69 1472.74 Q1734.63 1468.13 1734.63 1459.41 Q1734.63 1450.66 1737.69 1446.07 Q1740.77 1441.47 1746.58 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1763.59 1470.77 L1768.47 1470.77 L1768.47 1476.65 L1763.59 1476.65 L1763.59 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1772.36 1442.09 L1794.58 1442.09 L1794.58 1444.08 L1782.04 1476.65 L1777.15 1476.65 L1788.96 1446.03 L1772.36 1446.03 L1772.36 1442.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1799.7 1442.09 L1818.06 1442.09 L1818.06 1446.03 L1803.98 1446.03 L1803.98 1454.5 Q1805 1454.15 1806.02 1453.99 Q1807.04 1453.8 1808.06 1453.8 Q1813.84 1453.8 1817.22 1456.98 Q1820.6 1460.15 1820.6 1465.56 Q1820.6 1471.14 1817.13 1474.24 Q1813.66 1477.32 1807.34 1477.32 Q1805.16 1477.32 1802.89 1476.95 Q1800.65 1476.58 1798.24 1475.84 L1798.24 1471.14 Q1800.33 1472.28 1802.55 1472.83 Q1804.77 1473.39 1807.25 1473.39 Q1811.25 1473.39 1813.59 1471.28 Q1815.93 1469.18 1815.93 1465.56 Q1815.93 1461.95 1813.59 1459.85 Q1811.25 1457.74 1807.25 1457.74 Q1805.37 1457.74 1803.5 1458.16 Q1801.64 1458.57 1799.7 1459.45 L1799.7 1442.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M2249.53 1472.72 L2257.16 1472.72 L2257.16 1446.35 L2248.85 1448.02 L2248.85 1443.76 L2257.12 1442.09 L2261.79 1442.09 L2261.79 1472.72 L2269.43 1472.72 L2269.43 1476.65 L2249.53 1476.65 L2249.53 1472.72 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M2274.5 1470.77 L2279.39 1470.77 L2279.39 1476.65 L2274.5 1476.65 L2274.5 1470.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M2294.46 1445.17 Q2290.84 1445.17 2289.02 1448.74 Q2287.21 1452.28 2287.21 1459.41 Q2287.21 1466.51 2289.02 1470.08 Q2290.84 1473.62 2294.46 1473.62 Q2298.09 1473.62 2299.9 1470.08 Q2301.72 1466.51 2301.72 1459.41 Q2301.72 1452.28 2299.9 1448.74 Q2298.09 1445.17 2294.46 1445.17 M2294.46 1441.47 Q2300.27 1441.47 2303.32 1446.07 Q2306.4 1450.66 2306.4 1459.41 Q2306.4 1468.13 2303.32 1472.74 Q2300.27 1477.32 2294.46 1477.32 Q2288.65 1477.32 2285.57 1472.74 Q2282.51 1468.13 2282.51 1459.41 Q2282.51 1450.66 2285.57 1446.07 Q2288.65 1441.47 2294.46 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M2321.47 1445.17 Q2317.86 1445.17 2316.03 1448.74 Q2314.22 1452.28 2314.22 1459.41 Q2314.22 1466.51 2316.03 1470.08 Q2317.86 1473.62 2321.47 1473.62 Q2325.1 1473.62 2326.91 1470.08 Q2328.74 1466.51 2328.74 1459.41 Q2328.74 1452.28 2326.91 1448.74 Q2325.1 1445.17 2321.47 1445.17 M2321.47 1441.47 Q2327.28 1441.47 2330.34 1446.07 Q2333.41 1450.66 2333.41 1459.41 Q2333.41 1468.13 2330.34 1472.74 Q2327.28 1477.32 2321.47 1477.32 Q2315.66 1477.32 2312.58 1472.74 Q2309.53 1468.13 2309.53 1459.41 Q2309.53 1450.66 2312.58 1446.07 Q2315.66 1441.47 2321.47 1441.47 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M131.538 1389.01 L139.177 1389.01 L139.177 1362.64 L130.867 1364.31 L130.867 1360.05 L139.13 1358.38 L143.806 1358.38 L143.806 1389.01 L151.445 1389.01 L151.445 1392.94 L131.538 1392.94 L131.538 1389.01 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M135.126 1177.32 L151.445 1177.32 L151.445 1181.26 L129.501 1181.26 L129.501 1177.32 Q132.163 1174.57 136.746 1169.94 Q141.353 1165.28 142.533 1163.94 Q144.779 1161.42 145.658 1159.68 Q146.561 1157.92 146.561 1156.23 Q146.561 1153.48 144.617 1151.74 Q142.695 1150.01 139.593 1150.01 Q137.394 1150.01 134.941 1150.77 Q132.51 1151.53 129.732 1153.08 L129.732 1148.36 Q132.556 1147.23 135.01 1146.65 Q137.464 1146.07 139.501 1146.07 Q144.871 1146.07 148.066 1148.76 Q151.26 1151.44 151.26 1155.93 Q151.26 1158.06 150.45 1159.98 Q149.663 1161.88 147.556 1164.47 Q146.978 1165.14 143.876 1168.36 Q140.774 1171.56 135.126 1177.32 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M144.316 950.933 Q147.672 951.65 149.547 953.919 Q151.445 956.187 151.445 959.52 Q151.445 964.636 147.927 967.437 Q144.408 970.238 137.927 970.238 Q135.751 970.238 133.436 969.798 Q131.144 969.382 128.691 968.525 L128.691 964.011 Q130.635 965.145 132.95 965.724 Q135.265 966.303 137.788 966.303 Q142.186 966.303 144.478 964.567 Q146.792 962.831 146.792 959.52 Q146.792 956.465 144.64 954.752 Q142.51 953.016 138.691 953.016 L134.663 953.016 L134.663 949.173 L138.876 949.173 Q142.325 949.173 144.154 947.808 Q145.982 946.419 145.982 943.826 Q145.982 941.164 144.084 939.752 Q142.209 938.317 138.691 938.317 Q136.769 938.317 134.57 938.734 Q132.371 939.15 129.732 940.03 L129.732 935.863 Q132.394 935.122 134.709 934.752 Q137.047 934.382 139.107 934.382 Q144.431 934.382 147.533 936.812 Q150.635 939.22 150.635 943.34 Q150.635 946.21 148.992 948.201 Q147.348 950.169 144.316 950.933 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M141.862 727.392 L130.056 745.841 L141.862 745.841 L141.862 727.392 M140.635 723.318 L146.515 723.318 L146.515 745.841 L151.445 745.841 L151.445 749.73 L146.515 749.73 L146.515 757.878 L141.862 757.878 L141.862 749.73 L126.26 749.73 L126.26 745.216 L140.635 723.318 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M130.543 511.629 L148.899 511.629 L148.899 515.564 L134.825 515.564 L134.825 524.036 Q135.843 523.689 136.862 523.527 Q137.88 523.342 138.899 523.342 Q144.686 523.342 148.066 526.513 Q151.445 529.684 151.445 535.101 Q151.445 540.679 147.973 543.781 Q144.501 546.86 138.181 546.86 Q136.005 546.86 133.737 546.49 Q131.492 546.119 129.084 545.379 L129.084 540.679 Q131.168 541.814 133.39 542.369 Q135.612 542.925 138.089 542.925 Q142.093 542.925 144.431 540.818 Q146.769 538.712 146.769 535.101 Q146.769 531.49 144.431 529.383 Q142.093 527.277 138.089 527.277 Q136.214 527.277 134.339 527.693 Q132.487 528.11 130.543 528.99 L130.543 511.629 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M139.917 315.356 Q136.769 315.356 134.918 317.509 Q133.089 319.662 133.089 323.412 Q133.089 327.139 134.918 329.315 Q136.769 331.467 139.917 331.467 Q143.066 331.467 144.894 329.315 Q146.746 327.139 146.746 323.412 Q146.746 319.662 144.894 317.509 Q143.066 315.356 139.917 315.356 M149.2 300.704 L149.2 304.963 Q147.441 304.13 145.635 303.69 Q143.853 303.25 142.093 303.25 Q137.464 303.25 135.01 306.375 Q132.58 309.5 132.232 315.819 Q133.598 313.805 135.658 312.741 Q137.718 311.653 140.195 311.653 Q145.404 311.653 148.413 314.824 Q151.445 317.972 151.445 323.412 Q151.445 328.736 148.297 331.953 Q145.149 335.171 139.917 335.171 Q133.922 335.171 130.751 330.588 Q127.58 325.981 127.58 317.254 Q127.58 309.06 131.468 304.199 Q135.357 299.315 141.908 299.315 Q143.667 299.315 145.45 299.662 Q147.255 300.009 149.2 300.704 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M129.223 88.2507 L151.445 88.2507 L151.445 90.2414 L138.899 122.811 L134.015 122.811 L145.82 92.1859 L129.223 92.1859 L129.223 88.2507 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M943.269 1527.24 Q945.465 1523.29 948.521 1521.41 Q951.576 1519.54 955.714 1519.54 Q961.284 1519.54 964.308 1523.45 Q967.331 1527.33 967.331 1534.53 L967.331 1556.04 L961.443 1556.04 L961.443 1534.72 Q961.443 1529.59 959.629 1527.11 Q957.815 1524.63 954.091 1524.63 Q949.539 1524.63 946.897 1527.65 Q944.256 1530.68 944.256 1535.9 L944.256 1556.04 L938.367 1556.04 L938.367 1534.72 Q938.367 1529.56 936.553 1527.11 Q934.739 1524.63 930.951 1524.63 Q926.463 1524.63 923.822 1527.68 Q921.18 1530.71 921.18 1535.9 L921.18 1556.04 L915.292 1556.04 L915.292 1520.4 L921.18 1520.4 L921.18 1525.93 Q923.185 1522.66 925.986 1521.1 Q928.787 1519.54 932.638 1519.54 Q936.521 1519.54 939.227 1521.51 Q941.964 1523.48 943.269 1527.24 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1003.97 1536.76 L1003.97 1539.62 L977.039 1539.62 Q977.421 1545.67 980.667 1548.85 Q983.946 1552 989.77 1552 Q993.144 1552 996.295 1551.17 Q999.478 1550.35 1002.6 1548.69 L1002.6 1554.23 Q999.446 1555.57 996.136 1556.27 Q992.826 1556.97 989.42 1556.97 Q980.89 1556.97 975.893 1552 Q970.928 1547.04 970.928 1538.57 Q970.928 1529.82 975.639 1524.69 Q980.381 1519.54 988.402 1519.54 Q995.595 1519.54 999.765 1524.18 Q1003.97 1528.8 1003.97 1536.76 M998.11 1535.04 Q998.046 1530.23 995.404 1527.37 Q992.794 1524.5 988.465 1524.5 Q983.564 1524.5 980.604 1527.27 Q977.676 1530.04 977.23 1535.07 L998.11 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1030.77 1525.87 Q1029.78 1525.3 1028.6 1525.04 Q1027.46 1524.76 1026.05 1524.76 Q1021.09 1524.76 1018.42 1528 Q1015.77 1531.22 1015.77 1537.27 L1015.77 1556.04 L1009.89 1556.04 L1009.89 1520.4 L1015.77 1520.4 L1015.77 1525.93 Q1017.62 1522.69 1020.58 1521.13 Q1023.54 1519.54 1027.77 1519.54 Q1028.38 1519.54 1029.11 1519.63 Q1029.84 1519.7 1030.73 1519.85 L1030.77 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1059.22 1537.81 Q1059.22 1531.44 1056.58 1527.94 Q1053.97 1524.44 1049.23 1524.44 Q1044.52 1524.44 1041.87 1527.94 Q1039.26 1531.44 1039.26 1537.81 Q1039.26 1544.14 1041.87 1547.64 Q1044.52 1551.14 1049.23 1551.14 Q1053.97 1551.14 1056.58 1547.64 Q1059.22 1544.14 1059.22 1537.81 M1065.08 1551.62 Q1065.08 1560.72 1061.03 1565.15 Q1056.99 1569.6 1048.65 1569.6 Q1045.57 1569.6 1042.83 1569.13 Q1040.09 1568.68 1037.51 1567.72 L1037.51 1562.03 Q1040.09 1563.43 1042.61 1564.1 Q1045.12 1564.76 1047.73 1564.76 Q1053.49 1564.76 1056.36 1561.74 Q1059.22 1558.75 1059.22 1552.67 L1059.22 1549.77 Q1057.41 1552.92 1054.57 1554.48 Q1051.74 1556.04 1047.79 1556.04 Q1041.24 1556.04 1037.23 1551.05 Q1033.22 1546.05 1033.22 1537.81 Q1033.22 1529.53 1037.23 1524.53 Q1041.24 1519.54 1047.79 1519.54 Q1051.74 1519.54 1054.57 1521.1 Q1057.41 1522.66 1059.22 1525.81 L1059.22 1520.4 L1065.08 1520.4 L1065.08 1551.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1101.71 1536.76 L1101.71 1539.62 L1074.78 1539.62 Q1075.17 1545.67 1078.41 1548.85 Q1081.69 1552 1087.52 1552 Q1090.89 1552 1094.04 1551.17 Q1097.22 1550.35 1100.34 1548.69 L1100.34 1554.23 Q1097.19 1555.57 1093.88 1556.27 Q1090.57 1556.97 1087.17 1556.97 Q1078.64 1556.97 1073.64 1552 Q1068.67 1547.04 1068.67 1538.57 Q1068.67 1529.82 1073.38 1524.69 Q1078.13 1519.54 1086.15 1519.54 Q1093.34 1519.54 1097.51 1524.18 Q1101.71 1528.8 1101.71 1536.76 M1095.85 1535.04 Q1095.79 1530.23 1093.15 1527.37 Q1090.54 1524.5 1086.21 1524.5 Q1081.31 1524.5 1078.35 1527.27 Q1075.42 1530.04 1074.98 1535.07 L1095.85 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1134.94 1566.87 L1134.94 1571.42 L1101.07 1571.42 L1101.07 1566.87 L1134.94 1566.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1146.75 1550.7 L1146.75 1569.6 L1140.86 1569.6 L1140.86 1520.4 L1146.75 1520.4 L1146.75 1525.81 Q1148.59 1522.62 1151.4 1521.1 Q1154.23 1519.54 1158.14 1519.54 Q1164.64 1519.54 1168.68 1524.69 Q1172.75 1529.85 1172.75 1538.25 Q1172.75 1546.65 1168.68 1551.81 Q1164.64 1556.97 1158.14 1556.97 Q1154.23 1556.97 1151.4 1555.44 Q1148.59 1553.88 1146.75 1550.7 M1166.67 1538.25 Q1166.67 1531.79 1164 1528.13 Q1161.36 1524.44 1156.71 1524.44 Q1152.06 1524.44 1149.39 1528.13 Q1146.75 1531.79 1146.75 1538.25 Q1146.75 1544.71 1149.39 1548.4 Q1152.06 1552.07 1156.71 1552.07 Q1161.36 1552.07 1164 1548.4 Q1166.67 1544.71 1166.67 1538.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1178.29 1541.98 L1178.29 1520.4 L1184.15 1520.4 L1184.15 1541.75 Q1184.15 1546.81 1186.12 1549.36 Q1188.09 1551.87 1192.04 1551.87 Q1196.78 1551.87 1199.52 1548.85 Q1202.29 1545.83 1202.29 1540.61 L1202.29 1520.4 L1208.15 1520.4 L1208.15 1556.04 L1202.29 1556.04 L1202.29 1550.57 Q1200.16 1553.82 1197.32 1555.41 Q1194.52 1556.97 1190.8 1556.97 Q1184.66 1556.97 1181.47 1553.15 Q1178.29 1549.33 1178.29 1541.98 M1193.03 1519.54 L1193.03 1519.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1234.95 1525.87 Q1233.96 1525.3 1232.78 1525.04 Q1231.64 1524.76 1230.23 1524.76 Q1225.27 1524.76 1222.6 1528 Q1219.95 1531.22 1219.95 1537.27 L1219.95 1556.04 L1214.07 1556.04 L1214.07 1520.4 L1219.95 1520.4 L1219.95 1525.93 Q1221.8 1522.69 1224.76 1521.13 Q1227.72 1519.54 1231.95 1519.54 Q1232.56 1519.54 1233.29 1519.63 Q1234.02 1519.7 1234.91 1519.85 L1234.95 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1241.09 1520.4 L1246.94 1520.4 L1246.94 1556.04 L1241.09 1556.04 L1241.09 1520.4 M1241.09 1506.52 L1246.94 1506.52 L1246.94 1513.93 L1241.09 1513.93 L1241.09 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1258.88 1510.27 L1258.88 1520.4 L1270.94 1520.4 L1270.94 1524.95 L1258.88 1524.95 L1258.88 1544.3 Q1258.88 1548.66 1260.06 1549.9 Q1261.27 1551.14 1264.93 1551.14 L1270.94 1551.14 L1270.94 1556.04 L1264.93 1556.04 Q1258.15 1556.04 1255.57 1553.53 Q1252.99 1550.98 1252.99 1544.3 L1252.99 1524.95 L1248.7 1524.95 L1248.7 1520.4 L1252.99 1520.4 L1252.99 1510.27 L1258.88 1510.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1291.92 1559.35 Q1289.44 1565.72 1287.08 1567.66 Q1284.73 1569.6 1280.78 1569.6 L1276.1 1569.6 L1276.1 1564.7 L1279.54 1564.7 Q1281.96 1564.7 1283.29 1563.56 Q1284.63 1562.41 1286.25 1558.14 L1287.3 1555.47 L1272.89 1520.4 L1279.09 1520.4 L1290.23 1548.28 L1301.37 1520.4 L1307.58 1520.4 L1291.92 1559.35 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1340.81 1566.87 L1340.81 1571.42 L1306.94 1571.42 L1306.94 1566.87 L1340.81 1566.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1352.74 1510.27 L1352.74 1520.4 L1364.81 1520.4 L1364.81 1524.95 L1352.74 1524.95 L1352.74 1544.3 Q1352.74 1548.66 1353.92 1549.9 Q1355.13 1551.14 1358.79 1551.14 L1364.81 1551.14 L1364.81 1556.04 L1358.79 1556.04 Q1352.01 1556.04 1349.43 1553.53 Q1346.85 1550.98 1346.85 1544.3 L1346.85 1524.95 L1342.56 1524.95 L1342.56 1520.4 L1346.85 1520.4 L1346.85 1510.27 L1352.74 1510.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1400.58 1534.53 L1400.58 1556.04 L1394.72 1556.04 L1394.72 1534.72 Q1394.72 1529.66 1392.75 1527.14 Q1390.78 1524.63 1386.83 1524.63 Q1382.09 1524.63 1379.35 1527.65 Q1376.61 1530.68 1376.61 1535.9 L1376.61 1556.04 L1370.73 1556.04 L1370.73 1506.52 L1376.61 1506.52 L1376.61 1525.93 Q1378.72 1522.72 1381.55 1521.13 Q1384.41 1519.54 1388.14 1519.54 Q1394.28 1519.54 1397.43 1523.36 Q1400.58 1527.14 1400.58 1534.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1427.38 1525.87 Q1426.39 1525.3 1425.22 1525.04 Q1424.07 1524.76 1422.67 1524.76 Q1417.71 1524.76 1415.03 1528 Q1412.39 1531.22 1412.39 1537.27 L1412.39 1556.04 L1406.5 1556.04 L1406.5 1520.4 L1412.39 1520.4 L1412.39 1525.93 Q1414.24 1522.69 1417.2 1521.13 Q1420.16 1519.54 1424.39 1519.54 Q1424.99 1519.54 1425.73 1519.63 Q1426.46 1519.7 1427.35 1519.85 L1427.38 1525.87 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1462.58 1536.76 L1462.58 1539.62 L1435.66 1539.62 Q1436.04 1545.67 1439.28 1548.85 Q1442.56 1552 1448.39 1552 Q1451.76 1552 1454.91 1551.17 Q1458.1 1550.35 1461.21 1548.69 L1461.21 1554.23 Q1458.06 1555.57 1454.75 1556.27 Q1451.44 1556.97 1448.04 1556.97 Q1439.51 1556.97 1434.51 1552 Q1429.55 1547.04 1429.55 1538.57 Q1429.55 1529.82 1434.26 1524.69 Q1439 1519.54 1447.02 1519.54 Q1454.21 1519.54 1458.38 1524.18 Q1462.58 1528.8 1462.58 1536.76 M1456.73 1535.04 Q1456.66 1530.23 1454.02 1527.37 Q1451.41 1524.5 1447.08 1524.5 Q1442.18 1524.5 1439.22 1527.27 Q1436.29 1530.04 1435.85 1535.07 L1456.73 1535.04 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1491.45 1521.45 L1491.45 1526.98 Q1488.97 1525.71 1486.3 1525.07 Q1483.62 1524.44 1480.76 1524.44 Q1476.4 1524.44 1474.2 1525.77 Q1472.04 1527.11 1472.04 1529.79 Q1472.04 1531.82 1473.6 1533 Q1475.16 1534.15 1479.87 1535.2 L1481.87 1535.64 Q1488.11 1536.98 1490.72 1539.43 Q1493.36 1541.85 1493.36 1546.21 Q1493.36 1551.17 1489.41 1554.07 Q1485.5 1556.97 1478.62 1556.97 Q1475.76 1556.97 1472.64 1556.39 Q1469.55 1555.85 1466.12 1554.74 L1466.12 1548.69 Q1469.36 1550.38 1472.51 1551.24 Q1475.66 1552.07 1478.75 1552.07 Q1482.89 1552.07 1485.12 1550.66 Q1487.35 1549.23 1487.35 1546.65 Q1487.35 1544.27 1485.72 1542.99 Q1484.13 1541.72 1478.69 1540.54 L1476.65 1540.07 Q1471.21 1538.92 1468.79 1536.56 Q1466.37 1534.18 1466.37 1530.04 Q1466.37 1525.01 1469.94 1522.27 Q1473.5 1519.54 1480.06 1519.54 Q1483.3 1519.54 1486.17 1520.01 Q1489.03 1520.49 1491.45 1521.45 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1529.14 1534.53 L1529.14 1556.04 L1523.28 1556.04 L1523.28 1534.72 Q1523.28 1529.66 1521.31 1527.14 Q1519.33 1524.63 1515.39 1524.63 Q1510.64 1524.63 1507.91 1527.65 Q1505.17 1530.68 1505.17 1535.9 L1505.17 1556.04 L1499.28 1556.04 L1499.28 1506.52 L1505.17 1506.52 L1505.17 1525.93 Q1507.27 1522.72 1510.1 1521.13 Q1512.97 1519.54 1516.69 1519.54 Q1522.83 1519.54 1525.99 1523.36 Q1529.14 1527.14 1529.14 1534.53 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1549.09 1524.5 Q1544.38 1524.5 1541.65 1528.19 Q1538.91 1531.85 1538.91 1538.25 Q1538.91 1544.65 1541.61 1548.34 Q1544.35 1552 1549.09 1552 Q1553.77 1552 1556.51 1548.31 Q1559.25 1544.62 1559.25 1538.25 Q1559.25 1531.92 1556.51 1528.23 Q1553.77 1524.5 1549.09 1524.5 M1549.09 1519.54 Q1556.73 1519.54 1561.09 1524.5 Q1565.45 1529.47 1565.45 1538.25 Q1565.45 1547 1561.09 1552 Q1556.73 1556.97 1549.09 1556.97 Q1541.42 1556.97 1537.06 1552 Q1532.73 1547 1532.73 1538.25 Q1532.73 1529.47 1537.06 1524.5 Q1541.42 1519.54 1549.09 1519.54 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1571.6 1506.52 L1577.45 1506.52 L1577.45 1556.04 L1571.6 1556.04 L1571.6 1506.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M1607.05 1525.81 L1607.05 1506.52 L1612.91 1506.52 L1612.91 1556.04 L1607.05 1556.04 L1607.05 1550.7 Q1605.21 1553.88 1602.37 1555.44 Q1599.57 1556.97 1595.63 1556.97 Q1589.17 1556.97 1585.09 1551.81 Q1581.05 1546.65 1581.05 1538.25 Q1581.05 1529.85 1585.09 1524.69 Q1589.17 1519.54 1595.63 1519.54 Q1599.57 1519.54 1602.37 1521.1 Q1605.21 1522.62 1607.05 1525.81 M1587.1 1538.25 Q1587.1 1544.71 1589.74 1548.4 Q1592.41 1552.07 1597.06 1552.07 Q1601.71 1552.07 1604.38 1548.4 Q1607.05 1544.71 1607.05 1538.25 Q1607.05 1531.79 1604.38 1528.13 Q1601.71 1524.44 1597.06 1524.44 Q1592.41 1524.44 1589.74 1528.13 Q1587.1 1531.79 1587.1 1538.25 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M44.1444 904.492 L50.9239 904.492 Q47.9002 907.739 46.4043 911.431 Q44.9083 915.091 44.9083 919.229 Q44.9083 927.377 49.9054 931.705 Q54.8707 936.034 64.2919 936.034 Q73.6813 936.034 78.6784 931.705 Q83.6436 927.377 83.6436 919.229 Q83.6436 915.091 82.1477 911.431 Q80.6518 907.739 77.6281 904.492 L84.3439 904.492 Q86.6355 907.866 87.7814 911.653 Q88.9272 915.409 88.9272 919.611 Q88.9272 930.4 82.3387 936.607 Q75.7183 942.814 64.2919 942.814 Q52.8336 942.814 46.2451 936.607 Q39.6248 930.4 39.6248 919.611 Q39.6248 915.346 40.7706 911.59 Q41.8846 907.802 44.1444 904.492 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M57.8307 877.692 Q57.2578 878.679 57.0032 879.857 Q56.7167 881.003 56.7167 882.403 Q56.7167 887.368 59.9632 890.042 Q63.1779 892.684 69.2253 892.684 L88.0042 892.684 L88.0042 898.572 L52.3562 898.572 L52.3562 892.684 L57.8944 892.684 Q54.6479 890.838 53.0883 887.878 Q51.4968 884.917 51.4968 880.684 Q51.4968 880.08 51.5923 879.347 Q51.656 878.615 51.8151 877.724 L57.8307 877.692 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M56.4621 859.168 Q56.4621 863.879 60.1542 866.616 Q63.8145 869.353 70.212 869.353 Q76.6095 869.353 80.3017 866.648 Q83.9619 863.911 83.9619 859.168 Q83.9619 854.489 80.2698 851.752 Q76.5777 849.015 70.212 849.015 Q63.8781 849.015 60.186 851.752 Q56.4621 854.489 56.4621 859.168 M51.4968 859.168 Q51.4968 851.529 56.4621 847.169 Q61.4273 842.808 70.212 842.808 Q78.9649 842.808 83.9619 847.169 Q88.9272 851.529 88.9272 859.168 Q88.9272 866.839 83.9619 871.199 Q78.9649 875.528 70.212 875.528 Q61.4273 875.528 56.4621 871.199 Q51.4968 866.839 51.4968 859.168 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M53.4065 813.94 L58.9447 813.94 Q57.6716 816.422 57.035 819.096 Q56.3984 821.77 56.3984 824.634 Q56.3984 828.995 57.7352 831.191 Q59.072 833.355 61.7456 833.355 Q63.7826 833.355 64.9603 831.796 Q66.1061 830.236 67.1565 825.525 L67.6021 823.52 Q68.9389 817.282 71.3897 814.672 Q73.8086 812.03 78.1691 812.03 Q83.1344 812.03 86.0308 815.977 Q88.9272 819.892 88.9272 826.767 Q88.9272 829.631 88.3543 832.751 Q87.8132 835.838 86.6992 839.275 L80.6518 839.275 Q82.3387 836.029 83.198 832.878 Q84.0256 829.727 84.0256 826.639 Q84.0256 822.502 82.6251 820.274 Q81.1929 818.046 78.6147 818.046 Q76.2276 818.046 74.9545 819.669 Q73.6813 821.26 72.5037 826.703 L72.0262 828.74 Q70.8804 834.183 68.5251 836.602 Q66.138 839.021 62.0002 839.021 Q56.9713 839.021 54.2341 835.456 Q51.4968 831.891 51.4968 825.334 Q51.4968 822.088 51.9743 819.223 Q52.4517 816.359 53.4065 813.94 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M53.4065 783.162 L58.9447 783.162 Q57.6716 785.644 57.035 788.318 Q56.3984 790.991 56.3984 793.856 Q56.3984 798.217 57.7352 800.413 Q59.072 802.577 61.7456 802.577 Q63.7826 802.577 64.9603 801.017 Q66.1061 799.458 67.1565 794.747 L67.6021 792.742 Q68.9389 786.504 71.3897 783.894 Q73.8086 781.252 78.1691 781.252 Q83.1344 781.252 86.0308 785.199 Q88.9272 789.114 88.9272 795.989 Q88.9272 798.853 88.3543 801.972 Q87.8132 805.06 86.6992 808.497 L80.6518 808.497 Q82.3387 805.251 83.198 802.1 Q84.0256 798.949 84.0256 795.861 Q84.0256 791.724 82.6251 789.496 Q81.1929 787.268 78.6147 787.268 Q76.2276 787.268 74.9545 788.891 Q73.6813 790.482 72.5037 795.925 L72.0262 797.962 Q70.8804 803.405 68.5251 805.824 Q66.138 808.243 62.0002 808.243 Q56.9713 808.243 54.2341 804.678 Q51.4968 801.113 51.4968 794.556 Q51.4968 791.31 51.9743 788.445 Q52.4517 785.581 53.4065 783.162 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M40.4842 754.134 L40.4842 724.088 L45.895 724.088 L45.895 747.705 L59.9632 747.705 L59.9632 725.075 L65.3741 725.075 L65.3741 747.705 L82.5933 747.705 L82.5933 723.515 L88.0042 723.515 L88.0042 754.134 L40.4842 754.134 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M66.4881 687.74 L88.0042 687.74 L88.0042 693.596 L66.679 693.596 Q61.6183 693.596 59.1038 695.57 Q56.5894 697.543 56.5894 701.49 Q56.5894 706.232 59.6131 708.969 Q62.6368 711.707 67.8567 711.707 L88.0042 711.707 L88.0042 717.595 L52.3562 717.595 L52.3562 711.707 L57.8944 711.707 Q54.6797 709.606 53.0883 706.773 Q51.4968 703.909 51.4968 700.185 Q51.4968 694.042 55.3163 690.891 Q59.1038 687.74 66.4881 687.74 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M42.2347 675.804 L52.3562 675.804 L52.3562 663.741 L56.9077 663.741 L56.9077 675.804 L76.2594 675.804 Q80.6199 675.804 81.8613 674.626 Q83.1026 673.417 83.1026 669.757 L83.1026 663.741 L88.0042 663.741 L88.0042 669.757 Q88.0042 676.536 85.4897 679.114 Q82.9434 681.692 76.2594 681.692 L56.9077 681.692 L56.9077 685.989 L52.3562 685.989 L52.3562 681.692 L42.2347 681.692 L42.2347 675.804 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M57.8307 636.941 Q57.2578 637.928 57.0032 639.106 Q56.7167 640.252 56.7167 641.652 Q56.7167 646.617 59.9632 649.291 Q63.1779 651.933 69.2253 651.933 L88.0042 651.933 L88.0042 657.821 L52.3562 657.821 L52.3562 651.933 L57.8944 651.933 Q54.6479 650.087 53.0883 647.127 Q51.4968 644.166 51.4968 639.933 Q51.4968 639.329 51.5923 638.596 Q51.656 637.864 51.8151 636.973 L57.8307 636.941 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M56.4621 618.417 Q56.4621 623.128 60.1542 625.865 Q63.8145 628.602 70.212 628.602 Q76.6095 628.602 80.3017 625.897 Q83.9619 623.16 83.9619 618.417 Q83.9619 613.738 80.2698 611.001 Q76.5777 608.264 70.212 608.264 Q63.8781 608.264 60.186 611.001 Q56.4621 613.738 56.4621 618.417 M51.4968 618.417 Q51.4968 610.778 56.4621 606.418 Q61.4273 602.057 70.212 602.057 Q78.9649 602.057 83.9619 606.418 Q88.9272 610.778 88.9272 618.417 Q88.9272 626.088 83.9619 630.448 Q78.9649 634.777 70.212 634.777 Q61.4273 634.777 56.4621 630.448 Q51.4968 626.088 51.4968 618.417 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M82.657 590.249 L101.563 590.249 L101.563 596.137 L52.3562 596.137 L52.3562 590.249 L57.7671 590.249 Q54.5842 588.403 53.0564 585.602 Q51.4968 582.769 51.4968 578.854 Q51.4968 572.361 56.6531 568.319 Q61.8093 564.245 70.212 564.245 Q78.6147 564.245 83.771 568.319 Q88.9272 572.361 88.9272 578.854 Q88.9272 582.769 87.3994 585.602 Q85.8398 588.403 82.657 590.249 M70.212 570.324 Q63.7508 570.324 60.0905 572.998 Q56.3984 575.64 56.3984 580.287 Q56.3984 584.934 60.0905 587.607 Q63.7508 590.249 70.212 590.249 Q76.6732 590.249 80.3653 587.607 Q84.0256 584.934 84.0256 580.287 Q84.0256 575.64 80.3653 572.998 Q76.6732 570.324 70.212 570.324 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M91.3143 543.27 Q97.68 545.753 99.6216 548.108 Q101.563 550.463 101.563 554.41 L101.563 559.089 L96.6615 559.089 L96.6615 555.651 Q96.6615 553.232 95.5157 551.896 Q94.3699 550.559 90.1048 548.935 L87.4312 547.885 L52.3562 562.303 L52.3562 556.097 L80.238 544.957 L52.3562 533.817 L52.3562 527.61 L91.3143 543.27 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip042)\" style=\"stroke:#009af9; stroke-width:8; stroke-opacity:1; fill:none\" points=\"\n",
       "  237.067,1384.24 257.608,1384.24 278.149,1384.24 298.689,1384.24 319.23,1384.24 339.771,1384.24 360.311,1384.24 380.852,1384.24 401.393,1384.24 421.933,1384.24 \n",
       "  442.474,1384.24 463.015,1384.24 483.555,1384.24 504.096,1384.24 524.637,1384.24 545.177,1384.24 565.718,1384.24 586.259,1384.24 606.799,1384.24 627.34,1384.24 \n",
       "  647.881,1384.24 668.421,1384.24 688.962,1384.24 709.503,1384.24 730.043,1384.24 750.584,1384.24 771.125,1384.24 791.665,1384.24 812.206,1384.24 832.747,1384.24 \n",
       "  853.287,1384.24 873.828,1384.24 894.369,1384.24 914.909,1384.24 935.45,1178.35 955.991,1154.39 976.531,1140.89 997.072,1166.06 1017.61,1163.82 1038.15,1151.77 \n",
       "  1058.69,1164.2 1079.23,1154.93 1099.78,1107.11 1120.32,1107.34 1140.86,1108.03 1161.4,1105.02 1181.94,1094.3 1202.48,1069.91 1223.02,1082.44 1243.56,1082.54 \n",
       "  1264.1,1094.72 1284.64,230.283 1305.18,242.001 1325.72,218.371 1346.26,242.001 1366.8,230.283 1387.34,230.186 1407.89,230.283 1428.43,230.283 1448.97,206.653 \n",
       "  1469.51,242.001 1490.05,218.468 1510.59,230.283 1531.13,218.371 1551.67,230.186 1572.21,218.468 1592.75,242.001 1613.29,86.1857 1633.83,86.1857 1654.37,86.1857 \n",
       "  1674.91,86.1857 1695.45,86.1857 1716,86.1857 1736.54,86.1857 1757.08,86.1857 1777.62,86.1857 1798.16,86.2658 1818.7,86.2658 1839.24,86.2658 1859.78,86.2658 \n",
       "  1880.32,86.2658 1900.86,86.2658 1921.4,86.2658 1941.94,86.2658 1962.48,86.2658 1983.02,86.2658 2003.56,86.2658 2024.11,86.2658 2044.65,86.2658 2065.19,86.2658 \n",
       "  2085.73,86.2658 2106.27,86.2658 2126.81,86.2658 2147.35,86.2658 2167.89,86.2658 2188.43,86.2658 2208.97,86.2658 2229.51,86.2658 2250.05,86.2658 2270.59,86.2658 \n",
       "  2291.13,86.2658 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip040)\" d=\"\n",
       "M1838.93 214.069 L2280.18 214.069 L2280.18 93.1086 L1838.93 93.1086  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1838.93,214.069 2280.18,214.069 2280.18,93.1086 1838.93,93.1086 1838.93,214.069 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip040)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1863.12,153.589 2008.27,153.589 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip040)\" d=\"M 0 0 M2045.66 170.869 L2032.47 136.309 L2037.35 136.309 L2048.3 165.406 L2059.27 136.309 L2064.13 136.309 L2050.96 170.869 L2045.66 170.869 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M2076.7 157.836 Q2071.54 157.836 2069.55 159.017 Q2067.56 160.197 2067.56 163.045 Q2067.56 165.313 2069.04 166.656 Q2070.55 167.975 2073.11 167.975 Q2076.66 167.975 2078.79 165.475 Q2080.94 162.952 2080.94 158.785 L2080.94 157.836 L2076.7 157.836 M2085.2 156.077 L2085.2 170.869 L2080.94 170.869 L2080.94 166.933 Q2079.48 169.295 2077.3 170.429 Q2075.13 171.54 2071.98 171.54 Q2068 171.54 2065.64 169.318 Q2063.3 167.072 2063.3 163.322 Q2063.3 158.947 2066.22 156.725 Q2069.16 154.503 2074.97 154.503 L2080.94 154.503 L2080.94 154.086 Q2080.94 151.147 2078.99 149.549 Q2077.07 147.929 2073.58 147.929 Q2071.36 147.929 2069.25 148.461 Q2067.14 148.994 2065.2 150.059 L2065.2 146.123 Q2067.54 145.221 2069.73 144.781 Q2071.93 144.318 2074.02 144.318 Q2079.64 144.318 2082.42 147.234 Q2085.2 150.151 2085.2 156.077 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M2089.67 134.85 L2093.92 134.85 L2093.92 170.869 L2089.67 170.869 L2089.67 134.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M2098.39 144.943 L2102.65 144.943 L2102.65 170.869 L2098.39 170.869 L2098.39 144.943 M2098.39 134.85 L2102.65 134.85 L2102.65 140.244 L2098.39 140.244 L2098.39 134.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M2124.18 148.878 L2124.18 134.85 L2128.44 134.85 L2128.44 170.869 L2124.18 170.869 L2124.18 166.98 Q2122.84 169.295 2120.78 170.429 Q2118.74 171.54 2115.87 171.54 Q2111.17 171.54 2108.21 167.79 Q2105.27 164.04 2105.27 157.929 Q2105.27 151.818 2108.21 148.068 Q2111.17 144.318 2115.87 144.318 Q2118.74 144.318 2120.78 145.452 Q2122.84 146.563 2124.18 148.878 M2109.67 157.929 Q2109.67 162.628 2111.59 165.313 Q2113.53 167.975 2116.91 167.975 Q2120.29 167.975 2122.23 165.313 Q2124.18 162.628 2124.18 157.929 Q2124.18 153.23 2122.23 150.568 Q2120.29 147.883 2116.91 147.883 Q2113.53 147.883 2111.59 150.568 Q2109.67 153.23 2109.67 157.929 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M2144.69 157.836 Q2139.53 157.836 2137.54 159.017 Q2135.54 160.197 2135.54 163.045 Q2135.54 165.313 2137.03 166.656 Q2138.53 167.975 2141.1 167.975 Q2144.64 167.975 2146.77 165.475 Q2148.92 162.952 2148.92 158.785 L2148.92 157.836 L2144.69 157.836 M2153.18 156.077 L2153.18 170.869 L2148.92 170.869 L2148.92 166.933 Q2147.47 169.295 2145.29 170.429 Q2143.11 171.54 2139.97 171.54 Q2135.98 171.54 2133.62 169.318 Q2131.29 167.072 2131.29 163.322 Q2131.29 158.947 2134.2 156.725 Q2137.14 154.503 2142.95 154.503 L2148.92 154.503 L2148.92 154.086 Q2148.92 151.147 2146.98 149.549 Q2145.06 147.929 2141.56 147.929 Q2139.34 147.929 2137.23 148.461 Q2135.13 148.994 2133.18 150.059 L2133.18 146.123 Q2135.52 145.221 2137.72 144.781 Q2139.92 144.318 2142 144.318 Q2147.63 144.318 2150.41 147.234 Q2153.18 150.151 2153.18 156.077 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M2161.86 137.582 L2161.86 144.943 L2170.64 144.943 L2170.64 148.253 L2161.86 148.253 L2161.86 162.327 Q2161.86 165.498 2162.72 166.401 Q2163.6 167.304 2166.26 167.304 L2170.64 167.304 L2170.64 170.869 L2166.26 170.869 Q2161.33 170.869 2159.46 169.04 Q2157.58 167.188 2157.58 162.327 L2157.58 148.253 L2154.46 148.253 L2154.46 144.943 L2157.58 144.943 L2157.58 137.582 L2161.86 137.582 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M2175.1 144.943 L2179.36 144.943 L2179.36 170.869 L2175.1 170.869 L2175.1 144.943 M2175.1 134.85 L2179.36 134.85 L2179.36 140.244 L2175.1 140.244 L2175.1 134.85 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M2193.88 147.929 Q2190.45 147.929 2188.46 150.614 Q2186.47 153.276 2186.47 157.929 Q2186.47 162.582 2188.44 165.267 Q2190.43 167.929 2193.88 167.929 Q2197.28 167.929 2199.27 165.244 Q2201.26 162.558 2201.26 157.929 Q2201.26 153.322 2199.27 150.637 Q2197.28 147.929 2193.88 147.929 M2193.88 144.318 Q2199.43 144.318 2202.6 147.929 Q2205.78 151.54 2205.78 157.929 Q2205.78 164.295 2202.6 167.929 Q2199.43 171.54 2193.88 171.54 Q2188.3 171.54 2185.13 167.929 Q2181.98 164.295 2181.98 157.929 Q2181.98 151.54 2185.13 147.929 Q2188.3 144.318 2193.88 144.318 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip040)\" d=\"M 0 0 M2231.79 155.221 L2231.79 170.869 L2227.53 170.869 L2227.53 155.359 Q2227.53 151.679 2226.1 149.85 Q2224.66 148.022 2221.79 148.022 Q2218.35 148.022 2216.35 150.221 Q2214.36 152.42 2214.36 156.216 L2214.36 170.869 L2210.08 170.869 L2210.08 144.943 L2214.36 144.943 L2214.36 148.971 Q2215.89 146.633 2217.95 145.475 Q2220.04 144.318 2222.74 144.318 Q2227.21 144.318 2229.5 147.096 Q2231.79 149.85 2231.79 155.221 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(curve.parameter_values,\n",
    "     curve.measurements,\n",
    "     xlab=curve.parameter_name,\n",
    "     ylab=\"Cross Entropy\",\n",
    "     label=\"Validation\", lw=2)\n",
    "# plot!(Net2.report.training_losses, label=\"Training\", lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9595"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = round(minimum(curve.measurements), digits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJBase.NumericRange(Float64, :merge_purity_threshold, ... )"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param1 = :merge_purity_threshold\n",
    "\n",
    "r1 = range(dt2, param1, lower=0, upper=1, scale=:linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "    model = DecisionTreeClassifier(\n",
       "            max_depth = -1,\n",
       "            min_samples_leaf = 1,\n",
       "            min_samples_split = 2,\n",
       "            min_purity_increase = 0.0,\n",
       "            n_subfeatures = 0,\n",
       "            post_prune = true,\n",
       "            merge_purity_threshold = 1.0,\n",
       "            pdf_smoothing = 0.0,\n",
       "            display_depth = 5),\n",
       "    tuning = Grid(\n",
       "            goal = 100,\n",
       "            resolution = 10,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = CV(\n",
       "            nfolds = 6,\n",
       "            shuffle = false,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = cross_entropy(\n",
       "            eps = 2.220446049250313e-16),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = MLJBase.NumericRange{Float64,MLJBase.Bounded,Symbol}[\u001b[34mNumericRange{Float64,…} @027\u001b[39m],\n",
       "    train_best = true,\n",
       "    repeats = 1,\n",
       "    n = nothing,\n",
       "    acceleration = CPUThreads{Int64}(1),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true)\u001b[34m @081\u001b[39m"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_dt_model = TunedModel(model=dt2,\n",
    "                                    tuning=Grid(goal=100),\n",
    "                                    resampling=CV(), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @579\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @516\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @966\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_dt = machine(self_tuning_dt_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @579\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n",
      "┌ Info: Attempting to evaluate 100 models.\n",
      "└ @ MLJTuning /home/andrew/.julia/packages/MLJTuning/Bbgvk/src/tuned_models.jl:494\n",
      "\u001b[33mEvaluating over 100 metamodels: 100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @579\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @516\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @966\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = fit!(self_tuning_dt, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(best_model = \u001b[34mDecisionTreeClassifier @336\u001b[39m,\n",
       " best_fitted_params = (tree = Decision Leaf\n",
       "Majority: 2\n",
       "Samples:  438,\n",
       "                       encoding = Dict{CategoricalValue{String,UInt32},UInt32}(\"B\" => 0x00000001,\"L\" => 0x00000002,\"R\" => 0x00000003),),)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = fitted_params(self_tuning_dt)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = true,\n",
       "    merge_purity_threshold = 0.2828282828282828,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 5)\u001b[34m @336\u001b[39m"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.28283"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_loss = round(z.report.best_result.measurement[1],digits=5)\n",
    "best_mpt = round(best.best_model.merge_purity_threshold,digits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"Figures/LearningCurve_DT_merge_purity_thresh:$(best_mpt)_loss:$(best_loss)\"\n",
    "png(replace(fn,'.' => ','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (10, 0.7, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (11, 0.6363636363636364, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (12, 0.5833333333333334, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (13, 0.6153846153846154, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (14, 0.6428571428571429, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (15, 0.6, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (16, 0.5625, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (17, 0.5882352941176471, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (18, 0.6111111111111112, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (19, 0.5789473684210527, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (20, 0.55, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (21, 0.5238095238095238, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (22, 0.5, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (23, 0.4782608695652174, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (24, 0.5, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (25, 0.48, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (26, 0.46153846153846156, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (27, 0.4444444444444444, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (28, 0.4642857142857143, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (29, 0.4482758620689655, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (30, 0.4666666666666667, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (31, 0.45161290322580644, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (32, 0.4375, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (33, 0.45454545454545453, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (34, 0.47058823529411764, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (35, 0.45714285714285713, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (36, 0.4722222222222222, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (37, 0.4594594594594595, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (38, 0.47368421052631576, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (39, 0.46153846153846156, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (40, 0.475, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (41, 0.4634146341463415, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (42, 0.4523809523809524, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (43, 0.46511627906976744, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (44, 0.45454545454545453, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (45, 0.4666666666666667, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (46, 0.4782608695652174, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (47, 0.46808510638297873, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (48, 0.4583333333333333, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (49, 0.46938775510204084, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (50, 0.46, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (51, 0.45098039215686275, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (52, 0.4423076923076923, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (53, 0.4528301886792453, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (54, 0.46296296296296297, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (55, 0.4727272727272727, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (56, 0.48214285714285715, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (57, 0.49122807017543857, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (58, 0.4827586206896552, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (59, 0.4745762711864407, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (60, 0.48333333333333334, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (61, 0.47540983606557374, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (62, 0.46774193548387094, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (63, 0.4603174603174603, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (64, 0.453125, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (65, 0.4461538461538462, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (66, 0.45454545454545453, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (67, 0.4626865671641791, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (68, 0.47058823529411764, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (69, 0.4782608695652174, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (70, 0.4857142857142857, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (71, 0.49295774647887325, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (72, 0.4861111111111111, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (73, 0.4794520547945205, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (74, 0.4864864864864865, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (75, 0.48, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (76, 0.4868421052631579, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (77, 0.4805194805194805, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (78, 0.48717948717948717, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (79, 0.4810126582278481, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (80, 0.475, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (81, 0.4691358024691358, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (82, 0.4634146341463415, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (83, 0.4578313253012048, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (84, 0.4642857142857143, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (85, 0.4588235294117647, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (86, 0.45348837209302323, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (87, 0.4482758620689655, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (88, 0.4431818181818182, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (89, 0.449438202247191, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (90, 0.45555555555555555, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (91, 0.45054945054945056, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (92, 0.44565217391304346, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (93, 0.44086021505376344, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (94, 0.44680851063829785, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (95, 0.45263157894736844, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (96, 0.4479166666666667, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (97, 0.44329896907216493, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (98, 0.4489795918367347, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (99, 0.4444444444444444, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (100, 0.44, 0.5340909090909091)\n",
      "(d, train_metric, valid_metric) = (101, 0.44554455445544555, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (102, 0.45098039215686275, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (103, 0.44660194174757284, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (104, 0.4519230769230769, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (105, 0.45714285714285713, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (106, 0.4528301886792453, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (107, 0.45794392523364486, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (108, 0.46296296296296297, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (109, 0.45871559633027525, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (110, 0.45454545454545453, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (111, 0.4594594594594595, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (112, 0.45535714285714285, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (113, 0.45132743362831856, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (114, 0.45614035087719296, 0.42045454545454547)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (115, 0.4608695652173913, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (116, 0.46551724137931033, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (117, 0.4700854700854701, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (118, 0.4661016949152542, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (119, 0.46218487394957986, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (120, 0.4666666666666667, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (121, 0.4628099173553719, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (122, 0.4672131147540984, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (123, 0.4715447154471545, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (124, 0.46774193548387094, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (125, 0.472, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (126, 0.46825396825396826, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (127, 0.47244094488188976, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (128, 0.4765625, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (129, 0.4728682170542636, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (130, 0.46923076923076923, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (131, 0.4732824427480916, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (132, 0.4772727272727273, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (133, 0.47368421052631576, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (134, 0.47761194029850745, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (135, 0.48148148148148145, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (136, 0.47794117647058826, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (137, 0.4744525547445255, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (138, 0.4782608695652174, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (139, 0.4748201438848921, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (140, 0.4785714285714286, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (141, 0.475177304964539, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (142, 0.4788732394366197, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (143, 0.4755244755244755, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (144, 0.4791666666666667, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (145, 0.47586206896551725, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (146, 0.4726027397260274, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (147, 0.46938775510204084, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (148, 0.47297297297297297, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (149, 0.4697986577181208, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (150, 0.47333333333333333, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (151, 0.47019867549668876, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (152, 0.47368421052631576, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (153, 0.477124183006536, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (154, 0.4805194805194805, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (155, 0.4774193548387097, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (156, 0.47435897435897434, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (157, 0.4713375796178344, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (158, 0.46835443037974683, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (159, 0.46540880503144655, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (160, 0.46875, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (161, 0.4658385093167702, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (162, 0.46296296296296297, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (163, 0.4601226993865031, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (164, 0.4573170731707317, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (165, 0.46060606060606063, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (166, 0.463855421686747, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (167, 0.46107784431137727, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (168, 0.4642857142857143, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (169, 0.46153846153846156, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (170, 0.4588235294117647, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (171, 0.4619883040935672, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (172, 0.46511627906976744, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (173, 0.4682080924855491, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (174, 0.46551724137931033, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (175, 0.4685714285714286, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (176, 0.4659090909090909, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (177, 0.4689265536723164, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (178, 0.46629213483146065, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (179, 0.46368715083798884, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (180, 0.46111111111111114, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (181, 0.4585635359116022, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (182, 0.46153846153846156, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (183, 0.4644808743169399, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (184, 0.46195652173913043, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (185, 0.4648648648648649, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (186, 0.46236559139784944, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (187, 0.46524064171123, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (188, 0.46808510638297873, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (189, 0.4708994708994709, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (190, 0.46842105263157896, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (191, 0.46596858638743455, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (192, 0.46875, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (193, 0.47150259067357514, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (194, 0.4690721649484536, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (195, 0.4666666666666667, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (196, 0.46938775510204084, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (197, 0.467005076142132, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (198, 0.46464646464646464, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (199, 0.46733668341708545, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (200, 0.47, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (201, 0.46766169154228854, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (202, 0.46534653465346537, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (203, 0.46798029556650245, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (204, 0.47058823529411764, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (205, 0.4682926829268293, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (206, 0.470873786407767, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (207, 0.46859903381642515, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (208, 0.47115384615384615, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (209, 0.47368421052631576, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (210, 0.47619047619047616, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (211, 0.47393364928909953, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (212, 0.47641509433962265, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (213, 0.4788732394366197, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (214, 0.4766355140186916, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (215, 0.4744186046511628, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (216, 0.47685185185185186, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (217, 0.4792626728110599, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (218, 0.47706422018348627, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (219, 0.4794520547945205, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (220, 0.4818181818181818, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (221, 0.4841628959276018, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (222, 0.4864864864864865, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (223, 0.484304932735426, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (224, 0.48214285714285715, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (225, 0.48, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (226, 0.4778761061946903, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (227, 0.47577092511013214, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (228, 0.4780701754385965, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (229, 0.48034934497816595, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (230, 0.4826086956521739, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (231, 0.48484848484848486, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (232, 0.4827586206896552, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (233, 0.48068669527896996, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (234, 0.47863247863247865, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (235, 0.4808510638297872, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (236, 0.4830508474576271, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (237, 0.4810126582278481, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (238, 0.4789915966386555, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (239, 0.4811715481171548, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (240, 0.48333333333333334, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (241, 0.4854771784232365, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (242, 0.4834710743801653, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (243, 0.48148148148148145, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (244, 0.48360655737704916, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (245, 0.4857142857142857, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (246, 0.483739837398374, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (247, 0.4817813765182186, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (248, 0.4838709677419355, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (249, 0.4859437751004016, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (250, 0.488, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (251, 0.4860557768924303, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (252, 0.48412698412698413, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (253, 0.48221343873517786, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (254, 0.48031496062992124, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (255, 0.4823529411764706, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (256, 0.48046875, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (257, 0.4785992217898833, 0.42045454545454547)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (258, 0.47674418604651164, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (259, 0.4749034749034749, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (260, 0.47307692307692306, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (261, 0.47509578544061304, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (262, 0.4732824427480916, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (263, 0.4714828897338403, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (264, 0.4696969696969697, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (265, 0.4679245283018868, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (266, 0.46616541353383456, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (267, 0.46441947565543074, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (268, 0.4626865671641791, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (269, 0.46096654275092935, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (270, 0.45925925925925926, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (271, 0.4612546125461255, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (272, 0.4632352941176471, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (273, 0.46153846153846156, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (274, 0.45985401459854014, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (275, 0.4581818181818182, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (276, 0.4601449275362319, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (277, 0.4620938628158845, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (278, 0.46402877697841727, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (279, 0.46236559139784944, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (280, 0.4607142857142857, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (281, 0.4626334519572954, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (282, 0.4645390070921986, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (283, 0.4664310954063604, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (284, 0.4647887323943662, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (285, 0.4666666666666667, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (286, 0.46853146853146854, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (287, 0.47038327526132406, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (288, 0.46875, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (289, 0.47058823529411764, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (290, 0.4689655172413793, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (291, 0.46735395189003437, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (292, 0.4691780821917808, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (293, 0.4709897610921502, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (294, 0.46938775510204084, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (295, 0.46779661016949153, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (296, 0.46621621621621623, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (297, 0.468013468013468, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (298, 0.4664429530201342, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (299, 0.46488294314381273, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (300, 0.4666666666666667, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (301, 0.4684385382059801, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (302, 0.47019867549668876, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (303, 0.46864686468646866, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (304, 0.47039473684210525, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (305, 0.46885245901639344, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (306, 0.4673202614379085, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (307, 0.46579804560260585, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (308, 0.4642857142857143, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (309, 0.4627831715210356, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (310, 0.4645161290322581, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (311, 0.4662379421221865, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (312, 0.46794871794871795, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (313, 0.4696485623003195, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (314, 0.4713375796178344, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (315, 0.46984126984126984, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (316, 0.47151898734177217, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (317, 0.47003154574132494, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (318, 0.46855345911949686, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (319, 0.4702194357366771, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (320, 0.46875, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (321, 0.470404984423676, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (322, 0.468944099378882, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (323, 0.47058823529411764, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (324, 0.4691358024691358, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (325, 0.4676923076923077, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (326, 0.4662576687116564, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (327, 0.4648318042813456, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (328, 0.46646341463414637, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (329, 0.46808510638297873, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (330, 0.4666666666666667, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (331, 0.46827794561933533, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (332, 0.46987951807228917, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (333, 0.47147147147147145, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (334, 0.47305389221556887, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (335, 0.4716417910447761, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (336, 0.47023809523809523, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (337, 0.4688427299703264, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (338, 0.46745562130177515, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (339, 0.4690265486725664, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (340, 0.47058823529411764, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (341, 0.47214076246334313, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (342, 0.47368421052631576, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (343, 0.47230320699708456, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (344, 0.4738372093023256, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (345, 0.47246376811594204, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (346, 0.47398843930635837, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (347, 0.4755043227665706, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (348, 0.47413793103448276, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (349, 0.47277936962750716, 0.42045454545454547)\n",
      "(d, train_metric, valid_metric) = (350, 0.4714285714285714, 0.42045454545454547)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10:1:350, Any[0.7, 0.6363636363636364, 0.5833333333333334, 0.6153846153846154, 0.6428571428571429, 0.6, 0.5625, 0.5882352941176471, 0.6111111111111112, 0.5789473684210527  …  0.47214076246334313, 0.47368421052631576, 0.47230320699708456, 0.4738372093023256, 0.47246376811594204, 0.47398843930635837, 0.4755043227665706, 0.47413793103448276, 0.47277936962750716, 0.4714285714285714], Any[0.42045454545454547, 0.42045454545454547, 0.42045454545454547, 0.42045454545454547, 0.42045454545454547, 0.42045454545454547, 0.42045454545454547, 0.42045454545454547, 0.42045454545454547, 0.42045454545454547  …  0.42045454545454547, 0.42045454545454547, 0.42045454545454547, 0.42045454545454547, 0.42045454545454547, 0.42045454545454547, 0.42045454545454547, 0.42045454545454547, 0.42045454545454547, 0.42045454545454547])"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_schedule, training_losses, valid_losses = learn_curve(best.best_model, X[train,:], y[train], acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip080\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip080)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip081\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip080)\" d=\"\n",
       "M175.024 1486.45 L2352.76 1486.45 L2352.76 47.2441 L175.024 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip082\">\n",
       "    <rect x=\"175\" y=\"47\" width=\"2179\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  176.233,1486.45 176.233,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  780.487,1486.45 780.487,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1384.74,1486.45 1384.74,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1988.99,1486.45 1988.99,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.024,1302.21 2352.76,1302.21 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.024,1059.37 2352.76,1059.37 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.024,816.519 2352.76,816.519 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.024,573.672 2352.76,573.672 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.024,330.824 2352.76,330.824 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  175.024,87.9763 2352.76,87.9763 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.024,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.024,1486.45 175.024,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  176.233,1486.45 176.233,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  780.487,1486.45 780.487,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1384.74,1486.45 1384.74,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1988.99,1486.45 1988.99,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.024,1302.21 201.157,1302.21 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.024,1059.37 201.157,1059.37 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.024,816.519 201.157,816.519 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.024,573.672 201.157,573.672 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.024,330.824 201.157,330.824 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  175.024,87.9763 201.157,87.9763 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M176.233 1508.44 Q172.621 1508.44 170.793 1512 Q168.987 1515.55 168.987 1522.67 Q168.987 1529.78 170.793 1533.35 Q172.621 1536.89 176.233 1536.89 Q179.867 1536.89 181.672 1533.35 Q183.501 1529.78 183.501 1522.67 Q183.501 1515.55 181.672 1512 Q179.867 1508.44 176.233 1508.44 M176.233 1504.73 Q182.043 1504.73 185.098 1509.34 Q188.177 1513.92 188.177 1522.67 Q188.177 1531.4 185.098 1536.01 Q182.043 1540.59 176.233 1540.59 Q170.422 1540.59 167.344 1536.01 Q164.288 1531.4 164.288 1522.67 Q164.288 1513.92 167.344 1509.34 Q170.422 1504.73 176.233 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M743.855 1535.98 L751.494 1535.98 L751.494 1509.62 L743.184 1511.29 L743.184 1507.03 L751.447 1505.36 L756.123 1505.36 L756.123 1535.98 L763.762 1535.98 L763.762 1539.92 L743.855 1539.92 L743.855 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M778.832 1508.44 Q775.221 1508.44 773.392 1512 Q771.586 1515.55 771.586 1522.67 Q771.586 1529.78 773.392 1533.35 Q775.221 1536.89 778.832 1536.89 Q782.466 1536.89 784.271 1533.35 Q786.1 1529.78 786.1 1522.67 Q786.1 1515.55 784.271 1512 Q782.466 1508.44 778.832 1508.44 M778.832 1504.73 Q784.642 1504.73 787.697 1509.34 Q790.776 1513.92 790.776 1522.67 Q790.776 1531.4 787.697 1536.01 Q784.642 1540.59 778.832 1540.59 Q773.021 1540.59 769.943 1536.01 Q766.887 1531.4 766.887 1522.67 Q766.887 1513.92 769.943 1509.34 Q773.021 1504.73 778.832 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M805.845 1508.44 Q802.234 1508.44 800.406 1512 Q798.6 1515.55 798.6 1522.67 Q798.6 1529.78 800.406 1533.35 Q802.234 1536.89 805.845 1536.89 Q809.48 1536.89 811.285 1533.35 Q813.114 1529.78 813.114 1522.67 Q813.114 1515.55 811.285 1512 Q809.48 1508.44 805.845 1508.44 M805.845 1504.73 Q811.656 1504.73 814.711 1509.34 Q817.79 1513.92 817.79 1522.67 Q817.79 1531.4 814.711 1536.01 Q811.656 1540.59 805.845 1540.59 Q800.035 1540.59 796.957 1536.01 Q793.901 1531.4 793.901 1522.67 Q793.901 1513.92 796.957 1509.34 Q800.035 1504.73 805.845 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1352.38 1535.98 L1368.7 1535.98 L1368.7 1539.92 L1346.75 1539.92 L1346.75 1535.98 Q1349.42 1533.23 1354 1528.6 Q1358.61 1523.95 1359.79 1522.61 Q1362.03 1520.08 1362.91 1518.35 Q1363.81 1516.59 1363.81 1514.9 Q1363.81 1512.14 1361.87 1510.41 Q1359.95 1508.67 1356.85 1508.67 Q1354.65 1508.67 1352.19 1509.43 Q1349.76 1510.2 1346.99 1511.75 L1346.99 1507.03 Q1349.81 1505.89 1352.26 1505.31 Q1354.72 1504.73 1356.75 1504.73 Q1362.13 1504.73 1365.32 1507.42 Q1368.51 1510.11 1368.51 1514.6 Q1368.51 1516.73 1367.7 1518.65 Q1366.92 1520.54 1364.81 1523.14 Q1364.23 1523.81 1361.13 1527.03 Q1358.03 1530.22 1352.38 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1383.77 1508.44 Q1380.16 1508.44 1378.33 1512 Q1376.52 1515.55 1376.52 1522.67 Q1376.52 1529.78 1378.33 1533.35 Q1380.16 1536.89 1383.77 1536.89 Q1387.4 1536.89 1389.21 1533.35 Q1391.04 1529.78 1391.04 1522.67 Q1391.04 1515.55 1389.21 1512 Q1387.4 1508.44 1383.77 1508.44 M1383.77 1504.73 Q1389.58 1504.73 1392.63 1509.34 Q1395.71 1513.92 1395.71 1522.67 Q1395.71 1531.4 1392.63 1536.01 Q1389.58 1540.59 1383.77 1540.59 Q1377.96 1540.59 1374.88 1536.01 Q1371.82 1531.4 1371.82 1522.67 Q1371.82 1513.92 1374.88 1509.34 Q1377.96 1504.73 1383.77 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1410.78 1508.44 Q1407.17 1508.44 1405.34 1512 Q1403.54 1515.55 1403.54 1522.67 Q1403.54 1529.78 1405.34 1533.35 Q1407.17 1536.89 1410.78 1536.89 Q1414.42 1536.89 1416.22 1533.35 Q1418.05 1529.78 1418.05 1522.67 Q1418.05 1515.55 1416.22 1512 Q1414.42 1508.44 1410.78 1508.44 M1410.78 1504.73 Q1416.59 1504.73 1419.65 1509.34 Q1422.73 1513.92 1422.73 1522.67 Q1422.73 1531.4 1419.65 1536.01 Q1416.59 1540.59 1410.78 1540.59 Q1404.97 1540.59 1401.89 1536.01 Q1398.84 1531.4 1398.84 1522.67 Q1398.84 1513.92 1401.89 1509.34 Q1404.97 1504.73 1410.78 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1966.23 1521.29 Q1969.59 1522 1971.46 1524.27 Q1973.36 1526.54 1973.36 1529.87 Q1973.36 1534.99 1969.84 1537.79 Q1966.32 1540.59 1959.84 1540.59 Q1957.66 1540.59 1955.35 1540.15 Q1953.06 1539.73 1950.6 1538.88 L1950.6 1534.36 Q1952.55 1535.5 1954.86 1536.08 Q1957.18 1536.66 1959.7 1536.66 Q1964.1 1536.66 1966.39 1534.92 Q1968.71 1533.18 1968.71 1529.87 Q1968.71 1526.82 1966.55 1525.11 Q1964.42 1523.37 1960.6 1523.37 L1956.58 1523.37 L1956.58 1519.53 L1960.79 1519.53 Q1964.24 1519.53 1966.07 1518.16 Q1967.9 1516.77 1967.9 1514.18 Q1967.9 1511.52 1966 1510.11 Q1964.12 1508.67 1960.6 1508.67 Q1958.68 1508.67 1956.48 1509.09 Q1954.28 1509.5 1951.65 1510.38 L1951.65 1506.22 Q1954.31 1505.48 1956.62 1505.11 Q1958.96 1504.73 1961.02 1504.73 Q1966.34 1504.73 1969.45 1507.17 Q1972.55 1509.57 1972.55 1513.69 Q1972.55 1516.56 1970.9 1518.55 Q1969.26 1520.52 1966.23 1521.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M1988.43 1508.44 Q1984.82 1508.44 1982.99 1512 Q1981.18 1515.55 1981.18 1522.67 Q1981.18 1529.78 1982.99 1533.35 Q1984.82 1536.89 1988.43 1536.89 Q1992.06 1536.89 1993.87 1533.35 Q1995.7 1529.78 1995.7 1522.67 Q1995.7 1515.55 1993.87 1512 Q1992.06 1508.44 1988.43 1508.44 M1988.43 1504.73 Q1994.24 1504.73 1997.29 1509.34 Q2000.37 1513.92 2000.37 1522.67 Q2000.37 1531.4 1997.29 1536.01 Q1994.24 1540.59 1988.43 1540.59 Q1982.62 1540.59 1979.54 1536.01 Q1976.48 1531.4 1976.48 1522.67 Q1976.48 1513.92 1979.54 1509.34 Q1982.62 1504.73 1988.43 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2015.44 1508.44 Q2011.83 1508.44 2010 1512 Q2008.2 1515.55 2008.2 1522.67 Q2008.2 1529.78 2010 1533.35 Q2011.83 1536.89 2015.44 1536.89 Q2019.08 1536.89 2020.88 1533.35 Q2022.71 1529.78 2022.71 1522.67 Q2022.71 1515.55 2020.88 1512 Q2019.08 1508.44 2015.44 1508.44 M2015.44 1504.73 Q2021.25 1504.73 2024.31 1509.34 Q2027.39 1513.92 2027.39 1522.67 Q2027.39 1531.4 2024.31 1536.01 Q2021.25 1540.59 2015.44 1540.59 Q2009.63 1540.59 2006.55 1536.01 Q2003.5 1531.4 2003.5 1522.67 Q2003.5 1513.92 2006.55 1509.34 Q2009.63 1504.73 2015.44 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M75.6078 1288.01 Q71.9967 1288.01 70.168 1291.58 Q68.3625 1295.12 68.3625 1302.25 Q68.3625 1309.36 70.168 1312.92 Q71.9967 1316.46 75.6078 1316.46 Q79.242 1316.46 81.0476 1312.92 Q82.8763 1309.36 82.8763 1302.25 Q82.8763 1295.12 81.0476 1291.58 Q79.242 1288.01 75.6078 1288.01 M75.6078 1284.31 Q81.418 1284.31 84.4735 1288.92 Q87.5522 1293.5 87.5522 1302.25 Q87.5522 1310.98 84.4735 1315.58 Q81.418 1320.17 75.6078 1320.17 Q69.7976 1320.17 66.719 1315.58 Q63.6634 1310.98 63.6634 1302.25 Q63.6634 1293.5 66.719 1288.92 Q69.7976 1284.31 75.6078 1284.31 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M92.6216 1313.62 L97.5058 1313.62 L97.5058 1319.49 L92.6216 1319.49 L92.6216 1313.62 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M115.422 1289.01 L103.617 1307.46 L115.422 1307.46 L115.422 1289.01 M114.196 1284.93 L120.075 1284.93 L120.075 1307.46 L125.006 1307.46 L125.006 1311.35 L120.075 1311.35 L120.075 1319.49 L115.422 1319.49 L115.422 1311.35 L99.8206 1311.35 L99.8206 1306.83 L114.196 1284.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M130.121 1284.93 L148.478 1284.93 L148.478 1288.87 L134.404 1288.87 L134.404 1297.34 Q135.422 1296.99 136.441 1296.83 Q137.459 1296.65 138.478 1296.65 Q144.265 1296.65 147.644 1299.82 Q151.024 1302.99 151.024 1308.41 Q151.024 1313.99 147.552 1317.09 Q144.08 1320.17 137.76 1320.17 Q135.584 1320.17 133.316 1319.8 Q131.07 1319.43 128.663 1318.68 L128.663 1313.99 Q130.746 1315.12 132.969 1315.68 Q135.191 1316.23 137.668 1316.23 Q141.672 1316.23 144.01 1314.12 Q146.348 1312.02 146.348 1308.41 Q146.348 1304.8 144.01 1302.69 Q141.672 1300.58 137.668 1300.58 Q135.793 1300.58 133.918 1301 Q132.066 1301.42 130.121 1302.3 L130.121 1284.93 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M76.0939 1045.17 Q72.4828 1045.17 70.6541 1048.73 Q68.8486 1052.27 68.8486 1059.4 Q68.8486 1066.51 70.6541 1070.07 Q72.4828 1073.61 76.0939 1073.61 Q79.7281 1073.61 81.5337 1070.07 Q83.3624 1066.51 83.3624 1059.4 Q83.3624 1052.27 81.5337 1048.73 Q79.7281 1045.17 76.0939 1045.17 M76.0939 1041.46 Q81.9041 1041.46 84.9596 1046.07 Q88.0383 1050.65 88.0383 1059.4 Q88.0383 1068.13 84.9596 1072.74 Q81.9041 1077.32 76.0939 1077.32 Q70.2838 1077.32 67.2051 1072.74 Q64.1495 1068.13 64.1495 1059.4 Q64.1495 1050.65 67.2051 1046.07 Q70.2838 1041.46 76.0939 1041.46 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M93.1077 1070.77 L97.9919 1070.77 L97.9919 1076.65 L93.1077 1076.65 L93.1077 1070.77 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M103.108 1042.09 L121.464 1042.09 L121.464 1046.02 L107.39 1046.02 L107.39 1054.49 Q108.409 1054.15 109.427 1053.99 Q110.446 1053.8 111.464 1053.8 Q117.251 1053.8 120.631 1056.97 Q124.01 1060.14 124.01 1065.56 Q124.01 1071.14 120.538 1074.24 Q117.066 1077.32 110.746 1077.32 Q108.571 1077.32 106.302 1076.95 Q104.057 1076.58 101.649 1075.84 L101.649 1071.14 Q103.733 1072.27 105.955 1072.83 Q108.177 1073.38 110.654 1073.38 Q114.659 1073.38 116.996 1071.28 Q119.334 1069.17 119.334 1065.56 Q119.334 1061.95 116.996 1059.84 Q114.659 1057.74 110.654 1057.74 Q108.779 1057.74 106.904 1058.15 Q105.052 1058.57 103.108 1059.45 L103.108 1042.09 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M139.08 1045.17 Q135.469 1045.17 133.64 1048.73 Q131.834 1052.27 131.834 1059.4 Q131.834 1066.51 133.64 1070.07 Q135.469 1073.61 139.08 1073.61 Q142.714 1073.61 144.519 1070.07 Q146.348 1066.51 146.348 1059.4 Q146.348 1052.27 144.519 1048.73 Q142.714 1045.17 139.08 1045.17 M139.08 1041.46 Q144.89 1041.46 147.945 1046.07 Q151.024 1050.65 151.024 1059.4 Q151.024 1068.13 147.945 1072.74 Q144.89 1077.32 139.08 1077.32 Q133.27 1077.32 130.191 1072.74 Q127.135 1068.13 127.135 1059.4 Q127.135 1050.65 130.191 1046.07 Q133.27 1041.46 139.08 1041.46 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M77.0893 802.318 Q73.4782 802.318 71.6495 805.883 Q69.8439 809.425 69.8439 816.554 Q69.8439 823.661 71.6495 827.225 Q73.4782 830.767 77.0893 830.767 Q80.7235 830.767 82.5291 827.225 Q84.3578 823.661 84.3578 816.554 Q84.3578 809.425 82.5291 805.883 Q80.7235 802.318 77.0893 802.318 M77.0893 798.614 Q82.8994 798.614 85.955 803.221 Q89.0337 807.804 89.0337 816.554 Q89.0337 825.281 85.955 829.887 Q82.8994 834.471 77.0893 834.471 Q71.2791 834.471 68.2004 829.887 Q65.1449 825.281 65.1449 816.554 Q65.1449 807.804 68.2004 803.221 Q71.2791 798.614 77.0893 798.614 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M94.1031 827.92 L98.9873 827.92 L98.9873 833.799 L94.1031 833.799 L94.1031 827.92 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M104.103 799.239 L122.459 799.239 L122.459 803.175 L108.385 803.175 L108.385 811.647 Q109.404 811.299 110.422 811.137 Q111.441 810.952 112.459 810.952 Q118.246 810.952 121.626 814.124 Q125.006 817.295 125.006 822.711 Q125.006 828.29 121.533 831.392 Q118.061 834.471 111.742 834.471 Q109.566 834.471 107.297 834.1 Q105.052 833.73 102.645 832.989 L102.645 828.29 Q104.728 829.424 106.95 829.98 Q109.172 830.536 111.649 830.536 Q115.654 830.536 117.992 828.429 Q120.33 826.323 120.33 822.711 Q120.33 819.1 117.992 816.994 Q115.654 814.887 111.649 814.887 Q109.774 814.887 107.899 815.304 Q106.047 815.721 104.103 816.6 L104.103 799.239 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M130.121 799.239 L148.478 799.239 L148.478 803.175 L134.404 803.175 L134.404 811.647 Q135.422 811.299 136.441 811.137 Q137.459 810.952 138.478 810.952 Q144.265 810.952 147.644 814.124 Q151.024 817.295 151.024 822.711 Q151.024 828.29 147.552 831.392 Q144.08 834.471 137.76 834.471 Q135.584 834.471 133.316 834.1 Q131.07 833.73 128.663 832.989 L128.663 828.29 Q130.746 829.424 132.969 829.98 Q135.191 830.536 137.668 830.536 Q141.672 830.536 144.01 828.429 Q146.348 826.323 146.348 822.711 Q146.348 819.1 144.01 816.994 Q141.672 814.887 137.668 814.887 Q135.793 814.887 133.918 815.304 Q132.066 815.721 130.121 816.6 L130.121 799.239 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M74.9365 559.47 Q71.3254 559.47 69.4967 563.035 Q67.6912 566.577 67.6912 573.706 Q67.6912 580.813 69.4967 584.378 Q71.3254 587.919 74.9365 587.919 Q78.5707 587.919 80.3763 584.378 Q82.205 580.813 82.205 573.706 Q82.205 566.577 80.3763 563.035 Q78.5707 559.47 74.9365 559.47 M74.9365 555.767 Q80.7467 555.767 83.8022 560.373 Q86.8809 564.956 86.8809 573.706 Q86.8809 582.433 83.8022 587.04 Q80.7467 591.623 74.9365 591.623 Q69.1264 591.623 66.0477 587.04 Q62.9921 582.433 62.9921 573.706 Q62.9921 564.956 66.0477 560.373 Q69.1264 555.767 74.9365 555.767 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M91.9503 585.072 L96.8345 585.072 L96.8345 590.952 L91.9503 590.952 L91.9503 585.072 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M112.483 571.808 Q109.334 571.808 107.483 573.961 Q105.654 576.114 105.654 579.864 Q105.654 583.591 107.483 585.767 Q109.334 587.919 112.483 587.919 Q115.631 587.919 117.459 585.767 Q119.311 583.591 119.311 579.864 Q119.311 576.114 117.459 573.961 Q115.631 571.808 112.483 571.808 M121.765 557.156 L121.765 561.415 Q120.006 560.581 118.2 560.142 Q116.418 559.702 114.659 559.702 Q110.029 559.702 107.575 562.827 Q105.145 565.952 104.797 572.271 Q106.163 570.257 108.223 569.193 Q110.284 568.105 112.76 568.105 Q117.969 568.105 120.978 571.276 Q124.01 574.424 124.01 579.864 Q124.01 585.188 120.862 588.405 Q117.714 591.623 112.483 591.623 Q106.487 591.623 103.316 587.04 Q100.145 582.433 100.145 573.706 Q100.145 565.512 104.034 560.651 Q107.922 555.767 114.473 555.767 Q116.233 555.767 118.015 556.114 Q119.821 556.461 121.765 557.156 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M139.08 559.47 Q135.469 559.47 133.64 563.035 Q131.834 566.577 131.834 573.706 Q131.834 580.813 133.64 584.378 Q135.469 587.919 139.08 587.919 Q142.714 587.919 144.519 584.378 Q146.348 580.813 146.348 573.706 Q146.348 566.577 144.519 563.035 Q142.714 559.47 139.08 559.47 M139.08 555.767 Q144.89 555.767 147.945 560.373 Q151.024 564.956 151.024 573.706 Q151.024 582.433 147.945 587.04 Q144.89 591.623 139.08 591.623 Q133.27 591.623 130.191 587.04 Q127.135 582.433 127.135 573.706 Q127.135 564.956 130.191 560.373 Q133.27 555.767 139.08 555.767 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M75.9319 316.623 Q72.3208 316.623 70.4921 320.187 Q68.6865 323.729 68.6865 330.859 Q68.6865 337.965 70.4921 341.53 Q72.3208 345.072 75.9319 345.072 Q79.5661 345.072 81.3717 341.53 Q83.2004 337.965 83.2004 330.859 Q83.2004 323.729 81.3717 320.187 Q79.5661 316.623 75.9319 316.623 M75.9319 312.919 Q81.742 312.919 84.7976 317.525 Q87.8763 322.109 87.8763 330.859 Q87.8763 339.586 84.7976 344.192 Q81.742 348.775 75.9319 348.775 Q70.1217 348.775 67.043 344.192 Q63.9875 339.586 63.9875 330.859 Q63.9875 322.109 67.043 317.525 Q70.1217 312.919 75.9319 312.919 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M92.9457 342.224 L97.8299 342.224 L97.8299 348.104 L92.9457 348.104 L92.9457 342.224 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M113.478 328.961 Q110.33 328.961 108.478 331.113 Q106.649 333.266 106.649 337.016 Q106.649 340.743 108.478 342.919 Q110.33 345.072 113.478 345.072 Q116.626 345.072 118.455 342.919 Q120.307 340.743 120.307 337.016 Q120.307 333.266 118.455 331.113 Q116.626 328.961 113.478 328.961 M122.76 314.308 L122.76 318.567 Q121.001 317.734 119.196 317.294 Q117.413 316.854 115.654 316.854 Q111.024 316.854 108.571 319.979 Q106.14 323.104 105.793 329.424 Q107.159 327.41 109.219 326.345 Q111.279 325.257 113.756 325.257 Q118.964 325.257 121.973 328.428 Q125.006 331.576 125.006 337.016 Q125.006 342.34 121.858 345.558 Q118.709 348.775 113.478 348.775 Q107.483 348.775 104.311 344.192 Q101.14 339.586 101.14 330.859 Q101.14 322.664 105.029 317.803 Q108.918 312.919 115.469 312.919 Q117.228 312.919 119.01 313.266 Q120.816 313.613 122.76 314.308 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M130.121 313.544 L148.478 313.544 L148.478 317.479 L134.404 317.479 L134.404 325.951 Q135.422 325.604 136.441 325.442 Q137.459 325.257 138.478 325.257 Q144.265 325.257 147.644 328.428 Q151.024 331.599 151.024 337.016 Q151.024 342.595 147.552 345.697 Q144.08 348.775 137.76 348.775 Q135.584 348.775 133.316 348.405 Q131.07 348.035 128.663 347.294 L128.663 342.595 Q130.746 343.729 132.969 344.285 Q135.191 344.84 137.668 344.84 Q141.672 344.84 144.01 342.734 Q146.348 340.627 146.348 337.016 Q146.348 333.405 144.01 331.299 Q141.672 329.192 137.668 329.192 Q135.793 329.192 133.918 329.609 Q132.066 330.025 130.121 330.905 L130.121 313.544 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M76.0013 73.775 Q72.3902 73.775 70.5615 77.3398 Q68.756 80.8814 68.756 88.011 Q68.756 95.1174 70.5615 98.6822 Q72.3902 102.224 76.0013 102.224 Q79.6356 102.224 81.4411 98.6822 Q83.2698 95.1174 83.2698 88.011 Q83.2698 80.8814 81.4411 77.3398 Q79.6356 73.775 76.0013 73.775 M76.0013 70.0713 Q81.8115 70.0713 84.867 74.6777 Q87.9457 79.261 87.9457 88.011 Q87.9457 96.7378 84.867 101.344 Q81.8115 105.928 76.0013 105.928 Q70.1912 105.928 67.1125 101.344 Q64.0569 96.7378 64.0569 88.011 Q64.0569 79.261 67.1125 74.6777 Q70.1912 70.0713 76.0013 70.0713 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M93.0151 99.3767 L97.8993 99.3767 L97.8993 105.256 L93.0151 105.256 L93.0151 99.3767 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M101.788 70.6963 L124.01 70.6963 L124.01 72.687 L111.464 105.256 L106.58 105.256 L118.385 74.6314 L101.788 74.6314 L101.788 70.6963 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M139.08 73.775 Q135.469 73.775 133.64 77.3398 Q131.834 80.8814 131.834 88.011 Q131.834 95.1174 133.64 98.6822 Q135.469 102.224 139.08 102.224 Q142.714 102.224 144.519 98.6822 Q146.348 95.1174 146.348 88.011 Q146.348 80.8814 144.519 77.3398 Q142.714 73.775 139.08 73.775 M139.08 70.0713 Q144.89 70.0713 147.945 74.6777 Q151.024 79.261 151.024 88.011 Q151.024 96.7378 147.945 101.344 Q144.89 105.928 139.08 105.928 Q133.27 105.928 130.191 101.344 Q127.135 96.7378 127.135 88.011 Q127.135 79.261 130.191 74.6777 Q133.27 70.0713 139.08 70.0713 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip082)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  236.658,87.9763 242.701,397.055 248.743,654.621 254.786,498.949 260.828,365.517 266.871,573.672 272.913,755.807 278.956,630.812 284.998,519.706 291.041,675.923 \n",
       "  297.083,816.519 303.126,943.725 309.168,1059.37 315.211,1164.95 321.254,1059.37 327.296,1156.51 333.339,1246.17 339.381,1329.2 345.424,1232.83 351.466,1310.59 \n",
       "  357.509,1221.27 363.551,1294.38 369.594,1362.93 375.636,1280.14 381.679,1202.22 387.722,1267.52 393.764,1194.28 399.807,1256.27 405.849,1187.18 411.892,1246.17 \n",
       "  417.934,1180.79 423.977,1237.06 430.019,1290.65 436.062,1228.8 442.104,1280.14 448.147,1221.27 454.189,1164.95 460.232,1214.38 466.275,1261.74 472.317,1208.05 \n",
       "  478.36,1253.65 484.402,1297.45 490.445,1339.58 496.487,1288.47 502.53,1239.25 508.572,1191.83 514.615,1146.1 520.657,1101.97 526.7,1143.11 532.742,1182.85 \n",
       "  538.785,1140.32 544.828,1178.8 550.87,1216.04 556.913,1252.1 562.955,1287.04 568.998,1320.9 575.04,1280.14 581.083,1240.6 587.125,1202.22 593.168,1164.95 \n",
       "  599.21,1128.75 605.253,1093.57 611.296,1126.82 617.338,1159.17 623.381,1125 629.423,1156.51 635.466,1123.27 641.508,1153.98 647.551,1121.64 653.593,1151.59 \n",
       "  659.636,1180.79 665.678,1209.27 671.721,1237.06 677.763,1264.18 683.806,1232.83 689.849,1259.36 695.891,1285.27 701.934,1310.59 707.976,1335.33 714.019,1304.94 \n",
       "  720.061,1275.23 726.104,1299.55 732.146,1323.33 738.189,1346.61 744.231,1317.72 750.274,1289.43 756.317,1312.33 762.359,1334.76 768.402,1307.17 774.444,1329.2 \n",
       "  780.487,1350.78 786.529,1323.85 792.572,1297.45 798.614,1318.72 804.657,1292.87 810.699,1267.52 816.742,1288.47 822.784,1263.63 828.827,1239.25 834.87,1259.88 \n",
       "  840.912,1280.14 846.955,1256.27 852.997,1276.2 859.04,1295.77 865.082,1272.39 871.125,1249.42 877.167,1226.85 883.21,1204.66 889.252,1224.01 895.295,1243.03 \n",
       "  901.338,1221.27 907.38,1240 913.423,1218.61 919.465,1197.57 925.508,1216.04 931.55,1195.36 937.593,1213.56 943.635,1193.22 949.678,1173.2 955.72,1191.14 \n",
       "  961.763,1208.81 967.805,1189.13 973.848,1169.75 979.891,1187.18 985.933,1168.1 991.976,1149.31 998.018,1166.51 1004.06,1183.45 1010.1,1164.95 1016.15,1181.66 \n",
       "  1022.19,1163.44 1028.23,1179.93 1034.27,1161.98 1040.32,1178.24 1046.36,1160.55 1052.4,1176.6 1058.44,1192.43 1064.49,1208.05 1070.53,1190.64 1076.57,1206.05 \n",
       "  1082.61,1188.89 1088.66,1204.11 1094.7,1187.18 1100.74,1170.47 1106.78,1153.98 1112.83,1169.04 1118.87,1183.9 1124.91,1198.58 1130.95,1213.07 1137,1227.37 \n",
       "  1143.04,1211.15 1149.08,1225.29 1155.12,1239.25 1161.17,1253.05 1167.21,1266.68 1173.25,1250.7 1179.29,1234.92 1185.34,1248.41 1191.38,1232.83 1197.42,1246.17 \n",
       "  1203.46,1259.36 1209.51,1243.99 1215.55,1228.8 1221.59,1213.78 1227.63,1226.85 1233.68,1212.01 1239.72,1224.95 1245.76,1210.29 1251.8,1223.08 1257.85,1235.74 \n",
       "  1263.89,1248.25 1269.93,1260.62 1275.98,1246.17 1282.02,1231.88 1288.06,1244.14 1294.1,1230.02 1300.15,1242.16 1306.19,1228.19 1312.23,1214.38 1318.27,1200.71 \n",
       "  1324.32,1212.74 1330.36,1224.66 1336.4,1211.15 1342.44,1197.78 1348.49,1209.58 1354.53,1221.27 1360.57,1208.05 1366.61,1219.62 1372.66,1231.08 1378.7,1218.01 \n",
       "  1384.74,1205.08 1390.78,1216.43 1396.83,1227.68 1402.87,1214.89 1408.91,1202.22 1414.95,1213.37 1421,1200.83 1427.04,1211.88 1433.08,1199.47 1439.12,1187.18 \n",
       "  1445.17,1175.01 1451.21,1185.97 1457.25,1173.92 1463.29,1161.98 1469.34,1172.85 1475.38,1183.61 1481.42,1171.8 1487.46,1160.09 1493.51,1170.77 1499.55,1159.17 \n",
       "  1505.59,1147.68 1511.63,1136.29 1517.68,1125 1523.72,1135.6 1529.76,1146.1 1535.8,1156.51 1541.85,1166.82 1547.89,1177.05 1553.93,1165.88 1559.97,1154.81 \n",
       "  1566.02,1143.84 1572.06,1132.96 1578.1,1143.11 1584.14,1153.17 1590.19,1163.15 1596.23,1152.37 1602.27,1141.69 1608.31,1151.59 1614.36,1161.4 1620.4,1150.82 \n",
       "  1626.44,1140.32 1632.48,1129.9 1638.53,1139.65 1644.57,1149.31 1650.61,1138.99 1656.66,1128.75 1662.7,1138.34 1668.74,1147.85 1674.78,1137.71 1680.83,1127.64 \n",
       "  1686.87,1117.65 1692.91,1127.09 1698.95,1136.46 1705,1145.76 1711.04,1154.98 1717.08,1145.08 1723.12,1154.23 1729.17,1163.31 1735.21,1172.32 1741.25,1181.26 \n",
       "  1747.29,1190.13 1753.34,1180.33 1759.38,1189.13 1765.42,1197.87 1771.46,1206.55 1777.51,1215.16 1783.55,1223.7 1789.59,1232.18 1795.63,1240.6 1801.68,1248.95 \n",
       "  1807.72,1257.24 1813.76,1247.55 1819.8,1237.93 1825.85,1246.17 1831.89,1254.35 1837.93,1262.48 1843.97,1252.94 1850.02,1243.48 1856.06,1234.08 1862.1,1242.16 \n",
       "  1868.14,1250.18 1874.19,1240.85 1880.23,1231.6 1886.27,1222.41 1892.31,1230.39 1898.36,1221.27 1904.4,1212.21 1910.44,1203.21 1916.48,1211.15 1922.53,1202.22 \n",
       "  1928.57,1210.1 1934.61,1217.93 1940.65,1209.07 1946.7,1200.27 1952.74,1208.05 1958.78,1215.78 1964.82,1223.45 1970.87,1214.72 1976.91,1222.35 1982.95,1229.93 \n",
       "  1988.99,1221.27 1995.04,1212.66 2001.08,1204.11 2007.12,1211.65 2013.17,1203.16 2019.21,1210.65 2025.25,1218.09 2031.29,1225.48 2037.34,1232.83 2043.38,1240.13 \n",
       "  2049.42,1231.71 2055.46,1223.35 2061.51,1215.04 2067.55,1206.78 2073.59,1198.58 2079.63,1205.85 2085.68,1197.7 2091.72,1204.92 2097.76,1212.1 2103.8,1204.01 \n",
       "  2109.85,1211.15 2115.89,1203.11 2121.93,1210.2 2127.97,1202.22 2134.02,1209.27 2140.06,1216.28 2146.1,1223.25 2152.14,1230.18 2158.19,1222.25 2164.23,1214.38 \n",
       "  2170.27,1221.27 2176.31,1213.44 2182.36,1205.66 2188.4,1197.93 2194.44,1190.24 2200.48,1197.1 2206.53,1203.92 2212.57,1210.7 2218.61,1217.43 2224.65,1209.8 \n",
       "  2230.7,1202.22 2236.74,1194.68 2242.78,1187.18 2248.82,1193.89 2254.87,1186.44 2260.91,1193.11 2266.95,1185.7 2272.99,1178.34 2279.04,1184.98 2285.08,1191.58 \n",
       "  2291.12,1198.14 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip082)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  236.658,1445.72 242.701,1445.72 248.743,1445.72 254.786,1445.72 260.828,1445.72 266.871,1445.72 272.913,1445.72 278.956,1445.72 284.998,1445.72 291.041,1445.72 \n",
       "  297.083,1445.72 303.126,1445.72 309.168,1445.72 315.211,1445.72 321.254,1445.72 327.296,1445.72 333.339,893.789 339.381,893.789 345.424,893.789 351.466,893.789 \n",
       "  357.509,1445.72 363.551,1445.72 369.594,893.789 375.636,1445.72 381.679,1445.72 387.722,1445.72 393.764,1445.72 399.807,1445.72 405.849,1445.72 411.892,1445.72 \n",
       "  417.934,1445.72 423.977,1445.72 430.019,1445.72 436.062,1445.72 442.104,1445.72 448.147,1445.72 454.189,1445.72 460.232,1445.72 466.275,1445.72 472.317,1445.72 \n",
       "  478.36,1445.72 484.402,1445.72 490.445,893.789 496.487,893.789 502.53,893.789 508.572,893.789 514.615,893.789 520.657,893.789 526.7,893.789 532.742,893.789 \n",
       "  538.785,893.789 544.828,893.789 550.87,893.789 556.913,893.789 562.955,893.789 568.998,893.789 575.04,893.789 581.083,893.789 587.125,893.789 593.168,893.789 \n",
       "  599.21,893.789 605.253,893.789 611.296,893.789 617.338,893.789 623.381,893.789 629.423,893.789 635.466,893.789 641.508,893.789 647.551,893.789 653.593,893.789 \n",
       "  659.636,893.789 665.678,893.789 671.721,893.789 677.763,893.789 683.806,893.789 689.849,893.789 695.891,893.789 701.934,893.789 707.976,893.789 714.019,893.789 \n",
       "  720.061,893.789 726.104,893.789 732.146,893.789 738.189,893.789 744.231,893.789 750.274,893.789 756.317,893.789 762.359,893.789 768.402,893.789 774.444,893.789 \n",
       "  780.487,893.789 786.529,1445.72 792.572,1445.72 798.614,1445.72 804.657,1445.72 810.699,1445.72 816.742,1445.72 822.784,1445.72 828.827,1445.72 834.87,1445.72 \n",
       "  840.912,1445.72 846.955,1445.72 852.997,1445.72 859.04,1445.72 865.082,1445.72 871.125,1445.72 877.167,1445.72 883.21,1445.72 889.252,1445.72 895.295,1445.72 \n",
       "  901.338,1445.72 907.38,1445.72 913.423,1445.72 919.465,1445.72 925.508,1445.72 931.55,1445.72 937.593,1445.72 943.635,1445.72 949.678,1445.72 955.72,1445.72 \n",
       "  961.763,1445.72 967.805,1445.72 973.848,1445.72 979.891,1445.72 985.933,1445.72 991.976,1445.72 998.018,1445.72 1004.06,1445.72 1010.1,1445.72 1016.15,1445.72 \n",
       "  1022.19,1445.72 1028.23,1445.72 1034.27,1445.72 1040.32,1445.72 1046.36,1445.72 1052.4,1445.72 1058.44,1445.72 1064.49,1445.72 1070.53,1445.72 1076.57,1445.72 \n",
       "  1082.61,1445.72 1088.66,1445.72 1094.7,1445.72 1100.74,1445.72 1106.78,1445.72 1112.83,1445.72 1118.87,1445.72 1124.91,1445.72 1130.95,1445.72 1137,1445.72 \n",
       "  1143.04,1445.72 1149.08,1445.72 1155.12,1445.72 1161.17,1445.72 1167.21,1445.72 1173.25,1445.72 1179.29,1445.72 1185.34,1445.72 1191.38,1445.72 1197.42,1445.72 \n",
       "  1203.46,1445.72 1209.51,1445.72 1215.55,1445.72 1221.59,1445.72 1227.63,1445.72 1233.68,1445.72 1239.72,1445.72 1245.76,1445.72 1251.8,1445.72 1257.85,1445.72 \n",
       "  1263.89,1445.72 1269.93,1445.72 1275.98,1445.72 1282.02,1445.72 1288.06,1445.72 1294.1,1445.72 1300.15,1445.72 1306.19,1445.72 1312.23,1445.72 1318.27,1445.72 \n",
       "  1324.32,1445.72 1330.36,1445.72 1336.4,1445.72 1342.44,1445.72 1348.49,1445.72 1354.53,1445.72 1360.57,1445.72 1366.61,1445.72 1372.66,1445.72 1378.7,1445.72 \n",
       "  1384.74,1445.72 1390.78,1445.72 1396.83,1445.72 1402.87,1445.72 1408.91,1445.72 1414.95,1445.72 1421,1445.72 1427.04,1445.72 1433.08,1445.72 1439.12,1445.72 \n",
       "  1445.17,1445.72 1451.21,1445.72 1457.25,1445.72 1463.29,1445.72 1469.34,1445.72 1475.38,1445.72 1481.42,1445.72 1487.46,1445.72 1493.51,1445.72 1499.55,1445.72 \n",
       "  1505.59,1445.72 1511.63,1445.72 1517.68,1445.72 1523.72,1445.72 1529.76,1445.72 1535.8,1445.72 1541.85,1445.72 1547.89,1445.72 1553.93,1445.72 1559.97,1445.72 \n",
       "  1566.02,1445.72 1572.06,1445.72 1578.1,1445.72 1584.14,1445.72 1590.19,1445.72 1596.23,1445.72 1602.27,1445.72 1608.31,1445.72 1614.36,1445.72 1620.4,1445.72 \n",
       "  1626.44,1445.72 1632.48,1445.72 1638.53,1445.72 1644.57,1445.72 1650.61,1445.72 1656.66,1445.72 1662.7,1445.72 1668.74,1445.72 1674.78,1445.72 1680.83,1445.72 \n",
       "  1686.87,1445.72 1692.91,1445.72 1698.95,1445.72 1705,1445.72 1711.04,1445.72 1717.08,1445.72 1723.12,1445.72 1729.17,1445.72 1735.21,1445.72 1741.25,1445.72 \n",
       "  1747.29,1445.72 1753.34,1445.72 1759.38,1445.72 1765.42,1445.72 1771.46,1445.72 1777.51,1445.72 1783.55,1445.72 1789.59,1445.72 1795.63,1445.72 1801.68,1445.72 \n",
       "  1807.72,1445.72 1813.76,1445.72 1819.8,1445.72 1825.85,1445.72 1831.89,1445.72 1837.93,1445.72 1843.97,1445.72 1850.02,1445.72 1856.06,1445.72 1862.1,1445.72 \n",
       "  1868.14,1445.72 1874.19,1445.72 1880.23,1445.72 1886.27,1445.72 1892.31,1445.72 1898.36,1445.72 1904.4,1445.72 1910.44,1445.72 1916.48,1445.72 1922.53,1445.72 \n",
       "  1928.57,1445.72 1934.61,1445.72 1940.65,1445.72 1946.7,1445.72 1952.74,1445.72 1958.78,1445.72 1964.82,1445.72 1970.87,1445.72 1976.91,1445.72 1982.95,1445.72 \n",
       "  1988.99,1445.72 1995.04,1445.72 2001.08,1445.72 2007.12,1445.72 2013.17,1445.72 2019.21,1445.72 2025.25,1445.72 2031.29,1445.72 2037.34,1445.72 2043.38,1445.72 \n",
       "  2049.42,1445.72 2055.46,1445.72 2061.51,1445.72 2067.55,1445.72 2073.59,1445.72 2079.63,1445.72 2085.68,1445.72 2091.72,1445.72 2097.76,1445.72 2103.8,1445.72 \n",
       "  2109.85,1445.72 2115.89,1445.72 2121.93,1445.72 2127.97,1445.72 2134.02,1445.72 2140.06,1445.72 2146.1,1445.72 2152.14,1445.72 2158.19,1445.72 2164.23,1445.72 \n",
       "  2170.27,1445.72 2176.31,1445.72 2182.36,1445.72 2188.4,1445.72 2194.44,1445.72 2200.48,1445.72 2206.53,1445.72 2212.57,1445.72 2218.61,1445.72 2224.65,1445.72 \n",
       "  2230.7,1445.72 2236.74,1445.72 2242.78,1445.72 2248.82,1445.72 2254.87,1445.72 2260.91,1445.72 2266.95,1445.72 2272.99,1445.72 2279.04,1445.72 2285.08,1445.72 \n",
       "  2291.12,1445.72 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"\n",
       "M1987.18 276.658 L2280.16 276.658 L2280.16 95.2176 L1987.18 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1987.18,276.658 2280.16,276.658 2280.16,95.2176 1987.18,95.2176 1987.18,276.658 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip080)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2011.37,155.698 2156.56,155.698 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M2194.6 175.385 Q2192.79 180.015 2191.08 181.427 Q2189.36 182.839 2186.49 182.839 L2183.09 182.839 L2183.09 179.274 L2185.59 179.274 Q2187.35 179.274 2188.32 178.44 Q2189.29 177.607 2190.47 174.505 L2191.24 172.561 L2180.75 147.052 L2185.27 147.052 L2193.37 167.329 L2201.47 147.052 L2205.98 147.052 L2194.6 175.385 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2211.86 169.042 L2219.5 169.042 L2219.5 142.677 L2211.19 144.343 L2211.19 140.084 L2219.46 138.418 L2224.13 138.418 L2224.13 169.042 L2231.77 169.042 L2231.77 172.978 L2211.86 172.978 L2211.86 169.042 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip080)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2011.37,216.178 2156.56,216.178 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip080)\" d=\"M 0 0 M2194.6 235.865 Q2192.79 240.495 2191.08 241.907 Q2189.36 243.319 2186.49 243.319 L2183.09 243.319 L2183.09 239.754 L2185.59 239.754 Q2187.35 239.754 2188.32 238.92 Q2189.29 238.087 2190.47 234.985 L2191.24 233.041 L2180.75 207.532 L2185.27 207.532 L2193.37 227.809 L2201.47 207.532 L2205.98 207.532 L2194.6 235.865 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip080)\" d=\"M 0 0 M2215.08 229.522 L2231.4 229.522 L2231.4 233.458 L2209.46 233.458 L2209.46 229.522 Q2212.12 226.768 2216.7 222.138 Q2221.31 217.485 2222.49 216.143 Q2224.73 213.62 2225.61 211.884 Q2226.52 210.124 2226.52 208.435 Q2226.52 205.68 2224.57 203.944 Q2222.65 202.208 2219.55 202.208 Q2217.35 202.208 2214.9 202.972 Q2212.47 203.735 2209.69 205.286 L2209.69 200.564 Q2212.51 199.43 2214.97 198.851 Q2217.42 198.273 2219.46 198.273 Q2224.83 198.273 2228.02 200.958 Q2231.22 203.643 2231.22 208.134 Q2231.22 210.263 2230.41 212.185 Q2229.62 214.083 2227.51 216.675 Q2226.93 217.347 2223.83 220.564 Q2220.73 223.759 2215.08 229.522 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(data_schedule, training_losses)\n",
    "plot!(data_schedule, valid_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(\n",
       "    max_depth = -1,\n",
       "    min_samples_leaf = 1,\n",
       "    min_samples_split = 2,\n",
       "    min_purity_increase = 0.0,\n",
       "    n_subfeatures = 0,\n",
       "    post_prune = true,\n",
       "    merge_purity_threshold = 0.2828282828282828,\n",
       "    pdf_smoothing = 0.0,\n",
       "    display_depth = 5)\u001b[34m @336\u001b[39m"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dt = best.best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @640\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @307\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @576\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Final_Tree = machine(final_dt, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 : 202/438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{DecisionTreeClassifier} @640\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/uKzAz/src/machines.jl:319\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{DecisionTreeClassifier} @640\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @307\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @576\u001b[39m ⏎ `AbstractArray{Multiclass{3},1}`\n"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(Final_Tree, rows=train, verbosity=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ2 = MLJ.predict(Final_Tree, X[test,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9168849434125446"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(ŷ2, y[test]) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45989304812834225"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(ŷ2, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(mode.(ŷ), y[test])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
