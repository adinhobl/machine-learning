{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "using DataFrames\n",
    "using CSV\n",
    "using MLJ\n",
    "using Plots\n",
    "using StatsBase\n",
    "\n",
    "include(\"../../lib.jl\")\n",
    "\n",
    "ENV[\"LINES\"]=30;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th> Mean of the integrated profile</th><th> Standard deviation of the integrated profile</th></tr><tr><th></th><th>Float64</th><th>Float64</th></tr></thead><tbody><p>17,898 rows × 9 columns (omitted printing of 7 columns)</p><tr><th>1</th><td>140.562</td><td>55.6838</td></tr><tr><th>2</th><td>102.508</td><td>58.8824</td></tr><tr><th>3</th><td>103.016</td><td>39.3416</td></tr><tr><th>4</th><td>136.75</td><td>57.1784</td></tr><tr><th>5</th><td>88.7266</td><td>40.6722</td></tr><tr><th>6</th><td>93.5703</td><td>46.6981</td></tr><tr><th>7</th><td>119.484</td><td>48.7651</td></tr><tr><th>8</th><td>130.383</td><td>39.8441</td></tr><tr><th>9</th><td>107.25</td><td>52.6271</td></tr><tr><th>10</th><td>107.258</td><td>39.4965</td></tr><tr><th>11</th><td>142.078</td><td>45.2881</td></tr><tr><th>12</th><td>133.258</td><td>44.0582</td></tr><tr><th>13</th><td>134.961</td><td>49.5543</td></tr><tr><th>14</th><td>117.945</td><td>45.5066</td></tr><tr><th>15</th><td>138.18</td><td>51.5245</td></tr><tr><th>16</th><td>114.367</td><td>51.9457</td></tr><tr><th>17</th><td>109.641</td><td>49.0177</td></tr><tr><th>18</th><td>100.852</td><td>51.7435</td></tr><tr><th>19</th><td>136.094</td><td>51.691</td></tr><tr><th>20</th><td>99.3672</td><td>41.5722</td></tr><tr><th>21</th><td>100.891</td><td>51.8904</td></tr><tr><th>22</th><td>105.445</td><td>41.14</td></tr><tr><th>23</th><td>95.8672</td><td>42.0599</td></tr><tr><th>24</th><td>117.367</td><td>53.9086</td></tr><tr><th>25</th><td>106.648</td><td>56.3672</td></tr><tr><th>26</th><td>112.719</td><td>50.3013</td></tr><tr><th>27</th><td>130.852</td><td>52.4329</td></tr><tr><th>28</th><td>119.438</td><td>52.8748</td></tr><tr><th>29</th><td>123.211</td><td>51.078</td></tr><tr><th>30</th><td>102.617</td><td>49.6924</td></tr><tr><th>&vellip;</th><td>&vellip;</td><td>&vellip;</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ccc}\n",
       "\t&  Mean of the integrated profile &  Standard deviation of the integrated profile & \\\\\n",
       "\t\\hline\n",
       "\t& Float64 & Float64 & \\\\\n",
       "\t\\hline\n",
       "\t1 & 140.562 & 55.6838 & $\\dots$ \\\\\n",
       "\t2 & 102.508 & 58.8824 & $\\dots$ \\\\\n",
       "\t3 & 103.016 & 39.3416 & $\\dots$ \\\\\n",
       "\t4 & 136.75 & 57.1784 & $\\dots$ \\\\\n",
       "\t5 & 88.7266 & 40.6722 & $\\dots$ \\\\\n",
       "\t6 & 93.5703 & 46.6981 & $\\dots$ \\\\\n",
       "\t7 & 119.484 & 48.7651 & $\\dots$ \\\\\n",
       "\t8 & 130.383 & 39.8441 & $\\dots$ \\\\\n",
       "\t9 & 107.25 & 52.6271 & $\\dots$ \\\\\n",
       "\t10 & 107.258 & 39.4965 & $\\dots$ \\\\\n",
       "\t11 & 142.078 & 45.2881 & $\\dots$ \\\\\n",
       "\t12 & 133.258 & 44.0582 & $\\dots$ \\\\\n",
       "\t13 & 134.961 & 49.5543 & $\\dots$ \\\\\n",
       "\t14 & 117.945 & 45.5066 & $\\dots$ \\\\\n",
       "\t15 & 138.18 & 51.5245 & $\\dots$ \\\\\n",
       "\t16 & 114.367 & 51.9457 & $\\dots$ \\\\\n",
       "\t17 & 109.641 & 49.0177 & $\\dots$ \\\\\n",
       "\t18 & 100.852 & 51.7435 & $\\dots$ \\\\\n",
       "\t19 & 136.094 & 51.691 & $\\dots$ \\\\\n",
       "\t20 & 99.3672 & 41.5722 & $\\dots$ \\\\\n",
       "\t21 & 100.891 & 51.8904 & $\\dots$ \\\\\n",
       "\t22 & 105.445 & 41.14 & $\\dots$ \\\\\n",
       "\t23 & 95.8672 & 42.0599 & $\\dots$ \\\\\n",
       "\t24 & 117.367 & 53.9086 & $\\dots$ \\\\\n",
       "\t25 & 106.648 & 56.3672 & $\\dots$ \\\\\n",
       "\t26 & 112.719 & 50.3013 & $\\dots$ \\\\\n",
       "\t27 & 130.852 & 52.4329 & $\\dots$ \\\\\n",
       "\t28 & 119.438 & 52.8748 & $\\dots$ \\\\\n",
       "\t29 & 123.211 & 51.078 & $\\dots$ \\\\\n",
       "\t30 & 102.617 & 49.6924 & $\\dots$ \\\\\n",
       "\t$\\dots$ & $\\dots$ & $\\dots$ &  \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "17898×9 DataFrame. Omitted printing of 8 columns\n",
       "│ Row   │  Mean of the integrated profile │\n",
       "│       │ \u001b[90mFloat64\u001b[39m                         │\n",
       "├───────┼─────────────────────────────────┤\n",
       "│ 1     │ 140.562                         │\n",
       "│ 2     │ 102.508                         │\n",
       "│ 3     │ 103.016                         │\n",
       "│ 4     │ 136.75                          │\n",
       "│ 5     │ 88.7266                         │\n",
       "│ 6     │ 93.5703                         │\n",
       "│ 7     │ 119.484                         │\n",
       "│ 8     │ 130.383                         │\n",
       "│ 9     │ 107.25                          │\n",
       "│ 10    │ 107.258                         │\n",
       "⋮\n",
       "│ 17888 │ 121.375                         │\n",
       "│ 17889 │ 98.7266                         │\n",
       "│ 17890 │ 126.625                         │\n",
       "│ 17891 │ 143.672                         │\n",
       "│ 17892 │ 118.484                         │\n",
       "│ 17893 │ 96.0                            │\n",
       "│ 17894 │ 136.43                          │\n",
       "│ 17895 │ 122.555                         │\n",
       "│ 17896 │ 119.336                         │\n",
       "│ 17897 │ 114.508                         │\n",
       "│ 17898 │ 57.0625                         │"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = CSV.read(\"data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at class labels to see if dataset is imbalanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Int64,Int64} with 2 entries:\n",
       "  0 => 16259\n",
       "  1 => 1639"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = countmap(data[:target_class])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.908425522404738\n",
       " 0.09157447759526204"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collect(label_counts[i] / size(data)[1] for i in keys(label_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get data ready for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"data-frame\"><thead><tr><th></th><th>variable</th><th>mean</th><th>min</th><th>median</th><th>max</th></tr><tr><th></th><th>Symbol</th><th>Float64</th><th>Real</th><th>Float64</th><th>Real</th></tr></thead><tbody><p>9 rows × 8 columns (omitted printing of 3 columns)</p><tr><th>1</th><td> Mean of the integrated profile</td><td>111.08</td><td>5.8125</td><td>115.078</td><td>192.617</td></tr><tr><th>2</th><td> Standard deviation of the integrated profile</td><td>46.5495</td><td>24.772</td><td>46.9475</td><td>98.7789</td></tr><tr><th>3</th><td> Excess kurtosis of the integrated profile</td><td>0.477857</td><td>-1.87601</td><td>0.22324</td><td>8.06952</td></tr><tr><th>4</th><td> Skewness of the integrated profile</td><td>1.77028</td><td>-1.79189</td><td>0.19871</td><td>68.1016</td></tr><tr><th>5</th><td> Mean of the DM-SNR curve</td><td>12.6144</td><td>0.213211</td><td>2.80184</td><td>223.392</td></tr><tr><th>6</th><td> Standard deviation of the DM-SNR curve</td><td>26.3265</td><td>7.37043</td><td>18.4613</td><td>110.642</td></tr><tr><th>7</th><td> Excess kurtosis of the DM-SNR curve</td><td>8.30356</td><td>-3.13927</td><td>8.43351</td><td>34.5398</td></tr><tr><th>8</th><td> Skewness of the DM-SNR curve</td><td>104.858</td><td>-1.97698</td><td>83.0646</td><td>1191.0</td></tr><tr><th>9</th><td>target_class</td><td>0.0915745</td><td>0</td><td>0.0</td><td>1</td></tr></tbody></table>"
      ],
      "text/latex": [
       "\\begin{tabular}{r|cccccc}\n",
       "\t& variable & mean & min & median & max & \\\\\n",
       "\t\\hline\n",
       "\t& Symbol & Float64 & Real & Float64 & Real & \\\\\n",
       "\t\\hline\n",
       "\t1 &  Mean of the integrated profile & 111.08 & 5.8125 & 115.078 & 192.617 & $\\dots$ \\\\\n",
       "\t2 &  Standard deviation of the integrated profile & 46.5495 & 24.772 & 46.9475 & 98.7789 & $\\dots$ \\\\\n",
       "\t3 &  Excess kurtosis of the integrated profile & 0.477857 & -1.87601 & 0.22324 & 8.06952 & $\\dots$ \\\\\n",
       "\t4 &  Skewness of the integrated profile & 1.77028 & -1.79189 & 0.19871 & 68.1016 & $\\dots$ \\\\\n",
       "\t5 &  Mean of the DM-SNR curve & 12.6144 & 0.213211 & 2.80184 & 223.392 & $\\dots$ \\\\\n",
       "\t6 &  Standard deviation of the DM-SNR curve & 26.3265 & 7.37043 & 18.4613 & 110.642 & $\\dots$ \\\\\n",
       "\t7 &  Excess kurtosis of the DM-SNR curve & 8.30356 & -3.13927 & 8.43351 & 34.5398 & $\\dots$ \\\\\n",
       "\t8 &  Skewness of the DM-SNR curve & 104.858 & -1.97698 & 83.0646 & 1191.0 & $\\dots$ \\\\\n",
       "\t9 & target\\_class & 0.0915745 & 0 & 0.0 & 1 & $\\dots$ \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/plain": [
       "9×8 DataFrame. Omitted printing of 5 columns\n",
       "│ Row │ variable                                      │ mean      │ min      │\n",
       "│     │ \u001b[90mSymbol\u001b[39m                                        │ \u001b[90mFloat64\u001b[39m   │ \u001b[90mReal\u001b[39m     │\n",
       "├─────┼───────────────────────────────────────────────┼───────────┼──────────┤\n",
       "│ 1   │  Mean of the integrated profile               │ 111.08    │ 5.8125   │\n",
       "│ 2   │  Standard deviation of the integrated profile │ 46.5495   │ 24.772   │\n",
       "│ 3   │  Excess kurtosis of the integrated profile    │ 0.477857  │ -1.87601 │\n",
       "│ 4   │  Skewness of the integrated profile           │ 1.77028   │ -1.79189 │\n",
       "│ 5   │  Mean of the DM-SNR curve                     │ 12.6144   │ 0.213211 │\n",
       "│ 6   │  Standard deviation of the DM-SNR curve       │ 26.3265   │ 7.37043  │\n",
       "│ 7   │  Excess kurtosis of the DM-SNR curve          │ 8.30356   │ -3.13927 │\n",
       "│ 8   │  Skewness of the DM-SNR curve                 │ 104.858   │ -1.97698 │\n",
       "│ 9   │ target_class                                  │ 0.0915745 │ 0        │"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m───────────────────────────────────────────────\u001b[0m┬\u001b[0m────────────────────────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.names                                       \u001b[0m│\u001b[0m\u001b[22m _.types                        \u001b[0m│\u001b[0m\u001b[22m _.scitypes    \u001b[0m│\u001b[0m\n",
       "├\u001b[0m───────────────────────────────────────────────\u001b[0m┼\u001b[0m────────────────────────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m  Mean of the integrated profile               \u001b[0m│\u001b[0m Float64                        \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m  Standard deviation of the integrated profile \u001b[0m│\u001b[0m Float64                        \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m  Excess kurtosis of the integrated profile    \u001b[0m│\u001b[0m Float64                        \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m  Skewness of the integrated profile           \u001b[0m│\u001b[0m Float64                        \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m  Mean of the DM-SNR curve                     \u001b[0m│\u001b[0m Float64                        \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m  Standard deviation of the DM-SNR curve       \u001b[0m│\u001b[0m Float64                        \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m  Excess kurtosis of the DM-SNR curve          \u001b[0m│\u001b[0m Float64                        \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m  Skewness of the DM-SNR curve                 \u001b[0m│\u001b[0m Float64                        \u001b[0m│\u001b[0m Continuous    \u001b[0m│\u001b[0m\n",
       "│\u001b[0m target_class                                  \u001b[0m│\u001b[0m CategoricalValue{Int64,UInt32} \u001b[0m│\u001b[0m Multiclass{2} \u001b[0m│\u001b[0m\n",
       "└\u001b[0m───────────────────────────────────────────────\u001b[0m┴\u001b[0m────────────────────────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┘\u001b[0m\n",
       "_.nrows = 17898\n"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coerce!(data, :target_class=>Multiclass)\n",
    "schema(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CategoricalValue{Int64,UInt32}[0, 0, 0, 0, 0, 0, 0, 0, 0, 0  …  0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 17898×8 DataFrame. Omitted printing of 7 columns\n",
       "│ Row   │  Mean of the integrated profile │\n",
       "│       │ \u001b[90mFloat64\u001b[39m                         │\n",
       "├───────┼─────────────────────────────────┤\n",
       "│ 1     │ 140.562                         │\n",
       "│ 2     │ 102.508                         │\n",
       "│ 3     │ 103.016                         │\n",
       "│ 4     │ 136.75                          │\n",
       "│ 5     │ 88.7266                         │\n",
       "│ 6     │ 93.5703                         │\n",
       "│ 7     │ 119.484                         │\n",
       "│ 8     │ 130.383                         │\n",
       "│ 9     │ 107.25                          │\n",
       "│ 10    │ 107.258                         │\n",
       "⋮\n",
       "│ 17888 │ 121.375                         │\n",
       "│ 17889 │ 98.7266                         │\n",
       "│ 17890 │ 126.625                         │\n",
       "│ 17891 │ 143.672                         │\n",
       "│ 17892 │ 118.484                         │\n",
       "│ 17893 │ 96.0                            │\n",
       "│ 17894 │ 136.43                          │\n",
       "│ 17895 │ 122.555                         │\n",
       "│ 17896 │ 119.336                         │\n",
       "│ 17897 │ 114.508                         │\n",
       "│ 17898 │ 57.0625                         │)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, X = unpack(data, ==(:target_class), colname->true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition train and test data accoring to class labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([2859, 3242, 436, 12919, 3279, 17651, 16341, 14975, 16060, 730  …  805, 13608, 9896, 11042, 13883, 3467, 112, 9612, 2792, 261], [1078, 16815, 4553, 12452, 10187, 17806, 8156, 16807, 2688, 1554  …  4126, 15656, 16102, 15077, 13659, 13276, 13241, 8204, 5417, 415])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data to use when trying to fit a single validation set\n",
    "train, test = partition(eachindex(y), 0.7, shuffle=true, rng=123, stratify=values(data[:target_class])) # gives 70:30 split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.9084450830140486\n",
       " 0.09155491698595147"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_counts = countmap(data[train,:target_class])\n",
    "collect(train_counts[i] / size(train)[1] for i in keys(train_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Array{Float64,1}:\n",
       " 0.9083798882681564\n",
       " 0.09162011173184358"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_counts = countmap(data[test,:target_class])\n",
    "collect(test_counts[i] / size(test)[1] for i in keys(test_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Five Learning Algorithms\n",
    "\n",
    "* Decision trees with some form of pruning\n",
    "* Neural networks\n",
    "* Boosting\n",
    "* Support Vector Machines\n",
    "* k-nearest neighbors\n",
    "\n",
    "\n",
    "##### Testing\n",
    "* Implement the algorithms\n",
    "* Design two *interesting* classification problems. For the purposes of this assignment, a classification problem is just a set of training examples and a set of test examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43-element Array{NamedTuple{(:name, :package_name, :is_supervised, :docstring, :hyperparameter_ranges, :hyperparameter_types, :hyperparameters, :implemented_methods, :is_pure_julia, :is_wrapper, :load_path, :package_license, :package_url, :package_uuid, :prediction_type, :supports_online, :supports_weights, :input_scitype, :target_scitype, :output_scitype),T} where T<:Tuple,1}:\n",
       " (name = AdaBoostClassifier, package_name = ScikitLearn, ... )\n",
       " (name = AdaBoostStumpClassifier, package_name = DecisionTree, ... )\n",
       " (name = BaggingClassifier, package_name = ScikitLearn, ... )\n",
       " (name = BayesianLDA, package_name = MultivariateStats, ... )\n",
       " (name = BayesianLDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianQDA, package_name = ScikitLearn, ... )\n",
       " (name = BayesianSubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = ConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DecisionTreeClassifier, package_name = DecisionTree, ... )\n",
       " (name = DeterministicConstantClassifier, package_name = MLJModels, ... )\n",
       " (name = DummyClassifier, package_name = ScikitLearn, ... )\n",
       " (name = EvoTreeClassifier, package_name = EvoTrees, ... )\n",
       " (name = ExtraTreesClassifier, package_name = ScikitLearn, ... )\n",
       " ⋮\n",
       " (name = ProbabilisticSGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RandomForestClassifier, package_name = DecisionTree, ... )\n",
       " (name = RandomForestClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeCVClassifier, package_name = ScikitLearn, ... )\n",
       " (name = RidgeClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SGDClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVC, package_name = LIBSVM, ... )\n",
       " (name = SVMClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMLinearClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SVMNuClassifier, package_name = ScikitLearn, ... )\n",
       " (name = SubspaceLDA, package_name = MultivariateStats, ... )\n",
       " (name = XGBoostClassifier, package_name = XGBoost, ... )"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models(matching(X,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 5,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Distances.Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = :uniform)\u001b[34m @061\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@load KNNClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbors\n",
    "* Use different values of k.\n",
    "\n",
    "1. https://alan-turing-institute.github.io/MLJ.jl/dev/composing_models/\n",
    "1. https://github.com/KristofferC/NearestNeighbors.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### No Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 5,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Distances.Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = :uniform)\u001b[34m @825\u001b[39m"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNNClassifier(K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{KNNClassifier} @645\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @259\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @953\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN = machine(knn, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{KNNClassifier} @645\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/machines.jl:322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{KNNClassifier} @645\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @259\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @953\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(KNN, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:04\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m───────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┬\u001b[0m───────────────────────────────────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.measure     \u001b[0m│\u001b[0m\u001b[22m _.measurement \u001b[0m│\u001b[0m\u001b[22m _.per_fold                                \u001b[0m│\u001b[0m\n",
       "├\u001b[0m───────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┼\u001b[0m───────────────────────────────────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m cross_entropy \u001b[0m│\u001b[0m 0.487         \u001b[0m│\u001b[0m [0.431, 0.411, 0.556, 0.59, 0.484, 0.452] \u001b[0m│\u001b[0m\n",
       "│\u001b[0m acc           \u001b[0m│\u001b[0m 0.972         \u001b[0m│\u001b[0m [0.969, 0.975, 0.972, 0.97, 0.973, 0.973] \u001b[0m│\u001b[0m\n",
       "└\u001b[0m───────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┴\u001b[0m───────────────────────────────────────────\u001b[0m┘\u001b[0m\n",
       "_.per_observation = [[[2.22e-16, 2.22e-16, ..., 0.223], [2.22e-16, 2.22e-16, ..., 2.22e-16], [0.511, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 0.223], [2.22e-16, 2.22e-16, ..., 0.223], [2.22e-16, 2.22e-16, ..., 36.0]], missing]\n"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_acc = evaluate!(KNN, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tree = NearestNeighbors.KDTree{StaticArrays.SArray{Tuple{8},Float64,1,8},Distances.Euclidean,Float64}\n",
       "  Number of points: 14915\n",
       "  Dimensions: 8\n",
       "  Metric: Distances.Euclidean(0.0)\n",
       "  Reordered: true,)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_params(KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 5,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Distances.Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = :uniform)\u001b[34m @106\u001b[39m"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNNClassifier(K=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{Standardizer} @878\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/machines.jl:322\n"
     ]
    }
   ],
   "source": [
    "standardizer = Standardizer()\n",
    "stand = machine(standardizer, X[train,:])\n",
    "fit!(stand)\n",
    "X_stand = MLJ.transform(stand, X);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{KNNClassifier} @819\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @942\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @697\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN = machine(knn, X_stand, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{KNNClassifier} @819\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/machines.jl:322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{KNNClassifier} @819\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @942\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @697\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(KNN, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33mEvaluating over 6 folds: 100%[=========================] Time: 0:00:00\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "┌\u001b[0m───────────────\u001b[0m┬\u001b[0m───────────────\u001b[0m┬\u001b[0m────────────────────────────────────────────\u001b[0m┐\u001b[0m\n",
       "│\u001b[0m\u001b[22m _.measure     \u001b[0m│\u001b[0m\u001b[22m _.measurement \u001b[0m│\u001b[0m\u001b[22m _.per_fold                                 \u001b[0m│\u001b[0m\n",
       "├\u001b[0m───────────────\u001b[0m┼\u001b[0m───────────────\u001b[0m┼\u001b[0m────────────────────────────────────────────\u001b[0m┤\u001b[0m\n",
       "│\u001b[0m cross_entropy \u001b[0m│\u001b[0m 0.393         \u001b[0m│\u001b[0m [0.485, 0.355, 0.401, 0.381, 0.282, 0.452] \u001b[0m│\u001b[0m\n",
       "│\u001b[0m acc           \u001b[0m│\u001b[0m 0.979         \u001b[0m│\u001b[0m [0.977, 0.981, 0.979, 0.979, 0.982, 0.978] \u001b[0m│\u001b[0m\n",
       "└\u001b[0m───────────────\u001b[0m┴\u001b[0m───────────────\u001b[0m┴\u001b[0m────────────────────────────────────────────\u001b[0m┘\u001b[0m\n",
       "_.per_observation = [[[2.22e-16, 0.223, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 0.223, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16], [2.22e-16, 2.22e-16, ..., 2.22e-16]], missing]\n"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_acc = evaluate!(KNN, resampling=CV(shuffle=true), measure=[cross_entropy, acc], verbosity=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate!(KNN, resampling=CV(shuffle=true), measure=[tnr,tpr,fnr,fpr], verbosity=1, operation=predict_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tree = NearestNeighbors.KDTree{StaticArrays.SArray{Tuple{8},Float64,1,8},Distances.Euclidean,Float64}\n",
       "  Number of points: 14915\n",
       "  Dimensions: 8\n",
       "  Metric: Distances.Euclidean(0.0)\n",
       "  Reordered: true,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitted_params(KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GridSearch / RandomSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 5,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Distances.Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = :uniform)\u001b[34m @287\u001b[39m"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_grid = KNNClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJBase.NumericRange(Int64, :K, ... )"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param1 = :K\n",
    "\n",
    "r1 = range(knn_grid, param1, lower=1, upper=9, scale=:linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "    model = KNNClassifier(\n",
       "            K = 5,\n",
       "            algorithm = :kdtree,\n",
       "            metric = Distances.Euclidean(0.0),\n",
       "            leafsize = 10,\n",
       "            reorder = true,\n",
       "            weights = :uniform),\n",
       "    tuning = Grid(\n",
       "            goal = 100,\n",
       "            resolution = 10,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = CV(\n",
       "            nfolds = 6,\n",
       "            shuffle = false,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = cross_entropy(\n",
       "            eps = 2.220446049250313e-16),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = MLJBase.NumericRange{Int64,MLJBase.Bounded,Symbol}[\u001b[34mNumericRange{Int64,…} @748\u001b[39m],\n",
       "    train_best = true,\n",
       "    repeats = 1,\n",
       "    n = nothing,\n",
       "    acceleration = CPUThreads{Int64}(1),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true)\u001b[34m @076\u001b[39m"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_knn_model = TunedModel(model=knn_grid,\n",
    "                                    tuning=Grid(goal=100),\n",
    "                                    resampling=CV(), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @436\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @052\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @526\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_knn = machine(self_tuning_knn_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @436\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/machines.jl:322\n",
      "┌ Info: Attempting to evaluate 9 models.\n",
      "└ @ MLJTuning /home/andrew/.julia/packages/MLJTuning/nuvTc/src/tuned_models.jl:501\n",
      "\u001b[33mEvaluating over 9 metamodels: 100%[=========================] Time: 0:00:01\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @436\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @052\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @526\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = fit!(self_tuning_knn, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(best_model = \u001b[34mKNNClassifier @233\u001b[39m,\n",
       " best_fitted_params = (tree = NearestNeighbors.KDTree{StaticArrays.SArray{Tuple{8},Float64,1,8},Distances.Euclidean,Float64}\n",
       "  Number of points: 12528\n",
       "  Dimensions: 8\n",
       "  Metric: Distances.Euclidean(0.0)\n",
       "  Reordered: true,),)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = fitted_params(self_tuning_knn)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 9,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Distances.Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = :uniform)\u001b[34m @233\u001b[39m"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manhattan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: Chebyshev not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: Chebyshev not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope at In[59]:1",
      " [2] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "knn_grid = KNNClassifier(metric=Chebyshev())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLJBase.NumericRange(Int64, :K, ... )"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param1 = :K\n",
    "\n",
    "r1 = range(knn_grid, param1, lower=1, upper=9, scale=:linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ProbabilisticTunedModel(\n",
       "    model = KNNClassifier(\n",
       "            K = 5,\n",
       "            algorithm = :kdtree,\n",
       "            metric = Distances.Euclidean(0.0),\n",
       "            leafsize = 10,\n",
       "            reorder = true,\n",
       "            weights = :uniform),\n",
       "    tuning = Grid(\n",
       "            goal = 100,\n",
       "            resolution = 10,\n",
       "            shuffle = true,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    resampling = CV(\n",
       "            nfolds = 6,\n",
       "            shuffle = false,\n",
       "            rng = Random._GLOBAL_RNG()),\n",
       "    measure = cross_entropy(\n",
       "            eps = 2.220446049250313e-16),\n",
       "    weights = nothing,\n",
       "    operation = MLJModelInterface.predict,\n",
       "    range = MLJBase.NumericRange{Int64,MLJBase.Bounded,Symbol}[\u001b[34mNumericRange{Int64,…} @748\u001b[39m],\n",
       "    train_best = true,\n",
       "    repeats = 1,\n",
       "    n = nothing,\n",
       "    acceleration = CPUThreads{Int64}(1),\n",
       "    acceleration_resampling = CPU1{Nothing}(nothing),\n",
       "    check_measure = true)\u001b[34m @186\u001b[39m"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_knn_model = TunedModel(model=knn_grid,\n",
    "                                    tuning=Grid(goal=100),\n",
    "                                    resampling=CV(), \n",
    "                                    measure=cross_entropy,\n",
    "                                    acceleration=CPUThreads(),\n",
    "                                    range=[r1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @286\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @728\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @734\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "self_tuning_knn = machine(self_tuning_knn_model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @286\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/machines.jl:322\n",
      "┌ Info: Attempting to evaluate 9 models.\n",
      "└ @ MLJTuning /home/andrew/.julia/packages/MLJTuning/nuvTc/src/tuned_models.jl:501\n",
      "\u001b[33mEvaluating over 9 metamodels: 100%[=========================] Time: 0:00:01\u001b[39m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{ProbabilisticTunedModel{Grid,…}} @286\u001b[39m trained 1 time.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @728\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @734\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = fit!(self_tuning_knn, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(best_model = \u001b[34mKNNClassifier @641\u001b[39m,\n",
       " best_fitted_params = (tree = NearestNeighbors.KDTree{StaticArrays.SArray{Tuple{8},Float64,1,8},Distances.Euclidean,Float64}\n",
       "  Number of points: 12528\n",
       "  Dimensions: 8\n",
       "  Metric: Distances.Euclidean(0.0)\n",
       "  Reordered: true,),)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best = fitted_params(self_tuning_knn)\n",
    "best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 9,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Distances.Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = :uniform)\u001b[34m @641\u001b[39m"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best.best_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (10, 0.8, 0.9130087789305666)\n",
      "(d, train_metric, valid_metric) = (60, 0.9666666666666667, 0.9349561053471668)\n",
      "(d, train_metric, valid_metric) = (110, 0.9818181818181818, 0.9588986432561851)\n",
      "(d, train_metric, valid_metric) = (160, 0.98125, 0.9692737430167597)\n",
      "(d, train_metric, valid_metric) = (210, 0.9809523809523809, 0.9696727853152434)\n",
      "(d, train_metric, valid_metric) = (260, 0.9769230769230769, 0.9672785315243416)\n",
      "(d, train_metric, valid_metric) = (310, 0.9709677419354839, 0.9676775738228253)\n",
      "(d, train_metric, valid_metric) = (360, 0.975, 0.9672785315243416)\n",
      "(d, train_metric, valid_metric) = (410, 0.9658536585365853, 0.9688747007182761)\n",
      "(d, train_metric, valid_metric) = (460, 0.9652173913043478, 0.9664804469273743)\n",
      "(d, train_metric, valid_metric) = (510, 0.9627450980392157, 0.9708699122106943)\n",
      "(d, train_metric, valid_metric) = (560, 0.9660714285714286, 0.9688747007182761)\n",
      "(d, train_metric, valid_metric) = (610, 0.9672131147540983, 0.9684756584197926)\n",
      "(d, train_metric, valid_metric) = (660, 0.9666666666666667, 0.9684756584197926)\n",
      "(d, train_metric, valid_metric) = (710, 0.9690140845070423, 0.966879489225858)\n",
      "(d, train_metric, valid_metric) = (760, 0.9644736842105263, 0.9664804469273743)\n",
      "(d, train_metric, valid_metric) = (810, 0.9641975308641976, 0.9664804469273743)\n",
      "(d, train_metric, valid_metric) = (860, 0.9651162790697675, 0.9672785315243416)\n",
      "(d, train_metric, valid_metric) = (910, 0.9637362637362638, 0.9652833200319234)\n",
      "(d, train_metric, valid_metric) = (960, 0.9645833333333333, 0.9660814046288907)\n",
      "(d, train_metric, valid_metric) = (1010, 0.9633663366336633, 0.966879489225858)\n",
      "(d, train_metric, valid_metric) = (1060, 0.9650943396226415, 0.9680766161213089)\n",
      "(d, train_metric, valid_metric) = (1110, 0.9666666666666667, 0.9688747007182761)\n",
      "(d, train_metric, valid_metric) = (1160, 0.968103448275862, 0.9692737430167597)\n",
      "(d, train_metric, valid_metric) = (1210, 0.9669421487603306, 0.9704708699122107)\n",
      "(d, train_metric, valid_metric) = (1260, 0.9682539682539683, 0.9704708699122107)\n",
      "(d, train_metric, valid_metric) = (1310, 0.9687022900763359, 0.9704708699122107)\n",
      "(d, train_metric, valid_metric) = (1360, 0.9691176470588235, 0.9704708699122107)\n",
      "(d, train_metric, valid_metric) = (1410, 0.9680851063829787, 0.9704708699122107)\n",
      "(d, train_metric, valid_metric) = (1460, 0.9678082191780822, 0.9704708699122107)\n",
      "(d, train_metric, valid_metric) = (1510, 0.9682119205298013, 0.970071827613727)\n",
      "(d, train_metric, valid_metric) = (1560, 0.9692307692307692, 0.970071827613727)\n",
      "(d, train_metric, valid_metric) = (1610, 0.9695652173913043, 0.970071827613727)\n",
      "(d, train_metric, valid_metric) = (1660, 0.9686746987951808, 0.9708699122106943)\n",
      "(d, train_metric, valid_metric) = (1710, 0.9690058479532164, 0.9704708699122107)\n",
      "(d, train_metric, valid_metric) = (1760, 0.9698863636363636, 0.9704708699122107)\n",
      "(d, train_metric, valid_metric) = (1810, 0.9707182320441989, 0.970071827613727)\n",
      "(d, train_metric, valid_metric) = (1860, 0.9709677419354839, 0.970071827613727)\n",
      "(d, train_metric, valid_metric) = (1910, 0.9712041884816754, 0.970071827613727)\n",
      "(d, train_metric, valid_metric) = (1960, 0.9714285714285714, 0.970071827613727)\n",
      "(d, train_metric, valid_metric) = (2010, 0.9706467661691542, 0.9716679968076616)\n",
      "(d, train_metric, valid_metric) = (2060, 0.970873786407767, 0.9716679968076616)\n",
      "(d, train_metric, valid_metric) = (2110, 0.9715639810426541, 0.9720670391061452)\n",
      "(d, train_metric, valid_metric) = (2160, 0.9717592592592592, 0.9728651237031125)\n",
      "(d, train_metric, valid_metric) = (2210, 0.971945701357466, 0.9728651237031125)\n",
      "(d, train_metric, valid_metric) = (2260, 0.9712389380530974, 0.9728651237031125)\n",
      "(d, train_metric, valid_metric) = (2310, 0.9714285714285714, 0.9732641660015962)\n",
      "(d, train_metric, valid_metric) = (2360, 0.9720338983050848, 0.9728651237031125)\n",
      "(d, train_metric, valid_metric) = (2410, 0.9726141078838174, 0.9724660814046289)\n",
      "(d, train_metric, valid_metric) = (2460, 0.973170731707317, 0.9720670391061452)\n",
      "(d, train_metric, valid_metric) = (2510, 0.9733067729083665, 0.9724660814046289)\n",
      "(d, train_metric, valid_metric) = (2560, 0.9734375, 0.9724660814046289)\n",
      "(d, train_metric, valid_metric) = (2610, 0.9739463601532568, 0.9720670391061452)\n",
      "(d, train_metric, valid_metric) = (2660, 0.9744360902255639, 0.9720670391061452)\n",
      "(d, train_metric, valid_metric) = (2710, 0.9730627306273063, 0.9720670391061452)\n",
      "(d, train_metric, valid_metric) = (2760, 0.9735507246376811, 0.9720670391061452)\n",
      "(d, train_metric, valid_metric) = (2810, 0.9736654804270463, 0.9724660814046289)\n",
      "(d, train_metric, valid_metric) = (2860, 0.9741258741258741, 0.9724660814046289)\n",
      "(d, train_metric, valid_metric) = (2910, 0.9745704467353952, 0.9732641660015962)\n",
      "(d, train_metric, valid_metric) = (2960, 0.9753378378378378, 0.9724660814046289)\n",
      "(d, train_metric, valid_metric) = (3010, 0.9764119601328903, 0.9728651237031125)\n",
      "(d, train_metric, valid_metric) = (3060, 0.9761437908496732, 0.9724660814046289)\n",
      "(d, train_metric, valid_metric) = (3110, 0.9758842443729904, 0.9732641660015962)\n",
      "(d, train_metric, valid_metric) = (3160, 0.975632911392405, 0.9732641660015962)\n",
      "(d, train_metric, valid_metric) = (3210, 0.9757009345794393, 0.9732641660015962)\n",
      "(d, train_metric, valid_metric) = (3260, 0.9754601226993865, 0.9732641660015962)\n",
      "(d, train_metric, valid_metric) = (3310, 0.9755287009063444, 0.9728651237031125)\n",
      "(d, train_metric, valid_metric) = (3360, 0.975297619047619, 0.9728651237031125)\n",
      "(d, train_metric, valid_metric) = (3410, 0.9750733137829912, 0.9728651237031125)\n",
      "(d, train_metric, valid_metric) = (3460, 0.9751445086705203, 0.9724660814046289)\n",
      "(d, train_metric, valid_metric) = (3510, 0.9749287749287749, 0.9724660814046289)\n",
      "(d, train_metric, valid_metric) = (3560, 0.9747191011235955, 0.9724660814046289)\n",
      "(d, train_metric, valid_metric) = (3610, 0.9745152354570638, 0.9728651237031125)\n",
      "(d, train_metric, valid_metric) = (3660, 0.9743169398907103, 0.9736632083000798)\n",
      "(d, train_metric, valid_metric) = (3710, 0.9743935309973046, 0.9736632083000798)\n",
      "(d, train_metric, valid_metric) = (3760, 0.9744680851063829, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (3810, 0.9748031496062992, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (3860, 0.9748704663212435, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (3910, 0.9749360613810741, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (3960, 0.975, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (4010, 0.9753117206982543, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (4060, 0.975615763546798, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (4110, 0.975425790754258, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (4160, 0.9754807692307692, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (4210, 0.9750593824228029, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (4260, 0.9753521126760564, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (4310, 0.9754060324825986, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (4360, 0.9752293577981651, 0.9732641660015962)\n",
      "(d, train_metric, valid_metric) = (4410, 0.97437641723356, 0.9736632083000798)\n",
      "(d, train_metric, valid_metric) = (4460, 0.9742152466367713, 0.9736632083000798)\n",
      "(d, train_metric, valid_metric) = (4510, 0.9740576496674057, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (4560, 0.9739035087719298, 0.9736632083000798)\n",
      "(d, train_metric, valid_metric) = (4610, 0.9741865509761388, 0.9736632083000798)\n",
      "(d, train_metric, valid_metric) = (4660, 0.9744635193133048, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (4710, 0.9749469214437367, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (4760, 0.9754201680672269, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (4810, 0.9756756756756757, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (4860, 0.9753086419753086, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (4910, 0.9745417515274949, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (4960, 0.9745967741935484, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (5010, 0.9746506986027944, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (5060, 0.974703557312253, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (5110, 0.9749510763209394, 0.9740622505985634)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(d, train_metric, valid_metric) = (5160, 0.9748062015503876, 0.9732641660015962)\n",
      "(d, train_metric, valid_metric) = (5210, 0.9744721689059501, 0.9732641660015962)\n",
      "(d, train_metric, valid_metric) = (5260, 0.9739543726235741, 0.9732641660015962)\n",
      "(d, train_metric, valid_metric) = (5310, 0.9740112994350283, 0.9732641660015962)\n",
      "(d, train_metric, valid_metric) = (5360, 0.9740671641791044, 0.9728651237031125)\n",
      "(d, train_metric, valid_metric) = (5410, 0.9739371534195933, 0.9732641660015962)\n",
      "(d, train_metric, valid_metric) = (5460, 0.973992673992674, 0.9732641660015962)\n",
      "(d, train_metric, valid_metric) = (5510, 0.9738656987295826, 0.9732641660015962)\n",
      "(d, train_metric, valid_metric) = (5560, 0.9739208633093526, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (5610, 0.9737967914438502, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (5660, 0.9740282685512367, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (5710, 0.9740805604203152, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (5760, 0.9741319444444444, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (5810, 0.9738382099827882, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (5860, 0.9737201365187713, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (5910, 0.9736040609137055, 0.9760574620909817)\n",
      "(d, train_metric, valid_metric) = (5960, 0.9736577181208054, 0.975658419792498)\n",
      "(d, train_metric, valid_metric) = (6010, 0.9738768718801997, 0.975658419792498)\n",
      "(d, train_metric, valid_metric) = (6060, 0.973927392739274, 0.975658419792498)\n",
      "(d, train_metric, valid_metric) = (6110, 0.9739770867430442, 0.975658419792498)\n",
      "(d, train_metric, valid_metric) = (6160, 0.9741883116883117, 0.975658419792498)\n",
      "(d, train_metric, valid_metric) = (6210, 0.9739130434782609, 0.975658419792498)\n",
      "(d, train_metric, valid_metric) = (6260, 0.9742811501597444, 0.975658419792498)\n",
      "(d, train_metric, valid_metric) = (6310, 0.9741679873217116, 0.975658419792498)\n",
      "(d, train_metric, valid_metric) = (6360, 0.9738993710691823, 0.975658419792498)\n",
      "(d, train_metric, valid_metric) = (6410, 0.974414976599064, 0.9752593774940144)\n",
      "(d, train_metric, valid_metric) = (6460, 0.9744582043343654, 0.9752593774940144)\n",
      "(d, train_metric, valid_metric) = (6510, 0.9745007680491552, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (6560, 0.9742378048780488, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (6610, 0.9744326777609682, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (6660, 0.9746246246246246, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (6710, 0.9743666169895678, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (6760, 0.9744082840236686, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (6810, 0.9741556534508077, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (6860, 0.9743440233236151, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (6910, 0.9738060781476121, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (6960, 0.9738505747126437, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7010, 0.974037089871612, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7060, 0.9739376770538244, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7110, 0.9741209563994374, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7160, 0.9738826815642458, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7210, 0.9740638002773925, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7260, 0.9742424242424242, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7310, 0.9738714090287278, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7360, 0.9740489130434783, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7410, 0.9739541160593792, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7460, 0.9741286863270777, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7510, 0.9743009320905459, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7560, 0.9744708994708995, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7610, 0.9745072273324573, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7660, 0.9745430809399478, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7710, 0.9747081712062257, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (7760, 0.9751288659793814, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (7810, 0.9750320102432779, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (7860, 0.9748091603053435, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (7910, 0.974968394437421, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (7960, 0.975, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (8010, 0.9747815230961299, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (8060, 0.9746898263027295, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (8110, 0.9745992601726264, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (8160, 0.9745098039215686, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (8210, 0.974299634591961, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (8260, 0.9742130750605327, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (8310, 0.9740072202166065, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (8360, 0.9740430622009569, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (8410, 0.9739595719381688, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (8460, 0.974113475177305, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (8510, 0.9736780258519389, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (8560, 0.9738317757009346, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (8610, 0.9737514518002323, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (8660, 0.9737875288683603, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (8710, 0.9735935706084959, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (8760, 0.9736301369863014, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (8810, 0.9737797956867197, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (8860, 0.9739277652370203, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (8910, 0.9737373737373738, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (8960, 0.9738839285714286, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (9010, 0.9739178690344062, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (9060, 0.9739514348785872, 0.9744612928970471)\n",
      "(d, train_metric, valid_metric) = (9110, 0.973874862788145, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (9160, 0.9737991266375546, 0.9752593774940144)\n",
      "(d, train_metric, valid_metric) = (9210, 0.9738327904451684, 0.9752593774940144)\n",
      "(d, train_metric, valid_metric) = (9260, 0.9739740820734342, 0.9752593774940144)\n",
      "(d, train_metric, valid_metric) = (9310, 0.9737916219119227, 0.9752593774940144)\n",
      "(d, train_metric, valid_metric) = (9360, 0.9738247863247863, 0.9752593774940144)\n",
      "(d, train_metric, valid_metric) = (9410, 0.9737513283740702, 0.9748603351955307)\n",
      "(d, train_metric, valid_metric) = (9460, 0.9734672304439747, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (9510, 0.9735015772870662, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (9560, 0.973744769874477, 0.9736632083000798)\n",
      "(d, train_metric, valid_metric) = (9610, 0.9737773152965661, 0.9736632083000798)\n",
      "(d, train_metric, valid_metric) = (9660, 0.9739130434782609, 0.9736632083000798)\n",
      "(d, train_metric, valid_metric) = (9710, 0.9741503604531411, 0.9736632083000798)\n",
      "(d, train_metric, valid_metric) = (9760, 0.9744877049180328, 0.9736632083000798)\n",
      "(d, train_metric, valid_metric) = (9810, 0.9747196738022426, 0.9736632083000798)\n",
      "(d, train_metric, valid_metric) = (9860, 0.9746450304259635, 0.9736632083000798)\n",
      "(d, train_metric, valid_metric) = (9910, 0.974066599394551, 0.9736632083000798)\n",
      "(d, train_metric, valid_metric) = (9960, 0.9739959839357429, 0.9740622505985634)\n",
      "(d, train_metric, valid_metric) = (10010, 0.9741258741258741, 0.9744612928970471)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10:50:10010, Any[0.8, 0.9666666666666667, 0.9818181818181818, 0.98125, 0.9809523809523809, 0.9769230769230769, 0.9709677419354839, 0.975, 0.9658536585365853, 0.9652173913043478  …  0.973744769874477, 0.9737773152965661, 0.9739130434782609, 0.9741503604531411, 0.9744877049180328, 0.9747196738022426, 0.9746450304259635, 0.974066599394551, 0.9739959839357429, 0.9741258741258741], Any[0.9130087789305666, 0.9349561053471668, 0.9588986432561851, 0.9692737430167597, 0.9696727853152434, 0.9672785315243416, 0.9676775738228253, 0.9672785315243416, 0.9688747007182761, 0.9664804469273743  …  0.9736632083000798, 0.9736632083000798, 0.9736632083000798, 0.9736632083000798, 0.9736632083000798, 0.9736632083000798, 0.9736632083000798, 0.9736632083000798, 0.9740622505985634, 0.9744612928970471])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_schedule, training_losses, valid_losses = learn_curve(best.best_model, X[train,:], y[train], acc, step=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n",
       "<defs>\n",
       "  <clipPath id=\"clip720\">\n",
       "    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip720)\" d=\"\n",
       "M0 1600 L2400 1600 L2400 0 L0 0  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip721\">\n",
       "    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<path clip-path=\"url(#clip720)\" d=\"\n",
       "M174.769 1486.45 L2352.76 1486.45 L2352.76 47.2441 L174.769 47.2441  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<defs>\n",
       "  <clipPath id=\"clip722\">\n",
       "    <rect x=\"174\" y=\"47\" width=\"2179\" height=\"1440\"/>\n",
       "  </clipPath>\n",
       "</defs>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  234.356,1486.45 234.356,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  748.032,1486.45 748.032,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1261.71,1486.45 1261.71,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  1775.38,1486.45 1775.38,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  2289.06,1486.45 2289.06,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.769,1445.72 2352.76,1445.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.769,1072.34 2352.76,1072.34 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.769,698.959 2352.76,698.959 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#000000; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n",
       "  174.769,325.581 2352.76,325.581 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.769,1486.45 2352.76,1486.45 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.769,1486.45 174.769,47.2441 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  234.356,1486.45 234.356,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  748.032,1486.45 748.032,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1261.71,1486.45 1261.71,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1775.38,1486.45 1775.38,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2289.06,1486.45 2289.06,1469.18 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.769,1445.72 200.905,1445.72 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.769,1072.34 200.905,1072.34 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.769,698.959 200.905,698.959 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  174.769,325.581 200.905,325.581 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip720)\" d=\"M 0 0 M234.356 1508.44 Q230.745 1508.44 228.916 1512 Q227.111 1515.55 227.111 1522.67 Q227.111 1529.78 228.916 1533.35 Q230.745 1536.89 234.356 1536.89 Q237.99 1536.89 239.796 1533.35 Q241.624 1529.78 241.624 1522.67 Q241.624 1515.55 239.796 1512 Q237.99 1508.44 234.356 1508.44 M234.356 1504.73 Q240.166 1504.73 243.222 1509.34 Q246.3 1513.92 246.3 1522.67 Q246.3 1531.4 243.222 1536.01 Q240.166 1540.59 234.356 1540.59 Q228.546 1540.59 225.467 1536.01 Q222.411 1531.4 222.411 1522.67 Q222.411 1513.92 225.467 1509.34 Q228.546 1504.73 234.356 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M702.662 1535.98 L718.981 1535.98 L718.981 1539.92 L697.037 1539.92 L697.037 1535.98 Q699.699 1533.23 704.282 1528.6 Q708.889 1523.95 710.069 1522.61 Q712.315 1520.08 713.194 1518.35 Q714.097 1516.59 714.097 1514.9 Q714.097 1512.14 712.152 1510.41 Q710.231 1508.67 707.129 1508.67 Q704.93 1508.67 702.477 1509.43 Q700.046 1510.2 697.268 1511.75 L697.268 1507.03 Q700.092 1505.89 702.546 1505.31 Q705 1504.73 707.037 1504.73 Q712.407 1504.73 715.602 1507.42 Q718.796 1510.11 718.796 1514.6 Q718.796 1516.73 717.986 1518.65 Q717.199 1520.54 715.092 1523.14 Q714.514 1523.81 711.412 1527.03 Q708.31 1530.22 702.662 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M724.097 1505.36 L742.453 1505.36 L742.453 1509.3 L728.379 1509.3 L728.379 1517.77 Q729.398 1517.42 730.416 1517.26 Q731.435 1517.07 732.453 1517.07 Q738.24 1517.07 741.62 1520.24 Q745 1523.42 745 1528.83 Q745 1534.41 741.527 1537.51 Q738.055 1540.59 731.736 1540.59 Q729.56 1540.59 727.291 1540.22 Q725.046 1539.85 722.639 1539.11 L722.639 1534.41 Q724.722 1535.54 726.944 1536.1 Q729.166 1536.66 731.643 1536.66 Q735.648 1536.66 737.986 1534.55 Q740.324 1532.44 740.324 1528.83 Q740.324 1525.22 737.986 1523.11 Q735.648 1521.01 731.643 1521.01 Q729.768 1521.01 727.893 1521.42 Q726.041 1521.84 724.097 1522.72 L724.097 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M760.069 1508.44 Q756.458 1508.44 754.629 1512 Q752.824 1515.55 752.824 1522.67 Q752.824 1529.78 754.629 1533.35 Q756.458 1536.89 760.069 1536.89 Q763.703 1536.89 765.509 1533.35 Q767.337 1529.78 767.337 1522.67 Q767.337 1515.55 765.509 1512 Q763.703 1508.44 760.069 1508.44 M760.069 1504.73 Q765.879 1504.73 768.935 1509.34 Q772.013 1513.92 772.013 1522.67 Q772.013 1531.4 768.935 1536.01 Q765.879 1540.59 760.069 1540.59 Q754.259 1540.59 751.18 1536.01 Q748.125 1531.4 748.125 1522.67 Q748.125 1513.92 751.18 1509.34 Q754.259 1504.73 760.069 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M787.083 1508.44 Q783.472 1508.44 781.643 1512 Q779.837 1515.55 779.837 1522.67 Q779.837 1529.78 781.643 1533.35 Q783.472 1536.89 787.083 1536.89 Q790.717 1536.89 792.522 1533.35 Q794.351 1529.78 794.351 1522.67 Q794.351 1515.55 792.522 1512 Q790.717 1508.44 787.083 1508.44 M787.083 1504.73 Q792.893 1504.73 795.948 1509.34 Q799.027 1513.92 799.027 1522.67 Q799.027 1531.4 795.948 1536.01 Q792.893 1540.59 787.083 1540.59 Q781.272 1540.59 778.194 1536.01 Q775.138 1531.4 775.138 1522.67 Q775.138 1513.92 778.194 1509.34 Q781.272 1504.73 787.083 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1211.47 1505.36 L1229.82 1505.36 L1229.82 1509.3 L1215.75 1509.3 L1215.75 1517.77 Q1216.77 1517.42 1217.78 1517.26 Q1218.8 1517.07 1219.82 1517.07 Q1225.61 1517.07 1228.99 1520.24 Q1232.37 1523.42 1232.37 1528.83 Q1232.37 1534.41 1228.9 1537.51 Q1225.42 1540.59 1219.1 1540.59 Q1216.93 1540.59 1214.66 1540.22 Q1212.41 1539.85 1210.01 1539.11 L1210.01 1534.41 Q1212.09 1535.54 1214.31 1536.1 Q1216.53 1536.66 1219.01 1536.66 Q1223.02 1536.66 1225.35 1534.55 Q1227.69 1532.44 1227.69 1528.83 Q1227.69 1525.22 1225.35 1523.11 Q1223.02 1521.01 1219.01 1521.01 Q1217.14 1521.01 1215.26 1521.42 Q1213.41 1521.84 1211.47 1522.72 L1211.47 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1247.44 1508.44 Q1243.83 1508.44 1242 1512 Q1240.19 1515.55 1240.19 1522.67 Q1240.19 1529.78 1242 1533.35 Q1243.83 1536.89 1247.44 1536.89 Q1251.07 1536.89 1252.88 1533.35 Q1254.71 1529.78 1254.71 1522.67 Q1254.71 1515.55 1252.88 1512 Q1251.07 1508.44 1247.44 1508.44 M1247.44 1504.73 Q1253.25 1504.73 1256.3 1509.34 Q1259.38 1513.92 1259.38 1522.67 Q1259.38 1531.4 1256.3 1536.01 Q1253.25 1540.59 1247.44 1540.59 Q1241.63 1540.59 1238.55 1536.01 Q1235.49 1531.4 1235.49 1522.67 Q1235.49 1513.92 1238.55 1509.34 Q1241.63 1504.73 1247.44 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1274.45 1508.44 Q1270.84 1508.44 1269.01 1512 Q1267.21 1515.55 1267.21 1522.67 Q1267.21 1529.78 1269.01 1533.35 Q1270.84 1536.89 1274.45 1536.89 Q1278.09 1536.89 1279.89 1533.35 Q1281.72 1529.78 1281.72 1522.67 Q1281.72 1515.55 1279.89 1512 Q1278.09 1508.44 1274.45 1508.44 M1274.45 1504.73 Q1280.26 1504.73 1283.32 1509.34 Q1286.4 1513.92 1286.4 1522.67 Q1286.4 1531.4 1283.32 1536.01 Q1280.26 1540.59 1274.45 1540.59 Q1268.64 1540.59 1265.56 1536.01 Q1262.51 1531.4 1262.51 1522.67 Q1262.51 1513.92 1265.56 1509.34 Q1268.64 1504.73 1274.45 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1301.46 1508.44 Q1297.85 1508.44 1296.02 1512 Q1294.22 1515.55 1294.22 1522.67 Q1294.22 1529.78 1296.02 1533.35 Q1297.85 1536.89 1301.46 1536.89 Q1305.1 1536.89 1306.9 1533.35 Q1308.73 1529.78 1308.73 1522.67 Q1308.73 1515.55 1306.9 1512 Q1305.1 1508.44 1301.46 1508.44 M1301.46 1504.73 Q1307.27 1504.73 1310.33 1509.34 Q1313.41 1513.92 1313.41 1522.67 Q1313.41 1531.4 1310.33 1536.01 Q1307.27 1540.59 1301.46 1540.59 Q1295.65 1540.59 1292.58 1536.01 Q1289.52 1531.4 1289.52 1522.67 Q1289.52 1513.92 1292.58 1509.34 Q1295.65 1504.73 1301.46 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1724.25 1505.36 L1746.47 1505.36 L1746.47 1507.35 L1733.93 1539.92 L1729.04 1539.92 L1740.85 1509.3 L1724.25 1509.3 L1724.25 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1751.59 1505.36 L1769.94 1505.36 L1769.94 1509.3 L1755.87 1509.3 L1755.87 1517.77 Q1756.89 1517.42 1757.91 1517.26 Q1758.93 1517.07 1759.94 1517.07 Q1765.73 1517.07 1769.11 1520.24 Q1772.49 1523.42 1772.49 1528.83 Q1772.49 1534.41 1769.02 1537.51 Q1765.55 1540.59 1759.23 1540.59 Q1757.05 1540.59 1754.78 1540.22 Q1752.54 1539.85 1750.13 1539.11 L1750.13 1534.41 Q1752.21 1535.54 1754.44 1536.1 Q1756.66 1536.66 1759.13 1536.66 Q1763.14 1536.66 1765.48 1534.55 Q1767.81 1532.44 1767.81 1528.83 Q1767.81 1525.22 1765.48 1523.11 Q1763.14 1521.01 1759.13 1521.01 Q1757.26 1521.01 1755.38 1521.42 Q1753.53 1521.84 1751.59 1522.72 L1751.59 1505.36 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1787.56 1508.44 Q1783.95 1508.44 1782.12 1512 Q1780.31 1515.55 1780.31 1522.67 Q1780.31 1529.78 1782.12 1533.35 Q1783.95 1536.89 1787.56 1536.89 Q1791.19 1536.89 1793 1533.35 Q1794.83 1529.78 1794.83 1522.67 Q1794.83 1515.55 1793 1512 Q1791.19 1508.44 1787.56 1508.44 M1787.56 1504.73 Q1793.37 1504.73 1796.43 1509.34 Q1799.5 1513.92 1799.5 1522.67 Q1799.5 1531.4 1796.43 1536.01 Q1793.37 1540.59 1787.56 1540.59 Q1781.75 1540.59 1778.67 1536.01 Q1775.62 1531.4 1775.62 1522.67 Q1775.62 1513.92 1778.67 1509.34 Q1781.75 1504.73 1787.56 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M1814.57 1508.44 Q1810.96 1508.44 1809.13 1512 Q1807.33 1515.55 1807.33 1522.67 Q1807.33 1529.78 1809.13 1533.35 Q1810.96 1536.89 1814.57 1536.89 Q1818.21 1536.89 1820.01 1533.35 Q1821.84 1529.78 1821.84 1522.67 Q1821.84 1515.55 1820.01 1512 Q1818.21 1508.44 1814.57 1508.44 M1814.57 1504.73 Q1820.38 1504.73 1823.44 1509.34 Q1826.52 1513.92 1826.52 1522.67 Q1826.52 1531.4 1823.44 1536.01 Q1820.38 1540.59 1814.57 1540.59 Q1808.76 1540.59 1805.68 1536.01 Q1802.63 1531.4 1802.63 1522.67 Q1802.63 1513.92 1805.68 1509.34 Q1808.76 1504.73 1814.57 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M2225.41 1535.98 L2233.05 1535.98 L2233.05 1509.62 L2224.74 1511.29 L2224.74 1507.03 L2233.01 1505.36 L2237.68 1505.36 L2237.68 1535.98 L2245.32 1535.98 L2245.32 1539.92 L2225.41 1539.92 L2225.41 1535.98 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M2260.39 1508.44 Q2256.78 1508.44 2254.95 1512 Q2253.15 1515.55 2253.15 1522.67 Q2253.15 1529.78 2254.95 1533.35 Q2256.78 1536.89 2260.39 1536.89 Q2264.03 1536.89 2265.83 1533.35 Q2267.66 1529.78 2267.66 1522.67 Q2267.66 1515.55 2265.83 1512 Q2264.03 1508.44 2260.39 1508.44 M2260.39 1504.73 Q2266.2 1504.73 2269.26 1509.34 Q2272.34 1513.92 2272.34 1522.67 Q2272.34 1531.4 2269.26 1536.01 Q2266.2 1540.59 2260.39 1540.59 Q2254.58 1540.59 2251.5 1536.01 Q2248.45 1531.4 2248.45 1522.67 Q2248.45 1513.92 2251.5 1509.34 Q2254.58 1504.73 2260.39 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M2287.4 1508.44 Q2283.79 1508.44 2281.97 1512 Q2280.16 1515.55 2280.16 1522.67 Q2280.16 1529.78 2281.97 1533.35 Q2283.79 1536.89 2287.4 1536.89 Q2291.04 1536.89 2292.84 1533.35 Q2294.67 1529.78 2294.67 1522.67 Q2294.67 1515.55 2292.84 1512 Q2291.04 1508.44 2287.4 1508.44 M2287.4 1504.73 Q2293.22 1504.73 2296.27 1509.34 Q2299.35 1513.92 2299.35 1522.67 Q2299.35 1531.4 2296.27 1536.01 Q2293.22 1540.59 2287.4 1540.59 Q2281.59 1540.59 2278.52 1536.01 Q2275.46 1531.4 2275.46 1522.67 Q2275.46 1513.92 2278.52 1509.34 Q2281.59 1504.73 2287.4 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M2314.42 1508.44 Q2310.81 1508.44 2308.98 1512 Q2307.17 1515.55 2307.17 1522.67 Q2307.17 1529.78 2308.98 1533.35 Q2310.81 1536.89 2314.42 1536.89 Q2318.05 1536.89 2319.86 1533.35 Q2321.69 1529.78 2321.69 1522.67 Q2321.69 1515.55 2319.86 1512 Q2318.05 1508.44 2314.42 1508.44 M2314.42 1504.73 Q2320.23 1504.73 2323.28 1509.34 Q2326.36 1513.92 2326.36 1522.67 Q2326.36 1531.4 2323.28 1536.01 Q2320.23 1540.59 2314.42 1540.59 Q2308.61 1540.59 2305.53 1536.01 Q2302.47 1531.4 2302.47 1522.67 Q2302.47 1513.92 2305.53 1509.34 Q2308.61 1504.73 2314.42 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M2341.43 1508.44 Q2337.82 1508.44 2335.99 1512 Q2334.19 1515.55 2334.19 1522.67 Q2334.19 1529.78 2335.99 1533.35 Q2337.82 1536.89 2341.43 1536.89 Q2345.07 1536.89 2346.87 1533.35 Q2348.7 1529.78 2348.7 1522.67 Q2348.7 1515.55 2346.87 1512 Q2345.07 1508.44 2341.43 1508.44 M2341.43 1504.73 Q2347.24 1504.73 2350.3 1509.34 Q2353.38 1513.92 2353.38 1522.67 Q2353.38 1531.4 2350.3 1536.01 Q2347.24 1540.59 2341.43 1540.59 Q2335.62 1540.59 2332.54 1536.01 Q2329.49 1531.4 2329.49 1522.67 Q2329.49 1513.92 2332.54 1509.34 Q2335.62 1504.73 2341.43 1504.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M74.9365 1431.51 Q71.3254 1431.51 69.4967 1435.08 Q67.6912 1438.62 67.6912 1445.75 Q67.6912 1452.86 69.4967 1456.42 Q71.3254 1459.96 74.9365 1459.96 Q78.5707 1459.96 80.3763 1456.42 Q82.205 1452.86 82.205 1445.75 Q82.205 1438.62 80.3763 1435.08 Q78.5707 1431.51 74.9365 1431.51 M74.9365 1427.81 Q80.7467 1427.81 83.8022 1432.42 Q86.8809 1437 86.8809 1445.75 Q86.8809 1454.48 83.8022 1459.08 Q80.7467 1463.67 74.9365 1463.67 Q69.1264 1463.67 66.0477 1459.08 Q62.9921 1454.48 62.9921 1445.75 Q62.9921 1437 66.0477 1432.42 Q69.1264 1427.81 74.9365 1427.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M91.9503 1457.12 L96.8345 1457.12 L96.8345 1463 L91.9503 1463 L91.9503 1457.12 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M111.904 1446.58 Q108.571 1446.58 106.649 1448.37 Q104.751 1450.15 104.751 1453.27 Q104.751 1456.4 106.649 1458.18 Q108.571 1459.96 111.904 1459.96 Q115.237 1459.96 117.159 1458.18 Q119.08 1456.38 119.08 1453.27 Q119.08 1450.15 117.159 1448.37 Q115.26 1446.58 111.904 1446.58 M107.228 1444.59 Q104.219 1443.85 102.529 1441.79 Q100.862 1439.73 100.862 1436.77 Q100.862 1432.63 103.802 1430.22 Q106.765 1427.81 111.904 1427.81 Q117.066 1427.81 120.006 1430.22 Q122.946 1432.63 122.946 1436.77 Q122.946 1439.73 121.256 1441.79 Q119.589 1443.85 116.603 1444.59 Q119.983 1445.38 121.858 1447.67 Q123.756 1449.96 123.756 1453.27 Q123.756 1458.3 120.677 1460.98 Q117.621 1463.67 111.904 1463.67 Q106.186 1463.67 103.108 1460.98 Q100.052 1458.3 100.052 1453.27 Q100.052 1449.96 101.95 1447.67 Q103.848 1445.38 107.228 1444.59 M105.515 1437.21 Q105.515 1439.89 107.182 1441.4 Q108.872 1442.9 111.904 1442.9 Q114.913 1442.9 116.603 1441.4 Q118.316 1439.89 118.316 1437.21 Q118.316 1434.52 116.603 1433.02 Q114.913 1431.51 111.904 1431.51 Q108.872 1431.51 107.182 1433.02 Q105.515 1434.52 105.515 1437.21 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M138.825 1431.51 Q135.214 1431.51 133.385 1435.08 Q131.58 1438.62 131.58 1445.75 Q131.58 1452.86 133.385 1456.42 Q135.214 1459.96 138.825 1459.96 Q142.459 1459.96 144.265 1456.42 Q146.094 1452.86 146.094 1445.75 Q146.094 1438.62 144.265 1435.08 Q142.459 1431.51 138.825 1431.51 M138.825 1427.81 Q144.635 1427.81 147.691 1432.42 Q150.769 1437 150.769 1445.75 Q150.769 1454.48 147.691 1459.08 Q144.635 1463.67 138.825 1463.67 Q133.015 1463.67 129.936 1459.08 Q126.881 1454.48 126.881 1445.75 Q126.881 1437 129.936 1432.42 Q133.015 1427.81 138.825 1427.81 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M75.9319 1058.14 Q72.3208 1058.14 70.4921 1061.7 Q68.6865 1065.24 68.6865 1072.37 Q68.6865 1079.48 70.4921 1083.04 Q72.3208 1086.58 75.9319 1086.58 Q79.5661 1086.58 81.3717 1083.04 Q83.2004 1079.48 83.2004 1072.37 Q83.2004 1065.24 81.3717 1061.7 Q79.5661 1058.14 75.9319 1058.14 M75.9319 1054.43 Q81.742 1054.43 84.7976 1059.04 Q87.8763 1063.62 87.8763 1072.37 Q87.8763 1081.1 84.7976 1085.71 Q81.742 1090.29 75.9319 1090.29 Q70.1217 1090.29 67.043 1085.71 Q63.9875 1081.1 63.9875 1072.37 Q63.9875 1063.62 67.043 1059.04 Q70.1217 1054.43 75.9319 1054.43 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M92.9457 1083.74 L97.8299 1083.74 L97.8299 1089.62 L92.9457 1089.62 L92.9457 1083.74 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M112.899 1073.21 Q109.566 1073.21 107.645 1074.99 Q105.747 1076.77 105.747 1079.9 Q105.747 1083.02 107.645 1084.8 Q109.566 1086.58 112.899 1086.58 Q116.233 1086.58 118.154 1084.8 Q120.075 1083 120.075 1079.9 Q120.075 1076.77 118.154 1074.99 Q116.256 1073.21 112.899 1073.21 M108.223 1071.21 Q105.214 1070.47 103.524 1068.41 Q101.858 1066.35 101.858 1063.39 Q101.858 1059.25 104.797 1056.84 Q107.76 1054.43 112.899 1054.43 Q118.061 1054.43 121.001 1056.84 Q123.941 1059.25 123.941 1063.39 Q123.941 1066.35 122.251 1068.41 Q120.584 1070.47 117.598 1071.21 Q120.978 1072 122.853 1074.29 Q124.751 1076.59 124.751 1079.9 Q124.751 1084.92 121.672 1087.6 Q118.617 1090.29 112.899 1090.29 Q107.182 1090.29 104.103 1087.6 Q101.047 1084.92 101.047 1079.9 Q101.047 1076.59 102.946 1074.29 Q104.844 1072 108.223 1071.21 M106.51 1063.83 Q106.51 1066.52 108.177 1068.02 Q109.867 1069.52 112.899 1069.52 Q115.909 1069.52 117.598 1068.02 Q119.311 1066.52 119.311 1063.83 Q119.311 1061.15 117.598 1059.64 Q115.909 1058.14 112.899 1058.14 Q109.867 1058.14 108.177 1059.64 Q106.51 1061.15 106.51 1063.83 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M129.867 1055.06 L148.223 1055.06 L148.223 1058.99 L134.149 1058.99 L134.149 1067.46 Q135.168 1067.12 136.186 1066.96 Q137.205 1066.77 138.223 1066.77 Q144.01 1066.77 147.39 1069.94 Q150.769 1073.11 150.769 1078.53 Q150.769 1084.11 147.297 1087.21 Q143.825 1090.29 137.506 1090.29 Q135.33 1090.29 133.061 1089.92 Q130.816 1089.55 128.408 1088.81 L128.408 1084.11 Q130.492 1085.24 132.714 1085.8 Q134.936 1086.35 137.413 1086.35 Q141.418 1086.35 143.756 1084.25 Q146.094 1082.14 146.094 1078.53 Q146.094 1074.92 143.756 1072.81 Q141.418 1070.71 137.413 1070.71 Q135.538 1070.71 133.663 1071.12 Q131.811 1071.54 129.867 1072.42 L129.867 1055.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M75.0291 684.758 Q71.418 684.758 69.5893 688.322 Q67.7838 691.864 67.7838 698.994 Q67.7838 706.1 69.5893 709.665 Q71.418 713.207 75.0291 713.207 Q78.6633 713.207 80.4689 709.665 Q82.2976 706.1 82.2976 698.994 Q82.2976 691.864 80.4689 688.322 Q78.6633 684.758 75.0291 684.758 M75.0291 681.054 Q80.8393 681.054 83.8948 685.66 Q86.9735 690.244 86.9735 698.994 Q86.9735 707.721 83.8948 712.327 Q80.8393 716.91 75.0291 716.91 Q69.2189 716.91 66.1403 712.327 Q63.0847 707.721 63.0847 698.994 Q63.0847 690.244 66.1403 685.66 Q69.2189 681.054 75.0291 681.054 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M92.0429 710.359 L96.9271 710.359 L96.9271 716.239 L92.0429 716.239 L92.0429 710.359 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M102.135 715.521 L102.135 711.262 Q103.895 712.096 105.7 712.535 Q107.506 712.975 109.242 712.975 Q113.871 712.975 116.302 709.873 Q118.756 706.748 119.103 700.406 Q117.76 702.396 115.7 703.461 Q113.64 704.526 111.14 704.526 Q105.955 704.526 102.922 701.401 Q99.9132 698.253 99.9132 692.813 Q99.9132 687.489 103.061 684.272 Q106.209 681.054 111.441 681.054 Q117.436 681.054 120.584 685.66 Q123.756 690.244 123.756 698.994 Q123.756 707.165 119.867 712.049 Q116.001 716.91 109.45 716.91 Q107.691 716.91 105.885 716.563 Q104.08 716.216 102.135 715.521 M111.441 700.869 Q114.589 700.869 116.418 698.716 Q118.27 696.563 118.27 692.813 Q118.27 689.086 116.418 686.934 Q114.589 684.758 111.441 684.758 Q108.293 684.758 106.441 686.934 Q104.612 689.086 104.612 692.813 Q104.612 696.563 106.441 698.716 Q108.293 700.869 111.441 700.869 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M138.825 684.758 Q135.214 684.758 133.385 688.322 Q131.58 691.864 131.58 698.994 Q131.58 706.1 133.385 709.665 Q135.214 713.207 138.825 713.207 Q142.459 713.207 144.265 709.665 Q146.094 706.1 146.094 698.994 Q146.094 691.864 144.265 688.322 Q142.459 684.758 138.825 684.758 M138.825 681.054 Q144.635 681.054 147.691 685.66 Q150.769 690.244 150.769 698.994 Q150.769 707.721 147.691 712.327 Q144.635 716.91 138.825 716.91 Q133.015 716.91 129.936 712.327 Q126.881 707.721 126.881 698.994 Q126.881 690.244 129.936 685.66 Q133.015 681.054 138.825 681.054 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M76.0245 311.379 Q72.4134 311.379 70.5847 314.944 Q68.7791 318.486 68.7791 325.615 Q68.7791 332.722 70.5847 336.287 Q72.4134 339.828 76.0245 339.828 Q79.6587 339.828 81.4642 336.287 Q83.2929 332.722 83.2929 325.615 Q83.2929 318.486 81.4642 314.944 Q79.6587 311.379 76.0245 311.379 M76.0245 307.676 Q81.8346 307.676 84.8902 312.282 Q87.9688 316.865 87.9688 325.615 Q87.9688 334.342 84.8902 338.949 Q81.8346 343.532 76.0245 343.532 Q70.2143 343.532 67.1356 338.949 Q64.0801 334.342 64.0801 325.615 Q64.0801 316.865 67.1356 312.282 Q70.2143 307.676 76.0245 307.676 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M93.0383 336.981 L97.9225 336.981 L97.9225 342.861 L93.0383 342.861 L93.0383 336.981 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M103.131 342.143 L103.131 337.884 Q104.89 338.717 106.696 339.157 Q108.501 339.597 110.237 339.597 Q114.867 339.597 117.297 336.495 Q119.751 333.37 120.098 327.027 Q118.756 329.018 116.696 330.083 Q114.635 331.148 112.135 331.148 Q106.95 331.148 103.918 328.023 Q100.909 324.875 100.909 319.435 Q100.909 314.111 104.057 310.893 Q107.205 307.676 112.436 307.676 Q118.432 307.676 121.58 312.282 Q124.751 316.865 124.751 325.615 Q124.751 333.787 120.862 338.671 Q116.996 343.532 110.446 343.532 Q108.686 343.532 106.881 343.185 Q105.075 342.838 103.131 342.143 M112.436 327.49 Q115.584 327.49 117.413 325.338 Q119.265 323.185 119.265 319.435 Q119.265 315.708 117.413 313.555 Q115.584 311.379 112.436 311.379 Q109.288 311.379 107.436 313.555 Q105.608 315.708 105.608 319.435 Q105.608 323.185 107.436 325.338 Q109.288 327.49 112.436 327.49 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M129.867 308.301 L148.223 308.301 L148.223 312.236 L134.149 312.236 L134.149 320.708 Q135.168 320.361 136.186 320.199 Q137.205 320.014 138.223 320.014 Q144.01 320.014 147.39 323.185 Q150.769 326.356 150.769 331.773 Q150.769 337.351 147.297 340.453 Q143.825 343.532 137.506 343.532 Q135.33 343.532 133.061 343.162 Q130.816 342.791 128.408 342.05 L128.408 337.351 Q130.492 338.486 132.714 339.041 Q134.936 339.597 137.413 339.597 Q141.418 339.597 143.756 337.49 Q146.094 335.384 146.094 331.773 Q146.094 328.162 143.756 326.055 Q141.418 323.949 137.413 323.949 Q135.538 323.949 133.663 324.365 Q131.811 324.782 129.867 325.662 L129.867 308.301 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip722)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  236.411,1445.72 246.684,201.121 256.958,87.9763 267.231,92.2192 277.505,94.4417 287.778,124.531 298.052,169.003 308.325,138.892 318.599,207.192 328.872,211.944 \n",
       "  339.146,230.406 349.419,205.566 359.693,197.041 369.966,201.121 380.24,183.592 390.513,217.497 400.787,219.56 411.06,212.699 421.334,223.004 431.607,216.679 \n",
       "  441.881,225.767 452.155,212.863 462.428,201.121 472.702,190.392 482.975,199.064 493.249,189.268 503.522,185.92 513.796,182.818 524.069,190.529 534.343,192.597 \n",
       "  544.616,189.582 554.89,181.974 565.163,179.476 575.437,186.126 585.71,183.653 595.984,177.078 606.257,170.866 616.531,169.003 626.804,167.237 637.078,165.561 \n",
       "  647.351,171.4 657.625,169.704 667.898,164.55 678.172,163.092 688.445,161.7 698.719,166.977 708.993,165.561 719.266,161.041 729.54,156.708 739.813,152.552 \n",
       "  750.087,151.536 760.36,150.56 770.634,146.76 780.907,143.103 791.181,153.358 801.454,149.714 811.728,148.857 822.001,145.419 832.275,142.099 842.548,136.369 \n",
       "  852.822,128.348 863.095,130.35 873.369,132.288 883.642,134.165 893.916,133.657 904.189,135.456 914.463,134.943 924.736,136.669 935.01,138.344 945.284,137.812 \n",
       "  955.557,139.423 965.831,140.989 976.104,142.512 986.378,143.992 996.651,143.42 1006.92,142.864 1017.2,140.361 1027.47,139.859 1037.75,139.369 1048.02,138.892 \n",
       "  1058.29,136.564 1068.57,134.293 1078.84,135.712 1089.11,135.301 1099.39,138.448 1109.66,136.262 1119.93,135.859 1130.21,137.179 1140.48,143.548 1150.75,144.752 \n",
       "  1161.03,145.929 1171.3,147.08 1181.57,144.966 1191.85,142.898 1202.12,139.288 1212.4,135.754 1222.67,133.846 1232.94,136.587 1243.22,142.314 1253.49,141.903 \n",
       "  1263.76,141.5 1274.04,141.105 1284.31,139.257 1294.58,140.339 1304.86,142.833 1315.13,146.7 1325.4,146.275 1335.68,145.858 1345.95,146.828 1356.22,146.414 \n",
       "  1366.5,147.362 1376.77,146.95 1387.04,147.877 1397.32,146.148 1407.59,145.757 1417.87,145.374 1428.14,147.567 1438.41,148.449 1448.69,149.316 1458.96,148.915 \n",
       "  1469.23,147.279 1479.51,146.901 1489.78,146.53 1500.05,144.953 1510.33,147.008 1520.6,144.26 1530.87,145.105 1541.15,147.111 1551.42,143.26 1561.69,142.937 \n",
       "  1571.97,142.62 1582.24,144.583 1592.52,143.128 1602.79,141.695 1613.06,143.621 1623.34,143.31 1633.61,145.197 1643.88,143.79 1654.16,147.807 1664.43,147.475 \n",
       "  1674.7,146.082 1684.98,146.824 1695.25,145.456 1705.52,147.235 1715.8,145.883 1726.07,144.549 1736.34,147.319 1746.62,145.994 1756.89,146.702 1767.17,145.398 \n",
       "  1777.44,144.112 1787.71,142.843 1797.99,142.571 1808.26,142.304 1818.53,141.071 1828.81,137.929 1839.08,138.652 1849.35,140.317 1859.63,139.128 1869.9,138.892 \n",
       "  1880.17,140.523 1890.45,141.208 1900.72,141.884 1910.99,142.552 1921.27,144.122 1931.54,144.768 1941.82,146.305 1952.09,146.038 1962.36,146.661 1972.64,145.512 \n",
       "  1982.91,148.763 1993.18,147.615 2003.46,148.215 2013.73,147.946 2024,149.394 2034.28,149.121 2044.55,148.003 2054.82,146.898 2065.1,148.32 2075.37,147.226 \n",
       "  2085.64,146.972 2095.92,146.722 2106.19,147.294 2116.46,147.859 2126.74,147.608 2137.01,146.553 2147.29,147.915 2157.56,147.667 2167.83,148.216 2178.11,150.338 \n",
       "  2188.38,150.081 2198.65,148.265 2208.93,148.022 2219.2,147.008 2229.47,145.236 2239.75,142.717 2250.02,140.985 2260.29,141.542 2270.57,145.862 2280.84,146.389 \n",
       "  2291.11,145.419 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip722)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  236.411,601.815 246.684,437.922 256.958,259.129 267.231,181.653 277.505,178.673 287.778,196.552 298.052,193.572 308.325,196.552 318.599,184.633 328.872,202.512 \n",
       "  339.146,169.733 349.419,184.633 359.693,187.612 369.966,187.612 380.24,199.532 390.513,202.512 400.787,202.512 411.06,196.552 421.334,211.451 431.607,205.492 \n",
       "  441.881,199.532 452.155,190.592 462.428,184.633 472.702,181.653 482.975,172.713 493.249,172.713 503.522,172.713 513.796,172.713 524.069,172.713 534.343,172.713 \n",
       "  544.616,175.693 554.89,175.693 565.163,175.693 575.437,169.733 585.71,172.713 595.984,172.713 606.257,175.693 616.531,175.693 626.804,175.693 637.078,175.693 \n",
       "  647.351,163.773 657.625,163.773 667.898,160.794 678.172,154.834 688.445,154.834 698.719,154.834 708.993,151.854 719.266,154.834 729.54,157.814 739.813,160.794 \n",
       "  750.087,157.814 760.36,157.814 770.634,160.794 780.907,160.794 791.181,160.794 801.454,160.794 811.728,157.814 822.001,157.814 832.275,151.854 842.548,157.814 \n",
       "  852.822,154.834 863.095,157.814 873.369,151.854 883.642,151.854 893.916,151.854 904.189,151.854 914.463,154.834 924.736,154.834 935.01,154.834 945.284,157.814 \n",
       "  955.557,157.814 965.831,157.814 976.104,154.834 986.378,148.874 996.651,148.874 1006.92,145.894 1017.2,145.894 1027.47,145.894 1037.75,145.894 1048.02,145.894 \n",
       "  1058.29,145.894 1068.57,142.914 1078.84,142.914 1089.11,142.914 1099.39,145.894 1109.66,145.894 1119.93,145.894 1130.21,151.854 1140.48,148.874 1150.75,148.874 \n",
       "  1161.03,145.894 1171.3,148.874 1181.57,148.874 1191.85,145.894 1202.12,145.894 1212.4,145.894 1222.67,145.894 1232.94,145.894 1243.22,142.914 1253.49,142.914 \n",
       "  1263.76,142.914 1274.04,145.894 1284.31,145.894 1294.58,151.854 1304.86,151.854 1315.13,151.854 1325.4,151.854 1335.68,154.834 1345.95,151.854 1356.22,151.854 \n",
       "  1366.5,151.854 1376.77,145.894 1387.04,145.894 1397.32,145.894 1407.59,145.894 1417.87,142.914 1428.14,142.914 1438.41,139.934 1448.69,130.995 1458.96,133.975 \n",
       "  1469.23,133.975 1479.51,133.975 1489.78,133.975 1500.05,133.975 1510.33,133.975 1520.6,133.975 1530.87,133.975 1541.15,133.975 1551.42,136.955 1561.69,136.955 \n",
       "  1571.97,142.914 1582.24,142.914 1592.52,142.914 1602.79,142.914 1613.06,142.914 1623.34,142.914 1633.61,139.934 1643.88,139.934 1654.16,139.934 1664.43,139.934 \n",
       "  1674.7,139.934 1684.98,139.934 1695.25,139.934 1705.52,139.934 1715.8,139.934 1726.07,139.934 1736.34,139.934 1746.62,139.934 1756.89,139.934 1767.17,139.934 \n",
       "  1777.44,139.934 1787.71,139.934 1797.99,139.934 1808.26,139.934 1818.53,139.934 1828.81,142.914 1839.08,142.914 1849.35,142.914 1859.63,142.914 1869.9,145.894 \n",
       "  1880.17,142.914 1890.45,142.914 1900.72,145.894 1910.99,145.894 1921.27,145.894 1931.54,145.894 1941.82,142.914 1952.09,142.914 1962.36,142.914 1972.64,142.914 \n",
       "  1982.91,139.934 1993.18,139.934 2003.46,139.934 2013.73,142.914 2024,142.914 2034.28,142.914 2044.55,142.914 2054.82,142.914 2065.1,142.914 2075.37,142.914 \n",
       "  2085.64,142.914 2095.92,142.914 2106.19,139.934 2116.46,136.955 2126.74,136.955 2137.01,136.955 2147.29,136.955 2157.56,136.955 2167.83,139.934 2178.11,145.894 \n",
       "  2188.38,145.894 2198.65,148.874 2208.93,148.874 2219.2,148.874 2229.47,148.874 2239.75,148.874 2250.02,148.874 2260.29,148.874 2270.57,148.874 2280.84,145.894 \n",
       "  2291.11,142.914 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip720)\" d=\"\n",
       "M1987.14 276.658 L2280.16 276.658 L2280.16 95.2176 L1987.14 95.2176  Z\n",
       "  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#000000; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  1987.14,276.658 2280.16,276.658 2280.16,95.2176 1987.14,95.2176 1987.14,276.658 \n",
       "  \"/>\n",
       "<polyline clip-path=\"url(#clip720)\" style=\"stroke:#009af9; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2011.34,155.698 2156.54,155.698 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip720)\" d=\"M 0 0 M2194.58 175.385 Q2192.78 180.015 2191.06 181.427 Q2189.35 182.839 2186.48 182.839 L2183.08 182.839 L2183.08 179.274 L2185.58 179.274 Q2187.34 179.274 2188.31 178.44 Q2189.28 177.607 2190.46 174.505 L2191.22 172.561 L2180.74 147.052 L2185.25 147.052 L2193.35 167.329 L2201.46 147.052 L2205.97 147.052 L2194.58 175.385 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M2211.85 169.042 L2219.49 169.042 L2219.49 142.677 L2211.18 144.343 L2211.18 140.084 L2219.44 138.418 L2224.12 138.418 L2224.12 169.042 L2231.76 169.042 L2231.76 172.978 L2211.85 172.978 L2211.85 169.042 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip720)\" style=\"stroke:#e26f46; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n",
       "  2011.34,216.178 2156.54,216.178 \n",
       "  \"/>\n",
       "<path clip-path=\"url(#clip720)\" d=\"M 0 0 M2194.58 235.865 Q2192.78 240.495 2191.06 241.907 Q2189.35 243.319 2186.48 243.319 L2183.08 243.319 L2183.08 239.754 L2185.58 239.754 Q2187.34 239.754 2188.31 238.92 Q2189.28 238.087 2190.46 234.985 L2191.22 233.041 L2180.74 207.532 L2185.25 207.532 L2193.35 227.809 L2201.46 207.532 L2205.97 207.532 L2194.58 235.865 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip720)\" d=\"M 0 0 M2215.07 229.522 L2231.39 229.522 L2231.39 233.458 L2209.44 233.458 L2209.44 229.522 Q2212.1 226.768 2216.69 222.138 Q2221.29 217.485 2222.47 216.143 Q2224.72 213.62 2225.6 211.884 Q2226.5 210.124 2226.5 208.435 Q2226.5 205.68 2224.56 203.944 Q2222.64 202.208 2219.53 202.208 Q2217.34 202.208 2214.88 202.972 Q2212.45 203.735 2209.67 205.286 L2209.67 200.564 Q2212.5 199.43 2214.95 198.851 Q2217.4 198.273 2219.44 198.273 Q2224.81 198.273 2228.01 200.958 Q2231.2 203.643 2231.2 208.134 Q2231.2 210.263 2230.39 212.185 Q2229.6 214.083 2227.5 216.675 Q2226.92 217.347 2223.82 220.564 Q2220.72 223.759 2215.07 229.522 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot(data_schedule, training_losses)\n",
    "plot!(data_schedule, valid_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNNClassifier(\n",
       "    K = 9,\n",
       "    algorithm = :kdtree,\n",
       "    metric = Distances.Euclidean(0.0),\n",
       "    leafsize = 10,\n",
       "    reorder = true,\n",
       "    weights = :uniform)\u001b[34m @795\u001b[39m"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_final = KNNClassifier(K=best.best_model.K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{KNNClassifier} @594\u001b[39m trained 0 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @816\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @044\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN_Final = machine(knn_final, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Training \u001b[34mMachine{KNNClassifier} @819\u001b[39m.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/machines.jl:322\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[34mMachine{KNNClassifier} @819\u001b[39m trained 8 times.\n",
       "  args: \n",
       "    1:\t\u001b[34mSource @942\u001b[39m ⏎ `Table{AbstractArray{Continuous,1}}`\n",
       "    2:\t\u001b[34mSource @697\u001b[39m ⏎ `AbstractArray{Multiclass{2},1}`\n"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit!(KNN, rows=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "ŷ = MLJ.predict(KNN, X_stand[test,:]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.45569859135832835"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(ŷ, y[test]) |> mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9783985102420857"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc(ŷ, y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Warning: The classes are un-ordered,\n",
      "│ using: negative='0' and positive='1'.\n",
      "│ To suppress this warning, consider coercing to OrderedFactor.\n",
      "└ @ MLJBase /home/andrew/.julia/packages/MLJBase/cJmIS/src/measures/confusion_matrix.jl:83\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "              ┌───────────────────────────┐\n",
       "              │       Ground Truth        │\n",
       "┌─────────────┼─────────────┬─────────────┤\n",
       "│  Predicted  │      0      │      1      │\n",
       "├─────────────┼─────────────┼─────────────┤\n",
       "│      0      │    4852     │     90      │\n",
       "├─────────────┼─────────────┼─────────────┤\n",
       "│      1      │     26      │     402     │\n",
       "└─────────────┴─────────────┴─────────────┘\n"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(mode.(ŷ), y[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.0",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
